% This file was created with JabRef 2.7.
% Encoding: Cp1252

@INPROCEEDINGS{5406409,
  author = {Qin kan and Yujiu Yang and Wenhuang Liu and Xiaodong Liu},
  title = {An integrated approach for detecting approximate duplicate records},
  booktitle = {Computational Intelligence and Industrial Applications, 2009. PACIIA
	2009. Asia-Pacific Conference on},
  year = {2009},
  volume = {1},
  pages = {381 -384},
  month = {nov.},
  abstract = {Detecting approximate duplicate records in database is a key problem
	related to data quality. Given two lists of records, the duplicate
	detection problem consists of determining all pairs that are similar
	to each other, where the overall similarity between two records is
	defined based on domain-specific similarities over individual attributes
	constituting the record. In this paper, we present a synthetic approach
	for recognizing clusters of approximate duplicate records of multi-language
	data. The key ideas are: (1) an efficient algorithm for pre-processing
	multi-language data consists of Chinese words segmentation and Chinese
	named entity recognition; (2) an efficient pair-wise comparison method
	based on domain- specific similarities, especially, the string kernel
	method; (3) using a priority queue of duplicate clusters and representative
	records strategy to respond adaptively to the data scale.},
  doi = {10.1109/PACIIA.2009.5406409},
  keywords = {Chinese words segmentation;approximate duplicate records detection;data
	quality;domain specific similarities;entity recognition;multilanguage
	data;string kernel method;data mining;database management systems;word
	processing;}
}

@INPROCEEDINGS{5159053,
  author = {Xu Jian-liang and Xiong Jing and Liu Yong},
  title = {A Query Optimization Strategy Based on Domain Ontology},
  booktitle = {Artificial Intelligence, 2009. JCAI '09. International Joint Conference
	on},
  year = {2009},
  pages = {510 -513},
  month = {april},
  abstract = {This paper proposes a strategy of query optimization based on domain
	ontology. Three kinds of query optimization respectively based on
	is-a relation, part-of relation, and equivalent-class relation in
	the domain ontology are investigated. By using this strategy, a domain-specific
	information searching system building upon GoogleAPI and domain ontology
	is built, where optimized user query is processed. Comparisons of
	searching results of optimized and non-optimized query show the effectiveness
	of the strategy in improving precision and recall of information
	searching.},
  doi = {10.1109/JCAI.2009.150},
  keywords = {GoogleAPI;domain ontology;domain-specific information searching system;query
	optimization;Internet;ontologies (artificial intelligence);query
	processing;}
}

@INPROCEEDINGS{5992375,
  author = {Abedmouleh, A. and Oubahssi, L. and Laforcade, P. and Choquet, C.},
  title = {Operationalization of Learning Scenarios on Open and Distance Learning
	Platforms: The Case of the Moodle Platform},
  booktitle = {Advanced Learning Technologies (ICALT), 2011 11th IEEE International
	Conference on},
  year = {2011},
  pages = {396 -398},
  month = {july},
  abstract = {This paper aims to facilitate the operationalization of scenarios
	on learning platforms. We propose an approach based on the explicitation
	and the formalization of the implicit business language in order
	to use it as a mean of communication with external design tools without
	losing the semantic of the designed scenario. The originality of
	our approach relies in performing the scenarios operationalization
	by the development of a simple API based on the implicit language
	of the platform. Our proposal is based on a theoretical framework
	inspired from the Domain Specific Modeling. Finally, we detail the
	implementation of our proposal on MOODLE.},
  doi = {10.1109/ICALT.2011.125},
  issn = {2161-3761},
  keywords = {API;Moodle platform;business language;distance learning platforms;domain
	specific modeling;learning management system;learning scenario operationalization;open
	learning platforms;application program interfaces;business data processing;distance
	learning;}
}

@INPROCEEDINGS{4698100,
  author = {Achilleos, A. and Yang, K. and Georgalas, N.},
  title = {A Model Driven Approach to Generate Service Creation Environments},
  booktitle = {Global Telecommunications Conference, 2008. IEEE GLOBECOM 2008. IEEE},
  year = {2008},
  pages = {1 -6},
  month = {30 2008-dec. 4},
  abstract = {The creation of services is a complex activity that involves several
	tasks. Furthermore this complexity is augmented by the fact that
	supporting service creation environments are technology-specific.
	Consequently a technology-independent approach and framework are
	required to generate service creation environments and drive service
	creation. In this paper we present such an approach and a generic
	framework for supporting service creation. The approach realizes
	service creation via the phases of: (i) domain specific language
	definition, (ii) model definition and validation, (iii) model-to-model
	transformation and (iv) model-to-code generation. Each phase maps
	to a corresponding phase in service creation starting from service
	analysis to service implementation. The applicability of the approach
	and its accompanying framework is demonstrated via an example scenario
	that illustrates the automatic generation of a service creation environment
	for an online survey system.},
  doi = {10.1109/GLOCOM.2008.ECP.325},
  issn = {1930-529X},
  keywords = {domain specific language definition;drive service creation;model-to-code
	generation;model-to-model transformation;service analysis;service
	creation environments;technology-independent approach;software engineering;specification
	languages;utility programs;}
}

@INPROCEEDINGS{4222642,
  author = {Adar, E. and Miryung Kim},
  title = {SoftGUESS: Visualization and Exploration of Code Clones in Context},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {762 -766},
  month = {may},
  abstract = {We introduce SoftGUESS, a code clone exploration system. SoftGUESS
	is built on the more general GUESS system which provides users with
	a mechanism to interactively explore graph structures both through
	direct manipulation as well as a domain-specific language. We demonstrate
	SoftGUESS through a number of mini-applications to analyze evolutionary
	code-clone behavior in software systems. The mini-applications of
	SoftGUESS represent a novel way of looking at code-clones in the
	context of many system features. It is our hope that SoftGUESS will
	form the basis for other analysis tools in the software- engineering
	domain.},
  doi = {10.1109/ICSE.2007.76},
  issn = {0270-5257},
  keywords = {SoftGUESS;code clone exploration system;domain-specific language;evolutionary
	code-clone behavior;graph structures;codes;evolutionary computation;graph
	theory;}
}

@INPROCEEDINGS{1245944,
  author = {Adavi, R.M.V. and Agarwal, N. and Gullapalli, S. and Kumar, P. and
	Sundaram, P.},
  title = {HADL: HUMS architectural description language},
  booktitle = {Digital Avionics Systems Conference, 2003. DASC '03. The 22nd},
  year = {2003},
  volume = {2},
  pages = { 11.D.3 - 111-10 vol.2},
  month = {oct.},
  abstract = { Specification of architectures is an important prerequisite for evaluation
	of architectures. With the increase in the growth of health usage
	and monitoring systems (HUMS) in commercial and military domains,
	the need for the design and evaluation of HUMS architectures has
	also been on the increase. In this paper, we describe HADL, HUMS
	Architectural Description Language, that we have designed for this
	purpose. In particular, we describe the features of the language,
	illustrate them with examples, and show how we use it in designing
	domain-specific HUMS architectures. A companion paper contains details
	on our design methodology of HUMS architectures.},
  doi = {10.1109/DASC.2003.1245944},
  keywords = { architectural description language; commercial domains; health usage
	and monitoring systems; military domains; condition monitoring; software
	architecture;}
}

@INPROCEEDINGS{5731189,
  author = {Adavi, R.M.V. and Agarwal, N. and Gullapalli, S. and Kumar, P. and
	Sundaram, P.},
  title = {HADL: HUMS architectural description language},
  booktitle = {Digital Avionics Systems Conference, 2003. DASC '03. The 22nd},
  year = {2003},
  volume = {2},
  pages = {11.D.3 -111-10 vol.2},
  month = {oct.},
  abstract = {Specification of architectures is an important prerequisite for evaluation
	of architectures. With the increase in the growth of health usage
	and monitoring systems (HUMS) in commercial and military domains,
	the need for the design and evaluation of HUMS architectures has
	also been on the increase. In this paper, we describe HADL, HUMS
	Architectural Description Language, that we have designed for this
	purpose. In particular, we describe the features of the language,
	illustrate them with examples, and show how we use it in designing
	domain-specific HUMS architectures. A companion paper contains details
	on our design methodology of HUMS architectures.},
  doi = {10.1109/DASC.2003.1245944},
  keywords = {architectural description language;commercial domains;health usage
	and monitoring systems;military domains;condition monitoring;software
	architecture;}
}

@ARTICLE{42931,
  author = {Adigun, M.O.},
  title = {A framework for teaching programming with reuse},
  journal = {Software Engineering Journal},
  year = {1989},
  volume = {4},
  pages = {159 -162},
  number = {3},
  month = {may},
  abstract = {A framework is proposed for introducing reusable programming as an
	engineering methodology in a typical programming class. The requirements
	of the teaching framework include a program design language a code-frame-driven
	implementation strategy of the design and a presentation approach
	which is software-domain specific. Reusable design fragments are
	defined and tools are provided to assist students or users in the
	process of building reusable fragments and constructing composite
	program logics from existing fragments},
  issn = {0268-6961},
  keywords = {code-frame-driven implementation strategy;composite program logics;computer
	science education;program design language;programming class;reusable
	fragments;reusable programming;software reusability;software-domain
	specific;teaching;computer science education;high level languages;programming;software
	reusability;teaching;}
}

@INPROCEEDINGS{6037562,
  author = {Agaram, M.K. and Chang Liu},
  title = {An Engine-Independent Framework for Business Rules Development},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2011 15th
	IEEE International},
  year = {2011},
  pages = {75 -84},
  month = {29 2011-sept. 2},
  abstract = {There is a compelling need for highly customized Domain Specific Languages
	and Business Vocabulary in certain industries such as insurance,
	mortgage, and finance to enable Knowledge Workers to articulate and
	to automate complex rules pertinent to their areas of function within
	their companies. Rule Engine vendors attempt to provide a solution
	to the problem by selling an integrated Rules Engine and Business
	Rules Management System. Usually, the BRMS's provided by vendors
	need to be customized and integrated into the overall Enterprise
	Architecture. This results in the Enterprise Architecture to be tightly
	coupled with the vendor's rule offering. Moreover, it poses a significant
	risk to the Enterprise as vendor solutions change between releases.
	The Enterprise Architecture needs a way to insulate itself from such
	impacts. This paper describes a framework that delivers the core
	BRMS functions of authoring and representation in a vendor neutral
	fashion. In addition, the paper sheds light on specific areas of
	the framework that can be standardized.},
  doi = {10.1109/EDOC.2011.20},
  issn = {1541-7719},
  keywords = {BRMS functions;business rules development;business rules management
	system;business vocabulary;domain specific languages;engine-independent
	framework;enterprise architecture;finance;insurance;mortgage;rules
	engine;business data processing;insurance data processing;knowledge
	based systems;mortgage processing;specification languages;vocabulary;}
}

@INPROCEEDINGS{5670051,
  author = {Ageishi, R. and Miura, T.},
  title = {Automatic Extraction of Synonyms Based on Statistical Machine Translation},
  booktitle = {Tools with Artificial Intelligence (ICTAI), 2010 22nd IEEE International
	Conference on},
  year = {2010},
  volume = {1},
  pages = {313 -317},
  month = {oct.},
  abstract = {In this work we discuss how to extract domain specific synonyms automatically.
	It is not easy to extract these synonyms or related words without
	hand-coding. We identify pairs of sentences from two corpus in Japanese,
	estimate translation probabilities and extract synonyms using statistical
	machine translation techniques.},
  doi = {10.1109/ICTAI.2010.52},
  issn = {1082-3409},
  keywords = {Japanese;automatic extraction;statistical machine translation;synonyms
	extraction;language translation;}
}

@INPROCEEDINGS{4151652,
  author = {Agosta, Giovanni and Breveglieri, Luca and Pelosi, Gerardo and Sykora,
	Martino},
  title = {Programming Highly Parallel Reconfigurable Architectures for Public-Key
	Cryptographic Applications},
  booktitle = {Information Technology, 2007. ITNG '07. Fourth International Conference
	on},
  year = {2007},
  pages = {3 -10},
  month = {april},
  abstract = {Tiled architectures are emerging as an architectural platform that
	allows high levels of instruction level parallelism. Traditional
	compiler parallelization techniques are usually employed to generate
	programs for these architectures. However, for specific application
	domains, the compiler is not able to effectively exploit the domain
	knowledge. In this paper, we propose a new programming model that,
	by means of the definition of software function units, allows domain-specific
	features to be explicitly modeled, achieving good performances while
	reducing development times with respect to low-level programming.
	Identity-based cryptographic algorithms are known to be computationally
	intensive and difficult to parallelize automatically. Recent advances
	have led to the adoption of embedded cryptographic coprocessors to
	speed up both traditional and identity-based public key algorithms.
	Custom-designed coprocessors have high development costs and times
	with respect to general purpose or DSP coprocessors. Therefore, the
	proposed methodology can be effectively employed to reduce time to
	market while preserving performances. It also represents a starting
	point for the definition of cryptography-oriented programming languages.
	We prove that tiled architecture well compare w.r.t. competitors
	implementations such as StrongARM and FPGAs},
  doi = {10.1109/ITNG.2007.160},
  keywords = {instruction level parallelism;low-level programming;parallel reconfigurable
	architecture;programming model;public-key cryptographic application;tiled
	architecture;cryptography;parallel architectures;programming;reconfigurable
	architectures;}
}

@INPROCEEDINGS{1508205,
  author = {Agostaro, F. and Pilato, G. and Vassallo, G. and Gaglio, S.},
  title = {A sub-symbolic approach to word modelling for domain specific speech
	recognition},
  booktitle = {Computer Architecture for Machine Perception, 2005. CAMP 2005. Proceedings.
	Seventh International Workshop on},
  year = {2005},
  pages = { 321 - 326},
  month = {july},
  abstract = { In this work a sub-symbolic technique for automatic, data driven
	language models construction is presented. Such a technique can be
	used to arrange a language-modelling module, which can be easily
	integrated in existing speech recognition architectures, such as
	the well-found HTK architecture. The proposed technique takes advantages
	from both the traditional LSA approach and from a novel application
	of a probability space metric known as "Hellinger's distance". Experimental
	trials are also presented, in order to validate the proposed approach.},
  doi = {10.1109/CAMP.2005.8},
  keywords = { Hellinger distance; data driven language models construction; domain
	specific speech recognition; language-modelling module; probability
	space metric; sub-symbolic approach; well-found HTK architecture;
	word modelling; probability; speech recognition;}
}

@INPROCEEDINGS{4151821,
  author = {Agrawal, Avinash J. and Chandak, Manoj B.},
  title = {Mobile Interface for Domain Specific Machine Translation Using Short
	Messaging Service},
  booktitle = {Information Technology, 2007. ITNG '07. Fourth International Conference
	on},
  year = {2007},
  pages = {957 -958},
  month = {april},
  abstract = {Machine aided translation system available on Web may certainly be
	proved to be more useful if become available on mobile phone with
	a different interface to provide interpersonal, domain specific communication
	across language barrier. Short message service which is easier, faster,
	reliable and available in all networks can be used as a method of
	communication with the server to facilitate machine translation on
	mobile phone. This paper discusses issues in development of such
	an application and also reports an experiment on "translation service"
	which translates English sentences of tourist's domain into Hindi
	on mobile phone using short messaging service on binary runtime environment
	for wireless (BREW) emulator},
  doi = {10.1109/ITNG.2007.126},
  keywords = {English sentences translation;Hindi;World Wide Web;binary runtime
	environment for wireless emulator;machine translation;mobile interface;mobile
	phone;short messaging service;translation service;electronic messaging;language
	translation;mobile computing;natural language processing;}
}

@INPROCEEDINGS{1454321,
  author = {Agrawal, D. and Giles, J. and Kang-Won Lee and Lobo, J.},
  title = {Policy ratification},
  booktitle = {Policies for Distributed Systems and Networks, 2005. Sixth IEEE International
	Workshop on},
  year = {2005},
  pages = { 223 - 232},
  month = {june},
  abstract = { It is not sufficient to merely check the syntax of new policies before
	they are deployed in a system; policies need to be analyzed for their
	interactions with each other and with their local environment. That
	is, policies need to go through a ratification process. We believe
	policy ratification becomes an essential part of system management
	as the number of policies in the system increases and as the system
	administration becomes more decentralized. In this paper, we focus
	on the basic tasks involved in policy ratification. To a large degree,
	these basic tasks can be performed independent of policy model and
	language and require little domain-specific knowledge. We present
	algorithms from constraint, linear, and logic programming disciplines
	to help perform ratification tasks. We provide an algorithm to efficiently
	assign priorities to the policies based on relative policy preferences
	indicated by policy administrators. Finally, with an example, we
	show how these algorithms have been integrated with our policy system
	to provide feedback to a policy administrator regarding potential
	interactions of policies with each other and with their deployment
	environment.},
  doi = {10.1109/POLICY.2005.25},
  keywords = { constraint programming; domain-specific knowledge; linear programming;
	logic programming; policy ratification; system administration; system
	management; computer network management; constraint handling; linear
	programming;}
}

@INPROCEEDINGS{5536629,
  author = {Ahmad, Manzoor},
  title = {First step towards a domain specific language for self-adaptive systems},
  booktitle = {New Technologies of Distributed Systems (NOTERE), 2010 10th Annual
	International Conference on},
  year = {2010},
  pages = {285 -290},
  month = {31 2010-june 2},
  abstract = {Self-adaptive systems are capable of autonomously modifying their
	behavior at run-time in response to changing environmental conditions.
	In order to modify the behavior, requirements play an important role,
	as they tend to change for these systems. For this we need to identify
	those requirements that are concerned with the adaptability features
	of the self-adaptive systems. In order to cope with the uncertainty
	inherent in self-adaptive systems, requirements engineering languages
	for these systems should include explicit constructs. RELAX is a
	requirement engineering language tor self-adaptive systems that incorporates
	uncertainty into the specification of these systems. To go one step
	further, we aim at developing a domain specific language that would
	bridge the gap between requirements and the overall system model.
	The first step that is illustrated in this paper is to build a textual
	editor for RELAX.},
  doi = {10.1109/NOTERE.2010.5536629}
}

@INPROCEEDINGS{4061490,
  author = {Raheel Ahmad and Shahram Rahimi},
  title = {A Perception Based, Domain Specific Expert System for Question-Answering
	Support},
  booktitle = {Web Intelligence, 2006. WI 2006. IEEE/WIC/ACM International Conference
	on},
  year = {2006},
  pages = {893 -896},
  month = {dec. },
  abstract = {The current search engine technologies mostly use a keyword based
	searching mechanism, which does not have any deductive abilities.
	There is an urgent need for a more intelligent question-answering
	system that will provide a more intuitive, natural language interface,
	and more accurate and direct search results. The introduction of
	computing with words (CwW) provides a new theoretical base for developing
	frameworks with support for dealing with information in natural language.
	This paper proposes a domain specific question-answering system based
	on fuzzy expert systems using CwW. In order to perform the translation
	of natural language based information into a standard format for
	use with CwW, probabilistic context-free grammar is used},
  doi = {10.1109/WI.2006.22},
  keywords = {fuzzy expert system;intelligent question-answering system;keyword
	based searching;natural language interface;natural language translation;probabilistic
	context-free grammar;search engine;context-free grammars;expert systems;fuzzy
	reasoning;information retrieval;language translation;natural language
	interfaces;probabilistic logic;search engines;}
}

@INPROCEEDINGS{1548578,
  author = {Raheel Ahmad and Shahram Rahimi},
  title = {A perception based, domain specific expert system for question-answering
	support},
  booktitle = {Fuzzy Information Processing Society, 2005. NAFIPS 2005. Annual Meeting
	of the North American},
  year = {2005},
  pages = { 454 - 459},
  month = {june},
  abstract = { The ability to search has become an integral part of our interaction
	with technology. However, the current search technologies mostly
	use a keyword based searching mechanism, which does not have any
	deductive abilities. The recent introduction of the idea of computing
	with words, which can give deduction abilities to existing technologies,
	provides a new base for developing frameworks. This paper proposes
	implementation of a domain specific fuzzy expert system based on
	a question-answer system, which employs computing with words. In
	order to perform the translation of natural language sentences into
	a standard format, probabilistic context-free grammar is used.},
  doi = {10.1109/NAFIPS.2005.1548578},
  keywords = { domain specific fuzzy expert system; keyword based searching mechanism;
	natural language sentences; probabilistic context-free grammar; question-answering
	support; context-free grammars; expert systems; fuzzy systems; natural
	languages;}
}

@INPROCEEDINGS{5475034,
  author = {Ahmadi, N. and Lelli, F. and Jazayeri, M.},
  title = {Supporting Domain-Specific Programming in Web 2.0: A Case Study of
	Smart Devices},
  booktitle = {Software Engineering Conference (ASWEC), 2010 21st Australian},
  year = {2010},
  pages = {215 -223},
  month = {april},
  abstract = {Web 2.0 communities emerge regularly with the growing need for domain-specific
	programming over Web APIs. Even though Web mashups provide access
	to Web APIs, they ignore domain-specific programming needs. On the
	other hand, developing domain-specific languages (DSLs) is costly
	and not feasible for such ad hoc communities. We propose User Language
	Domain (ULD): an intermediate Web-based architecture using a domain-specific
	embedded languages approach that reduces the cost of DSL development
	to plugging the Web APIs into a host end user programming language.
	We have implemented the proposed architecture in the context of smart
	devices, where we plug the functionality of different Lego Mindstorms
	devices into a Web-based visual programming language. We expect that
	several domains, such as smart homes or wearable computers can use
	the ULD architecture to reduce development effort.},
  doi = {10.1109/ASWEC.2010.36},
  issn = {1530-0803},
  keywords = {Lego Mindstorms devices;Web 2.0;Web APIs;Web-based architecture;Web-based
	visual programming language;application programming interfaces;domain-specific
	embedded language approach;domain-specific languages;domain-specific
	programming;host end user programming language;smart devices;user
	language domain;Web services;application program interfaces;programming
	languages;software architecture;}
}

@INPROCEEDINGS{5437669,
  author = {Aijaz, F. and Chaudhary, M.A. and Walke, B.},
  title = {Mobile Web Services in Health Care and Sensor Networks},
  booktitle = {Communication Software and Networks, 2010. ICCSN '10. Second International
	Conference on},
  year = {2010},
  pages = {254 -259},
  month = {feb.},
  abstract = {The Wireless Sensor Networks (WSN) environments are increasingly becoming
	intelligent with the rapid boost in the capabilities of sensor nodes.
	The Sun Small Programmable Object Technology (SPOT) platform offers
	a high-end sensing device in terms of processing, memory and battery
	configurations compared to other existing in the community. Within
	the scope of this paper, a health care use case is derived from the
	concept of service-oriented collector nodes represented by Sun SPOTs.
	The existing Mobile Web Services (MobWS) framework is ported to Sun
	SPOT devices that enables domain specific in-network computations
	enfolding synchronous or asynchronous classes of MobWSs. Based on
	that, the exemplary requirements of the Web Services (WS) in the
	health care domain are spotlighted and impressions of a comprehensive
	prototype, called MEDICARE, are shown.},
  doi = {10.1109/ICCSN.2010.42},
  keywords = {MEDICARE;SPOT;WSN;battery configurations;health care;high end sensing
	device;memory configurations;mobile Web services;network computations;sensor
	networks;service oriented collector nodes;sun small programmable
	object technology;wireless sensor networks;mobile computing;wireless
	sensor networks;}
}

@INPROCEEDINGS{685736,
  author = {Aitken, W. and Dickens, B. and Kwiatkowski, P. and De Moor, O. and
	Richter, D. and Simonyi, C.},
  title = {Transformation in intentional programming},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {114 -123},
  month = {jun},
  abstract = {Intentional programming is a new paradigm in software engineering
	that allows programming languages to be implemented in a highly extensible
	manner. In particular, the programmer can specify new abstractions
	that are specific to his problem domain, while simultaneously recording
	any domain specific optimizations that may apply to such new abstractions.
	This paper describes a system that implements intentional programming,
	focusing on the facilities for program transformation. The key difference
	with other approaches lies in the way the order of transformation
	is controlled: emphasis is placed on specifying that order in a compositional
	fashion, so that transformations are easily re-used},
  doi = {10.1109/ICSR.1998.685736},
  issn = {1085-9098},
  keywords = {C language;abstractions;domain specific optimizations;intentional
	programming;program transformation;programming languages;software
	engineering;software reuse;C language;optimisation;software reusability;}
}

@INPROCEEDINGS{1604762,
  author = {Alanen, M. and Lilius, J. and Porres, I. and Trascan, D. and Oliver,
	I. and Sandstrom, K.},
  title = {Design method support for domain specific SoC design},
  booktitle = {Model-Based Development of Computer-Based Systems and Model-Based
	Methodologies for Pervasive and Embedded Software, 2006. MBD/MOMPES
	2006. Fourth and Third International Workshop on},
  year = {2006},
  pages = {8 pp. -32},
  month = {march},
  abstract = {In this paper we introduce the idea of methodware (i.e., combination
	of language definitions and model transformations) as a framework
	for defining design methods. The approach should allow us to facilitate
	building of customized tools for domain specific problems. We apply
	our approach to the MICAS architecture, a novel SoC architecture
	for mobile peripherals},
  doi = {10.1109/MBD-MOMPES.2006.8},
  keywords = {design method support;domain specific SoC design;language definitions;methodware;model
	transformations;logic CAD;system-on-chip;}
}

@INPROCEEDINGS{4159734,
  author = {Aleksy, Markus and Schwind, Michael and Gitzel, Ralf},
  title = {Generating Families of Business Components from Metamodel Hierarchies},
  booktitle = {Complex, Intelligent and Software Intensive Systems, 2007. CISIS
	2007. First International Conference on},
  year = {2007},
  pages = {197 -204},
  month = {april},
  abstract = {Model-driven software development (MDSD) has gained acceptance in
	the software development industry over the last few years. Current
	approaches to MDSD favor targeting particular domains over general-purpose
	modeling and code generation solutions. The OMEGA toolset provides
	an infrastructure for the rapid development of domain-specific modeling
	and code generation facilities. It is based on the concept of metamodel
	hierarchies. In this paper we propose an extension to the OMEGA approach
	that is targeted at enabling the evolutionary development of families
	of business components by introducing additional model types. The
	paper provides an overview of the OMEGA approach and describes the
	integration of family and feature models as configuration data to
	the OMEGA code generator and the expected benefits. A short outlook
	at the end of the paper outlines future work on this matter},
  doi = {10.1109/CISIS.2007.30},
  keywords = {OMEGA toolset;code generation;metamodel hierarchies;model-driven software
	development;object-oriented programming;program compilers;software
	tools;}
}

@INPROCEEDINGS{5076630,
  author = {Ali, N.M. and Hosking, J. and Huh, J. and Grundy, J.},
  title = {Critic Authoring Templates for Specifying Domain-Specific Visual
	Language Tool Critics},
  booktitle = {Software Engineering Conference, 2009. ASWEC '09. Australian},
  year = {2009},
  pages = {81 -90},
  month = {april},
  abstract = {In recent years we have observed the extensive evolution of support
	tools that work with the user to achieve a range of computer-mediated
	tasks. One of these support tools is the critiquing system (also
	known as critics). Critics have evolved in the last years as specific
	tool features to support users in computer-mediated tasks by providing
	guidelines or suggestions for improvement to designs, code and other
	digital artifacts. While critic tools have been demonstrated to be
	effective in providing feedback, critic authoring continues to be
	a big challenge. We describe a visual design critic authoring template
	approach that facilitates the construction of critics for Marama-based
	domain-specific visual language tools. Our template approach provides
	end-users and tool designers with a new way to express design critics
	in a natural and efficient manner. We describe prototype tool support
	for specifying and realizing these design critics in Marama-based
	tools.},
  doi = {10.1109/ASWEC.2009.22},
  issn = {1530-0803},
  keywords = {Marama-based tools;computer-mediated tasks;critic authoring templates;domain-specific
	visual language tool critics;support tools;authoring systems;task
	analysis;visual languages;}
}

@INPROCEEDINGS{5295286,
  author = {Ali, N.M. and Hosking, J. and Huh, J. and Grundy, J.},
  title = {Template-based critic authoring for domain-specific visual language
	tools},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {111 -118},
  month = {sept.},
  abstract = {In recent years, there has been an increasing interest in using computer-based
	ldquocritic toolsrdquo for supporting the end-users or tool designers
	in analyzing and proactively improving their design artifacts. Several
	approaches have been applied to designing and realizing such critics,
	for example rule-based, pattern-matching and knowledge-based approaches.
	While many studies have found evidence that critic tools are an efficient
	feedback-providing mechanism, there is still insufficient knowledge
	about how to provide an effective critic authoring environment. In
	this paper we apply a business rule template approach as a mechanism
	to specify and realize critics for Marama-based domain-specific visual
	language tools. We describe a visual design critic authoring template
	approach that supports end-users or tool designers in the construction
	of critics for any Marama-based tools.},
  doi = {10.1109/VLHCC.2009.5295286},
  issn = {1943-6092},
  keywords = {Marama-based visual language tools;computer-based critic tools;critic
	tools;domain-specific visual language tools;feedback-providing mechanism;template-based
	critic authoring;visual design critic authoring template;authoring
	systems;visual languages;}
}

@INPROCEEDINGS{4976345,
  author = {Alonso, I.G. and Fuente, M.P.A.G. and Brugos, J.A.L.},
  title = {Using Sysml to Describe a New Methodology for Semiautomatic Software
	Generation from Inferred Behavioral and Data Models},
  booktitle = {Systems, 2009. ICONS '09. Fourth International Conference on},
  year = {2009},
  pages = {210 -215},
  month = {march},
  abstract = {This article describes a new methodology designed for semiautomatic
	generation of software applications using the new standard of OMG
	consortium: Sysml. The methodology has behavior and data model inference
	steps. Both data and behavior are inferred, the first by XSD-schema
	inference and the latter by business process mining inferences. The
	paper describes how by using Sysml a better description of the methodology
	is given, a description that allows making a better design than using
	UML standard tools.},
  doi = {10.1109/ICONS.2009.50},
  keywords = {OMG consortium;Sysml;UML tool;XSD-schema inference;behavioral model
	inference;business process mining inference;code generation;data
	model inference;semiautomatic software component generation methodology;Unified
	Modeling Language;XML;automatic programming;business data processing;data
	mining;data models;inference mechanisms;object-oriented programming;program
	compilers;}
}

@ARTICLE{6065739,
  author = {Altintas, N. and Cetin, S. and Dogru, A. and Oguztuzun, H.},
  title = {Modeling Product Line Software Assets Using Domain-Specific Kits},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2011},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {Software Product Line Engineering (SPLE) is a prominent paradigm for
	the assembly of a family of products using product line core assets.
	The modeling of software assets that together form the actual products
	is critical for achieving the strategic benefits of software product
	lines. We propose a feature-based approach to software asset modeling
	based on abstractions provided by Domain-Specific Kits. This approach
	involves a software asset metamodel used to derive asset modeling
	languages that define reusable software assets in domain-specific
	terms. The approach also prescribes a roadmap for modeling these
	software assets in conjunction with the product line reference architecture.
	Asset capabilities can be modeled using feature diagrams as the external
	views of the software assets. Internal views can be expressed in
	terms of domain-specific artifacts with variability points, where
	the domain-specific artifacts are created using Domain-Specific Kits.
	This approach produces loosely coupled and highly cohesive software
	assets that are reusable for multiple product lines. The approach
	is validated by assessing software asset reuse in two different product
	lines in the finance domain. We also evaluated the productivity gains
	in large-scale complex projects, and found that the approach yielded
	a significant reduction in the total project effort.},
  doi = {10.1109/TSE.2011.109},
  issn = {0098-5589}
}

@ARTICLE{5441292,
  author = {Amatriain, X. and Arumi, P.},
  title = {Frameworks Generate Domain-Specific Languages: A Case Study in the
	Multimedia Domain},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2011},
  volume = {37},
  pages = {544 -558},
  number = {4},
  month = {july-aug. },
  abstract = {We present an approach to software framework development that includes
	the generation of domain-specific languages (DSLs) and pattern languages
	as goals for the process. Our model is made of three workflows-framework,
	metamodel, and patterns-and three phases-inception, construction,
	and formalization. The main conclusion is that when developing a
	framework, we can produce with minimal overhead-almost as a side
	effect-a metamodel with an associated DSL and a pattern language.
	Both outputs will not only help the framework evolve in the right
	direction, but will also be valuable in themselves. In order to illustrate
	these ideas, we present a case study in the multimedia domain. For
	several years, we have been developing a multimedia framework. The
	process has produced a full-fledged domain-specific metamodel for
	the multimedia domain, with an associated DSL and a pattern language.},
  doi = {10.1109/TSE.2010.48},
  issn = {0098-5589},
  keywords = {associated DSL;domain-specific languages;domain-specific metamodel;multimedia
	domain;pattern languages;software framework development;visual programming;multimedia
	computing;software engineering;specification languages;visual programming;}
}

@ARTICLE{4543986,
  author = {Amelunxen, C. and Schurr, A.},
  title = {Formalising model transformation rules for UML/MOF 2},
  journal = {Software, IET},
  year = {2008},
  volume = {2},
  pages = {204 -222},
  number = {3},
  month = {june },
  abstract = {Model-driven software development, today's state-of-the-art approach
	to the design of software, can be applied in various domains and
	thus demands a variety of domain-specific modelling languages. The
	specification of a domain-specific modelling language's syntax and
	semantics can in turn be specified based on models, which represent
	the approach of metamodelling as a special form of language engineering.
	The latest version of the unified modelling language 2 (UML 2) and
	its subset the meta object facility 2 (MOF 2) provide sufficient
	support for metamodelling, a modelling language's abstract syntax.
	Furthermore, based on the description of the abstract syntax, a language's
	static semantics can simply be specified by the object constraint
	language (OCL) as UML/MOF's natural constraint language, whereas
	the description of an MOF compliant language's dynamic semantics
	is still not covered. The authors try to close this gap by integrating
	MOF/OCL with graph transformations for the specification of dynamic
	aspects of modelling languages and tools. The formalisation of such
	an integration is non-trivial because of the fact that UML/MOF 2
	offer a rather unusual and sophisticated association concept (graph
	model). Although there are many approaches, which formalise graph
	transformations in general and first approaches that offer a precise
	specification of the semantics of the association concepts of UML/MOF
	2, there is still a lack in bringing both together. Here, the authors
	close this gap by formalising graph transformations that work on
	a UML/MOF 2 compatible graph model.},
  doi = {10.1049/iet-sen:20070076},
  issn = {1751-8806},
  keywords = {UML;Unified Modelling Language;domain-specific modelling language
	semantics;domain-specific modelling language syntax;graph transformation;meta
	object facility 2;model-driven software development;object constraint
	language;Unified Modeling Language;computational linguistics;formal
	specification;graph theory;}
}

@INPROCEEDINGS{4279200,
  author = {Amor, M. and Garcia, A. and Fuentes, L.},
  title = {AGOL: An Aspect-Oriented Domain-Specific Language for MAS},
  booktitle = {Aspect-Oriented Requirements Engineering and Architecture Design,
	2007. Early Aspects at ICSE: Workshops in},
  year = {2007},
  pages = {4},
  month = {may},
  abstract = {Specific features of multi-agent systems (MAS), such as autonomy,
	learning, mobility, coordination, are driving development concerns,
	which make evident the need for new design abstractions. Up to now,
	agent-oriented modeling languages have delivered basic MAS design
	abstractions - such as goals and actions - that explicitly tackle
	some of these concerns. However, the modularization of a plethora
	of fundamental MAS features has been hindered throughout the software
	lifecycle. This paper presents a methodological framework to address
	enhanced modularity and traceability of such crosscutting concerns
	in MAS development. Our design framework is mainly rooted at the
	proposition of a new domain-specific language, called AGOL. In addition,
	the proposed framework is supported by a bench of transformation
	rules of AGOL artifacts, which can be effectively used to derive
	agent implementations in two concrete aspect-oriented implementation
	platforms, namely AspectT and Malaca.},
  doi = {10.1109/EARLYASPECTS.2007.3},
  keywords = {AGOL;AspectT;Malaca;agent-oriented modeling languages;aspect-oriented
	domain-specific language;design abstractions;multi-agent systems;software
	lifecycle;multi-agent systems;object-oriented languages;object-oriented
	programming;}
}

@INPROCEEDINGS{5954391,
  author = {van Amstel, M.F. and van den Brand, M.G.J. and Engelen, L.J.P.},
  title = {Using a DSL and Fine-Grained Model Transformations to Explore the
	Boundaries of Model Verification -- Extended Abstract},
  booktitle = {Software Testing, Verification and Validation Workshops (ICSTW),
	2011 IEEE Fourth International Conference on},
  year = {2011},
  pages = {63 -66},
  month = {march},
  abstract = {Traditionally, the state-space explosion problem in model checking
	is handled by applying abstractions and simplifications to the model
	that needs to be verified. In this paper, we propose a model-driven
	engineering approach that works the other way around. Instead of
	making a concrete model more abstract, we propose to refine an abstract
	model to make it more concrete. We propose to use fine-grained model
	transformations to enable model checking of models that are as close
	to the implementation model as possible. We applied our approach
	in a case study. The results show that models that are more concrete
	can be validated when fine-grained transformations are applied.},
  doi = {10.1109/ICSTW.2011.8},
  keywords = {DSL;abstract model;domain-specific language;fine-grained model transformation;model
	checking;model verification;model-driven engineering;state-space
	explosion problem;formal verification;programming languages;}
}

@INPROCEEDINGS{6004513,
  author = {van Amstel, M.F. and van den Brand, M.G.J. and Engelen, L.J.P.},
  title = {Using a DSL and Fine-Grained Model Transformations to Explore the
	Boundaries of Model Verification},
  booktitle = {Secure Software Integration Reliability Improvement Companion (SSIRI-C),
	2011 5th International Conference on},
  year = {2011},
  pages = {120 -127},
  month = {june},
  abstract = {Traditionally, the state-space explosion problem in model checking
	is handled by applying abstractions and simplifications to the model
	that needs to be verified. In this paper, we propose a model-driven
	engineering approach that works the other way around. Instead of
	making a concrete model more abstract, we propose to refine an abstract
	model to make it more concrete. We propose to use fine-grained model
	transformations to enable model checking of models that are as close
	to the implementation model as possible. We applied our approach
	in a case study. The results show that it is possible to validate
	models that are more concrete when fine-grained transformations are
	applied.},
  doi = {10.1109/SSIRI-C.2011.26},
  keywords = {DSL;fine grained model transformation;fine grained transformation;model
	checking;model driven engineering approach;model verification boundaries;state
	space explosion problem;formal verification;software engineering;}
}

@INPROCEEDINGS{5945429,
  author = {Kyoungho An and Trewyn, A. and Gokhale, A. and Sastry, S.},
  title = {Model-Driven Performance Analysis of Reconfigurable Conveyor Systems
	Used in Material Handling Applications},
  booktitle = {Cyber-Physical Systems (ICCPS), 2011 IEEE/ACM International Conference
	on},
  year = {2011},
  pages = {141 -150},
  month = {april},
  abstract = {Reconfigurable conveyors are increasingly being adopted in multiple
	industrial sectors for their immense flexibility in adapting to new
	products and product lines. Before modifying the layout of the conveyor
	system for the new product line, however, engineers and layout planners
	must be able to answer many questions about the system, such as maximum
	sustainable rate of flow of goods, prioritization among goods, and
	tolerances of failures. Any analysis capability that provides answers
	to these questions must account for both the physical and cyber artifacts
	of the reconfigurable system all at once. Moreover, the same system
	should enable the stakeholders to seamlessly change the layouts and
	be able to analyze the pros and cons of the layouts. This paper addresses
	these challenges by presenting a model-driven analysis tool that
	provides three important capabilities. First, a domain-specific modeling
	language provides the stakeholders with intuitive artifacts to model
	conveyor layouts. Second, an analysis engine embedded within the
	model-driven tool provides an accurate simulation of the modeled
	conveyor system accounting for both the physical and cyber issues.
	Third, generative capabilities within the tool help to automate the
	analysis process. The merits of our model-driven analysis tool are
	evaluated in the context of an example conveyor topology.},
  doi = {10.1109/ICCPS.2011.12},
  keywords = {domain-specific modeling language;immense flexibility;intuitive artifacts;layout
	planning;material handling;model-driven performance analysis;product
	lines;reconfigurable conveyor systems;stakeholders;conveyors;production
	engineering computing;production planning;simulation languages;}
}

@ARTICLE{1435421,
  author = {Anagnostakis, A.G. and Tzima, M. and Sakellaris, G.C. and Fotiadis,
	D.I. and Likas, A.C.},
  title = {Semantics-based information modeling for the health-care administration
	sector: the Citation platform},
  journal = {Information Technology in Biomedicine, IEEE Transactions on},
  year = {2005},
  volume = {9},
  pages = {239 -247},
  number = {2},
  month = {june },
  abstract = {An information brokerage environment for effective information structuring,
	indexing, and retrieval in the health-care administration sector
	is presented. The system is based on ontology modeling, natural language
	processing, extensible markup language, semantics analysis, and behavioral
	description. Semantics-based information acquisition is achieved
	through the uniform modeling, representation, and handling of domain-specific
	knowledge, both content-based and procedural. The system has been
	validated using information located on several repositories in the
	web and its performance is reported in terms of precision and recall.},
  doi = {10.1109/TITB.2005.847145},
  issn = {1089-7771},
  keywords = {Citation platform;behavioral description;content-based knowledge;domain-specific
	knowledge;extensible markup language;health-care administration sector;information
	brokerage environment;information indexing;information retrieval;information
	structuring;natural language processing;ontology modeling;procedural
	knowledge;semantics analysis;semantics-based information modeling;content-based
	retrieval;health care;indexing;information retrieval;medical information
	systems;natural languages;ontologies (artificial intelligence);page
	description languages;Artificial Intelligence;Health Services Administration;Programming
	Languages;Semantics;}
}

@ARTICLE{4731241,
  author = {Anand, C.K. and Kahl, W.},
  title = {An Optimized Cell BE Special Function Library Generated by Coconut},
  journal = {Computers, IEEE Transactions on},
  year = {2009},
  volume = {58},
  pages = {1126 -1138},
  number = {8},
  month = {aug. },
  abstract = {Coconut, a tool for developing high-assurance, high-performance kernels
	for scientific computing, contains an extensible domain-specific
	language (DSL) embedded in Haskell. The DSL supports interactive
	prototyping and unit testing, simplifying the process of designing
	efficient implementations of common patterns. Unscheduled C and scheduled
	assembly language output are supported. Using the patterns, even
	nonexpert users can write efficient function implementations, leveraging
	special hardware features. A production-quality library of elementary
	functions for the cell BE SPU compute engines has been developed.
	Coconut-generated and -scheduled vector functions were more than
	four times faster than commercially distributed functions written
	in C with intrinsics (a nicer syntax for in-line assembly), wrapped
	in loops and scheduled by spuxlc. All Coconut functions were faster,
	but the difference was larger for hard-to-approximate functions for
	which register-level SIMD lookups made a bigger difference. Other
	helpful features in the language include facilities for translating
	interval and polynomial descriptions between GHCi, a Haskell interpreter
	used to prototype in the DSL, and Maple, used for exploration and
	minimax polynomial generation. This makes it easier to match mathematical
	properties of the functions with efficient calculational patterns
	in the SPU ISA. By using single, literate source files, the resulting
	functions are remarkably readable.},
  doi = {10.1109/TC.2008.223},
  issn = {0018-9340},
  keywords = {Coconut;DSL;GHCi;Haskell interpreter;Maple;extensible domain-specific
	language;high-performance kernels;interactive prototyping;minimax
	polynomial generation;optimized cell BE special function library;register-level
	SIMD lookups;scheduled assembly language output;scientific computing;spuxlc;unit
	testing;unscheduled C language output;C language;assembly language;functional
	languages;software engineering;}
}

@INPROCEEDINGS{1631167,
  author = {Anderson, A.H.},
  title = {Domain-independent, composable Web services policy assertions},
  booktitle = {Policies for Distributed Systems and Networks, 2006. Policy 2006.
	Seventh IEEE International Workshop on},
  year = {2006},
  pages = {4 pp. -152},
  month = {june},
  abstract = {The current model for the predicates, or "Assertions", used in a WS-Policy
	instance is for each policy domain to design new schema elements
	for that domain's Assertions. Their semantics are defined in an associated
	specification and are domain-specific. This model leads to interoperability
	and maintenance problems and hinders dynamic service composition.
	WS-Policy constraints is a domain-independent language for writing
	Assertions that is based on the Web Services Policy Language subset
	of XACML; it differs in addressing only the Assertion layer. This
	paper describes problems with domain-specific Assertions, the WS-Policy
	constraints alternative, and problems encountered in the development
	of this language},
  doi = {10.1109/POLICY.2006.16},
  keywords = {Assertion layer;WS-Policy constraints;WS-Policy instance;Web Services
	Policy Language;XACML;domain-independent composable Web services
	policy assertions;domain-independent language;dynamic service composition;interoperability
	problems;maintenance problems;Internet;page description languages;programming
	language semantics;}
}

@INPROCEEDINGS{943833,
  author = {Andreasen, T.},
  title = {Query evaluation based on domain-specific ontologies},
  booktitle = {IFSA World Congress and 20th NAFIPS International Conference, 2001.
	Joint 9th},
  year = {2001},
  volume = {3},
  pages = {1844 -1849 vol.3},
  month = {july},
  abstract = {We describe here a query evaluation principle that is based on compound
	or hierarchical aggregation, where an aggregate may be an argument
	to another aggregate. A query is assumed to consist of or be decomposed
	into a set of query attributes that are divided into groups. During
	query evaluation, groups are treated for individual aggregation and
	group aggregates are in turn aggregated, leading to a query aggregate.
	The evaluation principle is considered in the context of an approach
	to ontology-based querying, where domain knowledge in the form of
	a dictionary and an ontology is utilized in the computation of groups
	of attributes and aggregation parameters. While the user may pose
	a query in a simple form as a list of words or in natural language,
	queries are evaluated as compound expressions. The groupings and
	operators applied are the results of a knowledge-based manipulation
	of the initial query},
  doi = {10.1109/NAFIPS.2001.943833},
  keywords = {OntoQuery;Ontolog;compound aggregation;content-based retrieval;dictionary;domain
	knowledge;hierarchical aggregation;knowledge-based manipulation;ontology;query
	attributes;query evaluation;query evaluation principle;text databases;knowledge
	based systems;query languages;query processing;}
}

@INPROCEEDINGS{972023,
  author = {Andreasen, T.},
  title = {Improving query-answers from domain knowledge},
  booktitle = {Systems, Man, and Cybernetics, 2001 IEEE International Conference
	on},
  year = {2001},
  volume = {5},
  pages = {3269 -3274 vol.5},
  abstract = {The paper introduces an approach to open knowledge-based query evaluation
	and describes a preliminary prototype taking this approach. A knowledge-base
	(KB), complementing the primary information base, that defines and
	relates words and concepts and exploits similarities between concepts,
	is used to improve answers. The knowledge base may introduce common
	and/or domain specific knowledge. The overall approach is to apply
	the KB in combination with a highly parameterised matching and aggregation,
	such that queries are transformed and expanded through the KB and
	parameters are resolved from the KB},
  doi = {10.1109/ICSMC.2001.972023},
  issn = {1062-922X},
  keywords = {KB;domain knowledge;domain specific-knowledge;highly parameterised
	matching;knowledge-base;open knowledge-based query evaluation;primary
	information base;query transformation;query-answer improvement;knowledge
	based systems;natural languages;query processing;text analysis;}
}

@INPROCEEDINGS{864130,
  author = {Angele, J.},
  title = {Propose-and-revise modeled in Karl},
  booktitle = {ISAI/IFIS 1996. Mexico-USA Collaboration in Intelligent Systems Technologies.
	Proceedings},
  year = {1996},
  pages = {278 -287},
  month = {nov.},
  abstract = {This paper reports an evaluation study for the specification of an
	average sized expert system for configuring elevator systems using
	the language KARL (Knowledge Acquisition and Representation Language).
	Two results have been gained in this study: (i) a formal model of
	the used problem-solving method (PSM) Propose-and-Revise has been
	developed and (ii) the adequacy of the language KARL for specifying
	such systems has been evaluated. KARL is based on a strong conceptual
	model: the KARL model of expertise, which represents different aspects
	of the model at different layers. It clearly separates domain specific
	knowledge fim probleni-solving-specific knowledge which allows to
	reuse both parts independenty from the other. KARL provides language
	primitives on a high level of abstraction, independent of implementation
	issues. KARL is a formal language which allows to represent knowledge
	unambiguously. KARL is an executable language which allows to validate
	the resulting model by testing and debugging. It turned out that
	KARL is well-suited for such specification issues. It also turned
	out that due to a flexible connection between domain knowledge and
	problem solving knowledge provided by KARL both different kinds of
	knowledge may be specified nearly independenty from the other which
	supports their reuse. This study gave us various insights into the
	adequacy of the language KARL for representing the knowledge on an
	abstract level. In spite of the encouraging results we gained this
	study also revealed some deficiencies of the language KARL which
	are currently eliminated for a future version of KARL.}
}

@INPROCEEDINGS{4492428,
  author = {Angyal, L. and Lengyel, L. and Charaf, H.},
  title = {A Synchronizing Technique for Syntactic Model-Code Round-Trip Engineering},
  booktitle = {Engineering of Computer Based Systems, 2008. ECBS 2008. 15th Annual
	IEEE International Conference and Workshop on the},
  year = {2008},
  pages = {463 -472},
  month = {31 2008-april 4},
  abstract = {The introduction of UML class diagrams has not raised the abstraction
	level of development to the extent that was intended: class diagrams
	are only the visual representations of source class skeletons implemented
	in a programming language. To improve the productivity, domain-specific
	languages are applied, which cover a narrow domain, and their high
	abstraction makes use of the domain experts easier. The simultaneous
	evolution of the source code and the software models causes the loss
	of synchronization. Round-tripping the domain-specific models is
	not supported by model-driven development tools, because the abstraction
	gap between the models and the generated code prevents the use of
	general approaches. However, developers should have the opportunity
	of choosing between the artifacts that are more efficient for applying
	the modifications. This paper introduces how different tools achieve
	the preservation of manually written code while the model is evolving.
	In contrast, we present our approach that allows the customization
	of the generated code. The abstraction gap is closed by performing
	model transformations and an incremental merge.},
  doi = {10.1109/ECBS.2008.33},
  keywords = {UML class diagrams;abstraction gap;model-driven development tools;programming
	language;syntactic model-code round-trip engineering;visual representations;Unified
	Modeling Language;programming languages;software engineering;software
	tools;}
}

@INPROCEEDINGS{706176,
  author = {Anlauff, M. and Kutter, P.W. and Pierantonio, A.},
  title = {Montages/Gem-Mex: a meta visual programming generator},
  booktitle = {Visual Languages, 1998. Proceedings. 1998 IEEE Symposium on},
  year = {1998},
  pages = {304 -305},
  month = {sep},
  abstract = {Last decade witnessed a disappointing lack in technology transfer
	from formal semantics to language design. Research in formal semantics
	has developed increasingly complex concepts and notation, at the
	expense of calculational clarity and applicability in the development
	of languages. Montages is a visual domain-specific formalism for
	specifying all the aspects of a programming language. It is intelligible
	to a broad range of people involved in the language life cycle, from
	design to programming language descriptions are fed to a rapid prototyping
	tool, called Gem-Mex, which generates a visual programming environment
	for the given language. Gem-Mex consists of a graphical front-end
	which allows a comfortable editing of the visual components of the
	specification. Starting from these visual descriptions the tool is
	able to generate in an automatic way high-quality documents, type-checkers,
	interpreters and a visual symbolic debugger. All these products form
	a powerful suite where the programmer can write, execute, animate
	and debug programs written in the specified language},
  doi = {10.1109/VL.1998.706176},
  issn = {1049-2615},
  keywords = {Gem-Mex;Montages;formal semantics;interpreters;language life cycle;meta
	visual programming generator;programs debugging;rapid prototyping
	tool;type-checkers;visual domain-specific formalism;visual programming
	environment;visual symbolic debugger;automatic programming;program
	debugging;program interpreters;programming environments;software
	prototyping;visual programming;}
}

@INPROCEEDINGS{5970480,
  author = {Ansari, J. and Xi Zhang and Mahonen, P.},
  title = {A compiler assisted approach for component based reconfigurable MAC
	design},
  booktitle = {Ad Hoc Networking Workshop (Med-Hoc-Net), 2011 The 10th IFIP Annual
	Mediterranean},
  year = {2011},
  pages = {135 -141},
  month = {june},
  abstract = {Cognitive radio networks require reconfiguration and adaptivity in
	order to efficiently meet the changing application demands and network
	conditions. We have developed a framework which allows composition
	of MAC protocols using a library of MAC components. These components
	are implemented with a hardware-software co-design approach so as
	to satisfy the timeliness requirements as well as to provide the
	desired degree of flexibility. A domain specific MAC language and
	corresponding MAC-meta compiler toolchain is developed to realize
	highly dynamic and reconfigurable MAC solutions using the MAC components.
	The prototype implementation on WARP SDR boards indicates that our
	approach eases the MAC development without compromising on the performance
	characteristics as compared to the monolithic way of implementing
	MAC protocols.},
  doi = {10.1109/Med-Hoc-Net.2011.5970480},
  keywords = {MAC protocols;MAC-metacompiler tool chain;WARP SDR boards;cognitive
	radio networks;compiler assisted approach;component based reconfigurable
	MAC design;domain specific MAC language;hardware-software codesign
	approach;access protocols;cognitive radio;hardware-software codesign;program
	compilers;software radio;specification languages;telecommunication
	computing;}
}

@INPROCEEDINGS{4019597,
  author = {Antkiewicz, M.},
  title = {Round-Trip Engineering of Framework-Based Software using Framework-Specific
	Modeling Languages},
  booktitle = {Automated Software Engineering, 2006. ASE '06. 21st IEEE/ACM International
	Conference on},
  year = {2006},
  pages = {323 -326},
  month = {sept.},
  abstract = {This research combines three distinct areas: domain-specific modeling,
	object-oriented application frameworks and round-trip engineering.
	We introduce framework-specific modeling languages (FSMLs) which
	are used for the modeling of framework-based software and enable
	automated round-trip engineering. We describe a prototype implementation
	of an example FSML. We also present research outline and future work},
  doi = {10.1109/ASE.2006.58},
  issn = {1527-1366},
  keywords = {domain-specific modeling;framework-based software;framework-specific
	modeling language;object-oriented application;round-trip engineering;formal
	specification;specification languages;}
}

@ARTICLE{4907004,
  author = {Antkiewicz, M. and Czarnecki, K. and Stephan, M.},
  title = {Engineering of Framework-Specific Modeling Languages},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2009},
  volume = {35},
  pages = {795 -824},
  number = {6},
  month = {nov.-dec. },
  abstract = {Framework-specific modeling languages (FSMLs) help developers build
	applications based on object-oriented frameworks. FSMLs model abstractions
	and rules of application programming interfaces (APIs) exposed by
	frameworks and can express models of how applications use APIs. Such
	models aid developers in understanding, creating, and evolving application
	code. We present four exemplar FSMLs and a method for engineering
	new FSMLs. The method was created postmortem by generalizing the
	experience of building the exemplars and by specializing existing
	approaches to domain analysis, software development, and quality
	evaluation of models and languages. The method is driven by the use
	cases that the FSML under development should support and the evaluation
	of the constructed FSML is guided by two existing quality frameworks.
	The method description provides concrete examples for the engineering
	steps, outcomes, and challenges. It also provides strategies for
	making engineering decisions. Our work offers a concrete example
	of software language engineering and its benefits. FSMLs capture
	existing domain knowledge in language form and support application
	code understanding through reverse engineering, application code
	creation through forward engineering, and application code evolution
	through round-trip engineering.},
  doi = {10.1109/TSE.2009.30},
  issn = {0098-5589},
  keywords = {application code creation through forward engineering;application
	code evolution through round-trip engineering;application programming
	interfaces;framework-specific modeling languages;object-oriented
	frameworks;reverse engineering;software development;software language
	engineering;application program interfaces;object-oriented programming;software
	engineering;}
}

@ARTICLE{1695541,
  author = {Arango, G. and Baxter, I. and Freeman, P. and Pidgeon, C.},
  title = {TMM: Software Maintenance by Transformation},
  journal = {Software, IEEE},
  year = {1986},
  volume = {3},
  pages = {27 -39},
  number = {3},
  month = {may },
  abstract = {Porting an undocumented program without any source changes demonstrates
	the value of a transformational theory of maintenance. The theory
	is based on the reuse of knowledge.},
  doi = {10.1109/MS.1986.233411},
  issn = {0740-7459}
}

@INPROCEEDINGS{4812468,
  author = {Arasu, A. and Re, C. and Suciu, D.},
  title = {Large-Scale Deduplication with Constraints Using Dedupalog},
  booktitle = {Data Engineering, 2009. ICDE '09. IEEE 25th International Conference
	on},
  year = {2009},
  pages = {952 -963},
  month = {29 2009-april 2},
  abstract = {We present a declarative framework for collective deduplication of
	entity references in the presence of constraints. Constraints occur
	naturally in many data cleaning domains and can improve the quality
	of deduplication. An example of a constraint is "each paper has a
	unique publication venue''; if two paper references are duplicates,
	then their associated conference references must be duplicates as
	well. Our framework supports collective deduplication, meaning that
	we can dedupe both paper references and conference references collectively
	in the example above. Our framework is based on a simple declarative
	Datalog-style language with precise semantics. Most previous work
	on deduplication either ignoreconstraints or use them in an ad-hoc
	domain-specific manner. We also present efficient algorithms to support
	the framework. Our algorithms have precise theoretical guarantees
	for a large subclass of our framework. We show, using a prototype
	implementation, that our algorithms scale to very large datasets.
	We provide thorough experimental results over real-world data demonstrating
	the utility of our framework for high-quality and scalable deduplication.},
  doi = {10.1109/ICDE.2009.43},
  issn = {1084-4627},
  keywords = {Dedupalog;ad-hoc domain-specific manner;collective deduplication;conference
	references;constraint;data cleaning domains;declarative Datalog-style
	language;large-scale deduplication;paper references;DATALOG;constraint
	handling;data analysis;}
}

@INPROCEEDINGS{4456835,
  author = {Armonas, A. and Nemuraite, L.},
  title = {Improving quality of code generated from OCL expressions},
  booktitle = {Computer and information sciences, 2007. iscis 2007. 22nd international
	symposium on},
  year = {2007},
  pages = {1 -6},
  month = {nov.},
  abstract = {In this paper, we briefly describe existing principles and stages
	for generating code from OCL expressions pointing out the drawbacks
	that cause inefficiencies of the resulting code. The proposed improvement
	of the transformation is based on extended abstract syntax trees
	(AST) with context-specific attributes. Principles for defining such
	attributes on AST trees and an example of transformation is presented.},
  doi = {10.1109/ISCIS.2007.4456835},
  keywords = {OCL expressions;abstract syntax trees;code generated;context-specific
	attributes;object constraint language;attribute grammars;computational
	linguistics;specification languages;}
}

@INPROCEEDINGS{5071016,
  author = {Arora, R. and Bangalore, P.},
  title = {A framework for raising the level of abstraction of explicit parallelization},
  booktitle = {Software Engineering - Companion Volume, 2009. ICSE-Companion 2009.
	31st International Conference on},
  year = {2009},
  pages = {339 -342},
  month = {may},
  abstract = {In this research, a Framework for Synthesizing Parallel Applications
	(FraSPA) in a user-guided manner is being developed. The FraSPA would
	facilitate the synthesis of parallel applications from existing sequential
	applications and middleware components for multiple-platforms and
	diverse domains. The framework design is based upon design patterns
	and generative programming techniques. The main goal of this research
	is to raise the level of abstraction of the widely used low-level
	parallel programming approaches. A technique to separate parallel
	and sequential concerns will be demonstrated through this work. Other
	contributions will be in the area of design patterns and Domain-Specific
	Languages (DSLs) for parallel computing. The design patterns, along
	with the DSLs, will promote code reuse and code correctness. There
	would be a reduction in code complexity and code maintenance would
	become easy. The productivity of the end-users will increase. This
	research can be broadly classified as ldquoSoftware Engineering for
	High Performance Computingrdquo.},
  doi = {10.1109/ICSE-COMPANION.2009.5071016},
  keywords = {abstraction level;design pattern;explicit parallelization;framework-for-synthesizing
	parallel application;generative programming technique;high performance
	computing;middleware component;parallel programming approach;sequential
	application;middleware;object-oriented programming;parallel programming;}
}

@INPROCEEDINGS{5069162,
  author = {Arora, R. and Bangalore, P. and Mernik, M.},
  title = {Developing scientific applications using Generative Programming},
  booktitle = {Software Engineering for Computational Science and Engineering, 2009.
	SECSE '09. ICSE Workshop on},
  year = {2009},
  pages = {51 -58},
  month = {may},
  abstract = {Scientific applications usually involve large number of distributed
	and dynamic resources and huge datasets. A mechanism like checkpointing
	is essential to make these applications resilient to failures. Using
	checkpointing as an example, this paper presents an approach for
	integrating the latest software engineering techniques with the development
	of scientific software. Generative programming is used in this research
	to achieve the goals of non-intrusive reengineering of existing applications
	to insert the checkpointing mechanism and to decouple the checkpointing-specifications
	from its actual implementation. The end-user specifies the checkpointing
	details at a higher level of abstraction, using which the necessary
	code is generated and woven into the application. The lessons learned
	and the implementation approach presented in this paper can be applied
	to the development of scientific applications in general. The paper
	also demonstrates that the generated code does not introduce any
	inaccuracies and its performance is comparable to the manually inserted
	code.},
  doi = {10.1109/SECSE.2009.5069162},
  keywords = {checkpointing specification;distributed resource;dynamic resource;generative
	programming;non intrusive reengineering;scientific application development;software
	engineering;checkpointing;formal specification;natural sciences computing;systems
	re-engineering;}
}

@INPROCEEDINGS{1167783,
  author = {Arsanjani, A.},
  title = {Business compilers: towards supporting a highly re-configurable architectural
	style for service-oriented architecture},
  booktitle = {Software Maintenance, 2002. Proceedings. International Conference
	on},
  year = {2002},
  pages = { 287 - 288},
  abstract = { Grammar-oriented Object design (GOOD) uses a business domain-specific
	language to model the flow an constraints on a set of collaborating
	enterprise components (EC). Maintenance of these components and their
	flow composition is a major issue. We present a software tool called
	the, Business Compiler (BC) that facilitates the definition, debugging
	and execution of business flow languages in order to help animate
	and execute the collaboration of components reflecting the business
	process steps defined by a business modeler. Architects enhance the
	grammar with component services that serve as actions in,the grammar.
	The combination of flow definition by modelers and component services
	by software architects provides a powerful collaborative environment
	for enabling the incremental creation of a highly re-configurable-architectural
	style. BC consists Of an application framework that supports component-based
	development and includes a GUI debugger front end This helps modelers
	by providing dynamic documentation an can be used by architects to
	create and execute a formal specification of business flow to facilitate
	maintainability through a highly adaptive and re-configurable architectural
	style.},
  doi = {10.1109/ICSM.2002.1167783},
  issn = {1063-6773 },
  keywords = { Business Compiler; GUI debugger front end; application framework;
	business domain-specific language; business flow languages; collaborative
	environment; component services; debugging; dynamic documentation;
	enterprise components; formal specification; grammar-oriented object
	design; highly reconfigurable architectural style; service-oriented
	architecture; software architects; software tool; object-oriented
	programming; program compilers; program debugging; software architecture;}
}

@INPROCEEDINGS{941667,
  author = {Arsanjani, A.},
  title = {A domain-language approach to designing dynamic enterprise component-based
	architectures to support business services},
  booktitle = {Technology of Object-Oriented Languages and Systems, 2001. TOOLS
	39. 39th International Conference and Exhibition on},
  year = {2001},
  pages = {130 -141},
  abstract = {Presents solutions to a major subset of problems facing component-based
	development and integration (CBDI). These solutions include patterns,
	techniques, design artifacts and activities across what we have identified
	as the five domains of CBDI, namely: organizational, methodological,
	architectural, technology implementation and infrastructure. We present
	a taxonomy of CBDI domains that transcends technology and tools to
	cover a wider spectrum of business and methodology concerns across
	an enterprise. Representative examples from the methodological and
	architectural domains are given. Domain-specific languages are combined
	with the object paradigm to yield grammar-oriented object design
	(GOOD). GOOD helps identify and map reusable subsystems in a business
	model to a well-mannered component-first software architecture. We
	then demonstrate how these manners should be added as first-class
	constructs to the component-based paradigm of software engineering},
  doi = {10.1109/TOOLS.2001.941667},
  keywords = {adaptive object models;architectural domain;business services;component-based
	architecture design;component-based development;component-based integration;component-based
	software engineering paradigm;component-first software architecture;design
	artifacts;design patterns;domain-specific languages;dynamic enterprise;first-class
	constructs;grammar-oriented object design;infrastructure;methodological
	domain;object paradigm;organizational domain;reusable subsystems;taxonomy;technology
	implementation;business data processing;grammars;integrated software;object-oriented
	methods;software architecture;specification languages;subroutines;}
}

@INPROCEEDINGS{4026978,
  author = {Arsanjani, A. and Ramanathan, S.},
  title = {Beyond SOA: Context-Aware Composite Services},
  booktitle = {Services Computing, 2006. SCC '06. IEEE International Conference
	on},
  year = {2006},
  pages = {514},
  month = {sept.},
  abstract = {Context aware services are the next generation of the service computing
	paradigm. In this paper we explore a case study of creating context-aware
	services for the telecommunications industry},
  doi = {10.1109/SCC.2006.26},
  keywords = {SOA;context-aware composite services;service computing;mobile computing;telecommunication
	services;}
}

@INPROCEEDINGS{4221085,
  author = {Ashraf, F. and Alhajj, R.},
  title = {ClusTex: Information Extraction from HTML Pages},
  booktitle = {Advanced Information Networking and Applications Workshops, 2007,
	AINAW '07. 21st International Conference on},
  year = {2007},
  volume = {1},
  pages = {355 -360},
  month = {may},
  abstract = {This paper propose ClusTex, a system which employs clustering techniques
	for automatic information extraction from HTML documents containing
	semi- structured data. Using domain-specific information provided
	by the user, ClusTex parses and tokenizes the data from an HTML document,
	partitions it into clusters containing similar elements, and estimates
	an extraction rule based on the pattern of occurrence of data tokens.
	The extraction rule is then used to refine clusters, and finally
	the output is reported. To demonstrate the effectiveness of this
	approach, the proposed approach is tested by conducting experiments
	on the University of Calgary Web-site; the results prove comparable
	to those reported in the literature.},
  doi = {10.1109/AINAW.2007.119},
  keywords = {ClusTex;HTML Web document;Web page;automatic information extraction;data
	parsing;data token;domain-specific information;semi structured data;Internet;grammars;hypermedia
	markup languages;information retrieval;pattern clustering;text analysis;}
}

@INPROCEEDINGS{1232441,
  author = {Aslam, J. and Bratus, S. and Kotz, D. and Peterson, R. and Rus, D.
	and Tofel, B.},
  title = {The Kerf toolkit for intrusion analysis},
  booktitle = {Information Assurance Workshop, 2003. IEEE Systems, Man and Cybernetics
	Society},
  year = {2003},
  pages = { 301 - 303},
  month = {june},
  abstract = { We consider the problem of intrusion analysis and present the Kerf
	toolkit, whose purpose is to provide an efficient and flexible infrastructure
	for the analysis of attacks. The Kerf toolkit includes a mechanism
	for securely recording host and network logging information for a
	network of workstations, a domain-specific language for querying
	this stored data, and an interface for viewing the results of such
	a query, providing feedback on these results, and generating new
	queries in an iterative fashion. We describe the architecture of
	Kerf in detail, present examples to demonstrate the power of our
	query language, and discuss the performance of our implementation
	of this system.},
  doi = {10.1109/SMCSIA.2003.1232441},
  issn = { },
  keywords = { Kerf toolkit; domain-specific query language; graphical interface;
	host information; information recording; intrusion analysis; network
	logging information; query processing; computer crime; data visualisation;
	query languages; query processing; system monitoring; workstation
	clusters;}
}

@INPROCEEDINGS{4312939,
  author = {Assali, A.A. and Lenne, D. and Debray, B.},
  title = {KoMIS: An Ontology-Based Knowledge Management System for Industrial
	Safety},
  booktitle = {Database and Expert Systems Applications, 2007. DEXA '07. 18th International
	Workshop on},
  year = {2007},
  pages = {475 -479},
  month = {sept.},
  abstract = {This paper presents an ontology-based knowledge management system
	for indexing and retrieving information about a domain-specific corpus
	of resources in an industrial enterprise. From our ongoing project
	KoMIS (knowledge management for industrial safety), we introduce
	in this paper our approach to index the internal resources of an
	enterprise using two ontologies; a domain ontology that describes
	the terms of the studied domain (the industrial safety domain), and
	an application ontology that describes the various types of indexed
	resources. In our indexing approach, the resulting index is an RDF
	file that can be interrogated using one of the RDF query languages.
	An ontology-based information retrieval system is built as a Web
	application to help and guide users in their research for resources.
	Thanks to our indexing and retrieval model, the returned Web page
	of results is built dynamically so that each potential modification
	of the application ontology will be taken into account without affecting
	the internal structure of the information retrieval system.},
  doi = {10.1109/DEXA.2007.34},
  issn = {1529-4188},
  keywords = {Web page;domain-specific corpus;industrial safety domain;knowledge
	management for industrial safety;ontology-based information retrieval
	system;information retrieval system evaluation;knowledge management;ontologies
	(artificial intelligence);safety;}
}

@ARTICLE{798323,
  author = {Atkins, D.L. and Ball, T. and Bruns, G. and Cox, K.},
  title = {Mawl: a domain-specific language for form-based services},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {334 -346},
  number = {3},
  month = {may/jun},
  abstract = {A form-based service is one in which the flow of data between service
	and user is described by a sequence of query/response interactions,
	or forms. Mawl is a domain-specific language for programming form-based
	services in a device-independent manner. We focus on Mawl's form
	abstraction, which is the means for separating service logic from
	user interface description, and show how this simple abstraction
	addresses seven issues in service creation, analysis, and maintenance:
	compile-time guarantees, implementation flexibility, rapid prototyping,
	testing and validation, support for multiple devices, composition
	of services, and usage analysis},
  doi = {10.1109/32.798323},
  issn = {0098-5589},
  keywords = {Mawl;compile-time guarantees;data flow;device-independent programming;domain-specific
	language;form abstraction;form-based services;implementation flexibility;multiple
	device support;query/response interaction sequence;rapid prototyping;service
	analysis;service creation;service logic;service maintenance;testing;usage
	analysis;user interface description;validation;distributed programming;high
	level languages;information resources;program testing;program verification;software
	maintenance;software prototyping;}
}

@INPROCEEDINGS{5630252,
  author = {Atkinson, C. and Draheim, D. and Geist, V.},
  title = {Typed Business Process Specification},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2010 14th
	IEEE International},
  year = {2010},
  pages = {69 -78},
  month = {oct.},
  abstract = {In this paper we propose a typed approach to business process specification
	based on typed workflow charts. These can be exploited as a domain-specific
	programming language and facilitate tight integration between workflow
	definition and system dialogue programming. The approach also supports
	the integration of business process modeling and business process
	automation. We discuss two ways of exploiting this potential for
	integration, one is the design of an integrated business process
	management suite and the other is a software artifact tracker based
	on a view-based, multi-dimensional software modeling tool.},
  doi = {10.1109/EDOC.2010.19},
  issn = {1541-7719},
  keywords = {business process automation;business process modeling;domain-specific
	programming language;multi-dimensional software modeling tool;system
	dialogue programming;typed business process specification;typed workflow
	charts;object-oriented programming;workflow management software;}
}

@ARTICLE{4907005,
  author = {Atkinson, C. and Gutheil, M. and Kennel, B.},
  title = {A Flexible Infrastructure for Multilevel Language Engineering},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2009},
  volume = {35},
  pages = {742 -755},
  number = {6},
  month = {nov.-dec. },
  abstract = {Although domain-specific modeling tools have come a long way since
	the modern era of model-driven development started in the early 1990s
	and now offer an impressive range of features, there is still significant
	room for enhancing the flexibility they offer to end users and for
	combining the advantages of domain-specific and general-purpose languages.
	To do this, however, it is necessary to enhance the way in which
	the current generation of tools view metamodeling and support the
	representation of the multiple, ?ontological? classification levels
	that often exist in subject domains. State-of-the-art tools essentially
	allow users to describe the abstract and concrete syntaxes of a language
	in the form of metamodels and to make statements in that language
	in the form of models. These statements typically convey information
	in terms of types and instances in the domain (e.g., the classes
	and objects of UML), but not in terms of types of types (i.e., domain
	metaclasses), and types of types of types, and so on, across multiple
	classification levels. In essence, therefore, while they provide
	rich support for ?linguistic? metamodeling, the current generation
	of tools provides little if any built-in support for modeling ?ontological?
	classification across more than one type/instance level in the subject
	domain. In this paper, we describe a prototype implementation of
	a new kind of modeling infrastructure that, by providing built-in
	support for multiple ontological as well as linguistic classification
	levels, offers various advantages over existing language engineering
	approaches and tools. These include the ability to view a single
	model from the perspective of both a general-purpose and a domain-specific
	modeling language, the ability to define constraints across multiple
	ontological classification levels, and the ability to tie the rendering
	of model elements to ontological as well as linguistic types over
	multiple classification levels. After first outlining the key conce-
	- ptual ingredients of this new infrastructure and presenting the
	main elements of our current realization, we show these benefits
	through two small examples.},
  doi = {10.1109/TSE.2009.31},
  issn = {0098-5589},
  keywords = {UML;domain-specific modeling language;linguistic metamodeling;model-driven
	development;multilevel language engineering;ontological classification
	levels;Unified Modeling Language;ontologies (artificial intelligence);software
	tools;}
}

@ARTICLE{1249336,
  author = {Attardi, G. and Cisternino, A.},
  title = {Multistage programming support in CLI},
  journal = {Software, IEE Proceedings -},
  year = {2003},
  volume = {150},
  pages = { 275 - 281},
  number = {5},
  month = {oct.},
  abstract = { Execution environments such as CLR and JVM provide many features
	needed by multi-stage programming languages, though there is no explicit
	support for them. Besides, staged computations are widely used in
	areas such as Web programming and generative programming. In the
	paper the authors present a possible CLR extension (which can also
	be ported to JVM) to provide support for multi-stage languages. The
	extension is based on CodeBricks - a framework for run-time code
	generation which allows expressing homogenous transformations of
	intermediate language as a composition of methods. They discuss the
	code generation strategy adopted by the framework and how an extension
	to CLR may improve the performance of multi-stage applications, although
	CodeBricks can also be implemented using the standard CLR. An informal
	discussion of how to translate MetaML staging annotations into CodeBricks
	is provided with a simple example.},
  doi = {10.1049/ip-sen:20030990},
  issn = {1462-5970},
  keywords = { CLI; CLR extension; CodeBricks; JVM; MetaML staging annotations;
	Web programming; generative programming; homogenous transformations;
	intermediate language; method composition; multistage programming
	support; performance; programming languages; run-time code generation;
	Java; program compilers; programming environments; software performance
	evaluation;}
}

@INPROCEEDINGS{6062708,
  author = {Attig, Michael and Brebner, Gordon},
  title = {400 Gb/s Programmable Packet Parsing on a Single FPGA},
  booktitle = {Architectures for Networking and Communications Systems (ANCS), 2011
	Seventh ACM/IEEE Symposium on},
  year = {2011},
  pages = {12 -23},
  month = {oct.},
  abstract = {Packet parsing is necessary at all points in the modern networking
	infrastructure, to support packet classification and security functions,
	as well as for protocol implementation. Increasingly high line rates
	call for advanced hardware packet processing solutions, while increasing
	rates of change call for high-level programmability of these solutions.
	This paper presents an approach for harnessing modern Field Programmable
	Gate Array (FPGA) devices, which are a natural technology for implementing
	the necessary high-speed programmable packet processing. The paper
	introduces PP: a simple high-level language for describing packet
	parsing algorithms in an implementation-independent manner. It demonstrates
	that this language can be compiled to give high-speed FPGA-based
	packet parsers that can be integrated alongside other packet processing
	components to build network nodes. Compilation involves generating
	virtual processing architectures tailored to specific packet parsing
	requirements. Scalability of these architectures allows parsing at
	line rates from 1 to 400 Gb/s as required in different network contexts.
	Run-time programmability of these architectures allows dynamic updating
	of parsing algorithms during operation in the field. Implementation
	results show that programmable packet parsing of 600 million small
	packets per second can be supported on a single Xilinx Virtex-7 FPGA
	device handling a 400 Gb/s line rate.},
  doi = {10.1109/ANCS.2011.12}
}

@INPROCEEDINGS{319146,
  author = {Auguin, M. and Boeri, F. and Carriere, C. and Menez, G.},
  title = {Incremental synthesis of application domain specific processors},
  booktitle = {Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993
	IEEE International Conference on},
  year = {1993},
  volume = {1},
  pages = {425 -428 vol.1},
  month = {april},
  abstract = {Synthesis of application domain specific processors may be achieved
	either by considering the set of applications as a macro-application
	or by performing an iterative synthesis process on the ordered list
	of applications. In the first approach, the whole set of basic blocks
	of all the applications are ordered according to the internal priority
	of the synthesis tool. The second one gives to the first applications
	the highest priority and forces adapted constructions in the final
	architecture. The authors deal with these approaches through the
	use of the synthesis aided tool called CAPSYS. In CAPSYS, applications
	are represented with an Ada-like language which includes abstract
	types, multidimensional dynamic arrays, vector processing, and recursivity.
	Associated with an application program, CAPSYS requires a library
	of functional units. A particular feature of CAPSYS is its ability
	to take into account a predefined architecture.<>},
  doi = {10.1109/ICASSP.1993.319146},
  issn = {1520-6149},
  keywords = {CAPSYS;abstract types;adapted constructions;application domain specific
	processors;architecture;incremental synthesis;iterative synthesis;library
	of functional units;multidimensional dynamic arrays;recursivity;vector
	processing;abstract data types;application specific integrated circuits;digital
	signal processing chips;iterative methods;multidimensional systems;vector
	processor systems;}
}

@INPROCEEDINGS{5558637,
  author = {Axelsson, E. and Claessen, K. and Dvai, G. and Horvth,
	Z. and Keijzer, K. and Lyckegd, B. and Persson, A. and Sheeran,
	M. and Svenningsson, J. and Vajdax, A.},
  title = {Feldspar: A domain specific language for digital signal processing
	algorithms},
  booktitle = {Formal Methods and Models for Codesign (MEMOCODE), 2010 8th IEEE/ACM
	International Conference on},
  year = {2010},
  pages = {169 -178},
  month = {july},
  abstract = {A new language, Feldspar, is presented, enabling high-level and platform-independent
	description of digital signal processing (DSP) algorithms. Feldspar
	is a pure functional language embedded in Haskell. It offers a high-level
	dataflow style of programming, as well as a more mathematical style
	based on vector indices. The key to generating efficient code from
	such descriptions is a high-level optimization technique called vector
	fusion. Feldspar is based on a low-level, functional core language
	which has a relatively small semantic gap to machine-oriented languages
	like C. The core language serves as the interface to the back-end
	code generator, which produces C. For very small examples, the generated
	code performs comparably to hand-written C code when run on a DSP
	target. While initial results are promising, to achieve good performance
	on larger examples, issues related to memory access patterns and
	array copying will have to be addressed.},
  doi = {10.1109/MEMCOD.2010.5558637},
  keywords = {Feldspar;digital signal processing;domain specific language;functional
	language;high-level dataflow;high-level optimization;machine-oriented
	languages;vector fusion;data flow analysis;functional languages;machine
	oriented languages;optimisation;signal processing;}
}

@ARTICLE{5685266,
  author = {Aydt, H. and Turner, S.J. and Wentong Cai and Low, M.Y.H. and Yew-Soon
	Ong and Ayani, R.},
  title = {Toward an Evolutionary Computing Modeling Language},
  journal = {Evolutionary Computation, IEEE Transactions on},
  year = {2011},
  volume = {15},
  pages = {230 -247},
  number = {2},
  month = {april },
  abstract = {The importance of domain knowledge in the design of effective evolutionary
	algorithms (EAs) is widely acknowledged in the meta-heuristics community.
	In the last few decades, a plethora of EAs has been manually designed
	by domain experts for solving domain-specific problems. Specialization
	has been achieved mainly by embedding available domain knowledge
	into the algorithms. Although programming libraries have been made
	available to construct EAs, a unifying framework for designing specialized
	EAs across different problem domains and branches of evolutionary
	computing does not exist yet. In this paper, we address this issue
	by introducing an evolutionary computing modeling language (ECML)
	which is based on the unified modeling language (UML). ECML incorporates
	basic UML elements and introduces new extensions that are specially
	needed for the evolutionary computation domain. Subsequently, the
	concept of meta evolutionary algorithms (MEAs) is introduced as a
	family of EAs that is capable of interpreting ECML. MEAs are solvers
	that are not restricted to a particular problem domain or branch
	of evolutionary computing through the use of ECML. By separating
	problem-specific domain knowledge from the EA implementation, we
	show that a unified framework for evolutionary computation can be
	attained. We demonstrate our approach by applying it to a number
	of examples.},
  doi = {10.1109/TEVC.2010.2081368},
  issn = {1089-778X},
  keywords = {domain specific problem;evolutionary computing modeling language;meta
	evolutionary algorithm;meta heuristics community;problem specific
	domain knowledge;programming library;unified modeling language;Unified
	Modeling Language;evolutionary computation;simulation languages;}
}

@ARTICLE{4061128,
  author = {Danny Ayers},
  title = {From Here to There},
  journal = {Internet Computing, IEEE},
  year = {2007},
  volume = {11},
  pages = {85 -89},
  number = {1},
  month = {jan.-feb. },
  abstract = {In a previous paper, the author described where he thought the Web
	would be headed over the next few years. He speculated that the trend
	seemed to be toward the semantic Web, although maybe not via the
	shortest path of directly deploying semantic Web technologies such
	as RDF and OWL. For this paper, he presents some concrete examples
	of technologies that support this prognosis. One key idea of the
	semantic Web is the Web of data, in which richly interconnected data
	collections appear alongside (and integrated with) the collections
	of hypertext documents. However, the Web supports linking, and with
	the various data languages available (often XML based), a Web of
	data without semantic Web technologies is entirely conceivable. In
	a sense, we already have such a thing, although the data are usually
	binary files such as images and audio files, which seriously limits
	linking potential. Without the ability to join pieces of information
	and work more on the level of knowledge representation, this naive
	Web of data offers little promise in itself. There is a possible
	shift under way, however, from the Web as (mostly) a document repository
	with generally limited granularity of addressability, to the Web
	as a generic, moderately interlinked data store (which includes documents
	as a subset of data types)},
  doi = {10.1109/MIC.2007.8},
  issn = {1089-7801},
  keywords = {data Web;moderately interlinked data store;semantic Web;semantic Web;}
}

@INPROCEEDINGS{5946149,
  author = {Ayral, H. and Yavuz, S.},
  title = {An automated domain specific stop word generation method for natural
	language text classification},
  booktitle = {Innovations in Intelligent Systems and Applications (INISTA), 2011
	International Symposium on},
  year = {2011},
  pages = {500 -503},
  month = {june},
  abstract = {In this paper we propose an automated method for generating domain
	specific stop words to improve classification of natural language
	content. Also we implemented a bayesian natural language classifier
	working on web pages, which is based on maximum a posteriori probability
	estimation of keyword distributions using bag-of-words model to test
	the generated stop words. We investigated the distribution of stop-word
	lists generated by our model and compared their contents against
	a generic stop-word list for English language. We also show that
	the document coverage rank and topic coverage rank of words belonging
	to natural language corpora follow Zipf's law, just like the word
	frequency rank is known to follow.},
  doi = {10.1109/INISTA.2011.5946149},
  keywords = {Bayesian natural language classifier;English language;Zipf law;automated
	domain specific stop word generation method;bag-of-words model;document
	coverage rank;keyword distributions;maximum a posteriori probability
	estimation;natural language text classification;topic coverage rank;Bayes
	methods;maximum likelihood estimation;natural language processing;pattern
	classification;text analysis;}
}

@INPROCEEDINGS{4444191,
  author = {Azarian, A. and Siadat, A. and Bauchat, J.-L.},
  title = {Domain-specific Information Retrieval system with a correspondence
	graph},
  booktitle = {Digital Information Management, 2007. ICDIM '07. 2nd International
	Conference on},
  year = {2007},
  volume = {1},
  pages = {1 -6},
  month = {oct.},
  abstract = {This paper describes different existing solutions and proposes a new
	approach for information retrieval with request specified in natural
	language within a specific domain and in a multilingual context.
	The experimental platform employed was the SIDIS- Enterprise car-diagnosis
	System of Siemens AG (Germany). The paper proposes a new methodology
	to retrieve car failures symptoms and is based on a correspondence
	graph. This methodology is more based on perception than on similarity
	computation between request and symptoms. A comparison study between
	this approach and usual retrieval methods (e.g. term frequency based)
	provide promising results.},
  doi = {10.1109/ICDIM.2007.4444191},
  keywords = {SIDIS- Enterprise car-diagnosis System;Siemens AG;car failures symptoms;correspondence
	graph;domain-specific information retrieval system;multilingual context;natural
	language;automobile manufacture;failure analysis;information retrieval
	systems;natural language processing;production engineering computing;}
}

@INPROCEEDINGS{5298731,
  author = {Azevedo, S. and Machado, R.J. and Muthig, D.},
  title = {Multistage Model Transformations in Software Product Lines},
  booktitle = {Software Engineering Advances, 2009. ICSEA '09. Fourth International
	Conference on},
  year = {2009},
  pages = {565 -569},
  month = {sept.},
  abstract = {Raising the level of abstraction for software engineers to write applications
	is still an undergoing issue. So, models will most likely become
	the dominant artifact in the development of software. However, models
	are nothing without the framing of a methodology, like the software
	factories methodology, which includes the software product lines
	approach. In the context of software product lines, model-driven
	development imposes the structuring of the software development process
	around models adequate to each one of the moments within the software
	supply chain. The different moments are the different stages that
	comprise different development teams, as well as the target user
	of the different software family members. This multistage process
	is a powerful vision of software development in general when compared
	with the current software development processes' state-of-the-art
	and this is the vision that feeds the Ph.D. work presented in this
	paper. This work is concerned with the transformations that models
	must suffer in the particular context of software product lines development,
	inside each stage and in between stages.},
  doi = {10.1109/ICSEA.2009.89},
  keywords = {model-driven development imposes;multistage model transformations;software
	development process;software product lines;software supply chain;software
	engineering;specification languages;}
}

@ARTICLE{4359173,
  author = {Azimi-Sadjadi, M.R. and Salazar, J. and Srinivasan, S. and Sheedvash,
	S.},
  title = {An Adaptable Connectionist Text-Retrieval System With Relevance Feedback},
  journal = {Neural Networks, IEEE Transactions on},
  year = {2007},
  volume = {18},
  pages = {1597 -1613},
  number = {6},
  month = {nov. },
  abstract = {This paper introduces a new connectionist network for certain domain-specific
	text-retrieval and search applications with expert end users. A new
	model reference adaptive system is proposed that involves three learning
	phases. Initial model-reference learning is first performed based
	upon an ensemble set of input-output of an initial reference model.
	Model-reference following is needed in dynamic environments where
	documents are added, deleted, or updated. Relevance feedback learning
	from multiple expert users then optimally maps the original query
	using either a score-based or a click-through selection process.
	The learning can be implemented, in regression or classification
	modes, using a three-layer network. The first layer is an adaptable
	layer that performs mapping from query domain to document space.
	The second and third layers perform document-to-term mapping, search/retrieval,
	and scoring tasks. The learning algorithms are thoroughly tested
	on a domain-specific text database that encompasses a wide range
	of Hewlett Packard (HP) products and for a large number of most commonly
	used single- and multiterm queries.},
  doi = {10.1109/TNN.2007.895912},
  issn = {1045-9227},
  keywords = {Hewlett Packard product;adaptable connectionist text-retrieval system;classification
	mode;click-through selection process;document handling;document-to-term
	mapping;domain-specific text database;model reference adaptive system;model-reference
	following;model-reference learning;multiterm query;regression mode;relevance
	feedback learning;score-based selection process;three-layer neural
	network;learning (artificial intelligence);neural nets;query formulation;relevance
	feedback;text analysis;Abstracting and Indexing as Topic;Algorithms;Artificial
	Intelligence;Automatic Data Processing;Database Management Systems;Expert
	Systems;Feedback;Fuzzy Logic;Information Systems;Logical Observation
	Identifiers Names and Codes;Neural Networks (Computer);Pattern Recognition,
	Automated;Programming Languages;Software;User-Computer Interface;}
}

@INPROCEEDINGS{5587810,
  author = {Azmi, A. and Bin Badia, N.},
  title = {iTree - Automating the construction of the narration tree of Hadiths
	(Prophetic Traditions)},
  booktitle = {Natural Language Processing and Knowledge Engineering (NLP-KE), 2010
	International Conference on},
  year = {2010},
  pages = {1 -7},
  month = {aug.},
  abstract = {The two fundamental sources of Islamic legislation are Qur'an and
	the Hadith. The Hadiths, or Prophetic Traditions, are narrations
	originating from the sayings and conducts of Prophet Muhammad. Each
	Hadith starts with a list of narrators involved in transmitting it
	followed by the transmitted text. The Hadith corpus is extremely
	huge and runs into hundreds of volumes. Due to its legislative importance,
	Hadiths have been carefully scrutinized by hadith scholars. One way
	a scholar may grade a Hadith is by its narration chain and the individual
	narrators in the chain. In this paper we report on a system that
	automatically generates the transmission chains of a Hadith and graphically
	display it. Computationally, this is a challenging problem. The text
	of Hadith is in Arabic, a morphologically rich language; and each
	Hadith has its own peculiar way of listing narrators. Our solution
	involves parsing and annotating the Hadith text and identifying the
	narrators' names. We use shallow parsing along with a domain specific
	grammar to parse the Hadith content. Experiments on sample Hadiths
	show our approach to have a very good success rate.},
  doi = {10.1109/NLPKE.2010.5587810},
  keywords = {Arabic language;Hadith corpus;Islamic legislation;domain specific
	grammar;iTree;narration tree;parsing;grammars;natural language processing;text
	analysis;trees (mathematics);}
}

@INPROCEEDINGS{4148882,
  author = {Baer, Philipp A. and Reichle, Roland and Zapf, Michael and Weise,
	Thomas and Geihs, Kurt},
  title = {A Generative Approach to the Development of Autonomous Robot Software},
  booktitle = {Engineering of Autonomic and Autonomous Systems, 2007. EASe '07.
	Fourth IEEE International Workshop on},
  year = {2007},
  pages = {43 -52},
  month = {march},
  abstract = {The integration of new or existing software components into established
	architectures and the ability to deal with heterogeneity are key
	requirements for middleware and development frameworks for robotic
	systems. This paper presents SPICA, a software development framework
	for communication infrastructures of autonomous mobile robots. Utilizing
	the model-driven software development paradigm, communication and
	data flow can be defined on an abstract level. For this purpose,
	domain-specific languages and tools are provided that allow specification
	and generation of module communication infrastructures for communication
	between modules along with primitives for data management. The high-level
	platform-independent specifications are automatically transformed
	into low-level platform and programming language-specific source
	code. We illustrate the applicability of our approach with an elaborate
	example describing the design of a soccer robot architecture that
	has proven its strength during RoboCup 2006. Our experiences have
	revealed that SPICA is advantageous for prototyping as well as for
	building high performance systems},
  doi = {10.1109/EASE.2007.2},
  keywords = {autonomous mobile robots;autonomous robot software;communication infrastructure;development
	frameworks;domain-specific language;high-level platform-independent
	specification;model-driven software development;software development
	framework;formal specification;mobile robots;robot programming;software
	architecture;}
}

@INPROCEEDINGS{1238032,
  author = {Bagge, O.S. and Kalleberg, K.T. and Haveraaen, M. and Visser, E.},
  title = {Design of the CodeBoost transformation system for domain-specific
	optimisation of C++ programs},
  booktitle = {Source Code Analysis and Manipulation, 2003. Proceedings. Third IEEE
	International Workshop on},
  year = {2003},
  pages = { 65 - 74},
  month = {sept.},
  abstract = { The use of a high-level, abstract coding style can greatly increase
	developer productivity. For numerical software, this can result in
	drastically reduced run-time performance. High-level, domain-specific
	optimisations can eliminate much of the overhead caused by an abstract
	coding style, but current compilers have poor support for domain-specific
	optimisation. We present CodeBoost, a source-to-source transformation
	tool for domain-specific optimisation of C++ programs. CodeBoost
	performs parsing, semantic analysis and pretty-printing, and transformations
	can be implemented either in the Stratego program transformation
	language, or as user-defined rewrite rules embedded within the C++
	program. CodeBoost has been used with great success to optimise numerical
	applications written in the Sophus high-level coding style. We discuss
	the overall design of the CodeBoost transformation framework, and
	take a closer look at two important features of CodeBoost: user-defined
	rules and totem annotations. We also show briefly how CodeBoost is
	used to optimise Sophus code, resulting in applications that run
	twice as fast, or more.},
  doi = {10.1109/SCAM.2003.1238032},
  issn = { },
  keywords = { C++ program optimisation; CodeBoost transformation framework; Sophus
	code; Stratego program transformation language; abstract coding style;
	domain-specific optimization; parsing; program compiler; semantic
	analysis; source-to-source transformation tool; totem annotation;
	user-defined rewrite rules; C++ language; optimising compilers; software
	tools;}
}

@INPROCEEDINGS{5494940,
  author = {Shuanhu Bai and Chien-Lin Huang and Bin Ma and Haizhou Li},
  title = {Semi-supervised learning of language model using unsupervised topic
	model},
  booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International
	Conference on},
  year = {2010},
  pages = {5386 -5389},
  month = {march},
  abstract = {We present a semi-supervised learning (SSL) method for building domain-specific
	language models (LMs) from general-domain data using probabilistic
	latent semantic analysis (PLSA). The proposed technique first performs
	topic decomposition (TD) on the combined dataset of domain-specific
	and general-domain data. Then it derives latent topic distribution
	of the interested domain, and derives domain-specific word n-gram
	counts with a PLSA style mixture model. Finally, it uses traditional
	n-gram modeling to construct domain-specific LMs from the domain-specific
	word n-gram counts. Experimental results show that this technique
	outperforms both states-of-the-art relative entropy text selection
	and traditional supervised training methods.},
  doi = {10.1109/ICASSP.2010.5494940},
  issn = {1520-6149},
  keywords = {PLSA style mixture model;domain-specific language models;domain-specific
	word n-gram counts;language model learning;probabilistic latent semantic
	analysis;relative entropy text selection;semi-supervised learning;topic
	decomposition;unsupervised topic model;learning (artificial intelligence);natural
	language processing;statistical analysis;}
}

@INPROCEEDINGS{5380748,
  author = {Shuanhu Bai and Min Zhang and Haizhou Li},
  title = {Semi-supervised Learning of Domain-Specific Language Models from
	General Domain Data},
  booktitle = {Asian Language Processing, 2009. IALP '09. International Conference
	on},
  year = {2009},
  pages = {273 -279},
  month = {dec.},
  abstract = {We present a semi-supervised learning method for building domain-specific
	language models (LM) from general-domain data. This method is aimed
	to use small amount of domain-specific data as seeds to tap domain-specific
	resources residing in larger amount of general-domain data with the
	help of topic modeling technologies. The proposed algorithm first
	performs topic decomposition (TD) on the combined dataset of domain-specific
	and general-domain data using probabilistic latent semantic analysis
	(PLSA). Then it derives domain-specific word n-gram counts with mixture
	modeling scheme of PLSA. Finally, it uses traditional n-gram modeling
	approach to construct domain-specific LMs from the domain-specific
	word n-gram counts. Experimental results show that this approach
	can outperform both stat-of-the-art methods and the simulated supervised
	learning method with our data sets. In particular, the semi-supervised
	learning method can achieve better performance even with very small
	amount of domain-specific data.},
  doi = {10.1109/IALP.2009.65},
  keywords = {domain-specific language models;domain-specific word n-gram counts;general
	domain data;natural language processing;probabilistic latent semantic
	analysis;semisupervised learning;simulated supervised learning method;topic
	decomposition;traditional n-gram modeling approach;learning (artificial
	intelligence);natural language processing;}
}

@INPROCEEDINGS{808705,
  author = {Bailey-Kellogg, C. and Zhao, F.},
  title = {The SAL interpreter for large-scale optimization in distributed control
	systems},
  booktitle = {Computer Aided Control System Design, 1999. Proceedings of the 1999
	IEEE International Symposium on},
  year = {1999},
  pages = {539 -544},
  abstract = {Many sensor-rich control systems interact with spatially distributed
	physical environments. This paper describes the spatial aggregation
	language (SAL) interpreter, a programming environment to support
	data interpretation and control tasks for distributed physical systems.
	SAL provides a set of powerful, abstract components to represent
	and exploit spatial structures in distributed physical data at multiple
	levels of abstraction. The programming environment supports rapid
	prototyping of application programs and interactive manipulation
	of spatial structures. In comparison with existing tools, the SAL
	environment is centered on geometric and topological representations
	that encode and utilize domain-specific knowledge, such as metrics,
	adjacency relations, and equivalence predicates. We illustrate the
	use of SAL in decentralized control design for thermal regulation},
  doi = {10.1109/CACSD.1999.808705},
  keywords = {SAL interpreter;decentralized control;distributed control systems;equivalence
	predicates;optimization;programming environment;rapid prototyping;spatial
	aggregation language;thermal regulation;control system analysis computing;decentralised
	control;distributed control;optimisation;program interpreters;programming
	environments;software prototyping;}
}

@INPROCEEDINGS{5071058,
  author = {Baillargeon, Robert and Rumpe, Bernhard and Volkel, Steven and France,
	Robert and Georg, Geri and Zschaler, Steffen},
  title = {Modeling in Software Engineering (MiSE 09)},
  booktitle = {Software Engineering - Companion Volume, 2009. ICSE-Companion 2009.
	31st International Conference on},
  year = {2009},
  pages = {453 -454},
  month = {may},
  abstract = {The Modeling in Software Engineering (MiSE) workshop series provides
	a forum for discussing the challenges associated with modeling software
	and with incorporating modeling practices into the software development
	process. The main goal is to further promote cross-fertilization
	between the modeling communities (e.g., MODELS) and software-engineering
	communities.},
  doi = {10.1109/ICSE-COMPANION.2009.5071058}
}

@INPROCEEDINGS{5747539,
  author = {Bajaj, M. and Zwemer, D. and Peak, R. and Phung, A. and Scott, A.G.
	and Wilson, M.},
  title = {SLIM: collaborative model-based systems engineering workspace for
	next-generation complex systems},
  booktitle = {Aerospace Conference, 2011 IEEE},
  year = {2011},
  pages = {1 -15},
  month = {march},
  abstract = {Development of complex systems is a collaborative effort spanning
	disciplines, teams, processes, software tools, and modeling formalisms.
	It is the vision of model-based systems engineering (MBSE) to enable
	a consistent, coherent, interoperable, and evolving model of a system
	throughout its lifecycle. However, no currently available modeling
	language can represent all aspects of a system (including system-of-systems)
	at all levels of abstraction across the lifecycle.},
  doi = {10.1109/AERO.2011.5747539},
  issn = {1095-323X},
  keywords = {SLIM;SysML based analysis tool;SysML based integration tool;change
	management;collaborative model based system engineering;commercial-off-the-shelf
	tools;conceptual architecture;discipline specific model;domain specific
	model;front-end conceptual abstraction;next generation complex system;plug-and-play;requirement
	verification;risk analysis;seamless interoperability;software tools;system
	development;system lifecycle management;system simulation;computer
	aided software engineering;formal verification;groupware;large-scale
	systems;open systems;software development management;systems engineering;}
}

@INPROCEEDINGS{4196522,
  author = {Bajwa, I.S. and Siddique, I. and Choudhary, M.A.},
  title = {Automatic Domain Specific Terminology Extraction using a Decision
	Support System},
  booktitle = {Information Communications Technology, 2006. ICICT '06. ITI 4th International
	Conference on},
  year = {2006},
  pages = {1 -2},
  month = {dec.},
  abstract = {Speech languages or natural languages contents are major tools of
	communication. This research paper presents a natural language processing
	based automated system for understanding speech language text. A
	new rule based model is presented for analyzing the natural languages
	and extracting the relative meanings from the given text. User writes
	the natural language scenario in simple English in a few paragraphs
	and the designed system has an obvious capability of analyzing the
	given script by the user. After composite analysis and extraction
	of associated information, the designed system gives particular meanings
	to an assortment of speech language text on the basis of its context.
	The designed system uses standard speech language rules that are
	clearly defined for all speech languages as English, Urdu, Chinese,
	Arabic, French, ...etc. The designed system provides a quick and
	reliable way to comprehend speech language context and generates
	respective meanings. The application with such abilities can be more
	intelligent and pertinent specifically for the user to save the time.},
  doi = {10.1109/ITICT.2006.358298},
  keywords = {automatic domain specific terminology extraction;automatic text understanding;decision
	support system;natural language processing;speech language text;decision
	support systems;natural language processing;nomenclature;text analysis;}
}

@INPROCEEDINGS{5457806,
  author = {Bakera, M. and Wagner, C. and Margaria, T. and Vassev, E. and Hinchey,
	M. and Steffen, B.},
  title = {Extracting Component-Oriented Behaviour for Self-Healing Enabling},
  booktitle = {Engineering of Autonomic and Autonomous Systems (EASe), 2010 Seventh
	IEEE International Conference and Workshops on},
  year = {2010},
  pages = {152 -161},
  month = {march},
  abstract = {Rich and multifaceted domain specific specification languages like
	the Autonomic System Specification Language (ASSL) help to design
	reliable systems with self-healing capabilities. The GEAR game-based
	Model Checker has been used successfully to investigate in depth
	properties of the ESA ExoMars Rover. We show here how to enable GEAR's
	game-based verification techniques for ASSL via systematic model
	extraction from a behavioral subset of the language, and illustrate
	it on a description of the Voyager II space mission. This way, we
	close the gap between the design-time and the run-time techniques
	provided in the SHADOWS platform for self-healing of concurrency,
	performance, and functional issues.},
  doi = {10.1109/EASe.2010.23},
  keywords = {ASSL;GEAR game based model checker;SHADOWS platform;autonomic system
	specification language;component oriented behaviour extraction;gamebased
	verification;self healing enabling;computer games;formal verification;learning
	(artificial intelligence);object-oriented programming;software fault
	tolerance;specification languages;}
}

@INPROCEEDINGS{1598726,
  author = {Balakrishna, M. and Moldovan, D. and Cave, E.K.},
  title = {Automatic creation and tuning of context free grammars for interactive
	voice response systems},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2005. IEEE
	NLP-KE '05. Proceedings of 2005 IEEE International Conference on},
  year = {2005},
  pages = { 158 - 163},
  month = {oct.-1 nov.},
  abstract = { In this paper, we focus on the design of an AUTOCFGPROCESSOR procedure
	to automatically create and tune context free grammars (CFGs) for
	directed dialog speech applications without the use of any domain
	specific text corpora. A reranking mechanism is used to post-process
	the large vocabulary continuous speech recognizer (LVCSR) n-best
	lists with additional phonetic and higher level linguistic knowledge
	for transcribing the user utterances with improved word error rate
	(WER). We also depict the classification of LVCSR transcriptions
	into semantic categories and the use of a statistical filtering mechanism
	on the valid LVCSR-transcriptions for the CFG creation and tuning
	tasks. We also illustrate the importance of the additional improvements
	gained by using semantic classification strength in a feedback loop
	to the transcription mechanism.},
  doi = {10.1109/NLPKE.2005.1598726},
  keywords = { AUTOCFGPROCESSOR procedure; context free grammars; directed dialog
	speech applications; interactive voice response systems; large vocabulary
	continuous speech recognizer; linguistic knowledge; reranking mechanism;
	statistical filtering mechanism; user utterances; word error rate;
	context-free grammars; interactive systems; speech processing; speech
	recognition; speech-based user interfaces; statistical analysis;}
}

@INPROCEEDINGS{5959775,
  author = {Balasubramaniam, D. and de Silva, L. and Jefferson, C. and Kotthoff,
	L. and Miguel, I. and Nightingale, P.},
  title = {Dominion: An Architecture-Driven Approach to Generating Efficient
	Constraint Solvers},
  booktitle = {Software Architecture (WICSA), 2011 9th Working IEEE/IFIP Conference
	on},
  year = {2011},
  pages = {228 -231},
  month = {june},
  abstract = {Constraints are used to solve combinatorial problems in a variety
	of industrial and academic disciplines. However most constraint solvers
	are designed to be general and monolithic, leading to problems with
	efficiency, scalability and extensibility. We propose a novel, architecture-driven
	constraint solver generation framework called Dominion to tackle
	these issues. For any given problem, Dominion generates a lean and
	efficient solver tailored to that problem. In this paper, we outline
	the Dominion approach and its implications for software architecture
	specification of the solver.},
  doi = {10.1109/WICSA.2011.37},
  keywords = {Dominion approach;academic discipline;architecture driven approach;combinatorial
	problem;constraint solver;industrial discipline;software architecture
	specification;constraint handling;software architecture;}
}

@INPROCEEDINGS{5431716,
  author = {Balasubramanian, D. and Jackson, E.K.},
  title = {Lost in Translation: Forgetful Semantic Anchoring},
  booktitle = {Automated Software Engineering, 2009. ASE '09. 24th IEEE/ACM International
	Conference on},
  year = {2009},
  pages = {645 -649},
  month = {nov.},
  abstract = {Assigning behavioral semantics to domain-specific languages (DSLs)
	opens the door for the application of formal methods, yet is largely
	an unresolved problem. Previously proposed solutions include semantic
	anchoring, in which a transformation from the DSL to an external
	framework that can supply both behavioral semantics and apply formal
	methods is constructed. The drawback of this approach is that it
	loses the structural constraints of the original DSL along with the
	details of the transformation, which can lead to erroneous results
	when formal methods are applied. We demonstrate this problem of forgetful
	semantic anchoring using existing approaches through a translation
	from dataflow systems to interface automata. We then describe our
	modeling tool FORMULA and apply it to the same example, showing how
	forgetful semantic anchoring can be avoided.},
  doi = {10.1109/ASE.2009.83},
  issn = {1527-1366},
  keywords = {FORMULA modeling tool;behavioral semantic;dataflow systems;domain
	specific languages;forgetful semantic anchoring;formal methods;interface
	automata;structural constraints;automata theory;formal specification;specification
	languages;}
}

@INPROCEEDINGS{1388386,
  author = {Balasubramanian, K. and Balasubramanian, J. and Parsons, J. and Gokhale,
	A. and Schmidt, D.C.},
  title = {A platform-independent component modeling language for distributed
	real-time and embedded systems},
  booktitle = {Real Time and Embedded Technology and Applications Symposium, 2005.
	RTAS 2005. 11th IEEE},
  year = {2005},
  pages = { 190 - 199},
  month = {march},
  abstract = { This paper provides two contributions to the study of developing
	and applying domain-specific modeling languages (DSMLS) to distributed
	real-time and embedded (DRE) systems - particularly those systems
	using standards-based QoS-enabled component middleware. First, it
	describes the platform-independent component modeling language (PICML),
	which is a DSML that enables developers to define component interfaces,
	QoS parameters and software building rules, and also generates descriptor
	files that facilitate system deployment. Second, it applies PICML
	to an unmanned air vehicle (UAV) application portion of an emergency
	response system to show how PICML resolves key component-based DRE
	system development challenges. Our results show that the capabilities
	provided by PICML - combined with its design and deployment-time
	validation capabilities - eliminates many common errors associated
	with conventional techniques, thereby increasing the effectiveness
	of applying QoS-enabled component middleware technologies to the
	DRE system domain.},
  doi = {10.1109/RTAS.2005.4},
  issn = {1080-1812},
  keywords = { DSML; component interface; distributed real-time embedded system;
	domain-specific modeling language; emergency response system; platform-independent
	component modeling language; software building rule; standards-based
	QoS-enabled component middleware; unmanned air vehicle application;
	aircraft; distributed object management; embedded systems; middleware;
	object-oriented programming; quality of service; remotely operated
	vehicles; software reusability;}
}

@INPROCEEDINGS{4550786,
  author = {Balasubramanian, K. and Schmidt, D.C.},
  title = {Physical Assembly Mapper: A Model-Driven Optimization Tool for QoS-Enabled
	Component Middleware},
  booktitle = {Real-Time and Embedded Technology and Applications Symposium, 2008.
	RTAS '08. IEEE},
  year = {2008},
  pages = {123 -134},
  month = {april},
  abstract = {This paper provides four contributions to the study of optimization
	techniques for component-based distributed real-time and embedded
	(DRE) systems. First, we describe key challenges of designing component-based
	DRE systems and identify key sources of overhead in a typical component-based
	DRE system from the domain of shipboard computing. Second, we describe
	a class of optimization techniques applicable to the deployment of
	component-based DRE systems. Third, we describe the physical assembly
	mapper (PAM), which is a model-driven optimization tool that implements
	these techniques to reduce footprint. Fourth, we evaluate the benefits
	of these optimization techniques empirically and analyze the results.
	Our results indicate that the deployment-time optimization techniques
	in PAM provides significant benefits, such as 45% improvement in
	footprint, when compared to conventional component middleware technologies.},
  doi = {10.1109/RTAS.2008.36},
  issn = {1080-1812},
  keywords = {QoS;component middleware;distributed real-time and embedded systems;model-driven
	optimization tool;physical assembly mapper;middleware;optimisation;quality
	of service;}
}

@INPROCEEDINGS{4148923,
  author = {Balasubramanian, Krishnakumar and Schmidt, Douglas C. and Molnar,
	Zoltan and Ledeczi, Akos},
  title = {Component-Based System Integration via (Meta)Model Composition},
  booktitle = {Engineering of Computer-Based Systems, 2007. ECBS '07. 14th Annual
	IEEE International Conference and Workshops on the},
  year = {2007},
  pages = {93 -102},
  month = {march},
  abstract = {This paper provides three contributions to the study of functional
	integration of distributed enterprise systems. First, we describe
	the challenges associated with functionally integrating the software
	of these systems. Second, we describe how the composition of domain-specific
	modeling languages (DSMLs) can simplify the functional integration
	of enterprise distributed systems by enabling the combination of
	diverse middleware technologies. Third, we demonstrate how composing
	DSMLs can solve functional integration problems by reverse engineering
	an existing CORBA component model (CCM) system and exposing it as
	Web service(s) to Web clients who use these services. This paper
	shows that functional integration done using (meta)model composition
	provides significant benefits with respect to automation, reusability,
	and scalability compared to conventional integration processes and
	methods},
  doi = {10.1109/ECBS.2007.24},
  keywords = {CORBA component model system;Web clients;Web services;component-based
	system;distributed enterprise systems;domain-specific modeling languages;functional
	integration;metamodel composition;middleware;reverse engineering;Web
	services;distributed object management;middleware;object-oriented
	programming;}
}

@INPROCEEDINGS{512784,
  author = {Balzer, R.},
  title = {Process definition formalism maturity levels},
  booktitle = {Software Process Workshop, 1994. Proceedings., Ninth International},
  year = {1994},
  pages = {132 -133},
  month = {oct},
  abstract = {Beginning with the observations that process definition formalisms
	are not very different from general purpose programming languages
	and that almost every type of programming language has been used
	as the basis of one or more process definition formalisms, we conclude
	that the field of process definition formalisms is quite immature.
	We speculate on the future maturation of this field by proposing
	a set of maturity levels by which to measure that maturation},
  doi = {10.1109/ISPW.1994.512784},
  keywords = { ad-hoc; domain specific; general purpose programming languages; internalized
	abstractions; multi-lingual infrastructure; process definition formalism
	maturity levels; stable abstractions; software engineering;}
}

@INPROCEEDINGS{346049,
  author = {Bandinelli, S. and Fuggetta, A.},
  title = {Computational reflection in software process modeling: The SLANG
	approach},
  booktitle = {Software Engineering, 1993. Proceedings., 15th International Conference
	on},
  year = {1993},
  pages = {144 -154},
  month = {may},
  abstract = {SLANG is a domain-specific language for software process modeling
	and enactment. The authors present the basic features provided by
	SLANG to support the enactment and, in particular, dynamic evolution
	of a process model. Software production processes are subject to
	changes during their lifetime. Therefore, software process formalism
	must include mechanisms to support the analysis and dynamic modification
	of process models, even while they are being enacted. It is thus
	necessary for a process model to have the ability to reason about
	its own structure. Petri net based process languages have been criticized
	because of the lack of these reflective features and their inability
	to effectively support process evolution. The reflective features
	offered by SLANG are outlined, which is a process formalism based
	on a high-level Petri net notation. In particular, the mechanisms
	are discussed to create and modify different net fragments while
	the modeled process is being enacted},
  doi = {10.1109/ICSE.1993.346049},
  issn = {0270-5257},
  keywords = { Petri net based process languages; SLANG approach; computational
	reflection; domain-specific language; dynamic evolution; dynamic
	modification; high-level Petri net notation; process evolution; process
	model; reflective features; software process formalism; software
	process modeling; Petri nets; specification languages;}
}

@INPROCEEDINGS{4228218,
  author = {Bhupesh Bansal and Catalyurek, U. and Chame, J. and Chen, C. and
	Deelman, E. and Gil, Y. and Hall, M. and Vijay Kumar and Kurc, T.
	and Lerman, K. and Nakano, A. and Nelson, Y.-J.L. and Saltz, J. and
	Ashish Sharma and Priya Vashishta},
  title = {Intelligent Optimization of Parallel and Distributed Applications},
  booktitle = {Parallel and Distributed Processing Symposium, 2007. IPDPS 2007.
	IEEE International},
  year = {2007},
  pages = {1 -6},
  month = {march},
  abstract = {This paper describes a new project that systematically addresses the
	enormous complexity of mapping applications to current and future
	parallel platforms. By integrating the system layers - domain-specific
	environment, application program, compiler, run-time environment,
	performance models and simulation, and workflow manager - and through
	a systematic strategy for application mapping, our approach exploit
	the vast machine resources available in such parallel platforms to
	dramatically increase the productivity of application programmers.
	This project brings together computer scientists in the areas represented
	by the system layers (i.e., language extensions, compilers, run-time
	systems, workflows) together with expertise in knowledge representation
	and machine learning. With expert domain scientists in molecular
	dynamics (MD) simulation, we are developing our approach in the context
	of a specific application class which already targets environments
	consisting of several hundreds of processors. In this way, we gain
	valuable insight into a generalizable strategy, while simultaneously
	producing performance benefits for existing and important applications.},
  doi = {10.1109/IPDPS.2007.370490},
  keywords = {application program;compilers;distributed applications;domain-specific
	environment;intelligent optimization;knowledge representation;machine
	learning;machine resources;molecular dynamics simulation;parallel
	platforms;run-time environment;workflow management;knowledge representation;learning
	(artificial intelligence);molecular dynamics method;optimising compilers;physics
	computing;resource allocation;}
}

@INPROCEEDINGS{6078240,
  author = {Barateiro, Jose and Borbinha, Jose},
  title = {Integrated management of risk information},
  booktitle = {Computer Science and Information Systems (FedCSIS), 2011 Federated
	Conference on},
  year = {2011},
  pages = {791 -798},
  month = {sept.},
  abstract = {Today's competitive environment requires effective risk management
	activities to create prevention and control mechanisms to address
	the risks attached to specific activities and valuable assets. One
	of the main challenges in this area is concerned with the analysis
	and modeling of risks, which increases with the fact that current
	efforts tend to operate in silos with narrowly focused, functionally
	driven, and disjointed activities. This leads to a fragmented view
	of risks, where each activity uses its own language, customs and
	metrics. The lack of interconnection and holistic view of risks limits
	an organization-wide perception of risks, where interdependent risks
	are not anticipated, controlled or managed. In order to address the
	Risk Management interoperability and standardization issues, this
	paper proposes an alignment between Risk Management, Governance and
	Enterprise Architecture activities, providing a systematic support
	to map and trace identified risks to enterprise artifacts modeled
	within the Enterprise Architecture, supporting the overall strategy
	and governance of any organization. We propose an architecture where
	risks are defined through a XML-based domain specific language, and
	integrated with a Metadata Registry to handle risk concerns in the
	overall organization environment.}
}

@ARTICLE{5432228,
  author = {Baresi, L. and Ghezzi, C. and Mottola, L.},
  title = {Loupe: Verifying Publish-Subscribe Architectures with a Magnifying
	Lens},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2011},
  volume = {37},
  pages = {228 -246},
  number = {2},
  month = {march-april },
  abstract = {The Publish-Subscribe (P/S) communication paradigm fosters high decoupling
	among distributed components. This facilitates the design of dynamic
	applications, but also impacts negatively on their verification,
	making it difficult to reason on the overall federation of components.
	In addition, existing P/S infrastructures offer radically different
	features to the applications, e.g., in terms of message reliability.
	This further complicates the verification as its outcome depends
	on the specific guarantees provided by the underlying P/S system.
	Although model checking has been proposed as a tool for the verification
	of P/S architectures, existing solutions overlook many characteristics
	of the underlying communication infrastructure to avoid state explosion
	problems. To overcome these limitations, the Loupe domain-specific
	model checker adopts a different approach. The P/S infrastructure
	is not modeled on top of a general-purpose model checker. Instead,
	it is embedded within the checking engine, and the traditional P/S
	operations become part of the modeling language. In this paper, we
	describe Loupe's design and the dedicated state abstractions that
	enable accurate verification without incurring state explosion problems.
	We also illustrate our use of state-of-the-art software verification
	tools to assess some key functionality in Loupe's current implementation.
	A complete case study shows how Loupe eases the verification of P/S
	architectures. Finally, we quantitatively compare Loupe's performance
	against alternative approaches. The results indicate that Loupe is
	effective and efficient in enabling accurate verification of P/S
	architectures.},
  doi = {10.1109/TSE.2010.39},
  issn = {0098-5589},
  keywords = {Loupe design;Loupe domain-specific model checker;P-S architectures
	verification;P-S infrastructures;P-S operation;communication infrastructure;dedicated
	state abstraction;distributed component;general purpose model checker;lens
	magnification;message reliability;modeling language;publish-subscribe
	architectures;publish-subscribe communication paradigm;state explosion
	problem;state-of-the-art software verification tool;formal verification;message
	passing;middleware;}
}

@INPROCEEDINGS{4222582,
  author = {Baresi, L. and Ghezzi, C. and Mottola, L.},
  title = {On Accurate Automatic Verification of Publish-Subscribe Architectures},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {199 -208},
  month = {may},
  abstract = {The paper presents a novel approach based on Bogor for the accurate
	verification of applications based on Publish- Subscribe infrastructures.
	Previous efforts adopted standard model checking techniques to verify
	the application behavior, but they introduce strong simplifications
	on the underlying infrastructure to cope with the state space explosion
	problem and make automatic verification feasible. Instead of building
	on top of existing model checkers, our proposal embeds the asynchronous
	communication mechanisms of Publish-Subscribe infrastructures within
	Bogor. This way, Publish-Subscribe primitives become part of the
	specification language as additional, domain-specific, constructs.
	Accurate models become feasible without incurring in state space
	explosion problems, thus enabling the automated verification of applications
	on top of realistic communication infrastructures.},
  doi = {10.1109/ICSE.2007.57},
  issn = {0270-5257},
  keywords = {asynchronous communication mechanisms;automatic verification;publish-subscribe
	architectures;publish-subscribe infrastructures;realistic communication
	infrastructures;specification language;middleware;program verification;software
	architecture;specification languages;}
}

@INPROCEEDINGS{990077,
  author = {Barreto, L.P. and Muller, G.},
  title = {Bossa: a DSL framework for application-specific scheduling policies},
  booktitle = {Hot Topics in Operating Systems, 2001. Proceedings of the Eighth
	Workshop on},
  year = {2001},
  pages = { 161},
  month = {may},
  abstract = { We present a framework for easing the development of adaptable process
	scheduling infrastructures. This framework permits the development
	and installation of basic scheduling policies, which can be specialized
	using application-specific policies. We base our approach on a Domain-Specific
	Language (DSL) named Bossa. A DSL is a high-level language that provides
	appropriate abstractions, which captures domain expertise and eases
	program development. Implementing an OS using a DSL improves OS robustness
	because code becomes more readable, maintainable and more amenable
	to verification of properties. Our target is to specialize process
	schedulers for an application with soft-real time requirements that
	is able to specify adequate regulation strategies for its CPU requirements.},
  doi = {10.1109/HOTOS.2001.990077},
  issn = { },
  keywords = { Bossa domain-specific language; CPU requirements; adaptable process
	scheduling infrastructures; application-specific policies; domain
	expertise; high-level language; process schedulers; program development;
	soft-real time requirements; high level languages; operating systems
	(computers); programming environments;}
}

@INPROCEEDINGS{4404174,
  author = {Barrett, K. and Davy, S. and Strassner, J. and Jennings, B. and van
	der Meer, S. and Donnelly, W.},
  title = {A Model Based Approach for Policy Tool Generation and Policy Analysis},
  booktitle = {Global Information Infrastructure Symposium, 2007. GIIS 2007. First
	International},
  year = {2007},
  pages = {99 -105},
  month = {july},
  abstract = {We outline an approach to policy specification and analysis in which
	an information model is used as the starting point for semi-automated
	generation of an integrated suite of languages, tools and an ontology.
	The suite includes separate domain-specific languages for the specification
	of systems structure and policies respectively, editors and checkers
	for these languages, and a baseline ontology that can be augmented
	with semantic information to support policy analyses processes. We
	describe a prototypical realisation of the approach, showing how
	the languages, tools and ontology are used to support policy transformation
	and conflict detection processes.},
  doi = {10.1109/GIIS.2007.4404174},
  keywords = {baseline ontology;domain-specific languages;model based approach;policy
	analysis;policy specification;policy tool generation;semi-automated
	generation;computer network management;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{5352773,
  author = {Barrientos, P.A. and Martinez Lopez, P.E.},
  title = {Developing DSLs using combinators. A design pattern},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {635 -642},
  month = {oct.},
  abstract = {The development of domain-specific languages (DSLs) is considered
	by many authors as a hard task. To simplify the design of DSLs we
	describe a design pattern based on the combinators technique, which
	can also provide guidelines for previous domain analysis phase because
	it is based on equational reasoning over the domain knowledge. Combinators
	is a common technique from functional programming to write programs.
	It was used many times to implement domain-specific embedded languages
	(DSELs) but that implementation approach is not the only one. In
	this paper we present the pattern together with the underlying and
	basic ideas behind it. We also show benefits of using it and illustrate
	the use of this pattern with some examples.},
  doi = {10.1109/IMCSIT.2009.5352773},
  keywords = {combinators technique;design pattern;domain analysis phase;domain
	knowledge;domain-specific embedded languages;domain-specific languages;equational
	reasoning;functional programming;programming languages;specification
	languages;}
}

@INPROCEEDINGS{5718444,
  author = {Bartelt, C.},
  title = {Conflict Analysis at Collaborative Development of Domain Specific
	Models using Description Logics},
  booktitle = {System Sciences (HICSS), 2011 44th Hawaii International Conference
	on},
  year = {2011},
  pages = {1 -9},
  month = {jan.},
  abstract = {Today the distribution of development locations, the co-evolution
	of models and the concurrency of work are typical for collaborative
	modeling in software projects. Software engineering teams demand
	modeling techniques at several abstraction levels to manage the complexity
	of software descriptions. Besides, software models are applied more
	and more for the specification of safety-critical systems. Hence
	software models take a hybrid role - as a matter of team communication
	and precise specification for refinement. Both aspects are considered
	in the research area of Model Driven Engineering (MDE). It provides
	methods to deal with formal specified meta-models of graphical (intuitive)
	modeling languages. Unfortunately the syntactical a semantically
	correct (consistent) integration of concurrently evolved models is
	poorly considered by the most MDE approaches. Especially the detection
	and analyzing of model merge conflicts can be automatized by using
	logical inference techniques. Therefore this paper proposes an approach
	based on description logics.},
  doi = {10.1109/HICSS.2011.126},
  issn = {1530-1605},
  keywords = {abstraction levels;collaborative development;conflict analysis;description
	logics;domain specific models;graphical modeling languages;intuitive
	modeling languages;model driven engineering;safety-critical systems;software
	description complexity;software engineering teams;software projects;work
	concurrency;safety-critical software;software development management;systems
	analysis;}
}

@INPROCEEDINGS{5285042,
  author = {Basili, Roberto and Croce, Danilo and Cao, Diego De and Giannone,
	Cristina},
  title = {Learning Semantic Roles for Ontology Patterns},
  booktitle = {Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT
	'09. IEEE/WIC/ACM International Joint Conferences on},
  year = {2009},
  volume = {3},
  pages = {291 -294},
  month = {sept.},
  abstract = {An ontology learning method, based on large scale linguistic ontologies,
	such as FrameNet and WordNet, is here discussed. A robust learning
	method is defined to assign semantic roles to domain specific grammatical
	patterns according to distributional models of lexical semantics.
	Large scale experimental results over an IE task show an accuracy
	around 85-90 #x025;.},
  doi = {10.1109/WI-IAT.2009.285}
}

@INPROCEEDINGS{1372102,
  author = {Bastarrica, M.C. and Ochoa, S.F. and Rossel, P.O.},
  title = {Integrated notation for software architecture specifications},
  booktitle = {Computer Science Society, 2004. SCCC 2004. 24th International Conference
	of the Chilean},
  year = {2004},
  pages = { 26 - 34},
  month = {nov.},
  abstract = { Currently, there are many notations to specify software architectures,
	which address a wide range of formality and completeness. Completely
	formal notations produce accurate and analyzable software architecture
	specifications, but the most formal and complete notations are also
	the most difficult to use and understand. Conversely, informal notations
	are easier to use and understand, but several design aspects may
	remain underspecified. This paper presents an integrated notation
	for specifying software architecture that reduces the complexity
	to use completely formal notations without resigning the formality
	required by software architecture specifications. The integrated
	notation proposes an architectural specification in three levels
	of abstraction: a graphical box-and-line diagram to specify the structure,
	a behavioral specification using input/output automata, and a basis
	of Larch traits describing the domain specific abstract data types.
	The proposed integrated notation has been used to specify the architecture
	of a complex mesh management tool and part of the specification is
	presented. Although more experimentation is required, the obtained
	results are encouraging.},
  doi = {10.1109/QEST.2004.12},
  keywords = { Larch traits; architectural specification; architecture definition
	languages; behavioral specification; complete notations; complex
	mesh management tool; domain specific abstract data types; formal
	notations; graphical box-and-line diagram; input-output automata;
	integrated notation; software architecture specifications; abstract
	data types; automata theory; formal specification; software architecture;}
}

@INPROCEEDINGS{1414596,
  author = {Anirban Basu and Swapan Bhattacharya},
  title = {A formal specification language for domain specific software development},
  booktitle = {TENCON 2004. 2004 IEEE Region 10 Conference},
  year = {2004},
  volume = {B},
  pages = { 322 - 325 Vol. 2},
  month = {nov.},
  abstract = { This paper introduces a formal specification language FIRST (Formal
	and Incremental Requirement Specification Technique) that supports
	prototyped software development process. The requirement specification
	of a system can be specified in such a way that the sequence of the
	timings of the operations can be implicitly expressed and for certain
	domains the specification can also be directly executed. The software
	can be incrementally designed i.e. at any stage a small portion of
	the software is specified and then incrementation is done over existing
	model. Consistency with the earlier model is also checked when incrementations
	are done.},
  doi = {10.1109/TENCON.2004.1414596},
  keywords = { FIRST; Formal and Incremental Requirement Specification Technique;
	domain specific software development; formal specification language;
	model checking; requirement specification; formal specification;
	software prototyping; specification languages;}
}

@INPROCEEDINGS{665062,
  author = {Basu, A. and Morrisett, G. and Von Eicken, T.},
  title = {Promela++: a language for constructing correct and efficient protocols},
  booktitle = {INFOCOM '98. Seventeenth Annual Joint Conference of the IEEE Computer
	and Communications Societies. Proceedings. IEEE},
  year = {1998},
  volume = {2},
  pages = {455 -462 vol.2},
  month = {mar-2 apr},
  abstract = {The challenge is to develop an easily usable protocol development
	framework that combines the flexibility of layered implementations,
	the efficiency of tightly-coupled monolithic implementations and
	the correctness achievable using high-level protocol validation languages.
	This challenge is addressed by a language-based framework that introduces
	a new protocol specification language called Promela++. The framework
	consists of a protocol verification tool and an optimizing compiler
	that generates efficient protocol code from Promela++ specifications.
	Promela++ is based on the Promela protocol validation language and
	has been designed with a rich set of domain-specific constructs.
	These constructs facilitate the task of protocol specification as
	well as enable the Promela++ compiler to perform domain-specific
	optimizations. The Promela++ compiler can also automatically transform
	protocol specifications in Promela++ to protocol models in Promela.
	The article presents a new language that unites the twin goals of
	checking protocol correctness using model checkers, and efficient
	protocol construction using optimizing compilers, under a single
	framework; exploits language design to provide mechanisms that simultaneously
	ease programming and enable generation of efficient protocol code;
	and demonstrates the effectiveness of this approach by doing a complete
	evaluation of multiple protocol implementations in Promela++},
  doi = {10.1109/INFCOM.1998.665062},
  issn = {0743-166X},
  keywords = {Promela;Promela++;TCP;compiler;domain-specific optimizations;high-level
	protocol validation languages;language-based framework;layered implementations;model
	checkers;optimizing compiler;protocol code generation;protocol correctness;protocol
	development;protocol specification language;protocol verification
	tool;tightly-coupled monolithic implementations;conformance testing;optimising
	compilers;program verification;specification languages;transport
	protocols;}
}

@INPROCEEDINGS{1311042,
  author = {Batory, D.},
  title = {Program comprehension in generative programming: a history of grand
	challenges},
  booktitle = {Program Comprehension, 2004. Proceedings. 12th IEEE International
	Workshop on},
  year = {2004},
  pages = { 2 - 11},
  month = {june},
  abstract = { The communities of generative programming (GP) and program comprehension
	(PC) look at similar problems: GP derives a program from a specification,
	PC derives a specification from a program. A basic difference between
	the two is GP's use of specific knowledge representations and mental
	models that are essential for program synthesis. In this paper, the
	author presents a historical review of the grand challenges, results,
	and outlook for GP as they pertain to PC.},
  doi = {10.1109/WPC.2004.1311042},
  issn = {1092-8138 },
  keywords = { generative programming; knowledge representations; mental models;
	program comprehension; program specification; program synthesis;
	automatic programming; formal specification; knowledge representation;
	reverse engineering;}
}

@INPROCEEDINGS{1317496,
  author = {Batory, D.},
  title = {Feature-oriented programming and the AHEAD tool suite},
  booktitle = {Software Engineering, 2004. ICSE 2004. Proceedings. 26th International
	Conference on},
  year = {2004},
  pages = { 702 - 703},
  month = {may},
  abstract = { Feature oriented programming (FOP) is an emerging paradigm for application
	synthesis, analysis, and optimization. A target application is specified
	declaratively as a set of features, like many consumer products (e.g.,
	personal computers, automobiles). FOP technology translates such
	declarative specifications into efficient programs.},
  doi = {10.1109/ICSE.2004.1317496},
  issn = {0270-5257 },
  keywords = { AHEAD tool suite; FOP technology; application analysis; application
	optimization; application synthesis; declarative specifications;
	feature-oriented programming; target application; object-oriented
	programming; software tools;}
}

@ARTICLE{846301,
  author = {Batory, D. and Gang Chen and Robertson, E. and Tao Wang},
  title = {Design wizards and visual programming environments for GenVoca generators},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2000},
  volume = {26},
  pages = {441 -452},
  number = {5},
  month = {may},
  abstract = {Domain-specific generators will increasingly rely on graphical languages
	for declarative specifications of target applications. Such languages
	will provide front-ends to generators and related tools to produce
	customized code on demand. Critical to the success of this approach
	will be domain-specific design wizards, tools that guide users in
	their selection of components for constructing particular applications.
	The authors present the P3 ContainerStore graphical language, its
	generator, and design wizard},
  doi = {10.1109/32.846301},
  issn = {0098-5589},
  keywords = {GenVoca generators;P3 ContainerStore graphical language;customized
	code;declarative specifications;design wizards;domain-specific generators;front-ends;graphical
	languages;target applications;visual programming environments;application
	generators;programming environments;user interfaces;visual languages;visual
	programming;}
}

@INPROCEEDINGS{685750,
  author = {Batory, D. and Gang Chen and Robertson, E. and Tao Wang},
  title = {Design wizards and visual programming environments for generators
	},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {255 -267},
  month = {jun},
  abstract = {Domain-specific generators will increasingly rely on graphical specification
	languages-applets-for declarative specifications of target applications.
	Applets will provide front-ends to generators and related tools to
	produce customized code on demand. Critical to the success of this
	approach will be domain-specific design wizards, tools that guide
	users in their selection of components for constructing particular
	applications. In this paper, we present the P3 ContainerStore applet,
	its generator and design wizard},
  doi = {10.1109/ICSR.1998.685750},
  issn = {1085-9098},
  keywords = {P3 ContainerStore applet;application building;component selection;customized
	code;declarative specifications;domain-specific application generators;domain-specific
	design wizards;graphical specification languages;visual programming
	environments;application generators;programming environments;specification
	languages;visual programming;}
}

@INPROCEEDINGS{685739,
  author = {Batory, D. and Lofaso, B. and Smaragdakis, Y.},
  title = {JTS: tools for implementing domain-specific languages},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {143 -153},
  month = {jun},
  abstract = {The Jakarta Tool Suite (JTS) aims to reduce substantially the cost
	of generator development by providing domain-independent tools for
	creating domain-specific languages and component-based generators
	called GenVoca generators. JTS is a set of precompiler-compiler tools
	for extending industrial programming languages (e.g., Java) with
	domain-specific constructs. JTS is itself a GenVoca generator where
	precompilers for JTS-extended languages are constructed from components
	},
  doi = {10.1109/ICSR.1998.685739},
  issn = {1085-9098},
  keywords = {GenVoca generators;JTS;Jakarta Tool Suite;Java;component-based generators;cost;domain-independent
	tools;domain-specific languages;generator development;industrial
	programming languages;precompiler-compiler tools;software reuse;compiler
	generators;high level languages;programming environments;software
	reusability;software tools;}
}

@INPROCEEDINGS{4211921,
  author = {Bauer, A. and Pister, M. and Tautschnig, M.},
  title = {Tool-support for the analysis of hybrid systems and models},
  booktitle = {Design, Automation Test in Europe Conference Exhibition, 2007. DATE
	'07},
  year = {2007},
  pages = {1 -6},
  month = {april},
  abstract = {This paper introduces a method and tool-support for the automatic
	analysis and verification of hybrid and embedded control systems,
	whose continuous dynamics are often modelled using MATLAB/Simulink.
	The method is based upon converting system models into the uniform
	input language of our efficient multi-domain constraint solving library,
	ABSOLVER, which is then used for subsequent analysis. Basically,
	ABSOLVER is an extensible SMT-solver which addresses mixed Boolean
	and (nonlinear) arithmetic constraint problems as they appear in
	the design of hybrid control systems. It allows the integration and
	semantic connection of various domain specific solvers via a logical
	circuit, such that almost arbitrary multi-domain constraint problems
	can be formulated and solved. Its design has been tailored for extensibility,
	and thus facilitates the reuse of expert knowledge, in that the most
	appropriate solver for a given task can be integrated and used. As
	such the only constraint over the problem domain is the capability
	of the employed solvers. Our approach to systems verification has
	been validated in an industrial case study using the model of a car's
	steering control system. However, additional benchmarks show that
	other hard instances of problems could also be solved by ABSOLVER
	in respectable time, and that for some instances, ABsOLVER's approach
	was the only means of solving a problem at all},
  doi = {10.1109/DATE.2007.364411},
  keywords = {ABSOLVER;Boolean constraint problems;MATLAB/Simulink;arithmetic constraint
	problems;car's steering control system;domain specific solvers;embedded
	control systems;hybrid control systems;logical circuit;multidomain
	constraint solving library;tool-support;Boolean functions;computability;constraint
	theory;control engineering computing;electronic engineering computing;embedded
	systems;formal verification;}
}

@INPROCEEDINGS{5428539,
  author = {Becker, J. and Bergener, P. and Breuker, D. and Rackers, M.},
  title = {Evaluating the Expressiveness of Domain Specific Modeling Languages
	Using the Bunge-Wand-Weber Ontology},
  booktitle = {System Sciences (HICSS), 2010 43rd Hawaii International Conference
	on},
  year = {2010},
  pages = {1 -10},
  month = {jan.},
  abstract = {Business Process Management is becoming an ever more important aspect
	for organizations alongside with Business Process Diagrams as a tool
	to describe business processes. So far process modeling has been
	mainly performed with generic process modeling languages. These approaches
	have however limitations when it comes to the needs of specific problem
	domains or automated process analysis. Semantic building block based
	languages (SBBL) aim to overcome those limitations by integrating
	domain semantics in the modeling language. However, this class of
	languages is only useful if they exhibit the same expressiveness
	as generic languages. In this paper we strive to answer this question
	by comparing the expressiveness of the SBBL language PICTURE with
	ARIS as a generic language based on the Bunge-Wand-Weber ontology,
	showing that PICTURE has hardly construct deficits compared to ARIS
	while showing less construct redundancy and construct overload in
	its constructs.},
  doi = {10.1109/HICSS.2010.190},
  issn = {1530-1605},
  keywords = {ARIS;Bunge Wand Weber ontology;automated process analysis;business
	process diagrams;business process management;domain specific modeling
	languages;generic process modeling language;semantic building block
	based languages;business data processing;business process re-engineering;ontologies
	(artificial intelligence);simulation languages;}
}

@INPROCEEDINGS{5416108,
  author = {Behalek, M. and Saloun, P.},
  title = {Usage of Embedded Process Functional Language as a Modeling Tool
	for Embedded Systems Development},
  booktitle = {Intelligent Systems, Modelling and Simulation (ISMS), 2010 International
	Conference on},
  year = {2010},
  pages = {116 -121},
  month = {jan.},
  abstract = {Demands on development process of embedded systems are increasing.
	To address these demands we can for example use different agile methodologies.
	Agile methodologies often try to eliminate different development
	risks as early as possible in development cycle. Solution can be
	a working model or prototype of at least critical system parts. Functional
	languages are very attractive from this perspective. They have several
	interesting properties like excellent abstraction mechanism; produced
	code is concise and extensible. These languages can be used as a
	tool producing a kind of executable design. In this paper we present
	our work on a domain specific functional language targeted for embedded
	systems - Embedded Process Functional Language. This high level language
	can be used like a modeling tool or a prototyping language in early
	development phases. It uses other technologies (even other functional
	languages) created for embedded systems development on lower levels.},
  doi = {10.1109/ISMS.2010.33},
  keywords = {Embedded Process Functional Language;abstraction mechanism;agile methodology;domain
	specific functional language;embedded systems development;executable
	design;high level language;modeling tool;prototyping language;embedded
	systems;functional languages;software prototyping;software tools;}
}

@INPROCEEDINGS{5350131,
  author = {Behalek, M. and Saloun, P.},
  title = {Simulation of Embedded Applications Implemented in Embedded Process
	Functional Language},
  booktitle = {Computational Intelligence, Modelling and Simulation, 2009. CSSim
	'09. International Conference on},
  year = {2009},
  pages = {253 -258},
  month = {sept.},
  abstract = {Functional programming paradigm is very attractive for implementation
	of embedded systems. Process functional language can be very useful
	from this perspective. Process functional language was developed
	to integrate good properties of functional and imperative languages.
	We have created a language mutation called Embedded process functional
	language designed for implementation of embedded applications. An
	emulator simulating embedded process functional programs is needed
	for purpose of development and also for debugging. This paper aims
	on developed emulator and created embedded process functional language.},
  doi = {10.1109/CSSim.2009.11},
  keywords = {embedded process functional language;embedded system;functional programming;imperative
	language;language mutation;program debugging;program emulator;embedded
	systems;functional languages;functional programming;program debugging;}
}

@INPROCEEDINGS{4135316,
  author = {Behnel, Stefan},
  title = {SLOSL--A Modelling Language for Topologies and Routing in Overlay
	Networks},
  booktitle = {Parallel, Distributed and Network-Based Processing, 2007. PDP '07.
	15th EUROMICRO International Conference on},
  year = {2007},
  pages = {498 -508},
  month = {feb.},
  abstract = {Overlay networks are a fascinating field in the area of distributed
	systems. They combine challenges from self-organisation to extreme
	scalability and provide an interesting middleware layer for server-free
	Internet applications. The design aspects of their implementations,
	however, remained largely at the prototype level, which renders their
	integration and deployment in real applications hard. This paper
	describes an integrative, platform independent design approach for
	overlay networks that models topologies as data management systems.
	Local decisions about neighbours and message forwarding are expressed
	in an SQL-like language. The mapping to runnable implementations
	follows the model driven architecture approach},
  doi = {10.1109/PDP.2007.75},
  issn = {1066-6192},
  keywords = {Internet;SQL;distributed systems;message forwarding;middleware layer;overlay
	networks;Internet;SQL;middleware;simulation languages;}
}

@INPROCEEDINGS{1639502,
  author = {Bengtsson, J. and Svensson, B.},
  title = {A configurable framework for stream programming exploration in baseband
	applications},
  booktitle = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006.
	20th International},
  year = {2006},
  pages = {8 pp.},
  month = {april},
  abstract = {This paper presents a configurable framework to be used for rapid
	prototyping of stream based languages. The framework is based on
	a set of design patterns defining the elementary structure of a domain
	specific language for high-performance signal processing. A stream
	language prototype for baseband processing has been implemented using
	the framework. We introduce language constructs to efficiently handle
	dynamic reconfiguration of distributed processing parameters. It
	is also demonstrated how new language specific primitive data types
	and operators can be used to efficiently and machine independently
	express computations on bitfields and data-parallel vectors. These
	types and operators yield code that is readable, compact and amenable
	to a stricter type checking than is common practice. They make it
	possible for a programmer to explicitly express parallelism to be
	exploited by a compiler. In short, they provide a programming style
	that is less error prone and has the potential to lead to more efficient
	implementations},
  doi = {10.1109/IPDPS.2006.1639502},
  keywords = {baseband processing;data-parallel vectors;distributed processing;domain
	specific language;dynamic reconfiguration;high-performance signal
	processing;primitive data types;program compiler;rapid prototyping;stream
	language prototype;stream programming;distributed processing;program
	compilers;software prototyping;telecommunication computing;telecommunication
	signalling;}
}

@INPROCEEDINGS{341267,
  author = {Bennett, K.H. and Ward, M.P.},
  title = {Theory and practice of middle-out programming to support program
	understanding},
  booktitle = {Program Comprehension, 1994. Proceedings., IEEE Third Workshop on},
  year = {1994},
  pages = {168 -175},
  month = {nov},
  abstract = {Theories of top-down and bottom-up program comprehension have existed
	for several years, but it has been recognised that understanding
	rarely happens in practice in such a well-ordered way. The paper
	describes recent work and results at Durham on what is termed middle-out
	programming. The objective is to avoid the problems of top-down and
	bottom-up approaches, by designing a very high level language specific
	to the application domain. Domain knowledge is captured in the design
	of this language, which retains a strong formal basis. This paper
	takes the view that software engineering will become strongly application
	domain based, and that knowledge representation of the domain will
	be a crucial factor in supporting program comprehension. An example
	of using this approach in the design of a large software system is
	presented},
  doi = {10.1109/WPC.1994.341267},
  keywords = {bottom-up program comprehension;domain knowledge;knowledge representation;large
	software system design;middle-out programming;program comprehension;program
	understanding;software engineering;top-down program comprehension;very
	high level language;high level languages;programming;reverse engineering;}
}

@INPROCEEDINGS{4536151,
  author = {Benson, G.D.},
  title = {State management for distributed Python applications},
  booktitle = {Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International
	Symposium on},
  year = {2008},
  pages = {1 -8},
  month = {april},
  abstract = {We present a novel state management mechanism that can be used to
	capture the complete execution state of distributed Python applications.
	This mechanism can serve as the foundation for a variety of dependability
	strategies including checkpointing, replication, and migration. Python
	is increasingly used for rapid prototyping parallel pro grams and,
	in some cases, used for high-performance application development
	using libraries such as NumPy. Building on Stackless Python and the
	River parallel and distributed programming environment, we have developed
	mechanisms for state capture at the language level. Our approach
	allows for migration and checkpointing of applications in heterogeneous
	environments. In addition, we allow for preemptive state capture
	so that programmers need not introduce explicit snapshot requests.
	Our mechanism can be extended to support application or domain-specific
	state capture. To our knowledge, this is the first general checkpointing
	scheme for Python. We describe our system, the implementation, and
	give some initial performance figures.},
  doi = {10.1109/IPDPS.2008.4536151},
  issn = {1530-2075},
  keywords = {NumPy;River parallel programming;Stackless Python;application checkpointing;application
	migration;dependability strategy;distributed Python applications;distributed
	programming environment;domain-specific state capture;heterogeneous
	environment;preemptive state capture;rapid prototyping;replication;state
	management;checkpointing;parallel programming;software prototyping;}
}

@INPROCEEDINGS{4273244,
  author = {Berenbach, B. and Konrad, S.},
  title = {Putting the "Engineering" into Software Engineering with Models},
  booktitle = {Modeling in Software Engineering, 2007. MISE '07: ICSE Workshop 2007.
	International Workshop on},
  year = {2007},
  pages = {4},
  month = {may},
  abstract = {Models are frequently used for illustrations in software design documents.
	Commonly they are used to show static structure and less often, external
	dynamic behavior. However, in software engineering, the lack of conceptual
	models often inhibits creativity and understanding, which may in
	turn lead to incomplete or poor design. This paper describes our
	experience using models for the architectural, conceptual and detailed
	design for software systems, identifies perceived weaknesses in traditional
	approaches and makes recommendations for future modeling tools and
	techniques.},
  doi = {10.1109/MISE.2007.13},
  keywords = {conceptual models;software design documents;software engineering;software
	development management;}
}

@INPROCEEDINGS{1254476,
  author = {Bergholz, A. and Childlovskii, B.},
  title = {Crawling for domain-specific hidden Web resources},
  booktitle = {Web Information Systems Engineering, 2003. WISE 2003. Proceedings
	of the Fourth International Conference on},
  year = {2003},
  pages = { 125 - 133},
  month = {dec.},
  abstract = { The Hidden Web, the part of the Web that remains unavailable for
	standard crawlers, has become an important research topic during
	recent years. Its size is estimated to 400 to 500 times larger than
	that of the publicly indexable Web (PIW). Furthermore, the information
	on the hidden Web is assumed to be more structured, because it is
	usually stored in databases. In this paper, we describe a crawler
	which starting from the PIW finds entry points into the hidden Web.
	The crawler is domain-specific and is initialized with pre-classified
	documents and relevant keywords. We describe our approach to the
	automatic identification of Hidden Web resources among encountered
	HTML forms. We conduct a series of experiments using the top-level
	categories in the Google directory and report our analysis of the
	discovered Hidden Web resources.},
  doi = {10.1109/WISE.2003.1254476},
  issn = { },
  keywords = { Google directory; HTML forms; Hidden Web resources; Web crawlers;
	Web pages; automatic identification; information searching; pre-classified
	documents; publicly indexable Web; relevant keywords; search engines;
	Internet; hypermedia markup languages; information resources; information
	retrieval; online front-ends; search engines;}
}

@ARTICLE{798324,
  author = {Bertrand, F. and Augeraud, M.},
  title = {BDL: a specialized language for per-object reactive control},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {347 -362},
  number = {3},
  month = {may/jun},
  abstract = {The problem of describing the concurrent behavior of objects in object
	oriented languages is addressed. The approach taken is to let methods
	be the behavior units whose synchronization is controlled separate
	from their specification. Our proposal is a domain-specific language
	called BDL for expressing constraints on this control and actually
	implementing its enforcement. We propose a model where each object
	includes a so-called ldquo;execution controller rdquo;, programmed
	in BDL. This separates cleanly the concepts of what the methods do,
	the object processes, from the circumstances in which they are allowed
	to do it, the control. The object controller ensures that scheduling
	constraints between the object's methods are met. Aggregate objects
	can be controlled in terms of their components. This language has
	a convenient formal base. Thus, using BDL expressions, behavioral
	properties of objects or groups of interesting objects can be verified.
	Our approach allows, for example, deadlock detection or verification
	of safety properties, while maintaining a reasonable code size for
	the running controller. A compiler from BDL has been implemented,
	automatically generating controller code in an Esterel program, i.e.,
	in a reactive programming language. From this code, the Esterel compiler,
	in turn, generates an automaton on which verifications are done.
	Then this automaton is translated into a C code to be executed. This
	multistage process typifies the method for successful use of a domain-specific
	language. This also allows high level concurrent programming},
  doi = {10.1109/32.798324},
  issn = {0098-5589},
  keywords = {BDL;BDL expressions;C code;Esterel compiler;Esterel program;aggregate
	objects;automaton;behavior units;behavioral properties;concurrent
	behavior;deadlock detection;domain-specific language;execution controller;formal
	base;high level concurrent programming;multistage process;object
	controller;object oriented languages;object processes;per-object
	reactive control;reactive programming language;safety properties;scheduling
	constraints;specialized language;automata theory;formal specification;object-oriented
	languages;parallel languages;program compilers;scheduling;synchronisation;}
}

@INPROCEEDINGS{868986,
  author = {Berzins, L.V. and Shing, M. and Riehle, R. and Nogueira, J.},
  title = {Evolutionary Computer Aided Prototyping System (CAPS)},
  booktitle = {Technology of Object-Oriented Languages and Systems, 2000. TOOLS
	34. Proceedings. 34th International Conference on},
  year = {2000},
  pages = {363 -372},
  abstract = {Describes a distributed development environment, called CAPS (Computer-Aided
	Prototyping System), to support rapid prototyping and automatic generation
	of source code based on designer specifications in an evolutionary
	software development process. The CAPS system uses a fifth-generation
	prototyping language to model the communication structure, timing
	constraints, I/O control and data buffering that comprise the requirements
	for an embedded software system. The language supports the specification
	of hard real-time systems with reusable components from domain-specific
	component libraries. CAPS has been used successfully as a research
	tool in prototyping large real-time control systems (e.g. a command-and-control
	station, Cruise missile flight control system, missile defense systems)
	and has demonstrated its capability to support the development of
	large, complex embedded software},
  doi = {10.1109/TOOLS.2000.868986},
  keywords = {5th-generation prototyping language;CAPS;Computer-Aided Prototyping
	System;Cruise missile flight control system;I/O control;automatic
	source code generation;command-and-control station;communication
	structure;data buffering;designer specifications;distributed development
	environment;domain-specific component libraries;embedded software
	system;evolutionary software development process;hard real-time systems;missile
	defense systems;rapid prototyping;real-time control systems;reusable
	components;timing constraints;automatic programming;buffer storage;computer
	aided software engineering;computerised control;development systems;distributed
	programming;embedded systems;fifth generation systems;input-output
	programs;missile control;software libraries;software prototyping;software
	reusability;subroutines;timing;}
}

@INPROCEEDINGS{1374299,
  author = {Bezivin, J.},
  title = {Model Engineering for Software Modernization},
  booktitle = {Reverse Engineering, 2004. Proceedings. 11th Working Conference on},
  year = {2004},
  pages = { 4},
  month = {nov.},
  doi = {10.1109/WCRE.2004.29},
  issn = {1095-1350}
}

@INPROCEEDINGS{1467881,
  author = {Bezivin, J. and Jouault, F. and Touzet, D.},
  title = {Principles, standards and tools for model engineering},
  booktitle = {Engineering of Complex Computer Systems, 2005. ICECCS 2005. Proceedings.
	10th IEEE International Conference on},
  year = {2005},
  pages = { 28 - 29},
  month = {june},
  abstract = {We take here a broad view of model engineering as encompassing different
	approaches such as the OMG MDA trade; proposal as stated in R. Soley
	(2000), the Microsoft Software Factories view based in J. Greenfield
	(2004), and many others. We distinguish the three levels of principles,
	standards and tools to facilitate the discussion. We proposed the
	idea that there exist a common set of principles that could be mapped
	to different implementation contexts through the help of common standards.
	We illustrate our claim with AMMA, a lightweight architectural style
	for a model-engineering platform that is currently mapped onto the
	eclipse modeling framework according to F. Budinsky et al. (2004).},
  doi = {10.1109/ICECCS.2005.68},
  keywords = { AMMA; conceptual architecture; eclipse modeling; model engineering;
	software architecture; software tools; standards; formal specification;
	software architecture; software standards; software tools;}
}

@INPROCEEDINGS{4144628,
  author = {Bharadwaj, Ramesh and Mukhopadhyay, Supratik},
  title = {SOLj: A Domain-Specific Language (DSL) for Secure Service-Based Systems},
  booktitle = {Future Trends of Distributed Computing Systems, 2007. FTDCS '07.
	11th IEEE International Workshop on},
  year = {2007},
  pages = {173 -180},
  month = {march},
  abstract = {We present SOLj (Secure Operations Language-Java), an event-driven
	domain-specific synchronous programming extension of Java for developing
	secure service-based systems. The language has capabilities for handling
	service invocations asynchronously, includes strong typing for the
	enforcement of information flow and security policies, and exception
	handling mechanisms to deal with failures of components or services
	(both benign and Byzantine). Applications written in SOLj are formally
	verifiable using static analysis techniques. SOLj programs may be
	deployed, configured, and run on SINS (Secure Infrastructure for
	Networked Systems) under development at the Naval Research Laboratory},
  doi = {10.1109/FTDCS.2007.32},
  issn = {1071-0483},
  keywords = {Secure Infrastructure for Networked Systems;Secure Operations Language-Java;domain-specific
	language;exception handling;service invocations;service-based system
	security;static analysis;synchronous programming;Java;exception handling;program
	diagnostics;security of data;specification languages;}
}

@INPROCEEDINGS{5158855,
  author = {Bhatti, S. and Brady, E. and Hammond, K. and McKinna, J.},
  title = {Domain Specific Languages (DSLs) for Network Protocols (Position
	Paper)},
  booktitle = {Distributed Computing Systems Workshops, 2009. ICDCS Workshops '09.
	29th IEEE International Conference on},
  year = {2009},
  pages = {208 -213},
  month = {june},
  abstract = {Next generation network architectures will benefit from the many years
	of practical experience that have been gained in designing, using
	and operating network protocols. Over time, the networking community
	has gradually improved its understanding of networked systems in
	terms of architecture, design, engineering and testing. However,
	as protocols and networked systems become more complex, it is our
	contention that it will be necessary for programming techniques to
	evolve similarly so that they better support the design, implementation
	and testing of both the functional and the non-functional requirements
	for the network protocols that will be used.We therefore envisage
	new levels of programming language support that permit: (a) the design
	and implementation of new protocols with provably correct construction;
	(b) inline testing; and (c) the expression of protocol behaviour
	within the design. Based on our ongoing work with both network protocols
	and programming language design, we believe that exploiting the capabilities
	of work in domain specific languages (DSLs) will allow us to meet
	such requirements, allowing straightforward and "correct-by-construction''
	design and implementation of next generation network protocols.},
  doi = {10.1109/ICDCSW.2009.64},
  issn = {1545-0678},
  keywords = {correct-by-construction design;domain specific languages;inline testing;network
	protocols;next generation network architectures;programming language
	design;programming language support;program testing;protocols;specification
	languages;}
}

@INPROCEEDINGS{4685758,
  author = {Bibbo, L.M. and Garcia, D. and Pons, C.},
  title = {A Domain Specific Language for the Development of Collaborative Systems},
  booktitle = {Chilean Computer Science Society, 2008. SCCC '08. International Conference
	of the},
  year = {2008},
  pages = {3 -12},
  month = {nov.},
  abstract = {Domain-specific languages (DSLs) are high level languages defined
	for combining expressivity and simplicity by means of constructs
	which are close to the problem domain and distant from the intricacies
	of underlying software implementation constraints. This paper presents
	a language to graphically document the analysis and design decisions
	embodied in collaborative system development. The language was designed
	as a conservative extension of the UML and it enables the application
	of the MDD approach to the development of such systems.},
  doi = {10.1109/SCCC.2008.12},
  issn = {1522-4902},
  keywords = {MDD approach;UML;collaborative system development;collaborative systems
	development;domain specific language;domain-specific languages;high
	level languages;software implementation constraints;Unified Modeling
	Language;groupware;high level languages;software engineering;}
}

@INPROCEEDINGS{5431720,
  author = {Biermann, E. and Ehrig, K. and Ermel, C. and Hurrelmann, J.},
  title = {Generation of Simulation Views for Domain Specific Modeling Languages
	Based on the Eclipse Modeling Framework},
  booktitle = {Automated Software Engineering, 2009. ASE '09. 24th IEEE/ACM International
	Conference on},
  year = {2009},
  pages = {625 -629},
  month = {nov.},
  abstract = {The generation of tools for domain specific modeling languages (DSMLs)
	is a key issue in model-driven development. Various tools already
	support the generation of domain-specific visual editors from models,
	but tool generation for visual behavior modeling languages is not
	yet supported in a satisfactory way. In this paper we propose a generic
	approach to specify DSML environments visually by models and transformation
	rules based on the Eclipse Modeling Framework (EMF). Editing rules
	define the behavior of generated visual editors, whereas simulation
	rules describe a model's operational semantics. From a DSML definition
	(model and transformation rules), an Eclipse plug-in is generated,
	implementing a visual DSML environment including an editor and (possibly
	multiple) simulators for different simulation views on the model.
	We present the basic components of Tiger2, our EMF-based generation
	environment, along the environment generation process for a small
	DSML modeling the behavior of ants in an ant hill.},
  doi = {10.1109/ASE.2009.46},
  issn = {1527-1366},
  keywords = {EMF based generation environment;Tiger2;domain specific modeling languages;domain
	specific visual editors;eclipse modeling framework;editing rules;model
	driven development;simulation views generation;visual behavior modeling
	languages;simulation languages;software engineering;}
}

@INPROCEEDINGS{5767620,
  author = {Bildhauer, D. and Ebert, J.},
  title = {DHHTGraphs - Modeling beyond plain graphs},
  booktitle = {Data Engineering Workshops (ICDEW), 2011 IEEE 27th International
	Conference on},
  year = {2011},
  pages = {100 -105},
  month = {april},
  abstract = {Graphs are known as a suitable representation of structured data in
	general and of models in particular. Most common graph databases
	and storage facilities are based on a relatively simple kind of graphs
	restricted to binary and often even untyped or non-attributed edges.
	However, for domain-specific modeling there is also a need for some
	natural advanced concepts such as n-ary edges, several kinds of hierarchy
	and distribution of a graph over several sites. While common graph
	approaches require to simulate those concepts by plain graphs, this
	paper introduces an enhanced approach offering a seamless integration
	of these concepts, called Distributed Hierarchical Hyper-TGraphs
	(DHHTGraphs). Based on a formal definition, DHHTGraphs are described
	in detail including a metamodeling language and an efficient implementation.},
  doi = {10.1109/ICDEW.2011.5767620},
  keywords = {DHHTGraph;distributed hierarchical hypertgraphs;domain-specific modeling;graph
	databases;metamodeling language;n-ary edges;plain graphs;seamless
	integration;distributed processing;graph theory;simulation languages;}
}

@ARTICLE{851757,
  author = {Blinn, J.F.},
  title = {Optimizing C++ vector expressions},
  journal = {Computer Graphics and Applications, IEEE},
  year = {2000},
  volume = {20},
  pages = {97 -103},
  number = {4},
  month = {jul/aug},
  abstract = {Recently, I've been entertaining myself by studying all the nifty
	new programming techniques that have been invented since I was in
	school. I started with C++ and object-oriented programming. I'm now
	progressing through generic programming, aspect-oriented programming,
	partial evaluation and generative programming. What I'm really interested
	in is whether all these tricks work in the real world of graphics
	programming. My answer so far is ldquo;Yes, but... rdquo;. To see
	ldquo;but what? rdquo;, I define one of the problems I want to solve
	in in this article. I wanted to have a programming language that
	defined vectors and the arithmetic operations between them. C++ allows
	me to do this, but there are various pitfalls in doing this well.
	This article addresses one of these pitfalls: the speed of execution
	of vector arithmetic. The conventional approach turns out to be somewhat
	slow, but there's a very tricky technique that can make vector arithmetic
	very fast. It's based on the work of T. Veldhuizen (1998) and uses
	the C++ template mechanism in bizarre and unexpected ways},
  doi = {10.1109/38.851757},
  issn = {0272-1716},
  keywords = {C++ template mechanism;C++ vector expression optimization;arithmetic
	operations;aspect-oriented programming;execution speed;generative
	programming;generic programming;graphics programming;object-oriented
	programming;partial evaluation;programming language;vector arithmetic;C++
	language;arithmetic;computer graphics;mathematics computing;object-oriented
	programming;optimisation;software performance evaluation;vectors;}
}

@INPROCEEDINGS{4076965,
  author = {Carsten Bock},
  title = {Model-Driven HMI Development: Can Meta-CASE Tools do the Job?},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {287b},
  month = {jan. },
  abstract = {Today metamodeling and domain-specific languages represent many promising
	beginnings to create non-generic tool support for project specific
	modeling tasks. Due to the inherent complexity and numerous variants
	of human-machine interfaces (HMIs) model-driven development becomes
	increasingly interesting for manufacturers and suppliers in the automobile
	industry. Particularly, the development of powerful user interfaces
	requires appropriate development processes as well as easy-to-use
	software tools. Since suitable tool kits are missing in the field
	of HMI development this paper describes the utilization of visual
	domain-specific languages for model-driven useware engineering in
	general and model-based specification of automotive HMIs in special.
	Moreover, results from a survey among developers are presented revealing
	the requirements for HMI specific tool support. Additionally, experiences
	with using current meta-CASE tools as well as standard office applications
	for creating a visual domain-specific language are presented. Based
	on these experiences requirements for future meta-CASE tools are
	derived},
  doi = {10.1109/HICSS.2007.385},
  issn = {1530-1605},
  keywords = {automobile industry;domain-specific language;meta-CASE tool;metamodeling
	language;model-based specification;model-driven human-machine interface
	development;model-driven useware engineering;project specific modeling;user
	interface development;visual domain-specific language;automobile
	industry;computer aided software engineering;formal specification;human
	computer interaction;user interfaces;visual languages;}
}

@INPROCEEDINGS{777981,
  author = {Boehm, B. and Abi-Antoun, M. and Port, D. and Kwan, J. and Lynch,
	A.},
  title = {Requirements engineering, expectations management, and the Two Cultures},
  booktitle = {Requirements Engineering, 1999. Proceedings. IEEE International Symposium
	on},
  year = {1999},
  pages = {14 -22},
  abstract = {One of the difficulties in requirements negotiation is to determine
	a feasible and mutually satisfactory set of requirements for the
	developer and the user, a problem related to C.P. Snow's (1959) ldquo;Two
	Cultures rdquo; problem. During the last year of our experience with
	an annual series of digital library projects, we have been experimenting
	with expectations management and domain specific lists of ldquo;simplifiers
	rdquo; and ldquo;complicators rdquo;, as a way to address the ldquo;Two
	Cultures rdquo; problem involving librarians and computer scientists.
	Initial results indicate that the simplifiers and complicators approach
	successfully reduced the number of projects having serious feasibility
	problems, and helped manage the expectations of both the developers
	and the customers/users. We see no obstacles to applying the approach
	to other domains},
  doi = {10.1109/ISRE.1999.777981},
  keywords = {Two Cultures;complicators approach;computer scientists;digital library;expectations
	management;librarians;requirements engineering;requirements negotiation;simplifiers
	approach;digital libraries;software development management;systems
	analysis;}
}

@INPROCEEDINGS{4591758,
  author = {Boender, J. and Di Cosmo, R. and Vouillon, J. and Durak, B. and Mancinelli,
	F.},
  title = {Improving the Quality of GNU/Linux Distributions},
  booktitle = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual
	IEEE International},
  year = {2008},
  pages = {1240 -1246},
  month = {28 2008-aug. 1},
  abstract = {The widespread adoption of free and open source software (FOSS) has
	lead to a freer and more agile marketplace where there is a higher
	number of components that can be used to build systems in many original
	and often unforeseen ways. One of the most prominent examples of
	complex systems built with FOSS components are GNU/Linux-based distributions.
	In this paper we present some tools that aim at helping distribution
	editors with maintaining the huge package bases associated with these
	distributions, and improving their quality, by detecting errors and
	inconsistencies in an effective, fast and automatic way.},
  doi = {10.1109/COMPSAC.2008.226},
  issn = {0730-3157},
  keywords = {GNU;Linux distributions;open source software;Linux;public domain software;}
}

@INPROCEEDINGS{5474053,
  author = {Bond, B. and Hammil, K. and Litchev, L. and Singh, S.},
  title = {FPGA Circuit Synthesis of Accelerator Data-Parallel Programs},
  booktitle = {Field-Programmable Custom Computing Machines (FCCM), 2010 18th IEEE
	Annual International Symposium on},
  year = {2010},
  pages = {167 -170},
  month = {may},
  abstract = {This paper describes the techniques used to describe and synthesize
	FPGA circuits expressed in a data-parallel domain specific language
	(DSL) called Accelerator. We identify the subset of data-parallel
	descriptions that are supported by our system and explain how we
	track memory access patterns which allow us to generate efficient
	FPGA circuits.},
  doi = {10.1109/FCCM.2010.51},
  keywords = {Accelerator data-parallel program;FPGA circuit synthesis;data-parallel
	description;domain specific language;memory access pattern;electronic
	engineering computing;field programmable gate arrays;logic design;network
	synthesis;parallel programming;specification languages;}
}

@INPROCEEDINGS{4554352,
  author = {Bone, M. and Nabicht, P.F. and Laufer, K. and Thiruvathukal, G.K.},
  title = {Taming XML: Objects first, then markup},
  booktitle = {Electro/Information Technology, 2008. EIT 2008. IEEE International
	Conference on},
  year = {2008},
  pages = {488 -493},
  month = {may},
  abstract = {Processing markup in object-oriented languages often requires the
	programmer to focus on the objects generating the markup rather than
	the more pertinent domain objects. The BetterXML framework aims to
	improve this situation by allowing the programmer to develop a domain-specific
	object model as usual and later bind this model to preexisting or
	newly generated markup. To this end, the framework provides two types
	of object trees, XElement and NaturalXML, for representing XML documents.
	XElement goes beyond DOM-like automatic parsing of XML by supporting
	the custom mapping of elements to domain objects; NaturalXML allows
	the mapping of existing domain objects to XML elements using class
	metadata. Both types of object trees can be inflated and deflated
	by means of a common intermediate representation in the form of an
	event stream. Finally, the framework includes the XML Intermediate
	Representation (XIR), a lossless record-oriented representation of
	XML documents for efficient streaming and other types of data exchange.},
  doi = {10.1109/EIT.2008.4554352},
  keywords = {XML document;XML intermediate representation;common intermediate representation;data
	exchange;lossless record-oriented representation;meta data;object
	trees;object-oriented language;XML;meta data;object-oriented languages;tree
	data structures;}
}

@INPROCEEDINGS{1417411,
  author = {Bonfe, M. and Fantuzzi, C.},
  title = {Application of object-oriented modeling tools to design the logic
	control system of a packaging machine},
  booktitle = {Industrial Informatics, 2004. INDIN '04. 2004 2nd IEEE International
	Conference on},
  year = {2004},
  pages = {569 -574},
  month = {june},
  abstract = {The paper presents the results of an application of object-oriented
	modeling techniques to design manufacturing systems logic controllers.
	In particular, the semantical aspects of specification languages
	like UML and Statecharts, widely used in many software engineering
	methods, are analysed and discussed, with regard to their adequacy
	for the industrial domain and their verification with formal methods.
	The paper ends with the description of a practical case of study,
	which shows that the proposed design and verification techniques
	can be successfully adopted in a real industrial framework, given
	domain-specific adaptation of object-oriented modeling languages},
  doi = {10.1109/INDIN.2004.1417411},
  keywords = {Statecharts;UML;formal methods;logic control system;manufacturing
	system;object-oriented modeling tools;packaging machine;software
	engineering methods;control system analysis computing;formal verification;industrial
	control;logic design;object-oriented languages;packaging machines;production
	engineering computing;}
}

@INPROCEEDINGS{1428812,
  author = {Bonfe, M. and Fantuzzi, C.},
  title = {A practical approach to object-oriented modeling of logic control
	systems for industrial applications},
  booktitle = {Decision and Control, 2004. CDC. 43rd IEEE Conference on},
  year = {2004},
  volume = {1},
  pages = {980 -985 Vol.1},
  month = {dec.},
  abstract = {The paper presents the results of an application of object-oriented
	modeling techniques to design manufacturing systems logic controllers.
	In particular, the semantical aspects of specification languages
	like UML and Statecharts, widely used in many software engineering
	methods, are analysed and discussed, with regard to their adequacy
	for the industrial domain and their verification with formal methods.
	The paper ends with the description of a practical case of study,
	which shows that the proposed design and verification techniques
	can be successfully adopted in a real industrial framework, given
	domain-specific adaptation of object-oriented modeling languages.},
  doi = {10.1109/CDC.2004.1428812},
  issn = {0191-2216},
  keywords = {Statechart;UML;formal method;logic control system;manufacturing systems
	logic controller;object-oriented modeling language;software engineering
	method;specification language;Unified Modeling Language;control system
	CAD;formal verification;object-oriented languages;simulation languages;}
}

@INPROCEEDINGS{1570729,
  author = { Bonfe, M. and Fantuzzi, C. and Secchi, C.},
  title = {Verification of Behavioral Substitutability in Object-Oriented Models
	for Industrial Controllers},
  booktitle = {Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005
	IEEE International Conference on},
  year = {2005},
  pages = { 3978 - 3983},
  month = {april},
  abstract = { The aim of the paper is to provide a practical method to introduce
	design principles typical of the object-oriented approach, like #8220;design
	by extension #8221;, to the application domain of manufacturing systems
	control design. The proposed method is based on a domain-specific
	extension of the modeling language UML and on the formalization of
	design models as transition systems for verification purposes. Object-oriented
	models, formalized according to the proposed semantics, can be analyzed
	with model checking techniques in order to verify the behavioral
	conformity of object classes, according to a notion of substitutability
	which is defined in the paper specifically for the proposed modeling
	language. },
  doi = {10.1109/ROBOT.2005.1570729},
  keywords = { Discrete-event systems ; Logic controllers ; Manufacturing systems
	; Verification;}
}

@INPROCEEDINGS{841028,
  author = {Borgida, A. and Devanbu, P.},
  title = {Adding more "DL" to IDL: towards more knowledgeable component inter-operability},
  booktitle = {Software Engineering, 1999. Proceedings of the 1999 International
	Conference on},
  year = {1999},
  pages = {378 -387},
  month = {may},
  abstract = {In an open component market place, interface description languages
	(IDLs), such as CORBA's, provide for the consumer only a weak guarantee
	(concerning type signatures) that a software service will work in
	a particular context as anticipated. Stronger guarantees, regarding
	the intended semantics of the service, would help, especially if
	formalized in a language that allowed effective, automatic and static
	checking of compatibility between the server and the client's service
	descriptions. We propose an approach based on a family of formalisms
	called description logics (DLs), providing three examples of the
	use of DLs to augment IDL: (1) for the CORBA Cos Relationship service;
	(2) for capturing information models described using STEP Express,
	the ISO standard language used in the manufacturing domain (and a
	basis of the OMG PDM effort); and (3) constraints involving methods.
	While traditional formal specification techniques are more powerful,
	DLs offer certain advantages: they have decidable, even efficient
	reasoning algorithms, yet they still excel at modeling natural domains,
	and are thus well-suited for specifying application and domain-specific
	services.},
  issn = {0270-5257},
  keywords = {CORBA;compatibility;decidable;description logics;formal specification;interface
	description languages;natural domains;reasoning;distributed object
	management;formal specification;}
}

@INPROCEEDINGS{6032508,
  author = {van den Bos, J. and van der Storm, T.},
  title = {Bringing domain-specific languages to digital forensics},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {671 -680},
  month = {may},
  abstract = {Digital forensics investigations often consist of analyzing large
	quantities of data. The software tools used for analyzing such data
	are constantly evolving to cope with a multiplicity of versions and
	variants of data formats. This process of customization is time consuming
	and error prone. To improve this situation we present DERRIC, a domain-specific
	language (DSL) for declaratively specifying data structures. This
	way, the specification of structure is separated from data processing.
	The resulting architecture encourages customization and facilitates
	reuse. It enables faster development through a division of labour
	between investigators and software engineers. We have performed an
	initial evaluation of DERRIC by constructing a data recovery tool.
	This so-called carver has been automatically derived from a declarative
	description of the structure of JPEG files. We compare it to existing
	carvers, and show it to be in the same league both with respect to
	recovered evidence, and runtime performance.},
  doi = {10.1145/1985793.1985887},
  issn = {0270-5257},
  keywords = {DERRIC;JPEG files;carver;data analysis;data recovery tool;data structure
	specification;declarative description;digital forensics;domain-specific
	languages;software tools;computer forensics;data analysis;data structures;formal
	specification;}
}

@ARTICLE{5290720,
  author = {Bostock, M. and Heer, J.},
  title = {Protovis: A Graphical Toolkit for Visualization},
  journal = {Visualization and Computer Graphics, IEEE Transactions on},
  year = {2009},
  volume = {15},
  pages = {1121 -1128},
  number = {6},
  month = {nov.-dec. },
  abstract = {Despite myriad tools for visualizing data, there remains a gap between
	the notational efficiency of high-level visualization systems and
	the expressiveness and accessibility of low-level graphical systems.
	Powerful visualization systems may be inflexible or impose abstractions
	foreign to visual thinking, while graphical systems such as rendering
	APIs and vector-based drawing programs are tedious for complex work.
	We argue that an easy-to-use graphical system tailored for visualization
	is needed. In response, we contribute Protovis, an extensible toolkit
	for constructing visualizations by composing simple graphical primitives.
	In Protovis, designers specify visualizations as a hierarchy of marks
	with visual properties defined as functions of data. This representation
	achieves a level of expressiveness comparable to low-level graphics
	systems, while improving efficiency - the effort required to specify
	a visualization - and accessibility - the effort required to learn
	and modify the representation. We substantiate this claim through
	a diverse collection of examples and comparative analysis with popular
	visualization tools.},
  doi = {10.1109/TVCG.2009.174},
  issn = {1077-2626},
  keywords = {Protovis;data visualization;graphical visualization toolkit;high-level
	visualization systems;low-level graphical systems;rendering API;vector-based
	drawing programs;application program interfaces;data visualisation;rendering
	(computer graphics);}
}

@INPROCEEDINGS{1260213,
  author = {Bottoni, P. and De Marsico, M. and Di Tommaso, P. and Levialdi, S.
	and Ventriglia, D.},
  title = {Configurations and transitions in visual languages},
  booktitle = {Human Centric Computing Languages and Environments, 2003. Proceedings.
	2003 IEEE Symposium on},
  year = {2003},
  pages = { 117 - 119},
  month = {oct.},
  abstract = { Domain specific visual languages express significant system configurations
	and behaviours. They are mainly used to express some form of system
	transformation, characterised by its pre-and post-conditions and
	by an execution policy. We propose an approach to management of transitions,
	independent from the adopted diagrammatic notation, and describe
	an application.},
  doi = {10.1109/HCC.2003.1260213},
  keywords = { system transformation; transition management; visual languages; systems
	analysis; visual languages;}
}

@INPROCEEDINGS{1372303,
  author = {Bottoni, P. and Grau, A.},
  title = {A Suite of Metamodels as a Basis for a Classification of Visual Languages},
  booktitle = {Visual Languages and Human Centric Computing, 2004 IEEE Symposium
	on},
  year = {2004},
  pages = {83 -90},
  month = {sept.},
  abstract = {Metamodeling frameworks for the definition and management of visual
	languages allow the implementation of visual environments based on
	some abstract notion of visual entity and of relations among them.
	We propose a suite of metamodels able to accommodate most commonly
	used visual paradigms, built as progressive specialisation of a root
	meta-meta model. This approach facilitates the design and implementation
	of new, general purpose as well as domain specific, visual languages
	by allowing the progressive construction of language-independent
	service layers},
  doi = {10.1109/VLHCC.2004.5},
  keywords = {classification;language-independent service layers;metamodels;visual
	entity;visual environments;visual language classification;visual
	languages;}
}

@INPROCEEDINGS{5954439,
  author = {Boubeta-Puig, J. and Medina-Bulo, I. and Garca-Domnguez,
	A.},
  title = {Analogies and Differences between Mutation Operators for WS-BPEL
	2.0 and Other Languages},
  booktitle = {Software Testing, Verification and Validation Workshops (ICSTW),
	2011 IEEE Fourth International Conference on},
  year = {2011},
  pages = {398 -407},
  month = {march},
  abstract = {Applying mutation testing to a program written in a certain language
	requires that a set of mutation operators is defined for that language.
	The mutation operators need to adequately cover the features of that
	language in order to be effective. In this work, we evaluate qualitatively
	the operators defined for the Web Services Business Process Execution
	Language 2.0 (WS-BPEL) and study the differences and similarities
	between WS-BPEL and other languages. We review the existing operators
	for several structured and object-oriented general-purpose programming
	languages, and for several domain-specific languages. Results confirm
	that WS-BPEL is very different from other languages, as half of the
	mutation operators for this language are equivalent to those of other
	languages. Our study concludes that the set of WS-BPEL mutation operators
	can be improved.},
  doi = {10.1109/ICSTW.2011.52},
  keywords = {WS-BPEL 2.0;Web Services Business Process Execution Language 2.0;domain-specific
	languages;mutation operators;mutation testing;object-oriented general-purpose
	programming languages;Web services;business data processing;program
	testing;programming languages;}
}

@INPROCEEDINGS{784626,
  author = {Bouchaffra, D. and Govindaraju, V. and Srihari, S.N.},
  title = {Recognition of strings using nonstationary Markovian models: an application
	in ZIP code recognition},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  pages = {2 vol. (xxiii+637+663)},
  abstract = {This paper presents nonstationary Markovian models and their application
	to recognition of strings of tokens, such as ZIP codes in the US
	mailstream. Unlike traditional approaches where digits are simply
	recognized in isolation, the novelty of our approach lies in the
	manner in which recognitions scores along with domain specific knowledge
	about the frequency distribution of various combination of digits
	are all integrated into one unified model. The domain knowledge is
	derived from postal directory files. This data feeds into the models
	as n-grams statistics that are seamlessly integrated with recognition
	scores of digit images. We present the recognition accuracy (90%)
	achieved on a set of 20,000 ZIP codes},
  doi = {10.1109/CVPR.1999.784626},
  keywords = {US mailstream;ZIP code recognition;digit images;domain knowledge;n-grams
	statistics;nonstationary Markovian models;postal directory file;strings
	recognition;Markov processes;pattern recognition;string matching;}
}

@ARTICLE{799906,
  author = {Bouchaffra, D. and Govindaraju, V. and Srihari, S.N.},
  title = {Postprocessing of recognized strings using nonstationary Markovian
	models},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {1999},
  volume = {21},
  pages = {990 -999},
  number = {10},
  month = {oct},
  abstract = {This paper presents nonstationary Markovian models and their application
	to recognition of strings of tokens. Domain specific knowledge is
	brought to bear on the application of recognizing zip codes in the
	US mailstream by the use of postal directory files. These files provide
	a wealth of information on the delivery points (mailstops) corresponding
	to each zip code. This data feeds into the models as n-grams, statistics
	that are integrated with recognition scores of digit images. An especially
	interesting facet of the model is its ability to excite and inhibit
	certain positions in the n-grams leading to the familiar area of
	Markov random fields. We empirically illustrate the success of Markovian
	modeling in postprocessing applications of string recognition. We
	present the recognition accuracy of the different models on a set
	of 20000 zip codes. The performance is superior to the present system
	which ignores all contextual information and simply relies on the
	recognition scores of the digit recognizers},
  doi = {10.1109/34.799906},
  issn = {0162-8828},
  keywords = {Bayes method;Markov random fields;class conditional probability;nonstationary
	Markovian models;postprocessing;statistical analysis;string recognition;zip
	codes;Bayes methods;Markov processes;postal services;probability;statistical
	analysis;string matching;}
}

@INPROCEEDINGS{5703714,
  author = {Boudjemil, Z. and Phelan, P. and de Leon, M.P. and van der Meer,
	S.},
  title = {A Case Study for Defining Interoperable Network Components Using
	MDD},
  booktitle = {Computer Modeling and Simulation (EMS), 2010 Fourth UKSim European
	Symposium on},
  year = {2010},
  pages = {381 -386},
  month = {nov.},
  abstract = {The current Internet is built on a set of protocols, but exhibits
	problems in supporting applications. The network is optimised for
	best-effort traffic, but other functional aspects are widely neglected.
	Applying concepts well-known in software engineering (abstraction,
	composition, separation of concerns) to design the future Internet
	architecture is seen as a promising way forward. This paper presents
	a case study using Model Driven Development addressing interoperability
	requirements in next generation networks. Our approach focuses on
	the specification of a high level Contract Domain Specific Language
	we combine Component-based Software Engineering for the design with
	our long-term experience of network resource management and performance
	optimisation. Part of our case study is a tool chain that supports
	the network engineers who deploy next generation networks.},
  doi = {10.1109/EMS.2010.69},
  keywords = {Internet architecture;MDD;best-effort traffic;component-based software
	engineering;high level contract domain specific language;interoperable
	network component;model driven development;network resource management;next
	generation networks;protocol;software engineering;Internet;high level
	languages;next generation networks;object-oriented programming;open
	systems;protocols;resource allocation;software architecture;telecommunication
	traffic;}
}

@INPROCEEDINGS{853868,
  author = {Bourret, R. and Bornhovd, C. and Buchmann, A.},
  title = {A generic load/extract utility for data transfer between XML documents
	and relational databases},
  booktitle = {Advanced Issues of E-Commerce and Web-Based Information Systems,
	2000. WECWIS 2000. Second International Workshop on},
  year = {2000},
  pages = {134 -143},
  abstract = {XML is rapidly gaining momentum in e-commerce and Internet-based information
	exchange, where its simplicity and custom-defined tags make it usable
	as a semantics-preserving data exchange format. However, to realize
	this potential it is necessary to be able to extract structured data
	from XML documents and store it in a database, as well as to generate
	XML documents from data extracted from a database. Although many
	DBMS vendors are scrambling to extend their products to handle XML,
	there is a need for a lightweight, DBMS- and platform-independent
	load/extract utility as well. In this paper, we describe such a utility
	that solves the following problems: (1) loading data from XML documents
	into relational tables with a known schema, (2) creating XML documents
	according to a known document type definition (DTD) from data extracted
	from a database, (3) generating relational schemas from XML DTDs
	for on-the-fly storage of XML documents, and (4) generating XML DTDs
	from relational schemas for on-the-fly extraction of relational data.
	We introduce a language to describe a mapping between an existing
	XML DTD and an existing relational schema and discuss some of the
	interesting issues arising from such a mapping},
  doi = {10.1109/WECWIS.2000.853868},
  keywords = {DBMS-independent utility;Internet-based information exchange;XML documents;custom-defined
	tags;data loading;data transfer;document type definition;electronic
	commerce;generic load/extract utility;mapping language;on-the-fly
	storage;platform-independent utility;relational databases;relational
	schemas;relational tables;semantics-preserving data exchange format;structured
	data extraction;Internet;electronic commerce;electronic data interchange;hypermedia
	markup languages;relational databases;utility programs;}
}

@INPROCEEDINGS{821511,
  author = {Bowen, T. and Chee, D. and Segal, M. and Sekar, R. and Shanbhag,
	T. and Uppuluri, P.},
  title = {Building survivable systems: an integrated approach based on intrusion
	detection and damage containment},
  booktitle = {DARPA Information Survivability Conference and Exposition, 2000.
	DISCEX '00. Proceedings},
  year = {2000},
  volume = {2},
  pages = {84 -99 vol.2},
  abstract = {Reliance on networked information systems to support critical infrastructures
	prompts interest in making network information systems survivable,
	so that they continue functioning even when under attack. To build
	survivable systems, attacks must be detected and reacted to before
	they impact performance or functionality. Previous survivable systems
	research focused primarily on detecting intrusions, rather than on
	preventing or containing damage due to intrusions. We have therefore
	developed a new approach that combines early attack detection with
	automated reaction for damage prevention and containment, as well
	as tracing and isolation of attack origination point(s). Our approach
	is based on specifying security-relevant behaviors using patterns
	over sequences of observable events, such as a process's system calls
	and their arguments, and the contents of network packets. By intercepting
	actual events at runtime and comparing them to specifications, attacks
	can be detected and operations associated with the deviant events
	can be modified to thwart the attack. Being based on security-relevant
	behaviors rather than known attack signatures, our approach can protect
	against unknown attacks. At the same time, our approach produces
	few false positives-a property that is critical for automating reactions.
	Our host-based mechanisms for attack detection and isolation coordinate
	with network routers enhanced with active networking technology in
	order to trace the origin of the attack and isolate the attacker},
  doi = {10.1109/DISCEX.2000.821511},
  keywords = {active networking;attack origination point;critical infrastructures;damage
	containment;early attack detection;host-based mechanisms;intrusion
	detection;network packets;network routers;networked information systems;runtime;security-relevant
	behavior;survivable systems;system calls;network operating systems;security
	of data;telecommunication network routing;telecommunication security;}
}

@INPROCEEDINGS{5546368,
  author = {Bradesko, L. and Dali, L. and Fortuna, B. and Grobelnik, M.
	and Mladenic, D. and Novalija, I. and Pajntar, B.},
  title = {Contextualized question answering},
  booktitle = {Information Technology Interfaces (ITI), 2010 32nd International
	Conference on},
  year = {2010},
  pages = {73 -78},
  month = {june},
  abstract = {The paper proposes an approach to contextualized answering of questions.
	The contextualization is achieved by using an ontology. The answers
	are provided based on a domain specific document collection. The
	approach consists of several phases as follows: data preparation,
	data enhancement, data indexing and handling questions. The functioning
	of the proposed approach is demonstrated on English document collections
	on Aquatic Sciences and Fisheries - ASFA, using Cyc ontology, ASFA
	thesaurus as domain specific ontology and WordNet as general ontology.
	The approach is in general applicable to other datasets in other
	languages, assuming the necessary natural language processing is
	handled and the match between the document collection language, domain
	ontology language and Cyc ontology is ensured.},
  issn = {1330-1012},
  keywords = {ASFA thesaurus;Cyc ontology;English document collections;WordNet;aquatic
	sciences and fisheries;contextualized question answering;data enhancement;data
	handling;data indexing;data preparation;datasets;domain specific
	document collection;domain specific ontology;natural language processing;data
	preparation;document handling;indexing;natural language processing;ontologies
	(artificial intelligence);}
}

@INPROCEEDINGS{1602362,
  author = {van den Brand, M.G.J. and Kooiker, A.T. and Vinju, J.J. and Veerman,
	N.P.},
  title = {A language independent framework for context-sensitive formatting},
  booktitle = {Software Maintenance and Reengineering, 2006. CSMR 2006. Proceedings
	of the 10th European Conference on},
  year = {2006},
  pages = {10 pp. -112},
  month = {march},
  abstract = {Automated formatting is an important technique for the software maintainer.
	It is either applied separately to improve the readability of source
	code, or as part of a source code transformation tool chain. In this
	paper we report on the application of generic tools for constructing
	formatters. In an industrial setting, automated formatters need to
	be tailored to the requirements of the customer. The (legacy) programming
	language or dialect and the corporate formatting conventions are
	specific and non-negotiable. Can generic formatting tools deal with
	such unexpected requirements? Driven by an industrial case of nearly
	80 thousand lines of COBOL code, several limitations in existing
	formatting technology have been addressed. We improved its flexibility
	by replacing a generative phase by a generic tool, and we added a
	little expressiveness to the formatting back end. Most importantly,
	we employed a multi-stage formatting framework that can cope with
	any kind of formatting convention using more computational power},
  doi = {10.1109/CSMR.2006.4},
  issn = {1052-8725},
  keywords = {COBOL code;context-sensitive formatting;corporate formatting conventions;formatters
	construction;generic formatting tools;language independent framework;legacy
	programming language;source code readability;source code transformation
	tool chain;COBOL;software maintenance;}
}

@INPROCEEDINGS{1510161,
  author = {van den Brand, M.G.J. and Kooiker, A.T. and Vinju, J.J. and Veerman,
	N.P.},
  title = {An architecture for context-sensitive formatting},
  booktitle = {Software Maintenance, 2005. ICSM'05. Proceedings of the 21st IEEE
	International Conference on},
  year = {2005},
  pages = { 631 - 634},
  month = {sept.},
  abstract = { We have taken a fixed set of formatting requirements for a Cobol
	system as spelled out in a standardization document, and applied
	generic formatting technology to implement them. It appeared that
	corporate conventions can dictate alignment that crosscuts the logical
	structure of a program, and can even dictate indentation that is
	dynamically computed from context information. We have developed
	and implemented a formatting architecture that allows arbitrary computational
	power for mapping language constructs to the Box language. The enabling
	feature is a hybrid format that merges Box expressions with parse
	trees. Much of the boilerplate part of formatting can still be automated
	by a default mapping to Box. Absolute tab stops, an important feature
	which is not found in many Box back-ends, is used extensively in
	our case study.},
  doi = {10.1109/ICSM.2005.17},
  issn = {1063-6773 },
  keywords = { Box back-end; Box expression; Box language; Cobol system; absolute
	tab stop; context-sensitive formatting architecture; formatting requirements;
	mapping language construct; parse tree; program logical structure;
	COBOL; software architecture; software maintenance;}
}

@INPROCEEDINGS{5557988,
  author = {Brandic, I. and Dustdar, S. and Anstett, T. and Schumm, D. and Leymann,
	F. and Konrad, R.},
  title = {Compliant Cloud Computing (C3): Architecture and Language Support
	for User-Driven Compliance Management in Clouds},
  booktitle = {Cloud Computing (CLOUD), 2010 IEEE 3rd International Conference on},
  year = {2010},
  pages = {244 -251},
  month = {july},
  abstract = {Cloud computing represents a promising computing paradigm, where computational
	power is provided similar to utilities like water, electricity or
	gas. While most of the Cloud providers can guarantee some measurable
	non-functional performance metrics e.g., service availability or
	throughput, there is lack of adequate mechanisms for guaranteeing
	certifiable and auditable security, trust, and privacy of the applications
	and the data they process. This lack represents an obstacle for moving
	most business relevant applications into the Cloud. In this paper
	we devise a novel approach for compliance management in Clouds, which
	we termed Compliant Cloud Computing (C3). On one hand, we propose
	novel languages for specifying compliance requirements concerning
	security, privacy, and trust by leveraging domain specific languages
	and compliance level agreements. On the other hand, we propose the
	C3 middleware responsible for the deployment of certifiable and auditable
	applications, for provider selection in compliance with the user
	requirements, and for enactment and enforcement of compliance level
	agreements. We underpin our approach with a use case discussing various
	techniques necessary for achieving security, privacy, and trust in
	Clouds as for example data fragmentation among different protection
	domains or among different geographical regions.},
  doi = {10.1109/CLOUD.2010.42},
  keywords = {C3 middleware;cloud providers;compliance level agreements;compliant
	cloud computing;domain specific languages;nonfunctional performance
	metrics;service privacy;service security;service trust;user requirements;user-driven
	compliance management;Internet;data privacy;middleware;programming
	languages;security of data;}
}

@INPROCEEDINGS{5641365,
  author = {Braun, S. and Obermeier, M. and Vogel-Heuser, B.},
  title = {Usability evaluation of modeling notations for software engineering
	in machine and plant automation},
  booktitle = {Emerging Technologies and Factory Automation (ETFA), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {1 -8},
  month = {sept.},
  abstract = {During the last eight years we investigated the benefit of modeling
	notations, e.g. UML and domain specific UML profiles as well as specialized
	modeling languages (Idiomatic Control Language - ICL) in software
	engineering (IEC 61131-3) in machine and plant automation with three
	different experiments and approximately more than 120 participants.
	The results of the conducted experiments revealed the benefits and
	the shortcomings of UML in the field of supporting control programming.
	Furthermore we can show the impact of different modeling notations
	on software quality (IEC 61131-3) and time consumption for the software
	development process as well as some results on how to evaluate model
	driven engineering under usability aspects.},
  doi = {10.1109/ETFA.2010.5641365},
  issn = {1946-0740},
  keywords = {IEC 61131-3;Idiomatic Control Language;domain specific UML profiles;machine;model
	driven engineering;modeling notation;plant automation;software development
	process;software engineering;software quality;specialized modeling
	languages;usability evaluation;IEC standards;Unified Modeling Language;industrial
	plants;production engineering computing;software quality;}
}

@INPROCEEDINGS{4941,
  author = {Brezocnik, Z. and Horvat, B. and Gerkes, M.},
  title = {Tool for system design verification},
  booktitle = {CompEuro '88. 'Design: Concepts, Methods and Tools'},
  year = {1988},
  pages = {100 -107},
  month = {apr},
  abstract = {An approach is presented for automatic formal verification of digital
	hardware designs using Prolog. Validation of design correctness is
	made by formal proof as an alternative to the traditional approach
	which utilizes simulation. A hardware design methodology based on
	this framework entails: writing a specification of required design,
	designing a circuit intended to implement it, and proving mathematically
	that the design meets its specification. Prolog is used both as a
	representational language for describing the design specification
	and implementation and also as an inference mechanism for proving
	its functional correctness. A developed verification system has enough
	domain specific and general mathematical knowledge to perform the
	proofs largely automatically. Designs can be handled from the transistor
	level up to the architectural levels. Some large designs, including
	a simple computer, have already been verified},
  doi = {10.1109/CMPEUR.1988.4941},
  keywords = {Prolog;automatic formal verification;circuit design;design correctness
	validation;design specification;digital hardware designs;formal proof;functional
	correctness;hardware design methodology;inference mechanism;mathematical
	proof;representational language;system design verification;tool;PROLOG;VLSI;circuit
	CAD;integrated circuit testing;}
}

@INPROCEEDINGS{5185390,
  author = {Briand, Xavier and Jeannet, Bertrand},
  title = {Combining control and data abstraction in the verification of hybrid
	systems},
  booktitle = {Formal Methods and Models for Co-Design, 2009. MEMOCODE '09. 7th
	IEEE/ACM International Conference on},
  year = {2009},
  pages = {141 -150},
  month = {july},
  abstract = {We address the verification of hybrid systems built as the composition
	of a discrete software controller interacting with a physical environment
	exhibiting a continuous behavior. Our goal is to attack the problem
	of the combinatorial explosion of discrete states that may happen
	if a complex software controller is considered. We propose as a solution
	to extend an existing abstract interpretation technique, namely dynamic
	partitioning, to hybrid systems described in a symbolic formalism.
	Dynamic partitioning allows to finely tune the tradeoff between precision
	and efficiency in the analysis. We show the effectiveness of the
	approach by a case study that combines a non trivial controller specified
	in the synchronous dataflow programming language Lustre with its
	physical environment.},
  doi = {10.1109/MEMCOD.2009.5185390}
}

@INPROCEEDINGS{4581424,
  author = {Brkic, M. and Matetic, M.},
  title = {VoiceXML for Slavic languages application development},
  booktitle = {Human System Interactions, 2008 Conference on},
  year = {2008},
  pages = {147 -151},
  month = {may},
  abstract = {Since Croatian language, as well as other Slavic languages, is essentially
	very different from English, we are in the need of developing language
	specific tools and systems. In this paper we point out the benefits
	that speech applications have and describe our future platform. Furthermore,
	we create domain-specific dialogues using VoiceXML. VoiceXML is a
	language for creating voice user interfaces. We opted for VoiceXML
	because it simplifies application development by permitting developers
	to use familiar Web techniques, tools and infrastructure.},
  doi = {10.1109/HSI.2008.4581424},
  keywords = {Croatian language;Slavic Languages;VoiceXML;Web techniques;language
	specific systems;language specific tools;voice user interfaces;XML;natural
	language interfaces;speech-based user interfaces;}
}

@INPROCEEDINGS{5961699,
  author = {Bromberg, Y. and Grace, P. and Rveillre, L.},
  title = {Starlink: Runtime Interoperability between Heterogeneous Middleware
	Protocols},
  booktitle = {Distributed Computing Systems (ICDCS), 2011 31st International Conference
	on},
  year = {2011},
  pages = {446 -455},
  month = {june},
  abstract = {Interoperability remains a challenging and growing problem within
	distributed systems. A range of heterogeneous network and middleware
	protocols which cannot interact with one another are now widely used,
	for example, the set of remote method invocation protocols, and the
	set of service discovery protocols. In environments where systems
	and services are composed dynamically, e.g. pervasive computing and
	systems-of-systems, the protocols used by two systems wishing to
	interact is unknown until runtime and hence interoperability cannot
	be guaranteed. In such situations, dynamic solutions are required
	to identify the differences between heterogeneous protocols and generate
	middleware connectors (or bridges) that will allow the systems to
	inter operate. In this paper, we present the Starlink middleware,
	a general framework into which runtime generated interoperability
	logic (in the form of higher level models) can be deployed to connect
	two heterogeneous protocols. For this, it provides: i) an abstract
	representation of network messages with a corresponding generic parser
	and composer, ii) an engine to execute coloured automata that represent
	the required interoperability behaviour between protocols, and iii)
	translation logic to describe the exchange of message content from
	one protocol to another. We show through case-study based evaluation
	that Starlink can bridge heterogeneous protocol types. Starlink is
	also compared against base-line protocol benchmarks to show that
	acceptable performance can still be achieved in spite of the high-level
	nature of the solution.},
  doi = {10.1109/ICDCS.2011.65},
  issn = {1063-6927},
  keywords = {Starlink middleware;abstract representation;coloured automata;distributed
	system;heterogeneous middleware protocol;heterogeneous network;remote
	method invocation protocol;runtime generated interoperability logic;runtime
	interoperability;service discovery protocol;translation logic;automata
	theory;middleware;open systems;protocols;}
}

@INPROCEEDINGS{923187,
  author = {Broom, B. and Fowler, R. and Kennedy, K.},
  title = {KelpIO: a telescope-ready domain-specific I/O library for irregular
	block-structured applications},
  booktitle = {Cluster Computing and the Grid, 2001. Proceedings. First IEEE/ACM
	International Symposium on},
  year = {2001},
  pages = {148 -155},
  abstract = {To ameliorate the need to spend significant programmer time modifying
	parallel programs to achieve high-performance, while maintaining
	compact, comprehensible source codes, the paper advocates the use
	of telescoping language technology to automatically apply, during
	the normal compilation process, high-level performance enhancing
	transformations to applications using a high-level domain-specific
	I/O library. We believe that this approach will be more acceptable
	to application developers than new language extensions, but will
	be just as amenable to optimization by advanced compilers, effectively
	making it a domain-specific language extension for I/O. The paper
	describes a domain-specific I/O library for irregular block-structured
	applications based on the KeLP library, describes high-level transformations
	of the library primitives for improving performance, and describes
	how a high-level domain-specific optimizer for applying these transformations
	could be constructed rising the telescoping languages framework},
  doi = {10.1109/CCGRID.2001.923187},
  keywords = {KeLP library;KelpIO;advanced compilers;application developers;comprehensible
	source codes;domain-specific I/O library;domain-specific language
	extension;high-level domain-specific I/O library;high-level domain-specific
	optimizer;high-level performance enhancing transformations;high-level
	transformations;high-performance;irregular block-structured applications;library
	primitives;normal compilation process;parallel programs;telescope-ready
	domain-specific I/O library;telescoping language technology;telescoping
	languages framework;high level languages;input-output programs;optimising
	compilers;parallel programming;software libraries;}
}

@INPROCEEDINGS{17203,
  author = {Bruno, W.F. and Narayanaswami, G. and Aoyama, M. and Chang, C.K.},
  title = {A knowledge-based system approach to the development of a system
	functional requirement specification processor},
  booktitle = {Computer Software and Applications Conference, 1988. COMPSAC 88.
	Proceedings., Twelfth International},
  year = {1988},
  pages = {387 -394},
  month = {oct},
  abstract = {The authors have designed a system functional requirement (SFR) specification
	processor using a knowledge-based system approach in conjunction
	with other artificial intelligence techniques, this processor can
	be used for a variety of software systems. Domain-specific expert
	knowledge is defined for each software system application. The adopted
	specification language can easily be read by a variety of people
	such as customers, managers, requirement writers, software designers,
	and the test team. The processor, for a given software system application,
	not only checks for consistency, completeness, nonredundancy, and
	nonambiguity, but also uses domain-specific expert knowledge to verify
	that the SFR specification document is correct from the domain expert's
	point of view. To illustrate the proposed approach, the authors give
	an example of a component of a telephone switching system and the
	processor system methodology},
  doi = {10.1109/CMPSAC.1988.17203},
  keywords = {artificial intelligence techniques;completeness;consistency;domain-specific
	expert knowledge;knowledge-based system;nonambiguity;nonredundancy;processor
	system methodology;software system application;specification document
	verification;specification language;system functional requirement
	specification processor;telephone switching system;electronic switching
	systems;formal specification;knowledge based systems;software tools;specification
	languages;telecommunications computing;}
}

@INPROCEEDINGS{1510101,
  author = {Bruntink, M. and van Deursen, A. and Tourwe, T.},
  title = {Isolating idiomatic crosscutting concerns},
  booktitle = {Software Maintenance, 2005. ICSM'05. Proceedings of the 21st IEEE
	International Conference on},
  year = {2005},
  pages = { 37 - 46},
  month = {sept.},
  abstract = { This paper reports on our experience in automatically migrating the
	crosscutting concerns of a large-scale software system, written in
	C, to an aspect-oriented implementation. We present a systematic
	approach for isolating crosscutting concerns, and illustrate this
	approach by zooming in on one particular crosscutting concern. Additionally,
	we compare the already existing solution to the aspect-oriented solution,
	and discuss advantages as well as disadvantages of both in terms
	of selected quality attributes. Our results show that automated migration
	is feasible, and that adopting an aspect-oriented approach can lead
	to significant improvements in source code quality, if carefully
	designed and managed.},
  doi = {10.1109/ICSM.2005.57},
  issn = {1063-6773 },
  keywords = { C language; aspect-oriented implementation; idiomatic crosscutting
	concern isolation; large-scale software system; source code quality;
	C language; embedded systems; object-oriented programming; software
	process improvement; software quality;}
}

@INPROCEEDINGS{4159851,
  author = {Buckl, Christian and Regensburger, Matthias and Knoll, Alois and
	Schrott, Gerhard},
  title = {Models for automatic generation of safety-critical real-time systems},
  booktitle = {Availability, Reliability and Security, 2007. ARES 2007. The Second
	International Conference on},
  year = {2007},
  pages = {580 -587},
  month = {april},
  abstract = {Model-based development has become state of the art in software engineering.
	A number of tools, like Mat-lab/Simulink or SCADE, are available
	for the automatic generation of application code on basis of models.
	Unfortunately, system aspects like process management, communication
	or fault-tolerance mechanisms are not covered by these tools. One
	main reason is the non-existence of appropriate models with an explicit
	semantic to allow the automatic code generation. In addition, there
	is a great need to have the possibility to extend both the model
	and the code generation abilities to allow a high coverage of the
	used platforms, since such code is platform dependent. In this paper,
	we will present an approach applying meta code generators using template-based
	code generation to achieve this extensibility and will discuss the
	properties of models required for the use in model-based development
	of system aspects for safety-critical real-time systems},
  doi = {10.1109/ARES.2007.106},
  keywords = {Matlab;SCADE;Simulink;application code;automatic code generation;metacode
	generators;model-based development;process management;safety-critical
	real-time systems;software engineering;formal specification;program
	compilers;safety-critical software;}
}

@INPROCEEDINGS{5533443,
  author = {Budiselic, Ivan and Zuzak, Ivan and Benc, Ivan},
  title = {Application middleware for convergence of IP Multimedia system and
	Web Services},
  booktitle = {MIPRO, 2010 Proceedings of the 33rd International Convention},
  year = {2010},
  pages = {507 -512},
  month = {may},
  abstract = {Current network applications are typically created for one of two
	worlds. Communication applications targeting mobile devices usually
	communicate using the SIP protocol and are integrated into IP Multimedia
	systems of mobile network operators. On the other hand, applications
	targeting the enterprise market typically adhere to the SOAP protocol
	and integrate with Web Services exposed on the Internet. However,
	existing and future applications would benefit from access to services
	exposed by both of these protocols in both the mobile network and
	the Internet. In this paper we present the architecture of an application
	middleware that acts as a bidirectional gateway among IP Multimedia
	and Web Services systems. The middleware provides infrastructure
	for SIP and SOAP message handling, and session and network resource
	management. The middleware exposes interfaces for defining application
	specific rules for communication between protocol domains. Lastly,
	we outline a domain specific language that simplifies definition
	of such rules.}
}

@INPROCEEDINGS{927267,
  author = {Buffenberger, J. and Gruell, K.},
  title = {A language for software subsystem composition},
  booktitle = {System Sciences, 2001. Proceedings of the 34th Annual Hawaii International
	Conference on},
  year = {2001},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { A software system often consists of thousands of source files, which
	must be translated into thousands of intermediate files, which eventually
	must be translated into some small number of library and executable
	files. Collectively, these steps compose its build process. A large
	software system can be difficult to build. The steps can be numerous
	and complex. Of course, there are a variety of tools to assist us
	(e.g. MAKE), but their languages emphasize the specification of low-level
	details (e.g. compiler names and options), rather than high-level
	attributes (e.g. host/target platforms and required subsystems).
	This paper describes a new domain-specific language for specifying
	the composition and construction of a software system, where the
	emphasis is on high-level attributes. A specification is processed
	by a pipeline of fairly simple tools to produce a set of makefiles,
	which are then processed by MAKE in the usual way.},
  doi = {10.1109/HICSS.2001.927267},
  issn = { },
  keywords = { MAKE tool; build process; domain-specific language; executable files;
	high-level attributes; host platforms; intermediate files; large
	software systems; library files; makefiles; pipeline; required subsystems;
	software subsystem composition language; software system specification;
	source files; target platforms; large-scale systems; specification
	languages; subroutines;}
}

@INPROCEEDINGS{4159670,
  author = {Bui, N.B. and Zhu, L. and Gorton, I. and Liu, Y.},
  title = {Benchmark Generation Using Domain Specific Modeling},
  booktitle = {Software Engineering Conference, 2007. ASWEC 2007. 18th Australian},
  year = {2007},
  pages = {169 -180},
  month = {april},
  abstract = {Performance benchmarks are domain specific applications that are specialized
	to a certain set of technologies and platforms. The development of
	a benchmark application requires mapping the performance specific
	domain concepts to an implementation and producing complex technology
	and platform specific code. Domain specific modeling (DSM) promises
	to bridge the gap between application domains and implementations
	by allowing designers to specify solutions in domain-specific abstractions
	and semantics through domain specific languages (DSL). This allows
	generation of a final implementation automatically from high level
	models. The modeling and task automation benefits obtained from this
	approach usually justify the upfront cost involved. This paper employs
	a DSM based approach to invent a new DSL, DSLBench, for benchmark
	generation. DSLBench and its associated code generation facilities
	allow the design and generation of a completely deployable benchmark
	application for performance testing from a high level model. DSLBench
	is implemented using Microsoft domain specific language toolkit.
	It is integrated with the Visual Studio 2005 Team Suite as a plug-in
	to provide extra modeling capabilities for performance testing. We
	illustrate the approach using a case study based on .Net and C#.},
  doi = {10.1109/ASWEC.2007.13},
  issn = {1530-0803},
  keywords = {DSLBench;Microsoft domain specific language toolkit;Visual Studio
	2005 Team Suite;benchmark generation;code generation;domain specific
	modeling;domain-specific abstractions;task automation;object-oriented
	programming;program compilers;program testing;software architecture;software
	performance evaluation;software tools;specification languages;}
}

@INPROCEEDINGS{4609424,
  author = {Buisson, J. and Dagnat, F.},
  title = {Experiments with Fractal on Modular Reflection},
  booktitle = {Software Engineering Research, Management and Applications, 2008.
	SERA '08. Sixth International Conference on},
  year = {2008},
  pages = {179 -186},
  month = {aug.},
  abstract = {In most reflective systems, the model of reflection objects often
	mirrors (a part of) the metamodel of the system. As a result, reflection
	is commonly tightly bound to the rest of the system. In this paper,
	we investigate the loosening of that coupling. With the rise of domain-specific
	modeling the need for separation of concerns and reuse when designing
	metamodels become critical. Therefore, we advocate the use of general
	design patterns abstracting the details of modeling languages when
	working on cross-cutting concerns (such as reflection) of a metamodel.
	Once the abstract patterns for reflection are built, they are mapped
	onto concrete modeling languages thanks to model engineering tools.
	In this paper, we apply this approach to the fractal component model.
	Following this process, reflection mechanisms built at the abstract
	level are straightforwardly reused and the resulting reflection system
	gains modularity.},
  doi = {10.1109/SERA.2008.19},
  keywords = {cross-cutting concerns;design patterns;domain-specific modeling;engineering
	tools;fractal component model;metamodels;modeling languages;modular
	reflection;reflective systems;fractals;object-oriented methods;software
	reusability;software tools;specification languages;}
}

@INPROCEEDINGS{5967523,
  author = {Bukhari, A.C. and Fareedi, A.A. and Yong Gi Kim},
  title = {HO2IEV: Heavyweight ontology based web information extraction technique
	for visionless users},
  booktitle = {Networked Computing and Advanced Information Management (NCM), 2011
	7th International Conference on},
  year = {2011},
  pages = {90 -95},
  month = {june},
  abstract = {As the internet grows rapidly, millions of web pages are being added
	on a daily basis. The extraction of precise information is becoming
	more and more difficult as the volume of data on the internet increases.
	Several search engines and information fetching tools are available
	on the internet, all of which claim to provide the best crawling
	facilities. For the most part, these search engines are keyword based.
	This poses a problem for visually impaired people who want to get
	the full use from online resources available to other users. Visually
	impaired users require special aid to get along with any given computer
	system. Interface and content management are no exception, and special
	tools are required to facilitate the extraction of relevant information
	from the internet for visually impaired users. The HO2IEV (Heavyweight
	Ontology Based Information Extraction for Visually impaired User)
	architecture provides a mechanism for highly precise information
	extraction using heavyweight ontology and built-in vocal command
	system for visually impaired internet users. Our prototype intelligent
	system not only integrates and communicates among different tools,
	such as voice command parsers, domain ontology extractors and short
	message engines, but also introduces an autonomous mechanism of information
	extraction (hereafter referred to as IE) using heavyweight ontology.
	In this paper we designed domain specific heavyweight ontology using
	OWL (Web Ontology Language) for ontology modeling and PAL (Prote
	#x0301;ge #x0301; Axiom Language) for axiom writing. We introduced
	a novel autonomous mechanism for IE by developing prototype software.
	A series of experiments were designed for the testing and analysis
	of the performance of heavyweight ontology in general, and our information
	extraction prototype specifically.},
  keywords = {HO2IEV;Internet;OWL;PAL;Prote #x0301;ge #x0301; Axiom Language;Web
	Ontology Language;axiom writing;content management;crawling facilities;domain
	ontology extractors;heavyweight ontology based information extraction
	for visually impaired user;information fetching tools;interface management;online
	resources;prototype intelligent system;search engines;short message
	engines;voice command parsers;web pages;Internet;knowledge representation
	languages;ontologies (artificial intelligence);search engines;}
}

@INPROCEEDINGS{4117723,
  author = {Bulatewicz, T. and Cuny, J.},
  title = {A Domain-Specific Language for Model Coupling},
  booktitle = {Simulation Conference, 2006. WSC 06. Proceedings of the Winter},
  year = {2006},
  pages = {1091 -1100},
  month = {dec.},
  abstract = {There is an increasing need for the comprehensive simulation of complex,
	dynamic, physical systems. Often such simulations are built by coupling
	existing, component models so that their concurrent simulations affect
	each other. The process of model coupling is, however, a nontrivial
	task that is not adequately supported by existing frameworks. To
	provide better support, we have developed an approach to model coupling
	that uses high level model interfaces called potential coupling interfaces.
	In this work, we present a visual, domain-specific language for model
	coupling, called the coupling description language, based on these
	interfaces. We show that it supports the resolution of model incompatibilities
	and allows for the fast-prototyping of coupled models},
  doi = {10.1109/WSC.2006.323199},
  keywords = {coupling description language;domain-specific language;model coupling;potential
	coupling interfaces;digital simulation;high level languages;}
}

@INPROCEEDINGS{501126,
  author = {Bull, T.},
  title = {Comprehension of safety-critical systems using domain-specific languages},
  booktitle = {Program Comprehension, 1996, Proceedings., Fourth Workshop on},
  year = {1996},
  pages = {108 -122},
  month = {mar},
  abstract = {Software, which is expressed in a programming language, seeks to solve
	real-world problems, which are expressed in terms of physical objects
	and laws. Thus, software, unlike traditionally-engineered artifacts,
	uses quite disjoint concepts in the problem and solution spaces.
	A key difficulty of software comprehension is understanding the relationship
	between these levels. We argue that we can nevertheless bridge certain
	aspects of this gap in software design and construction. This is
	particularly important when the software is safety-critical. Our
	strategy is to make explicit the connection between the physical
	world and the machine implementation, by using formally-defined domain-specific
	languages, based on program transformations. The design criteria
	for such languages are described, a simple language is shown as an
	example, and comparisons are made with similar work},
  doi = {10.1109/WPC.1996.501126},
  keywords = {domain-specific languages;program transformations;program understanding;programming
	language;safety-critical systems;software comprehension;software
	design;software engineering;program diagnostics;reverse engineering;safety-critical
	software;specification languages;}
}

@INPROCEEDINGS{4556612,
  author = {Bunch, L. and Bradshaw, J.M. and Young, C.O.},
  title = {Policy-Governed Information Exchange in a U.S. Army Operational Scenario},
  booktitle = {Policies for Distributed Systems and Networks, 2008. POLICY 2008.
	IEEE Workshop on},
  year = {2008},
  pages = {243 -244},
  month = {june},
  abstract = {The authors are investigating how emerging policy and semantic Web
	technologies can be used to help provide the best set of available
	tactical information to the Soldier in the field. In this initial
	effort, Researchers from the U.S. Army Research Labs (ARL) and the
	Florida Institute for Human and Machine Cognition (IHMC) have developed
	a system that demonstrates the potential of these technologies in
	a small-scale U.S. army mockup scenario. The system represents and
	reasons about domain-specific policies to help recognize what documents
	the end soldier is allowed to receive given the current mission context.
	The system also relies on policies to help recognize when appropriate
	human approval can be obtained or a specific transformation of the
	information can be performed to allow the information to be sent.
	Semantic Web technologies are further used to describe the properties
	and features of each document and relate these features to mission
	contexts in which the information is likely to be appropriate. The
	result is a compelling demonstration of the role that policies and
	semantic Web technologies can play in promoting the Army's need to
	share information while remaining vigilant of the requirements to
	protect methods and sources.},
  doi = {10.1109/POLICY.2008.26},
  keywords = {Florida Institute for Human and Machine Cognition;US Army Research
	Labs;US Army operational scenario;domain-specific policies;information
	sharing;policy-governed information exchange;semantic Web technologies;tactical
	information;electronic data interchange;military computing;semantic
	Web;}
}

@ARTICLE{671084,
  author = {Buntine, W. and Norvig, P. and Van Baalen, J. and Spiegelhalter,
	D. and Thomas, A.},
  title = {Will domain-specific code synthesis become a silver bullet?},
  journal = {Intelligent Systems and their Applications, IEEE},
  year = {1998},
  volume = {13},
  pages = {9 -15},
  number = {2},
  month = {mar/apr},
  abstract = {In the commercial world, the biggest hurdles to delivering embedded
	intelligence in software are the software-engineering costs. We members
	of the intelligent-systems community have brilliant techniques available,
	and no end of applications is in sight, but the gap between intelligent-systems
	techniques and delivered software is too great. Standard programming
	languages and tools are too general-purpose, and the standard means
	of delivering a technology-as code libraries or new-fangled componentware-is
	simply too restricting for some specialties. One way would be through
	domain-specific code synthesis. Support for the development of domain-specific
	extensions to programming languages and of adaptive software-where
	code synthesis is in the inner-loop of a self-modifying programming
	system-would have a profound effect in this regard. This is the future
	of intelligent systems development},
  doi = {10.1109/5254.671084},
  issn = {1094-7167},
  keywords = {code synthesis;domain-specific code synthesis;domain-specific extensions;intelligent
	systems development;intelligent-systems techniques;software-engineering;software
	development management;software engineering;}
}

@INPROCEEDINGS{1612858,
  author = {Bunus, P.},
  title = {A simulation and decision framework for selection of numerical solvers
	in scientific computing},
  booktitle = {Simulation Symposium, 2006. 39th Annual},
  year = {2006},
  pages = { 10 pp.},
  month = {april},
  abstract = { Selecting the right numerical solver or the most appropriate numerical
	package for a particular simulation problem it is increasingly difficult
	for users without an extensive mathematical background and deeper
	knowledge in numerical analysis. In this paper, we propose a model-driven
	combined decision-simulation framework for automatically selecting
	a numerical method for a given set of equation system. We also propose
	a formal paradigm based on domain-specific languages for specification
	of structural and behavioral aspects of the numerical equation solving
	process. Starting from a declarative description of the equation
	system that need to be solved, our system is able to detect the nature
	of the equations, perform symbolic manipulations of the equations,
	and transform them into a domain-specific model. We describe the
	motivation for such a system, its main features, and a prototype
	environment together with a usage example.},
  doi = {10.1109/ANSS.2006.9},
  issn = {1080-241X},
  keywords = { decision-simulation framework; domain-specific language; equation
	system; numerical equation solving process; numerical package; numerical
	solver; scientific computing; symbolic manipulation; decision making;
	mathematics computing; numerical analysis; symbol manipulation;}
}

@INPROCEEDINGS{4464018,
  author = {Bures, T. and Malohlava, M. and Hnetynka, P.},
  title = {Using DSL for Automatic Generation of Software Connectors},
  booktitle = {Composition-Based Software Systems, 2008. ICCBSS 2008. Seventh International
	Conference on},
  year = {2008},
  pages = {138 -147},
  month = {feb.},
  abstract = {Component-based engineering is a recognized paradigm, which models
	an application as a collection of reusable components. The key idea
	behind components is that they contain only the business logic and
	communicate with one another only via well-defined interfaces. The
	communication paths among components (so called bindings) are in
	modern component systems realized by software connectors, which allow
	explicit modeling of communication and also its implementation at
	runtime. An important aspect of using connectors is the possibility
	of their automatic generation, which saves a significant amount of
	development work. However, the generation itself is not a trivial
	task, since there is a big semantic gap between the abstract specification
	of a connector at design time and its implementation at runtime.
	In this paper, we present an approach to generating implementations
	of software connectors. The approach is based on a new domain specific
	language for describing templates of connector implementations and
	a transformation framework using the Strate-go/XT term rewriting
	system for generating source code of connectors.},
  doi = {10.1109/ICCBSS.2008.17},
  keywords = {business logic;component-based engineering;connectors source code;domain
	specific language;modern component systems;reusable components collection;rewriting
	system;software connectors automatic generation;rewriting systems;software
	reusability;}
}

@INPROCEEDINGS{4024413,
  author = {Burgy, L. and Consel, C. and Latry, F. and Lawall, J. and Palix,
	N. and Reveillere, L.},
  title = {Language Technology for Internet-Telephony Service Creation},
  booktitle = {Communications, 2006. ICC '06. IEEE International Conference on},
  year = {2006},
  volume = {4},
  pages = {1795 -1800},
  month = {june },
  abstract = {Telephony is evolving at a frantic pace, critically relying on the
	development of services to offer a host of new functionalities. However,
	programming Internet telephony services requires an intimate knowledge
	of a variety of protocols and technologies, which can be a challenge
	for many programmers. Furthermore, because telephony is a resource
	heavily relied on, programmability of telephony platforms should
	not compromise their robustness. This paper presents an approach
	to creating telephony services that builds on programming language
	technology (i.e., language design and implementation, language semantics,
	and program analysis). We have developed a language, named Session
	Processing Language (SPL), that offers domain-specific constructs,
	abstracting over the intricacies of the underlying technologies.
	By design, SPL guarantees critical properties that cannot be verified
	in general-purpose languages. SPL relies on a Service Logic Execution
	Environment for SIP (SIP-SLEE) that introduces a design framework
	for service development based around the notion of session. SPL and
	SIP-SLEE have been implemented and they are now being used to develop
	and deploy real services, demonstrating the practical benefits of
	our approach.},
  doi = {10.1109/ICC.2006.254980},
  issn = {8164-9547}
}

@ARTICLE{5487528,
  author = {Burgy, L. and Reveillere, L. and Lawall, J. and Muller, G.},
  title = {Zebu: A Language-Based Approach for Network Protocol Message Processing},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2011},
  volume = {37},
  pages = {575 -591},
  number = {4},
  month = {july-aug. },
  abstract = {A network application communicates with other applications according
	to a set of rules known as a protocol. This communication is managed
	by the part of the application known as the protocol-handling layer,
	which enables the manipulation of protocol messages. The protocol-handling
	layer is a critical component of a network application since it represents
	the interface between the application and the outside world. It must
	thus satisfy two constraints: It must be efficient to be able to
	treat a large number of messages and it must be robust to face various
	attacks targeting the application itself or the underlying platform.
	Despite these constraints, the development process of this layer
	still remains rudimentary and requires a high level of expertise.
	It includes translating the protocol specification written in a high-level
	formalism such as ABNF toward low-level code such as C. The gap between
	these abstraction levels can entail many errors. This paper proposes
	a new language-based approach to developing protocol-handling layers,
	to improve their robustness without compromising their performance.
	Our approach is based on the use of a domain-specific language, Zebu,
	to specify the protocol-handling layer of network applications that
	use textual HTTP-like application protocols. The Zebu syntax is very
	close to that of ABNF, facilitating the adoption of Zebu by domain
	experts. By annotating the original ABNF specification of a protocol,
	the Zebu user can dedicate the protocol-handling layer to the needs
	of a given application. The Zebu compiler first checks the annotated
	specification for inconsistencies, and then generates a protocol-handling
	layer according to the annotations. This protocol-handling layer
	is made up of a set of data structures that represent a message,
	a parser that fills in these data structures, and various stub functions
	to access these data structures or drive the parsing of a message.},
  doi = {10.1109/TSE.2010.64},
  issn = {0098-5589},
  keywords = {ABNF;Internet era;Zebu compiler;Zebu syntax;data structures;domain-specific
	languages;language-based approach;message parsing;network protocol
	message processing;protocol specification;protocol-handling layer;textual
	HTTP-like application protocols;Internet;computational linguistics;formal
	specification;program compilers;protocols;specification languages;telecommunication
	computing;}
}

@INPROCEEDINGS{4365692,
  author = {Burgy, L. and Reveillere, L. and Lawall, J.L. and Muller, G.},
  title = {A Language-Based Approach for Improving the Robustness of Network
	Application Protocol Implementations},
  booktitle = {Reliable Distributed Systems, 2007. SRDS 2007. 26th IEEE International
	Symposium on},
  year = {2007},
  pages = {149 -160},
  month = {oct.},
  abstract = {The secure and robust functioning of a network relies on the defect-free
	implementation of network applications. As network protocols have
	become increasingly complex, however, hand-writing network message
	processing code has become increasingly error-prone. In this paper,
	we present a domain-specific language, Zebu, for generating robust
	and efficient message processing layers. A Zebu specification, based
	on the notation used in RFCs, describes protocol message formats
	and related processing constraints. Zebu-based applications are efficient,
	since message fragments can be specified to be processed on demand.
	Zebu-based applications are also robust, as the Zebu compiler automatically
	checks specification consistency and generates parsing stubs that
	include validation of the message structure. Using a message torture
	suite in the context of SIP and RTSP, we show that Zebu-generated
	code is both complete and defect-free.},
  doi = {10.1109/SRDS.2007.26},
  issn = {1060-9857},
  keywords = {domain-specific languages;message processing;network application protocol
	implementations;protocols;}
}

@INPROCEEDINGS{919165,
  author = {Butler, G. and Batory, D. and Czarnecki, K. and Eisenecker, U.},
  title = {Generative techniques for product lines},
  booktitle = {Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International
	Conference on},
  year = {2001},
  pages = {760 -761},
  abstract = {Not available},
  doi = {10.1109/ICSE.2001.919165},
  issn = {0270-5257}
}

@INPROCEEDINGS{638026,
  author = {Gui Cabral and DeBellis, M.},
  title = {Domain-specific Representations In The KBSA Concept Demo},
  booktitle = {Knowledge-Based Software Engineering Conference, 1991. Proceedings.,
	6th Annual},
  year = {1991},
  pages = {97 -106},
  month = {sep},
  abstract = {Not available},
  doi = {10.1109/KBSE.1991.638026}
}

@INPROCEEDINGS{5566223,
  author = {Li Cai and Cheng Lu and Chunjie Zhang},
  title = {Privacy Domain-Specific Ontology Building and Consistency Analysis},
  booktitle = {Internet Technology and Applications, 2010 International Conference
	on},
  year = {2010},
  pages = {1 -6},
  month = {aug.},
  abstract = {With the rapid development of the Internet, people increasingly pay
	attention to the users' online privacy issues. And so , building
	privacy domain-specific ontology is a necessary method for using
	and protecting privacy data among computer systems. Ontology is an
	explicit conceptualization for specific domain and is used in natural
	language processing, information retrieval, knowledge sharing and
	other fields widely. This paper introduces the principles and steps
	of building a privacy ontology, and studies a basic concept of privacy
	ontology and describes the ontology with OWL, and then gives the
	methods system of the privacy ontology establishment. At the same
	time, this paper makes a preliminary analysis of the consistency
	of privacy ontology. The privacy ontology building realizes knowledge
	sharing and the reuse of privacy domain and helps to protect and
	manage the online privacy of Internet users.},
  doi = {10.1109/ITAPP.2010.5566223},
  keywords = {Internet;OWL;consistency analysis;knowledge sharing;privacy data protection;privacy
	domain-specific ontology building;users online privacy issues;Internet;data
	integrity;data privacy;knowledge representation languages;ontologies
	(artificial intelligence);}
}

@INPROCEEDINGS{581980,
  author = {Calmet, J. and Jekutsch, S. and Schu, J.},
  title = {A generic query-translation framework for a mediator architecture
	},
  booktitle = {Data Engineering, 1997. Proceedings. 13th International Conference
	on},
  year = {1997},
  pages = {434 -443},
  month = {apr},
  abstract = {A mediator is a domain-specific tool to support uniform access to
	multiple heterogeneous information sources and to abstract and combine
	data from different but related databases to gain new information.
	This middleware product is urgently needed for these frequently occurring
	tasks in a decision support environment. In order to provide a front
	end, a mediator usually defines a new language. If an application
	or a user submits a question to the mediator, it has to be decomposed
	into several queries to the underlying information sources. Since
	these sources can only be accessed using their own query language,
	a query translator is needed. This paper presents a new approach
	for implementing query translators. It supports conjunctive queries
	as well as negation. Care is taken to enable information sources
	of which processing capabilities do not allow conjunctive queries
	in general. Rapid implementation is guided by reusing previously
	prepared code. The specification of the translator is done declaratively
	and domain-independently},
  doi = {10.1109/ICDE.1997.581980},
  keywords = {code reuse;conjunctive queries;data abstraction;data combination;decision
	support environment;declarative domain-independent specification;domain-specific
	tool;front end;generic query-translation framework;information source
	accessing;language definition;mediator architecture;middleware product;multiple
	heterogeneous information sources;negation;query decomposition;query
	language;query translators;rapid implementation;related databases;uniform
	access;decision support systems;deductive databases;distributed databases;online
	front-ends;query languages;query processing;software reusability;}
}

@INPROCEEDINGS{10180,
  author = {Cameron, R.D.},
  title = {The introspection technique in maintenance metaprogramming},
  booktitle = {Software Maintenance, 1988., Proceedings of the Conference on},
  year = {1988},
  pages = {298 -301},
  month = {oct},
  abstract = {Deals with a specific metaprogramming technique which is useful in
	the maintenance of data-driven software. Data-driven software includes
	programs whose algorithms are controlled by tables of data, such
	as table-driven parsers. A maintenance metaprogram for such software
	must have the ability to inspect and process both the program source
	code as well as the data tables developed by the software at run
	time. The easiest way to make the data tables available to the maintenance
	metaprogram is to run the original program until the data tables
	are developed in memory. Control is then passed to the maintenance
	metaprogram, which inspects these tables and the source code of the
	original program to carry out the maintenance operations. In essence,
	this involves the construction of a hybrid cross between the original
	program and the maintenance metaprogram. A case study is considered
	in which the introspection technique was used in maintenance of a
	metaprogramming system},
  doi = {10.1109/ICSM.1988.10180},
  keywords = {data-driven software;hybrid cross;introspection technique;maintenance
	metaprogramming;program source code;table-driven parsers;software
	engineering;software tools;}
}

@INPROCEEDINGS{564993,
  author = {Canfora, G. and Cimitile, A. and De Lucia, A.},
  title = {Specifying code analysis tools},
  booktitle = {Software Maintenance 1996, Proceedings., International Conference
	on},
  year = {1996},
  pages = {95 -103},
  month = {nov},
  abstract = {Customised code analysis tools for the maintenance and evolution of
	existing software systems can be created by storing program information
	in a database, and using an application generator to translate the
	high-level specifications of the analyses the tools are intended
	to perform. We present a high-level domain-specific language for
	the specification of program analysis tools that exploit an algebraic
	program representation called F(p). The algebraic representation
	is a compact program view which describes the static composition
	of the control structures and the set of the resulting potential
	executions. Operands of the algebraic expression (that represent
	the program's constructs) are used as indexes to access information
	stored in the database. The specification language provides facilities
	for the traversal of the program representation and access to the
	associated information in the database. The program model and the
	analysis results are integrated into a unique conceptual model, thus
	simplifying the reuse of the results of an analysis and the integration
	of the tools},
  doi = {10.1109/ICSM.1996.564993},
  issn = {1063-6773},
  keywords = {algebraic expression operands;algebraic program representation;analysis
	results reuse;application generator;code analysis tool specification;compact
	program view;conceptual model;control structures;database;high-level
	domain-specific language;high-level specification translation;indexes;information
	access;potential executions;program analysis tools;program information
	storage;software maintenance;software systems evolution;static composition;tools
	integration;algebraic specification;application generators;database
	management systems;indexing;program control structures;program diagnostics;software
	maintenance;software tools;specification languages;}
}

@INPROCEEDINGS{4228020,
  author = {Cao, J. and Goyal, A. and Midkiff, S.P. and Caruthers, J.M.},
  title = {An Optimizing Compiler for Parallel Chemistry Simulations},
  booktitle = {Parallel and Distributed Processing Symposium, 2007. IPDPS 2007.
	IEEE International},
  year = {2007},
  pages = {1 -10},
  month = {march},
  abstract = {Well designed domain specific languages enable the easy expression
	of problems, the application of domain specific optimizations, and
	dramatic improvements in productivity for their users. In this paper
	we describe a compiler for polymer chemistry, and in particular rubber
	chemistry, that achieves all of these goals. The compiler allows
	the development of a system of ordinary differential equations describing
	a complex rubber reaction - a task that used to require months -
	to be done in days. The generated code, like much machine generated
	code, is more complex than human written code, and stresses commercial
	compilers to the point of failure. However, because of knowledge
	of the form of ODEs generated, the compiler can perform specialized
	common sub-expression and other algebraic optimizations that simplifies
	the code sufficiently to allow it to be compiled (eliminating all
	but 6.9% of the operations in our largest program) and to provide
	five times faster performance on our largest benchmark codes.},
  doi = {10.1109/IPDPS.2007.370292},
  keywords = {domain specific languages;optimizing compiler;ordinary differential
	equations;parallel chemistry simulations;polymer chemistry;rubber
	chemistry;chemistry computing;differential equations;digital simulation;formal
	specification;mathematics computing;optimising compilers;parallel
	programming;rubber;}
}

@ARTICLE{5076454,
  author = {Lan Cao and Ramesh, B. and Rossi, M.},
  title = {Are Domain-Specific Models Easier to Maintain Than UML Models?},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {19 -21},
  number = {4},
  month = {july-aug. },
  abstract = {Although domain-specific modeling (DSM) languages have been adopted
	in industries such as telecommunications and insurance, they haven't
	yet gained wide acceptance in practice. This is because the claims
	of increased productivity and ease of understanding haven't yet been
	verified by independent studies. To address this concern, we examined
	a DSM language's performance for maintenance tasks. Maintenance in
	software-intensive systems is critical because software often continuously
	evolves during development as well as after delivery, to meet users'
	ever-changing needs. So, maintenance performance significantly impacts
	software development productivity.Experimental results show that
	maintenance can be significantly easier and faster with a DSM language
	than with a general- purpose modeling language.},
  doi = {10.1109/MS.2009.87},
  issn = {0740-7459},
  keywords = {UML model;domain-specific modeling language performance;software development
	productivity;software-intensive system maintenance;software architecture;software
	maintenance;software performance evaluation;specification languages;}
}

@INPROCEEDINGS{6004515,
  author = {Yan Cao and Qiuzi Lu and Tianhua Xu and Tao Tang and Haifeng Wang
	and Yongcheng Xu},
  title = {Integrating DSL-CBI and NuSMV for Modeling and Verifiying Interlocking
	Systems},
  booktitle = {Secure Software Integration Reliability Improvement Companion (SSIRI-C),
	2011 5th International Conference on},
  year = {2011},
  pages = {136 -143},
  month = {june},
  abstract = {The Computer Based Interlocking System (CBI) is used to ensure safe
	train movements at a railway station. For a given station, all the
	train routes and the concrete safety rules associated with these
	are defined in the interlocking table. Currently, the development
	and verification of interlocking tables is entirely manual process,
	which is inefficient and error-prone due to the complexity of the
	CBI and the human interferences. Besides, the complexity and volume
	of the verification results tend to make users feel extremely non-understandable.
	In order to tackle these problems, we introduce a toolset based on
	Domain Specific Language for Computer Based Interlocking Systems
	(DSL-CBI) to automatically generate and verify the interlocking table,
	and then mark the conflicting routes in the railway station. In this
	paper, we also discuss the advantages of the toolset and the significant
	contribution in developing CBI based on the proposed toolset.},
  doi = {10.1109/SSIRI-C.2011.28},
  keywords = {DSL-CBI;NuSMV;computer based interlocking system;domain specific language;railway
	station;safe train movements;railway safety;specification languages;}
}

@INPROCEEDINGS{5952519,
  author = {Yan Cao and Tianhua Xu and Tao Tang and Haifeng Wang and Lin Zhao},
  title = {Automatic generation and verification of interlocking tables based
	on Domain Specific Language for Computer Based Interlocking Systems
	(DSL-CBI)},
  booktitle = {Computer Science and Automation Engineering (CSAE), 2011 IEEE International
	Conference on},
  year = {2011},
  volume = {2},
  pages = {511 -515},
  month = {june},
  abstract = {Interlocking tables, as the function specification of the Computer
	Based Interlocking System (CBI), play an important role in ensuring
	safe train movements at a railway station. The development and verification
	of interlocking tables is entirely manual process currently, which
	is inefficient and error-prone due to the complexity of the CBI and
	the human interferences. In order to tackle these problems, we introduce
	a toolset based on Domain Specific Language for Computer Based Interlocking
	Systems (DSL-CBI) to automatically generate and verify the interlocking
	table. In this paper, we address how to use the algorithm to automatically
	generate the interlocking table by inputting the XML file of the
	railway station designed by DSL-CBI, and how to use model checking
	to verify whether there are any conflicting settings in it. We also
	discuss the advantages of the toolset and the significant contribution
	in developing CBI based on the proposed toolset.},
  doi = {10.1109/CSAE.2011.5952519},
  keywords = {DSL-CBI;XML file;automatic generation;automatic verification;computer
	based interlocking systems;domain specific language;human-computer
	interferences;interlocking tables;manual process;model checking;railway
	station;safe train movements;XML;formal specification;formal verification;human
	computer interaction;railway engineering;railway safety;specification
	languages;}
}

@INPROCEEDINGS{607452,
  author = {Carlson, R. and Hunnicutt, S.},
  title = {Generic and domain-specific aspects of the Waxholm NLP and dialog
	modules},
  booktitle = {Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International
	Conference on},
  year = {1996},
  volume = {2},
  pages = {677 -680 vol.2},
  month = {oct},
  abstract = {Gives an overview of the NLP (natural language processing) and dialog
	components in the Waxholm spoken dialog system. We discuss how the
	dialog and the natural language components are modeled from a generic
	and a domain-specific point of view. Dialog management based on grammatical
	rules and lexical semantic features is implemented in our parser.
	The notation to describe the syntactic rules has been expanded to
	cover some of our special needs to model the dialog. The parser runs
	with two different time scales, corresponding to the words in each
	utterance and to the turns in the dialog. Topic selection is accomplished
	based on probabilities calculated from user initiatives. Results
	from parser performance and topic prediction are included in this
	paper},
  doi = {10.1109/ICSLP.1996.607452},
  keywords = {Waxholm spoken dialogue system;dialogue module;domain-specific aspects;generic
	aspects;grammatical rules;lexical semantic features;natural language
	processing module;parser performance;probabilities;time scales;topic
	prediction;turn-taking;user initiatives;utterances;words;grammars;natural
	language interfaces;natural languages;probability;speech recognition;speech
	synthesis;}
}

@INPROCEEDINGS{6007391,
  author = {Carlsson, C. and Fuller, R. and Fedrizzi, M.},
  title = {A hierarchical approach to assess keyword dependencies in fuzzy keyword
	ontologies},
  booktitle = {Fuzzy Systems (FUZZ), 2011 IEEE International Conference on},
  year = {2011},
  pages = {1799 -1804},
  month = {june},
  abstract = {The Knowledge Mobilization project (KNOWMO BILE) has been a joint
	effort by Institute for Advanced Management Systems Research, Abo
	Akademi University and VTT Technical Research Centre of Finland.
	Its goal was to better "mobilize" knowledge stored in heterogeneous
	databases for users with various backgrounds, geographical locations
	and situations. The working hypothesis of the project was that fuzzy
	mathematics combined with domain-specific data models, in other words,
	fuzzy ontologies, would help manage the uncertainty in finding information
	that matches the users needs. In this way, KNOWMOBILE places itself
	in the domain of knowledge management. In this paper we describe
	an industrial demonstration of fuzzy ontologies in information retrieval
	in the paper industry where problem solving reports are annotated
	with keywords and then stored in a database for later use. Furthermore,
	using Bellmann-Zadeh's principle to fuzzy decision-making we will
	show a method for identifying keyword dependencies in the keyword
	taxonomic tree.},
  doi = {10.1109/FUZZY.2011.6007391},
  issn = {1098-7584},
  keywords = {Bellmann-Zadeh principle;KNOWMOBILE;Knowledge Mobilization project;domain-specific
	data models;fuzzy decision making;fuzzy keyword ontologies;fuzzy
	mathematics;heterogeneous databases;hierarchical approach;information
	retrieval;keyword dependencies;knowledge management;uncertainty management;data
	models;decision making;distributed databases;fuzzy reasoning;fuzzy
	set theory;information retrieval;knowledge management;ontologies
	(artificial intelligence);uncertainty handling;}
}

@INPROCEEDINGS{5974257,
  author = {Carvalho, N. and Almeida, J.J. and Simoes, A.},
  title = {OML: A scripting approach for manipulating ontologies},
  booktitle = {Information Systems and Technologies (CISTI), 2011 6th Iberian Conference
	on},
  year = {2011},
  pages = {1 -6},
  month = {june},
  abstract = {There are different definitions for ontologies. Different knowledge
	areas tend to define ontologies in a different way. For computer
	science, an ontology can be used to describe, in a well defined and
	structured way, knowledge about a specific domain. These artifacts
	store rich information that can be reasoned about, this information
	can also be target of many structured processing functions. There
	is a diversity of programs that can be implemented to take advantage
	of these features and produce applications in every area of knowledge.
	The Ontology Manipulation Language (OML) is a Domain Specific Language
	(DSL) designed to describe and execute operations that reason about
	ontologies. These reasoning operations can be used to manipulate
	and maintain the current information in the ontology, infer new knowledge
	or concepts, or even produce any kind of side effect. OML is a simple
	and descriptive language, yet it is powerful enough to implement
	complex operations or reasoning engines in a clear and efficient
	way. To actually run programs written in OML a standalone compiler
	is available, as well as a mechanism for embedding OML programs in
	a generic programming language. This allows the quick development
	of applications that make use of ontologies, by describing ontology
	related operations in wove OML snippets throughout the code. This
	mechanism has proven to be a very effective and clear approach for
	taking advantage of adopting ontologies to represent information,
	while maintaining the implicit advantages of using a general-goal
	programming language.},
  keywords = {Domain Specific Language;OML;Ontology Manipulation Language;reasoning
	operations;inference mechanisms;knowledge representation languages;ontologies
	(artificial intelligence);}
}

@INPROCEEDINGS{6032482,
  author = {Cassou, D. and Balland, E. and Consel, C. and Lawall, J.},
  title = {Leveraging software architectures to guide and verify the development
	of sense/compute/control applications},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {431 -440},
  month = {may},
  abstract = {A software architecture describes the structure of a computing system
	by specifying software components and their interactions. Mapping
	a software architecture to an implementation is a well known challenge.
	A key element of this mapping is the architecture's description of
	the data and control-flow interactions between components. The characterization
	of these interactions can be rather abstract or very concrete, providing
	more or less implementation guidance, programming support, and static
	verification. In this paper, we explore one point in the design space
	between abstract and concrete component interaction specifications.
	We introduce a notion of interaction contract that expresses allowed
	interactions between components, describing both data and control-flow
	constraints. This declaration is part of the architecture description,
	allows generation of extensive programming support, and enables various
	verifications. We instantiate our approach in an architecture description
	language for Sense/Compute/Control applications, and describe associated
	compilation and verification strategies.},
  doi = {10.1145/1985793.1985852},
  issn = {0270-5257},
  keywords = {abstract component interaction specifications;concrete component interaction
	specifications;development verification;sense/compute/control applications;software
	architectures;software component specification;formal specification;formal
	verification;software architecture;}
}

@INPROCEEDINGS{5470550,
  author = {Cassou, D. and Bruneau, J. and Consel, C.},
  title = {A tool suite to prototype pervasive computing applications},
  booktitle = {Pervasive Computing and Communications Workshops (PERCOM Workshops),
	2010 8th IEEE International Conference on},
  year = {2010},
  pages = {820 -822},
  month = {29 2010-april 2},
  abstract = {Despite much progress, developing a pervasive computing application
	remains a challenge because of a lack of conceptual frameworks and
	supporting tools. This challenge involves coping with heterogeneous
	entities, overcoming the intricacies of distributed systems technologies,
	working out an architecture for the application, encoding it in a
	program, writing specific code to test the application, and finally
	deploying it. We present DiaSuite, a tool suite covering the development
	lifecycle of a pervasive computing system. This tool suite comprises
	a domain-specific design language, a compiler for this language,
	which produces a Java programming framework, an editor to define
	simulation scenarios, and a 2D-renderer to simulate pervasive computing
	applications. We have validated our tool suite on a variety of comprehensive
	applications in areas including telecommunications, building automation,
	and health-care.},
  doi = {10.1109/PERCOMW.2010.5470550},
  keywords = {2D-renderer;DiaSuite;Java programming framework;application testing;building
	automation;compiler;development lifecycle;distributed systems technology;domain-specific
	design language;editor;health-care;pervasive computing application
	simulation;prototype pervasive computing application;simulation scenarios;telecommunications;tool
	suite;Java;program compilers;program testing;rendering (computer
	graphics);software tools;ubiquitous computing;}
}

@ARTICLE{6051438,
  author = {Cassou, D. and Bruneau, J. and Consel, C. and Balland, E.},
  title = {Towards A Tool-Based Development Methodology for Pervasive Computing
	Applications},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2011},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {Despite much progress, developing a pervasive computing application
	remains a challenge because of a lack of conceptual frameworks and
	supporting tools. This challenge involves coping with heterogeneous
	devices, overcoming the intricacies of distributed systems technologies,
	working out an architecture for the application, encoding it in a
	program, writing specific code to test the application, and finally
	deploying it. This paper presents a design language and a tool suite
	covering the development life-cycle of a pervasive computing system.
	The design language allows to define a taxonomy of area-specific
	building-blocks, abstracting over their heterogeneity. This language
	also includes a layer to define the architecture of an application,
	following an architectural pattern commonly used in the pervasive
	computing domain. Our underlying methodology assigns roles to the
	stakeholders, providing separation of concerns. Our tool suite includes
	a compiler that takes design artifacts written in our language as
	input, and generates a programming framework that supports the subsequent
	development stages, namely implementation, testing and deployment.
	Our methodology has been applied on a wide spectrum of areas. Based
	on these experiments, we assess our approach through three criteria:
	expressiveness, usability and productivity.},
  doi = {10.1109/TSE.2011.107},
  issn = {0098-5589}
}

@INPROCEEDINGS{6078184,
  author = {Celikovic, Milan and Lukovic, Ivan and Aleksic, Slavica and Ivancevic,
	Vladimir},
  title = {A MOF based meta-model of IIS "CHARP"x2217;Case PIM concepts},
  booktitle = {Computer Science and Information Systems (FedCSIS), 2011 Federated
	Conference on},
  year = {2011},
  pages = {825 -832},
  month = {sept.},
  abstract = {In this paper, we present platform independent model (PIM) concepts
	of IIS #x2217;Case tool for information system (IS) modeling and
	design. IIS #x2217;Case is a model driven software tool that provides
	generation of executable application prototypes. The concepts are
	described by Meta Object Facility (MOF) specification, one of the
	commonly sed approaches for describing meta-models. One of the main
	reasons for having IIS #x2217;Case PIM concepts specified through
	the meta-model, is to provide software documentation in a formal
	way, as well as a domain analysis purposed to create a domain specific
	langage to support IS design. Using the meta-model of PIM concepts,
	we can generate test cases that may assist in software tool verification.}
}

@INPROCEEDINGS{4756768,
  author = {Yonghua Cen and Zhe Han and PeiPei Ji},
  title = {Chinese Term Recognition and Extraction Based on Hidden Markov Model},
  booktitle = {Computational Intelligence and Industrial Application, 2008. PACIIA
	'08. Pacific-Asia Workshop on},
  year = {2008},
  volume = {2},
  pages = {219 -224},
  month = {dec.},
  abstract = {Motivated by the probabilistic characteristics of syntax compositions
	especially POS (part of speech) matching of Chinese textual information
	and the inner structures of most unlexicalized Chinese domain terms,
	a system framework to recognize and extract domain-specific Chinese
	terms based on hidden Markov model (HMM) was proposed and implemented.
	The system learns the HMM parameters by the input training corpus
	with words roughly segmented and POS tagged by the ICTCLAS system
	developed by Chinese Academy of Sciences and term boundaries manually
	labeled. Based on HMM with the learned parameters knowledge, the
	system conducts term boundaries labeling for Chinese textual information
	from different domains and recognizes terms according to these boundaries.
	The system shows good performance, and the terms recognized can be
	treated as candidate terms for false-eliminating and optimizing combining
	with other parameters such as mutual information and domain dependency.},
  doi = {10.1109/PACIIA.2008.242},
  keywords = {Chinese Academy of Sciences;Chinese term recognition;Chinese textual
	information;ICTCLAS system;POS tagged;domain-specific Chinese terms;hidden
	Markov model;learned parameters knowledge;part of speech matching;probabilistic
	characteristics;syntax compositions;training corpus;unlexicalized
	Chinese domain terms;computational linguistics;hidden Markov models;knowledge
	based systems;natural language processing;probability;text analysis;}
}

@INPROCEEDINGS{4812384,
  author = {Ceri, S.},
  title = {Search Computing},
  booktitle = {Data Engineering, 2009. ICDE '09. IEEE 25th International Conference
	on},
  year = {2009},
  pages = {1 -3},
  month = {29 2009-april 2},
  abstract = {ldquoWho are the strongest European competitors on software ideas?
	Who is the best doctor to cure insomnia in a nearby hospital? Where
	can I attend an interesting conference in my field close to a sunny
	beach?rdquo This information is available on the Web, but no software
	system can accept such queries nor compute the answer. We hereby
	propose search computing as a new multi-disciplinary science which
	will provide the abstractions, foundations, methods, and tools required
	to answer these and many similar queries. While state-of-art search
	systems answer generic or domain-specific queries, search computing
	enables answering questions via a constellation of dynamically selected,
	cooperating search services. Search computing requires innovation
	in software principles, languages, interfaces and protocols, as well
	as contributions from other sciences such as mathematics, operations
	research, psychology, sociology, economical and legal sciences.},
  doi = {10.1109/ICDE.2009.176},
  issn = {1084-4627},
  keywords = {domain-specific queries;service computing;software development;software
	engineering;}
}

@INPROCEEDINGS{5284880,
  author = {Ceri, Stefano},
  title = {Search Computing},
  booktitle = {Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT
	'09. IEEE/WIC/ACM International Joint Conferences on},
  year = {2009},
  volume = {2},
  pages = {1},
  month = {sept.},
  abstract = { #x0201C;Who are the strongest European competitors on software ideas?
	Who is the best doctor to cure insomnia in a nearby hospital? Where
	can I attend an interesting conference in my field closest to a sunny
	beach?" This information is available on the Web, but no software
	system can accept such queries nor compute the answer. We hereby
	propose search computing as a new multi-disciplinary science which
	will provide the abstractions, foundations, methods, and tools required
	to answer these and many similar queries. While state-of-art search
	systems answer generic or domain-specific queries, search computing
	enables answering questions via a constellation of dynamically selected,
	cooperating, search services. Search computing requires innovation
	in software principles, languages, interfaces, and protocols, as
	well as contributions from other sciences such as mathematics, operations
	research, psychology, sociology, economical and legal sciences.},
  doi = {10.1109/WI-IAT.2009.376}
}

@INPROCEEDINGS{5284929,
  author = {Ceri, Stefano},
  title = {Search Computing},
  booktitle = {Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT
	'09. IEEE/WIC/ACM International Joint Conferences on},
  year = {2009},
  volume = {1},
  pages = {1},
  month = {sept.},
  abstract = { #x0201C;Who are the strongest European competitors on software ideas?
	Who is the best doctor to cure insomnia in a nearby hospital? Where
	can I attend an interesting conference in my field closest to a sunny
	beach?" This information is available on the Web, but no software
	system can accept such queries nor compute the answer. We hereby
	propose search computing as a new multi-disciplinary science which
	will provide the abstractions, foundations, methods, and tools required
	to answer these and many similar queries. While state-of-art search
	systems answer generic or domain-specific queries, search computing
	enables answering questions via a constellation of dynamically selected,
	cooperating, search services. Search computing requires innovation
	in software principles, languages, interfaces, and protocols, as
	well as contributions from other sciences such as mathematics, operations
	research, psychology, sociology, economical and legal sciences.},
  doi = {10.1109/WI-IAT.2009.369}
}

@INPROCEEDINGS{5352768,
  author = {Cervelle, J. and Crepinsek, M. and Forax, R. and Kosar, T. and Mernik,
	M. and Roussel, G.},
  title = {On defining quality based grammar metrics},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {651 -658},
  month = {oct.},
  abstract = {Grammar metrics have been introduced to measure the quality and the
	complexity of the formal grammars. The aim of this paper is to explore
	the meaning of these notions and to experiment, on several grammars
	of domain specific languages and of general purpose languages, existing
	grammar metrics together with new metrics based on grammar LR automaton
	and on the produced language. We discuss the results of this experiment
	and focus on the comparison between domain specific languages and
	general purpose languages grammars and on the evolution of the metrics
	between several versions of the same language.},
  doi = {10.1109/IMCSIT.2009.5352768},
  keywords = {based grammar metrics;defining quality;domain specific languages;general
	purpose languages grammars;grammar engineering;grammarware;software
	language engineering;formal languages;grammars;software metrics;}
}

@INPROCEEDINGS{5230617,
  author = {Chaki, S. and Datta, A.},
  title = {ASPIER: An Automated Framework for Verifying Security Protocol Implementations},
  booktitle = {Computer Security Foundations Symposium, 2009. CSF '09. 22nd IEEE},
  year = {2009},
  pages = {172 -185},
  month = {july},
  abstract = {We present ASPIER - the first framework that combines software model
	checking with a standard protocol security model to automatically
	analyze authentication and secrecy properties of protocol implementations
	in C. The technical approach extends the iterative abstraction-refinement
	methodology for software model checking with a domain-specific protocol
	and symbolic attacker model. We have implemented the ASPIER tool
	and used it to verify authentication and secrecy properties of a
	part of an industrial strength protocol implementation - the handshake
	in OpenSSL - for configurations consisting of up to 3 servers and
	3 clients. We have also implemented two distinct methods for reasoning
	about attacker message derivations, and evaluated them in the context
	of OpenSSL verification. ASPIER detected the "version-rollback" vulnerability
	in OpenSSL 0.9.6c source code and successfully verified the implementation
	when clients and servers are only willing to run SSL 3.0.},
  doi = {10.1109/CSF.2009.20},
  issn = {1063-6900},
  keywords = {OpenSSL verification;SSL 3.0;aspier tool;iterative abstraction-refinement
	methodology;security protocol implementations verfication;software
	model checking;standard protocol security model;symbolic attacker
	model;version-rollback vulnerability;protocols;security of data;}
}

@INPROCEEDINGS{777969,
  author = {Chakravarty, S. and Shahar, Y.},
  title = {A constraint-based specification of periodic patterns in time-oriented
	data},
  booktitle = {Temporal Representation and Reasoning, 1999. TIME-99. Proceedings.
	Sixth International Workshop on},
  year = {1999},
  pages = {29 -40},
  abstract = {We use a constraint based language to specify periodic temporal patterns.
	The Constraint-based Pattern Specification Language (CAPSUL) is simple
	to use, but allows a wide variety of patterns to be expressed. CAPSUL
	solves problems such as: (1) how to use calendar based constraints
	to define repetition of a periodic event; (2) what temporal relations
	must exist between consecutive repeats of a pattern; and (3) how
	expressivity is limited if the same temporal relations must hold
	between each pair of intervals in the pattern. We implemented CAPSUL
	in a temporal-abstraction system called Resume, and used it in a
	graphical knowledge acquisition tool to acquire domain-specific knowledge
	from experts about patterns to be found in large databases. We summarize
	the results of preliminary experiments using the pattern specification
	and pattern detection tools on data about patients who have cancer
	and have been seen at the University of Chicago bone marrow transplantation
	center},
  doi = {10.1109/TIME.1999.777969},
  keywords = {CAPSUL;Constraint-based Pattern Specification Language;Resume;bone
	marrow transplantation center;calendar based constraints;cancer;constraint
	based language;constraint based specification;graphical knowledge
	acquisition tool;large databases;patients;pattern detection tools;pattern
	specification;periodic pattern specification;periodic temporal patterns;temporal
	relations;temporal-abstraction system;time oriented data;constraint
	handling;knowledge acquisition;logic programming languages;medical
	information systems;pattern recognition;specification languages;temporal
	logic;temporal reasoning;}
}

@ARTICLE{798322,
  author = {Chandra, S. and Richards, B. and Larus, J.R.},
  title = {Teapot: a domain-specific language for writing cache coherence protocols},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {317 -333},
  number = {3},
  month = {may/jun},
  abstract = {In this paper, we describe Teapot, a domain-specific language for
	writing cache coherence protocols. Cache coherence is of concern
	when parallel and distributed systems make local replicas of shared
	data to improve scalability and performance. In both distributed
	shared memory systems and distributed file systems, a coherence protocol
	maintains agreement among the replicated copies as the underlying
	data are modified by programs running on the system. Cache coherence
	protocols are notoriously difficult to implement, debug, and maintain.
	Moreover, protocols are not off-the-shelf, reusable components, because
	their details depend on the requirements of the system under consideration.
	The complexity of engineering coherence protocols can discourage
	users from experimenting with new, potentially more efficient protocols.
	We have designed and implemented Teapot, a domain-specific language
	that attempts to address this complexity. Teapot's language constructs,
	such as a state-centric control structure and continuations, are
	better suited to expressing protocol code than those of a typical
	systems programming language. Teapot also facilitates automatic verification
	of protocols, so hard to find protocol bugs, such as deadlocks, can
	be detected and fixed before encountering them on an actual execution.
	We describe the design rationale of Teapot, present an empirical
	evaluation of the language using two case studies, and relate the
	lessons that we learned in building a domain-specific language for
	systems programming },
  doi = {10.1109/32.798322},
  issn = {0098-5589},
  keywords = {Teapot;automatic verification;cache coherence protocol writing;continuations;deadlocks;debugging;distributed
	file systems;distributed shared memory systems;distributed systems;domain-specific
	language;local shared data replicas;parallel systems;performance;scalability;state-centric
	control structure;systems programming;cache storage;distributed databases;distributed
	programming;distributed shared memory systems;formal verification;high
	level languages;memory protocols;}
}

@INPROCEEDINGS{5587771,
  author = {Tao-Hsing Chang and Fu-Yuan Hsu and Chia-Hoang Lee and Hahn-Ming
	Lee},
  title = {Part-of-speech tagging for Chinese unknown words in a domain-specific
	small corpus using morphological and contextual rules},
  booktitle = {Natural Language Processing and Knowledge Engineering (NLP-KE), 2010
	International Conference on},
  year = {2010},
  pages = {1 -6},
  month = {aug.},
  abstract = {Many studies have tried to search useful information on the Internet
	by meaningful terms or words. The performance of these approaches
	is often affected by the accuracy of unknown word extraction and
	POS tagging, while the accuracy is affected by the size of training
	corpora and the characteristics of language. This work proposes and
	develops a method that concentrates on tagging the POS of Chinese
	unknown words for the domain of our interest, based on the integration
	of morphological, contextual rules and a statistics-based method.
	Experimental results indicate that the proposed method can overcome
	the difficulties resulting from small corpora in oriental languages,
	and can accurately tags unknown words with POS in domain-specific
	small corpora.},
  doi = {10.1109/NLPKE.2010.5587771},
  keywords = {Chinese Unknown Words;Internet;contextual rules;morphological rule;part-of-speech
	tagging;statistics-based method;natural language processing;statistical
	analysis;text analysis;}
}

@INPROCEEDINGS{885896,
  author = {Sun Chang-Ai and Liu Chao and Jin Mao-Zhong and Zhang Mei},
  title = {Architecture framework for software test tool},
  booktitle = {Technology of Object-Oriented Languages and Systems, 2000. TOOLS
	- Asia 2000. Proceedings. 36th International Conference on},
  year = {2000},
  pages = {40 -47},
  abstract = {A software test tool based on source code is an important tool to
	aid software quality assurance, and under those environments where
	the test technology and test requirements continuously vary, the
	software test tool itself should be endowed with extensibility, easy
	reusability and interoperability. In this paper, the necessity of
	research on an integrated framework for component-based software
	test tools is analyzed, based on the analysis of the traditional
	model of software test tools and the characteristics of such tools.
	We study the architectural design of such an integrated framework
	and come up with a reference model for the framework, which is based
	on components and which can be compatible with CORBA. Moreover, the
	technology for implementing a dynamic configuration based on different
	users' requirements is also introduced},
  doi = {10.1109/TOOLS.2000.885896},
  keywords = {CORBA;component-based software;continuously varying test requirements;continuously
	varying test technology;domain-specific modelling;dynamic configuration;integrated
	framework;interoperability;reference model;software architecture;software
	extensibility;software quality assurance;software reusability;software
	test tool;source code;user requirements;distributed object management;open
	systems;program testing;software architecture;software reusability;software
	tools;subroutines;}
}

@INPROCEEDINGS{4287953,
  author = {Hu Changjun and Zhang Xiaoming and Zhao Qian and Zhao Chongchong},
  title = {Ontology-Based Semantic Integration Method for Domain-Specific Scientific
	Data},
  booktitle = {Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed
	Computing, 2007. SNPD 2007. Eighth ACIS International Conference
	on},
  year = {2007},
  volume = {3},
  pages = {772 -777},
  month = {30 2007-aug. 1},
  abstract = {Sharing the huge amount of data existed and being produced in specific
	domain is of great significance for researches. The semantic heterogeneity
	has been identified as the most important and toughest problem when
	integrating various sources. This paper proposes a semantic data
	integration method to manipulate information scattered over many
	data sources in specific area of science. The method uses OWL ontology
	to represent the global semantics of domain model and the local semantics
	of heterogeneous sources respectively. Semantic mappings between
	global and local ontology are built in a mapping ontology. Semantic
	heterogeneity among the data sources is resolved by TBox reasoning,
	and there is no need to migrate data from data sources to ontology
	instances. Domain-specific markup language is used as the uniform
	format for query results to facilitate data exchange in the domain.
	The architecture and key components for implementing the method are
	presented. To further illustrate the method, the paper gives a case
	study to show how to integrate data semantically in material sciences.},
  doi = {10.1109/SNPD.2007.431},
  keywords = {OWL ontology;TBox reasoning;data exchange;data sharing;domain-specific
	markup language;domain-specific scientific data;material sciences;semantic
	integration method;semantic mappings;software architecture;inference
	mechanisms;knowledge representation languages;materials science;natural
	sciences computing;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{6037507,
  author = {Charaf, H.},
  title = {Developing Mobile Applications for Multiple Platforms},
  booktitle = {Engineering of Computer Based Systems (ECBS-EERC), 2011 2nd Eastern
	European Regional Conference on the},
  year = {2011},
  pages = {2},
  month = {sept.},
  abstract = {Developing software for mobile devices requires special attention,
	and it still requires more large effort compared to software development
	for desktop computers and servers. With the introduction and popularity
	of wireless devices, the diversity of the platforms has also been
	increased. There are different platforms and tools from different
	vendors such as Microsoft, Sun, Nokia, SonyEricsson and many more.
	Because of the relatively low-level programming interface, software
	development (e.g. for Symbian) platform is a tiresome and error prone
	task, whereas Android and Windows Mobile contains higher level structures.
	This keynote introduces the problem of the software development for
	incompatible mobile platforms. Moreover, it provides a Model-Driven
	Architecture (MDA) and Domain Specific Modeling Language (DSML)-based
	solution. We will also discuss the relevance of the model-based approach
	that facilitates a more efficient software development because the
	reuse and the generative techniques are key characteistics of model-based
	computing. In the presented approach, the platform-independence lies
	in the model transformation. This keynote illustrates the creation
	of model compliers on a metamodeling basis by a software package
	called Visual Modeling and Transformation System (VMTS), which is
	a multipurpose modeling and metamodel-based transformation system.
	A case study is also presented on how model compilers can be used
	to generate user interface handler code for different mobile platforms
	from the same platform-independent input models.},
  doi = {10.1109/ECBS-EERC.2011.43},
  keywords = {Android;Microsoft;Nokia;SonyEricsson;Sun;Symbian;VMTS software package;Windows
	Mobile;domain specific modeling language;mobile application;model-driven
	architecture;software development;software generative technique;software
	reuse technique;user interface handler code;visual modeling and transformation
	system;mobile computing;simulation languages;software architecture;software
	reusability;user interfaces;}
}

@INPROCEEDINGS{5463692,
  author = {Chatley, R. and Ayres, J. and White, T.},
  title = {LiFT: Driving Development Using a Business-Readable DSL for Web Testing},
  booktitle = {Software Testing, Verification, and Validation Workshops (ICSTW),
	2010 Third International Conference on},
  year = {2010},
  pages = {460 -468},
  month = {april},
  abstract = {This paper describes the development and evolution of LiFT, a framework
	for writing automated tests in a style that makes them very readable,
	even for non-programmers. We call this style 'literate testing'.
	By creating a domain-specific language embedded within Java, we were
	able to write automated tests that read almost like natural language,
	allowing business requirements to be expressed very clearly. This
	allows development to be driven from tests that are created by developers
	and customers together, helping give all stakeholders confidence
	that the right things are being tested and hence a correct system
	being built. We discuss the experiences of a team using these tools
	and techniques in a large commercial project, and the lessons learned
	from the experience.},
  doi = {10.1109/ICSTW.2010.12},
  keywords = {Java;LiFT;Web testing;business readable DSL;domain specific language;driving
	development;literate testing;natural language;Internet;Java;business
	data processing;}
}

@INPROCEEDINGS{1631611,
  author = {Chau, R. and Yeh, C.-H.},
  title = {Enabling a Semantic Smart WWW: A Soft Computing Framework for Automatic
	Ontology Development},
  booktitle = {Computational Intelligence for Modelling, Control and Automation,
	2005 and International Conference on Intelligent Agents, Web Technologies
	and Internet Commerce, International Conference on},
  year = {2005},
  volume = {2},
  pages = {1067 -1071},
  month = {nov.},
  abstract = {Text-based information accounts for more than 80% of today's Web content.
	They consist of Web pages written in different natural languages.
	As the semantic Web aims at turning the current Web into a machine-understandable
	knowledge repository, availability of multilingual ontology thus
	becomes an issue at the core of a multilingual semantic Web. However,
	multilingual ontology is too complex and resource intensive to be
	constructed manually. In this paper, we propose a three-layer model
	built on top of a soft computing framework to automatically acquire
	a multilingual ontology from domain specific parallel texts. The
	objective is to enable semantic smart information access regardless
	of language over the Web},
  doi = {10.1109/CIMCA.2005.1631611},
  keywords = {Web pages;automatic multilingual ontology development;machine-understandable
	knowledge repository;natural language;smart multilingual semantic
	Web;smart semantic information access;soft computing framework;text-based
	information;Web sites;fuzzy set theory;information retrieval;knowledge
	acquisition;natural languages;neural nets;ontologies (artificial
	intelligence);semantic Web;text analysis;}
}

@INPROCEEDINGS{1592954,
  author = { Chauhan, A. and McCosh, C. and Kennedy, K. and Hanson, R.},
  title = {Automatic Type-Driven Library Generation for Telescoping Languages},
  booktitle = {Supercomputing, 2003 ACM/IEEE Conference},
  year = {2003},
  pages = { 51},
  month = {nov.},
  abstract = { Telescoping languages is a strategy to automatically generate highly-optimized
	domain-specific libraries. The key idea is to create specialized
	variants of library procedures through extensive offline processing.
	This paper describes a telescoping system, called ARGen, which generates
	high-performance Fortran or C libraries from prototype Matlab code
	for the linear algebra library, ARPACK. ARGen uses variable types
	to guide procedure specializations on possible calling contexts.
	ARGen needs to infer Matlab types in order to speculate on the possible
	variants of library procedures, as well as to generate code. This
	paper shows that our type-inference system is powerful enough to
	generate all the variants needed for ARPACK automatically from the
	Matlab development code. The ideas demonstrated here provide a basis
	for building a more general telescoping system for Matlab.},
  doi = {10.1109/SC.2003.10038}
}

@INPROCEEDINGS{1427148,
  author = { Chavez, N.R. and Hartley, R.T.},
  title = {The role of object-oriented techniques and multi-agents in story
	understanding},
  booktitle = {Integration of Knowledge Intensive Multi-Agent Systems, 2005. International
	Conference on},
  year = {2005},
  pages = { 580 - 585},
  month = {18-21,},
  abstract = {Not available},
  doi = {10.1109/KIMAS.2005.1427148},
  keywords = { artificial intelligence; conceptual dependency; domain specific knowledge
	encapsulation; high-level comprehension process; knowledge base;
	memory; multiagent script-based story understanding system; object-oriented
	techniques; knowledge based systems; knowledge representation; multi-agent
	systems; object-oriented methods;}
}

@INPROCEEDINGS{5491337,
  author = {Checiu, Laurentiu and Ionescu, Dan},
  title = {A new algorithm for mapping XML Schema to XML Schema},
  booktitle = {Computational Cybernetics and Technical Informatics (ICCC-CONTI),
	2010 International Joint Conference on},
  year = {2010},
  pages = {625 -630},
  month = {may},
  abstract = {XML and its schema language mainly built to encode documents electronically
	are widely used for a variety of purposes in modern programming.
	XML Schema is developed in the context of a domain specific terminology.
	Schema mapping describes the semantic correspondences between the
	components of two XML Schema documents. An XML Schema document describes
	the information that resides in an XML document using the XML Schema
	Definition Language (XSD). Therefore, an XML Schema document should
	be parsed differently than an XML document, in order to extract the
	proper information from it. This paper introduces a new solution
	for the XML Schema matching and mapping problem consisting of a novel
	approach to the internal representation of XML Schema documents during
	the matching process. The solution is based on the XML object model.
	The use of an object model for the XML Schema document representation
	within the mapping and matching software tool produces the following
	benefits: (1) the XML Schema document internal representation conforms
	to the XML Schema Definition Language standard specifications, (2)
	a better scalability of the software tool, (3) the matching engine
	is developed based on object-oriented design patterns such as the
	Composite and the Chain of Responsibility, (4) flexibility of the
	matching engine and (5) tractability of very complex XML Schema documents.
	The given examples illustrate plainly the new method presented in
	this paper.},
  doi = {10.1109/ICCCYB.2010.5491337}
}

@INPROCEEDINGS{6032405,
  author = {Feng Chen and Hong Zhou and Hongji Yang and Ward, M. and Chu, W.C.-C.},
  title = {Requirements Recovery by Matching Domain Ontology and Program Ontology},
  booktitle = {Computer Software and Applications Conference (COMPSAC), 2011 IEEE
	35th Annual},
  year = {2011},
  pages = {602 -607},
  month = {july},
  abstract = {Users and systems requirements are fundamental for software development
	and maintenance. However, for most of existing systems, you may only
	find design documents without requirement specification. This paper
	presents an ontology-based reengineering approach to recovering requirements
	from existing systems. The proposed approach consists of three main
	parts: ontology development, ontology mapping and derivation of the
	requirements. Domain ontology is used to model domain specific requirements
	and program ontology is used to present system structure and behaviour.
	The algorithm of ontology mapping is developed to match domain ontology
	and program ontology for requirement recovery. A case study of Point
	of Sale Terminal (POST) system is used to illustrate the approach.
	Conclusions are drawn and further research directions are advocated.},
  doi = {10.1109/COMPSAC.2011.84},
  issn = {0730-3157},
  keywords = {design documents;matching domain ontology;ontology development;ontology
	mapping;ontology-based reengineering;point of sale terminal system;program
	ontology;requirement specification;requirements recovery;software
	development;software maintenance;formal specification;ontologies
	(artificial intelligence);software maintenance;systems re-engineering;}
}

@INPROCEEDINGS{1613349,
  author = { Kai Chen and Sztipanovits, J. and Abdelwahed, S.},
  title = {A Semantic Unit for Timed Automata Based Modeling Languages},
  booktitle = {Real-Time and Embedded Technology and Applications Symposium, 2006.
	Proceedings of the 12th IEEE},
  year = {2006},
  pages = { 347 - 360},
  month = {april},
  abstract = { Model-Integrated Computing (MIC) is an infrastructure for model-based
	design of real-time and embedded software and systems. MIC places
	strong emphasis on the use of domain-specific modeling languages
	(DSMLs) and model transformations in design flows. Building on our
	earlier work on transformational specification of semantics for DSMLs,
	the paper proposes a "semantic unit" - a common semantic model -
	for timed automata behavior. The semantic unit is defined using Abstract
	State Machine (ASM) formalism. We show that the precise semantics
	of a wide range of timed automata based modeling languages (TAMLs)
	can be defined through specifying model transformations between a
	domain-specific TAML and the semantic unit. The proposed method that
	we call semantic anchoring is demonstrated by developing the transformation
	rules from the UPPAAL and IF languages to the semantic unit.},
  doi = {10.1109/RTAS.2006.8}
}

@INPROCEEDINGS{4211918,
  author = {Kai Chen and Sztipanovits, J. and Sandeep Neema},
  title = {Compositional Specification of Behavioral Semantics},
  booktitle = {Design, Automation Test in Europe Conference Exhibition, 2007. DATE
	'07},
  year = {2007},
  pages = {1 -6},
  month = {april},
  abstract = {An emerging common trend in model-based design of embedded software
	and systems is the adoption of domain-specific modeling languages
	(DSMLs). While abstract syntax metamodeling enables the rapid and
	inexpensive development of DSMLs, the specification of DSML semantics
	is still a hard problem. In previous work, we have developed methods
	and tools for the semantic anchoring of DSMLs. Semantic anchoring
	introduces a set of reusable "semantic units" that provide reference
	semantics for basic behavioral categories using the abstract state
	machine (ASM) framework. In this paper, we extend the semantic anchoring
	framework to heterogeneous behaviors by developing a method for the
	composition of semantic units. Semantic unit composition reduces
	the required effort from DSML designers and improves the quality
	of the specification. The proposed method is demonstrated through
	a case study},
  doi = {10.1109/DATE.2007.364408},
  keywords = {DSML semantics;abstract state machine;abstract syntax metamodeling;behavioral
	semantics;compositional specification;domain-specific modeling languages;embedded
	software;embedded systems;reusable semantic units;semantic anchoring
	framework;semantic unit composition;Unified Modeling Language;embedded
	systems;programming language semantics;}
}

@INPROCEEDINGS{4273243,
  author = {Kai Chen and Sztipanovits, J. and Neema, S.},
  title = {A Case Study on Semantic Unit Composition},
  booktitle = {Modeling in Software Engineering, 2007. MISE '07: ICSE Workshop 2007.
	International Workshop on},
  year = {2007},
  pages = {3},
  month = {may},
  abstract = {In previous work we have discussed a semantic anchoring framework
	that enables the semantic specification of Domain-Specific Modeling
	languages by specifying semantic anchoring rules to predefined semantic
	units. This framework is further extended to support heterogeneous
	systems by developing a method for the composition of semantic units.
	In this paper, we explain the semantic unit composition through a
	case study.},
  doi = {10.1109/MISE.2007.1},
  keywords = {domain-specific modeling languages;semantic anchoring framework;semantic
	unit composition;programming language semantics;simulation languages;}
}

@INPROCEEDINGS{1276011,
  author = {Keh-Jiann Chen and Jia-Ming You and Dee-Hwa Kao and Cheng-Huei Wu},
  title = {Design of lexical database for financial domain},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2003. Proceedings.
	2003 International Conference on},
  year = {2003},
  pages = { 784 - 789},
  month = {oct.},
  abstract = { Here, we study the design of domain lexicons and address the problems
	of lexical knowledge representation and linking between different
	knowledge sources. A domain lexicon contains substantial domain specific
	vocabularies with associated phonological, morphological, syntactic
	and semantic/pragmatic information as well as links to general knowledge
	bases which are rich enough to support knowledge-intensive models
	for practical NLP systems. We take financial domain as an example
	to illustrate the representation structures for syntactic and semantic
	knowledge. In order to suit for both maximal reusability and deep
	analysis, our domain lexicon is designed with uniform knowledge representation
	and fine-grain feature encoding. We also address the issues of how
	to bridge the gaps between coarse-grain general lexicons and fine-grain
	domain lexicons.},
  doi = {10.1109/NLPKE.2003.1276011},
  issn = { },
  keywords = { NLP system; financial domain lexicon; knowledge base; lexical database
	design; lexical knowledge representation; natural language processing
	system; semantic knowledge; syntactic knowledge; vocabulary; database
	management systems; knowledge based systems; knowledge representation;
	natural languages; vocabulary;}
}

@ARTICLE{103328,
  author = {Cherneff, J.},
  title = {Communicating design representations: the role of interpretation
	},
  journal = {Computer-Aided Engineering Journal},
  year = {1991},
  volume = {8},
  pages = {153 -159},
  number = {4},
  month = {aug},
  abstract = {The development of integrated computer systems in the fields of architecture,
	engineering and construction (AEC) requires a viable scheme for data
	exchange between professional groups. Data exchange capability has
	two essential components: representation standards and feature extraction.
	A representation standard is an agreement among AEC professions about
	a common problem description language. Feature extraction is a process
	by which a particular professional entity derives a domain specific
	representation i.e. an interpretation, from a common representation.
	The roles of standards and feature extraction can be understood in
	terms of their relation to the overall problem-solving structure.
	This article presents a communication architecture for integrating
	AEC design data based on the principle of local interpretation of
	a low-level graphic exchange language. A prototype implementation
	is described},
  issn = {0263-9327},
  keywords = {AEC;architecture;communication architecture;construction;data exchange;design
	data;domain specific representation;engineering;feature extraction;integrated
	computer systems;low-level graphic exchange language;problem description
	language;problem-solving;representation standards;architectural CAD;computer
	graphics;computerised pattern recognition;electronic data interchange;}
}

@INPROCEEDINGS{479403,
  author = {Chevalier, H. and Ingold, C. and Kunz, C. and Moore, C. and Roven,
	C. and Yamron, J. and Baker, B. and Bamberg, P. and Bridle, S. and
	Bruce, T. and Weader, A.},
  title = {Large-vocabulary speech recognition in specialized domains},
  booktitle = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995
	International Conference on},
  year = {1995},
  volume = {1},
  pages = {217 -220 vol.1},
  month = {may},
  abstract = {We report on research into the discrete-word speech recognition performance
	of several specialized language models optimized for four large domains
	of professional discourse. We describe the construction of these
	models and report perplexity and recognition results for each of
	the specialized domains. The data indicate that such specialization
	may significantly improve performance both before and after adaptation},
  doi = {10.1109/ICASSP.1995.479403},
  issn = {1520-6149},
  keywords = { adaptation; discrete-word speech recognition performance; large-vocabulary
	speech recognition; perplexity results; professional discourse; recognition
	results; specialized domains; specialized language models; natural
	languages; speech processing; speech recognition;}
}

@ARTICLE{903116,
  author = {Chien, S. and Fisher, F. and Estlin, T.},
  title = {Automated software module reconfiguration through the use of artificial
	intelligence planning techniques},
  journal = {Software, IEE Proceedings -},
  year = {2000},
  volume = {147},
  pages = {186 -192},
  number = {5},
  month = {oct},
  abstract = {One important approach to enhancing software re-use is through the
	creation of large-scale software libraries. By modularising functionality,
	many complex specialised applications can be built up from smaller
	reusable general-purpose libraries. Consequently, many large software
	libraries have been formed for applications such as image processing
	and data analysis. However, knowing the requirements and formats
	of each of these routines requires considerable expertise thus limiting
	the usage of these libraries to experts. An approach is described
	to enable novices to use complex software libraries. In this approach,
	the interactions between, and requirements of, the software modules
	are represented in a declarative language based on artificial intelligence
	(AI) planning techniques. The user is then able to specify their
	goals in terms of this language-designating what they want accomplished
	instead of how to do it. The AI planning system then uses this model
	of the available subroutines to compose a domain specific script
	to fulfil the user request. Three such systems developed by the Artificial
	Intelligence Group of the Jet Propulsion Laboratory and described.
	The multimission VICAR planner (MVP) was deployed in 1994 and used
	to support image processing for science product generation for the
	Galileo mission. MVP reduced the time for filling certain classes
	of requests from 4 h to 15 min. The automated SAR image processing
	system (ASIP) was deployed in 1996 to the Department of Geology at
	Arizona State University to support aeolian science analysis of synthetic
	aperture radar images. ASIP reduces the number of manual inputs in
	science product generation tenfold. Finally, the DPLAN system reconfigures
	software modules that control complex antenna hardware in configuring
	antennas to support a wide range of tracks for NASA's Deep Space
	Network of communications and radio science antennas},
  doi = {10.1049/ip-sen:20000899},
  issn = {1462-5970},
  keywords = {ASIP;DPLAN system;Deep Space Network;Galileo mission;aeolian science
	analysis;artificial intelligence planning techniques;automated SAR
	image processing system;automated software module reconfiguration;complex
	antenna hardware;data analysis;declarative language;domain specific
	script;image processing;large-scale software libraries;multimission
	VICAR planner;radio science antennas;science product generation;software
	modules;software re-use;synthetic aperture radar images;user request;configuration
	management;deductive databases;knowledge based systems;planning (artificial
	intelligence);software libraries;}
}

@INPROCEEDINGS{1434889,
  author = {Cho, K.C. and Yeong-Tae Song},
  title = {Layered design of CORBA audio/video streaming service in a distributed
	Java ORB middleware multimedia platform},
  booktitle = {Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed
	Computing, 2005 and First ACIS International Workshop on Self-Assembling
	Wireless Networks. SNPD/SAWN 2005. Sixth International Conference
	on},
  year = {2005},
  pages = { 198 - 205},
  month = {may},
  abstract = { The present work practices a previously published layered middleware
	design architecture that can integrate a rich set of Java APIs into
	the OMG CORBA audio/video streaming framework. In this design, JMF,
	CORBA audio/video streaming service, CORBA, and JRE are functioned
	as a programming language domain-specific middleware, a common middleware
	service, a distribution middleware, and a host infrastructure middleware
	layers, respectively. The current work utilizes the OMG audio/video
	stream binding mechanism in order to provide a standardized interoperability
	of streams between CORBA and JMF environments. Through a generalized
	mapping, CORBA objects could directly manipulate JMF classes. Although
	the current high level layered design work is only applicable to
	OMG compliant Java ORBs, this design concept could be extended into
	other language specific ORBs by utilizing other programming language
	domain specific service layers, i.e., other programming language
	APIs.},
  doi = {10.1109/SNPD-SAWN.2005.47},
  keywords = { CORBA audio/video streaming service; JMF environment; Java API; distributed
	Java ORB middleware multimedia platform; distribution middleware;
	high level layered design; host infrastructure middleware; interoperability;
	programming language domain-specific middleware; Java; distributed
	object management; middleware; multimedia computing; open systems;
	video streaming;}
}

@INPROCEEDINGS{4368004,
  author = {Key-Sun Choi},
  title = {IT Ontology and Semantic Technology},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2007. NLP-KE
	2007. International Conference on},
  year = {2007},
  pages = {14 -15},
  month = {30 2007-sept. 1},
  abstract = {IT (information technology) ontology is to be used for analyzing the
	information technology as well as for enhancing it. Semantic technology
	is compared with the syntactic one. Ontology plays a backbone for
	meaning-centered reconfiguration of syntactic structure, which is
	one aspect of semantic technology. The purpose of use of IT ontology
	will be categorized into two things: to capture the right information
	and services for user requests, and on the other hand, to give insights
	for the future IT with their possible paths by interlinking relations
	on component classes and instances. Consider question-answering based
	on ontology to improve the performance of QA. Each question type
	(e.g., 5W1H) will seek its specific relation from the ontology that
	has already been acquired from the relevant information resources
	(e.g., Wikipedia or news articles). The question is whether such
	relations and related classes are so neutral independent of domain
	or they are affected by each specific-domain. The first step of ontology
	learning for question-answering application is to find such neutral
	relation discovery mechanism and to take care of the special distorted
	relation-instance mapping when populating on the domain resources.
	Then, we will consider the domain ontology acquisition by top-down
	manner from already made similar resources (e.g., domain-specific
	thesaurus) and also bottom-up manner from the relevant resources.
	But the already-made resources should be checked against the current
	available resources for their coverage. Problem is that thesaurus
	is comprised of classes, not the instances of terms that appear in
	corpora. They have little coverage over the resources, and even the
	mapping between classes and instances has not been established yet
	in this stage. Clustering technology could now filter out the irrelevant
	mappings. Features of clustering could be improved more accurate
	by using more semantic ones that have been accumulated during the
	steps. For example, discov- ery process based on patterns could be
	evolved by putting the discovered semantic features into the patterns.
	Keeping ontology use for question-answering in mind, it is asked
	for how much the acquired ontology can represent the resources used
	for acquisition processes. Derived questions are summarized into
	two about: (1) how such ideal complete ontology could be generated
	for each specification of use, and (2) how much ontology contributes
	to the intended problem-solving. The ideal case is to convert all
	of resources to their corresponding ontology. But if presupposing
	the gap between the meaning of resources and acquired ontology, a
	set of raw chunks in resources may be still effective to answer for
	given questions with some help from acquired ontology or even without
	resort to them. Definitions of classes and relations in ontology
	would be manifested through dual structure to supplement the complementary
	factors between the idealized complete noise-free ontology shape
	and incomplete error-prone knowledge. In the result, we now confront
	two problems: how to measure the ontology effectiveness for each
	situation, and how to compare with the use of ontology for each application
	and to transform into another shape of ontology depending on application,
	that could be helped by granularity control and even extended to
	reconfiguration of knowledge structure. In the result, the intended
	IT ontology is modularized enough to be compromised later for each
	purpose of use, and in efficient and effective ways. Still we have
	to solve definition questions and their translation to ontology forms.},
  doi = {10.1109/NLPKE.2007.4368004},
  keywords = {IT ontology learning;clustering technology;domain ontology acquisition;domain-specific
	thesaurus;information resources;information technology infrastructure;neutral
	relation discovery mechanism;pattern semantic feature discovery;problem
	solving;question answering application;relation-instance mapping;semantic
	technology;data mining;information filtering;learning (artificial
	intelligence);ontologies (artificial intelligence);pattern clustering;problem
	solving;thesauri;}
}

@INPROCEEDINGS{5738538,
  author = {Chowdhury, P. and Mandal, A. and Kumar, K.R.P. and Athithan, G.},
  title = {A Framework for VoIP Speech Data Generation Using Asterisk},
  booktitle = {Devices and Communications (ICDeCom), 2011 International Conference
	on},
  year = {2011},
  pages = {1 -4},
  month = {feb.},
  abstract = {State-of-the-art approaches for automatic recognition of speech, speaker
	or language specific information from spoken data rely on statistical
	techniques that require large databases for training and testing.
	Application of these techniques on Voice over Internet Protocol (VoIP)
	environment requires studying them under different codec and network
	conditions. Though earlier works have studied and reported the same,
	a framework for automatic generation of VoIP speech is lacking. A
	number of speech corpus for different applications are available
	for microphone speech. As domain specific performance needs to be
	evaluated in matched acoustic characteristics and application conditions,
	methods that enable automatic generation of target speech from the
	available microphone speech are important to a researcher in saving
	time and effort. We present a framework based on Asterisk, a freely
	available open source Internet Protocol-Public Exchange (IP-PBX)
	software for realization of VoIP speech from the available microphone
	speech corpus in network conditions that is reflective of actual
	VoIP channels.},
  doi = {10.1109/ICDECOM.2011.5738538},
  keywords = {VoIP speech data generation;asterisk;codec conditions;domain specific
	performance;language specific information;matched acoustic characteristics;microphone
	speech corpus;network conditions;open source Internet protocol-public
	exchange software;speaker automatic recognition;speech automatic
	recognition;statistical techniques;voice over Internet protocol environment;Internet
	telephony;speaker recognition;speech codecs;}
}

@INPROCEEDINGS{183302,
  author = {Christiansen, M.G. and Delcambre, S.N. and Demirors, E. and Demirors,
	O. and Tanik, M.M.},
  title = {Software development with transformable components},
  booktitle = {System Sciences, 1992. Proceedings of the Twenty-Fifth Hawaii International
	Conference on},
  year = {1992},
  volume = {ii},
  pages = {558 -559 vol.2},
  month = {jan},
  abstract = {A software development environment for describing and utilizing domain
	specific abstraction mechanisms is described. DARMS (Design Abstract
	Representation and Manipulation Shell) is a prototype environment
	which has been developed over the past three years},
  doi = {10.1109/HICSS.1992.183302},
  keywords = { DARMS; Design Abstract Representation and Manipulation Shell; domain
	specific abstraction mechanisms; software development environment;
	transformable components; programming environments; software reusability;}
}

@INPROCEEDINGS{4127225,
  author = {Cickovski, T. and Sweet, C. and Izaguirre, J.A.},
  title = {MDL, A Domain-Specific Language for Molecular Dynamics},
  booktitle = {Simulation Symposium, 2007. ANSS '07. 40th Annual},
  year = {2007},
  pages = {256 -266},
  month = {march },
  abstract = {Molecular dynamics (MD) involves solving Newton's equations of motion
	for a molecular system and propagating the system by time-dependent
	updates of atomic positions and velocities. As a severe limitation
	of molecular dynamics is the size of the timestep used for propagation,
	a key area of research is the development of efficient propagation
	algorithms which can maintain accuracy and stability with larger
	timesteps. We present MDL, an MD domain-specific language with the
	goals of allowing prototyping, testing and debugging of these algorithms.
	We illustrate the use of parallelism within MDL to implement the
	finite temperature string method, and interfacing to visualization
	and graphical tools},
  doi = {10.1109/ANSS.2007.26},
  issn = {1080-241X},
  keywords = {MDL;Newton's motion equations;atomic positions;atomic velocities;domain-specific
	language;finite temperature string method;molecular dynamics;molecular
	system;chemistry computing;high level languages;molecular dynamics
	method;}
}

@INPROCEEDINGS{5631527,
  author = {Cirilo, C.E. and do Prado, A.F. and de Souza, W.L. and Zaina, L.A.M.},
  title = {Model Driven RichUbi - A Model-Driven Process to Construct Rich Interfaces
	for Context-Sensitive Ubiquitous Applications},
  booktitle = {Software Engineering (SBES), 2010 Brazilian Symposium on},
  year = {2010},
  pages = {100 -109},
  month = {27 2010-oct. 1},
  abstract = {Software development that meets the demand of Ubiquitous Computing,
	in which access to applications occurs anywhere, anytime and from
	different devices, has raised new challenges for Software Engineering.
	Among these challenges it stands out the development of context-sensitive
	ubiquitous applications. Much of the effort required for building
	such applications can be reduced through the reuse of the application's
	modeling. Different parts of a ubiquitous application can be reused,
	such as the user interface. Generate the interfaces' code so that
	they can self-adapt according to the different access contexts makes
	the application more dynamic and personalized. Therefore, by combining
	the conceptions of rich interfaces, domain-specific modeling, and
	context sensitivity, this paper presents a development process, called
	Model Driven RichUbi, to support the construction of rich interfaces
	for context-sensitive ubiquitous applications.},
  doi = {10.1109/SBES.2010.20},
  keywords = {context sensitive ubiquitous application;model driven RichUbi;rich
	interface;software development;software engineering;ubiquitous computing;user
	interface;software engineering;ubiquitous computing;user interfaces;}
}

@INPROCEEDINGS{4637555,
  author = {Clark, T. and Sammut, P. and Willans, J.},
  title = {Beyond Annotations: A Proposal for Extensible Java (XJ)},
  booktitle = {Source Code Analysis and Manipulation, 2008 Eighth IEEE International
	Working Conference on},
  year = {2008},
  pages = {229 -238},
  month = {sept.},
  abstract = {Annotations provide a limited way of extending Java in order to tailor
	the language for specific tasks. This paper describes a proposal
	for a Java extension which generalises annotations to allow Java
	to be a platform for developing domain specific languages.},
  doi = {10.1109/SCAM.2008.34},
  keywords = {Java annotation;domain specific language development platform;extensible
	Java;Java;}
}

@ARTICLE{5035594,
  author = {Cleenewerck, T. and Noye, J.},
  title = {Editorial domain specific aspect languages},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {165 -166},
  number = {3},
  month = {june },
  abstract = {Most research in the AOSD community focuses on general-purpose aspect
	languages. Unfortunately, the trend towards ever more expressive
	languages has a major drawback: general-purpose aspect languages
	(GPALs) are losing their purposefullness. In the extreme, pointcut
	expressions, for instance, become Turing-complete queries. It seems
	that along the road a turn was taken, leading astray from simple
	and concise aspects. As a result, aspects are becoming increasingly
	complex to specify and their impact on the base code is getting more
	difficult to control, although these specifications are at the heart
	of aspect- orientation.},
  doi = {10.1049/iet-sen.2009.9033},
  issn = {1751-8806},
  keywords = {Turing-complete queries;domain specific aspect languages;general-purpose
	aspect languages;object-oriented languages;object-oriented programming;}
}

@INPROCEEDINGS{4591729,
  author = {Combemale, B. and Broto, L. and Tchana, A. and Hagimont, D.},
  title = {Metamodeling Autonomic System Management Policies - Ongoing Works},
  booktitle = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual
	IEEE International},
  year = {2008},
  pages = {1091 -1096},
  month = {28 2008-aug. 1},
  abstract = {Autonomic computing is recognized as one of the most promising solution
	to address the increasingly complex task of distributed environments'
	administration. In this context, many projects relied on software
	components and architectures to organize such an autonomic management
	software. However, we observed that the interfaces of a component
	model are too low-level, difficult to use and still error prone.
	Therefore, we introduced higher-level languages for the modeling
	of deployment and management policies. These domain specific languages
	enhance simplicity and consistency of the policies. Our current work
	is to formally describe the metamodels and the semantics associated
	with these languages.},
  doi = {10.1109/COMPSAC.2008.24},
  issn = {0730-3157},
  keywords = {autonomic computing;autonomic system management policy metamodeling;distributed
	environment administration;domain specific language;software architecture;software
	component;distributed processing;object-oriented programming;project
	management;software architecture;software management;}
}

@INPROCEEDINGS{5503308,
  author = {Comitz, P.},
  title = {A domain specific approach to aviation data},
  booktitle = {Integrated Communications Navigation and Surveillance Conference
	(ICNS), 2010},
  year = {2010},
  pages = {1 -14},
  month = {may},
  abstract = {Presents a collection of slides that discusses the following; aviation
	data; domain specific languages; ultra-large scale systems; and data
	management.},
  doi = {10.1109/ICNSURV.2010.5503308},
  issn = {2155-4943},
  keywords = {Lexer;Parser;TODO;aviation data;data generation capability;data management;domain
	specific languages;online real time system;ultra-large scale systems;aerospace
	computing;real-time systems;specification languages;}
}

@INPROCEEDINGS{4272181,
  author = {Comitz, P.H.},
  title = {A Software Factory for Air Traffic Data},
  booktitle = {Integrated Communications, Navigation and Surveillance Conference,
	2007. ICNS '07},
  year = {2007},
  pages = {1 -7},
  month = {30 2007-may 3},
  abstract = {Modern information systems require a flexible, scalable, and upgradable
	infrastructure that allows communication, and subsequently collaboration,
	between heterogeneous information processing and computing environments.
	Heterogeneous systems often use different data representations for
	the same data items, limiting collaboration and increasing the cost
	and complexity of system integration. Although this problem is conceptually
	straightforward, the process of data conversion is error prone, often
	dramatically underestimated, and surprisingly complex. The complexity
	is often the result of the non-standard data representations that
	are used by computing systems in the aviation domain. This paper
	describes work that is being done to address this challenge. A prototype
	software factory for air traffic data is being built and evaluated.
	The software factory provides the capability to create data and interface
	models for use in the air traffic domain. The model will allow the
	user to specify entities such as data items, scaling, units, headers
	and footers, representation, and coding. The factory automatically
	creates a machine usable data representation. A prototype for a Domain
	Specific Language to assist in this task is being developed. This
	paper describes the scope of the work and the overall approach.},
  doi = {10.1109/ICNSURV.2007.384149},
  keywords = {Domain Specific Language;air traffic data;data representation;heterogeneous
	information processing;prototype software factory;data handling;groupware;}
}

@ARTICLE{5737854,
  author = {Cong, J. and Bin Liu and Neuendorffer, S. and Noguera, J. and Vissers,
	K. and Zhiru Zhang},
  title = {High-Level Synthesis for FPGAs: From Prototyping to Deployment},
  journal = {Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions
	on},
  year = {2011},
  volume = {30},
  pages = {473 -491},
  number = {4},
  month = {april },
  abstract = {Escalating system-on-chip design complexity is pushing the design
	community to raise the level of abstraction beyond register transfer
	level. Despite the unsuccessful adoptions of early generations of
	commercial high-level synthesis (HLS) systems, we believe that the
	tipping point for transitioning to HLS msystem-on-chip design complexityethodology
	is happening now, especially for field-programmable gate array (FPGA)
	designs. The latest generation of HLS tools has made significant
	progress in providing wide language coverage and robust compilation
	technology, platform-based modeling, advancement in core HLS algorithms,
	and a domain-specific approach. In this paper, we use AutoESL's AutoPilot
	HLS tool coupled with domain-specific system-level implementation
	platforms developed by Xilinx as an example to demonstrate the effectiveness
	of state-of-art C-to-FPGA synthesis solutions targeting multiple
	application domains. Complex industrial designs targeting Xilinx
	FPGAs are also presented as case studies, including comparison of
	HLS solutions versus optimized manual designs. In particular, the
	experiment on a sphere decoder shows that the HLS solution can achieve
	an 11-31% reduction in FPGA resource usage with improved design productivity
	compared to hand-coded design.},
  doi = {10.1109/TCAD.2011.2110592},
  issn = {0278-0070},
  keywords = {AutoESL AutoPilot HLS tool;C-to-FPGA synthesis solutions;SoC;Xilinx
	FPGA;commercial high-level synthesis systems;domain-specific system-level
	implementation platforms;field-programmable gate array designs;hand-coded
	design;improved design productivity;platform-based modeling;register
	transfer level;robust compilation technology;sphere decoder;system-on-chip
	design complexity;wide language coverage;field programmable gate
	arrays;network synthesis;system-on-chip;}
}

@INPROCEEDINGS{5623168,
  author = {Wang Cong and Wang Xiao-guo and Yu Zhen-wei and Liu Jun and Wang
	Zhi-xue},
  title = {UML-based C4ISR capability requirement analysis},
  booktitle = {Computer Application and System Modeling (ICCASM), 2010 International
	Conference on},
  year = {2010},
  volume = {11},
  pages = {V11-445 -V11-448},
  month = {oct.},
  abstract = {The paper discusses the method for C4ISR capability requirement analysis.
	The method focuses on domain knowledge modeling and domain knowledge
	reusing. The approach uses three layers framework to describe capability
	requirement, which makes the domain knowledge reusable. Based on
	the framework, the process for C4ISR capability requirement modeling
	has been discussed. The meta-concept model has been discussed in
	the paper too. Finally, a case study of C4ISR architectural simulation
	modeling is provided to demonstrate the availability and applicability
	of the method.},
  doi = {10.1109/ICCASM.2010.5623168},
  keywords = {C4ISR capability requirement analysis;UML;domain knowledge modeling;domain
	knowledge reusing;meta-concept model;Unified Modeling Language;systems
	analysis;}
}

@INPROCEEDINGS{5565130,
  author = {Wang Cong and Wang Xiao-guo and Liu Jun and Tian Ming and Wang Zhi-xue},
  title = {Object modeling language for C4ISR capability requirement analysis},
  booktitle = {Computer Science and Information Technology (ICCSIT), 2010 3rd IEEE
	International Conference on},
  year = {2010},
  volume = {3},
  pages = {609 -613},
  month = {july},
  abstract = {The paper suggests a meta-ontology for C4ISR capability conceptualization.
	Under which, by taking advantage of the UML profile mechanism, the
	paper defines a domain-specific modeling language for C4ISR capability
	requirement. The abstract syntax, concrete syntax and formal semantic
	of the modeling language have been discussed in the paper. A case
	study of C4ISR architectural simulation modeling is provided to demonstrate
	the availability and applicability of the language.},
  doi = {10.1109/ICCSIT.2010.5565130},
  keywords = {C4ISR capability requirement analysis;UML profile mechanism;abstract
	syntax;concrete syntax;domain-specific modeling language;formal semantic;meta-ontology;object
	modeling language;Unified Modeling Language;abstract data types;formal
	verification;ontologies (artificial intelligence);programming language
	semantics;}
}

@INPROCEEDINGS{1240296,
  author = {Consel, C. and Reveillere, L.},
  title = {A programmable client-server model: robust extensibility via DSLs},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = { 70 - 79},
  month = {oct.},
  abstract = { The client-server model has been successfully used to support a wide
	variety of families of services in the context of distributed systems.
	However, its server-centric nature makes it insensitive to fast changing
	client characteristics like terminal capabilities, network features,
	user preferences and evolving needs. To overcome these key limitations,
	we present an approach to enabling a server to adapt to different
	clients by making it programmable. A service-description language
	is used to program server adaptations. This language is designed
	as a domain-specific language to offer expressiveness and conciseness
	without compromising safety and security. We show that requiring
	the deployment of new protocols or server implementations. We illustrate
	our approach with the Internet Message Access Protocol (IMAP). An
	IMAP server is made programmable and a language, named Pems, is introduced
	to program robust variations of e-mail services. Our approach is
	uniformly used to develop a platform for multimedia communication
	services. This platform is composed of programmable servers for telephony
	service, e-mail processing, remote-document processing and stream
	adapters.},
  doi = {10.1109/ASE.2003.1240296},
  issn = {1527-1366},
  keywords = { DSL; IMAP; Internet Message Access Protocol; Pems; client-server
	model; digital subscriber line; distributed systems; domain-specific
	language; e-mail processing; evolving needs; multimedia communication
	services; network features; program server adaptations; programmable
	model; remote-document processing; robust extensibility; server implementations;
	server-centric nature; service-description language; stream adapters;
	telephony service; terminal capabilities; user preferences; Internet;
	client-server systems; distributed programming; electronic mail;
	programming environments; telephony; transport protocols;}
}

@INPROCEEDINGS{4031894,
  author = {Daniel E. Cooke and Brad Nemanich and J. Nelson Rushton},
  title = {The Role of Theory and Experiment in Language Design--A 15 Year Perspective},
  booktitle = {Tools with Artificial Intelligence, 2006. ICTAI '06. 18th IEEE International
	Conference on},
  year = {2006},
  pages = {163 -168},
  month = {nov. },
  abstract = {This paper highlights a 15-year research effort during which the executable
	specification language SequenceL has been researched and developed.
	The paper covers early insights into how iterative control structures
	are used as well as advances involving simple and general semantics},
  doi = {10.1109/ICTAI.2006.112},
  issn = {1082-3409},
  keywords = {SequenceL;executable specification language;iterative control structure;language
	design;specification languages;}
}

@INPROCEEDINGS{1639716,
  author = {Cornwall, J.L.T. and Beckmann, O. and Kelly, P.H.J.},
  title = {Automatically translating a general purpose C++ image processing
	library for GPUs},
  booktitle = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006.
	20th International},
  year = {2006},
  pages = {8 pp.},
  month = {april},
  abstract = {This paper presents work-in-progress towards a C++ source-to-source
	translator that automatically seeks parallelizable code fragments
	and replaces them with code for a graphics co-processor. We report
	on our experience with accelerating an industrial image processing
	library. To increase the effectiveness of our approach, we exploit
	some domain-specific knowledge of the library's semantics. We outline
	the architecture of our translator and how it uses the ROSE source-to-source
	transformation library to overcome complexities in the C++ language.
	Techniques for parallel analysis and source transformation are presented
	in light of their uses in GPU code generation. We conclude with results
	from a performance evaluation of two examples, image blending and
	an erosion filter, hand-translated with our parallelization techniques.
	We show that our approach has potential and explain some of the remaining
	challenges in building an effective tool},
  doi = {10.1109/IPDPS.2006.1639716},
  keywords = {C++ image processing library;C++ language;C++ source-to-source translator;GPU
	code generation;ROSE source-to-source transformation library;domain-specific
	knowledge;erosion filter;graphics coprocessor;image blending;industrial
	image processing library;library semantics;parallel analysis;parallelizable
	code fragment;parallelization technique;source transformation;C++
	language;computer graphic equipment;coprocessors;image processing;program
	compilers;program interpreters;programming language semantics;software
	libraries;}
}

@INPROCEEDINGS{5655259,
  author = {Correia, A. and Brito e Abreu, F.},
  title = {Defining and Observing the Compliance of Service Level Agreements:
	A Model Driven Approach},
  booktitle = {Quality of Information and Communications Technology (QUATIC), 2010
	Seventh International Conference on the},
  year = {2010},
  pages = {165 -170},
  month = {29 2010-oct. 2},
  abstract = {IT Service Management (ITSM) is the set of processes that allow planning,
	organizing, directing and controlling the provisioning of IT services.
	Among the concerns of ITSM, namely within the service level management
	process, are the requirements for services availability, performance,
	accuracy, capacity and security, which are specified in terms of
	service-level agreements (SLA). SLA definition and monitoring are
	open issues within the ITSM domain. This paper overviews an ongoing
	research initiative concerned with three specific problems in this
	context: (1) SLAs in the context of ITSM are informally specified
	in natural language, (2) SLAs specifications are not grounded on
	models of ITSM processes, (3) SLAs compliance verification in IT
	services is not performed at the same level of abstraction as service
	design. To mitigate those problems, we propose a model-based approach
	to IT services SLA specification and compliance verification. The
	specification part will be based on a SLA language - a domain specific
	language (DSL) for defining quality attributes as non functional
	requirements (NFRs) in the context of ITSM. Its metamodel will be
	an extension of the metamodel of the adopted process modeling language.
	As such, it will be possible to ground SLA definition on the corresponding
	IT service model constructs. SLA monitoring and compliance validation
	will occur at the same abstraction level as service specification,
	therefore being understood by all stakeholders.},
  doi = {10.1109/QUATIC.2010.32},
  keywords = {IT service management;SLA monitoring;compliance verification;domain
	specific language;metamodel extension;model driven approach;natural
	language;nonfunctional requirement;process modeling language;service
	level agreement;service level management;business data processing;conformance
	testing;formal specification;formal verification;information technology;natural
	language processing;specification languages;}
}

@INPROCEEDINGS{1575738,
  author = {Costagliola, G. and Deufemia, V. and Risi, M.},
  title = {Sketch Grammars: a formalism for describing and recognizing diagrammatic
	sketch languages},
  booktitle = {Document Analysis and Recognition, 2005. Proceedings. Eighth International
	Conference on},
  year = {2005},
  pages = { 1226 - 1230 Vol. 2},
  month = {aug.-1 sept.},
  abstract = { Sketch-based user interfaces are increasingly common and are being
	built for a variety of different disciplines. However, at present
	the implementation of sketch recognizers is quite time consuming
	since they are mostly based on specific techniques, as opposed to
	several other fields such as textual/visual languages and speech
	recognition, which benefit from the availability of compiler generation
	techniques and tools. This paper proposes a grammar formalism, namely
	Sketch Grammars (SkGs), for describing both the shape of the symbols'
	language and the syntax of sketch languages. Recognizers are automatically
	generated from the sketch grammar descriptions.},
  doi = {10.1109/ICDAR.2005.218},
  issn = {1520-5263 },
  keywords = { SkGs; Sketch Grammars; diagrammatic sketch languages; grammar formalism;
	sketch recognizers; sketch-based user interfaces; grammars;}
}

@INPROCEEDINGS{37443,
  author = {Costantini, S. and Lanzarone, G.A.},
  title = {Problem solving in metalogic programming},
  booktitle = {Computers and Communications, 1989. Conference Proceedings., Eighth
	Annual International Phoenix Conference on},
  year = {1989},
  pages = {543 -548},
  month = {mar},
  abstract = {The objective of this study is to show how a metalogic programming
	language, i.e. a logic language with fully developed, built-in metalevel
	features, is a suitable tool for formalization and use of several
	forms of reasoning. The role of metaknowledge in expressing auxiliary
	inference strategies is emphasized. Two case studies are presented
	to illustrate the proposed approach: one dealing with domain-specific
	concepts (formalization of a kind of analogical reasoning), the other
	related to domain-independent sentences (expression and composing
	general properties of relations). The two cases are then considered
	in combination, in order to show how the interaction of different
	kinds of metaknowledge can be coped with. Solutions to these problems
	are worked out both in reflective Prolog, a language with extensive
	metalevel capabilities that the authors are developing, and also
	in Prolog, for comparison and assessment of the proposed language.
	The concept of preprocessing metaknowledge at program-consultation
	time for a significant improvement in efficiency is introduced and
	applied to the problem areas considered},
  doi = {10.1109/PCCC.1989.37443},
  keywords = {Prolog;auxiliary inference strategies;domain-independent sentences;domain-specific
	concepts;formalization;logic language;metaknowledge;metalevel features;metalogic
	programming;problem solving;PROLOG;logic programming;}
}

@INPROCEEDINGS{1612748,
  author = {Cote, D. and St-Denis, R. and Kerjean, S.},
  title = {Generative programming for programmable logic controllers},
  booktitle = {Emerging Technologies and Factory Automation, 2005. ETFA 2005. 10th
	IEEE Conference on},
  year = {2005},
  volume = {2},
  pages = {8 pp. -748},
  month = {sept.},
  abstract = {Many attempts have been made to implement supervisors derived by synthesis
	procedures peculiar to the supervisory control theory (SCT), most
	adopting the event-based supervisory control paradigm. However, when
	considering code generation schemata for programmable logic controllers
	(PLCs), hardware resources are limited and event tracking is hard
	to realize satisfactorily. Moreover, previous work has highlighted
	differences between the abstract model adopted by SCT and realistic
	process control situations. Inappropriate solutions to these issues
	may result in code generation schemata that produce unreliable PLC
	code. A generative programming approach for PLCs based on a dual
	paradigm, the state-based supervisory control paradigm, is investigated
	in this paper. Such an approach exhibits interesting properties.
	For instance, the maximum depth of the PLC stack as well as PLC cycle
	timing evaluations become possible. Furthermore, well-known code
	optimization techniques can be used to obtain more efficient code},
  doi = {10.1109/ETFA.2005.1612748},
  keywords = {code generation;code optimization;event tracking;generative programming;programmable
	logic controller;realistic process control;state-based supervisory
	control paradigm;control system analysis computing;optimising compilers;programmable
	controllers;}
}

@INPROCEEDINGS{1553549,
  author = {Courbis, C. and Finkelstein, A.},
  title = {Towards aspect weaving applications},
  booktitle = {Software Engineering, 2005. ICSE 2005. Proceedings. 27th International
	Conference on},
  year = {2005},
  pages = { 69 - 77},
  month = {may},
  abstract = { Software must be adapted to accommodate new features in the context
	of changing requirements. In this paper, we illustrate how applications
	with aspect weaving capabilities can be easily and dynamically adapted
	with unforeseen features. Aspects were used at three levels: in the
	context of semantic analysers, within a BPEL engine that orchestrates
	Web services, and finally within BPEL processes themselves. Each
	level uses its own tailored domain-specific aspect language that
	is easier to manipulate than a general-purpose one (close to the
	programming language) and the pointcuts are independent from the
	implementation.},
  doi = {10.1109/ICSE.2005.1553549},
  keywords = { BPEL engine; BPEL processes; Web services; aspect weaving applications;
	aspect-oriented programming; domain-specific aspect language; semantic
	analysers; software adaptability; Internet; object-oriented programming;}
}

@INPROCEEDINGS{114588,
  author = {Crabtree, B. and Crouch, R.S. and Moffat, D.C. and Pirie, N. and
	Pulman, S.G. and Ritchie, C.D. and Tate, A.},
  title = {Interacting with an intelligent planning system using English sentences},
  booktitle = {Expert Planning Systems, 1991., First International Conference on},
  year = {1990},
  pages = {169 -174},
  month = {jun},
  abstract = {An intelligent planning system is an example of a software aid which,
	although developed by specialists in artificial intelligence and
	customised for a particular application by knowledge engineers, is
	intended eventually to be used by non-programmers for a wide variety
	of tasks. The aim of the project described is to experiment with
	the use of English language as the medium of communication. The overall
	system consists of: a natural language front end (NLFE), which produces
	logical forms (LF) representing the meaning of an input sentence;
	a plan query language evaluator (PQLE) which accepts inputs in a
	plan query language and evaluates them against an internal representation
	of the current plan, or by manipulating the planner itself to try
	to produce further plans; and the planner, a hand-crafted, domain
	specific planner written in Prolog. The NLFE, PQLE and planner are
	all separate processes, controlled by a `debugger' interfaced to
	the Sun View Window system, enabling the course of operation of the
	system to be traced},
  keywords = { English language; English sentences; LF; NLFE; PQLE; Prolog; Sun
	View Window system; artificial intelligence; current plan; domain
	specific planner; hand-crafted; input sentence; intelligent planning
	system; internal representation; knowledge engineers; logical forms;
	natural language front end; non-programmers; plan query language
	evaluator; software aid; administrative data processing; knowledge
	based systems; natural languages; telecommunications computing; user
	interfaces;}
}

@INPROCEEDINGS{5295274,
  author = {Cramer, B. and Kastens, U.},
  title = {Animation automatically generated from simulation specifications},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {157 -164},
  month = {sept.},
  abstract = {Our generator framework DEVIL supports the development of visual languages.
	It generates complete language implementations from high-level specifications
	including advanced graphical structure editors. It has been successfully
	used for a wide range of domain specific visual languages. For a
	DSL that has an execution semantics, e.g. a processor specification
	language, it is desirable to simulate and to visualize program execution
	for purposes of analysis or evaluation. This paper shows how DEViL
	is extended to generate a simulator for a visual language from specification
	of its state transition model. Without the need of any further specification
	a smooth animation of program execution using a technique of graphical
	interpolation is generated automatically. Further advanced animations
	can easily be obtained by simply associating some "animated visual
	patterns" to standardized operations of the simulator. DEViL provides
	a large variety of such patterns which encapsulate the implementation
	of certain useful animation effects. Our approach has proven to be
	effective for the animation of several DSLs.},
  doi = {10.1109/VLHCC.2009.5295274},
  issn = {1943-6092},
  keywords = {DEViL;graphical interpolation;program execution animation;simulation
	specifications;state transition model;visual language;digital simulation;visual
	languages;}
}

@INPROCEEDINGS{77056,
  author = {Crimi, C. and Guercio, A. and Tortora, G. and Tucci, M.},
  title = {An intelligent iconic system to generate and to interpret visual
	languages},
  booktitle = {Visual Languages, 1989., IEEE Workshop on},
  year = {1989},
  pages = {144 -149},
  month = {oct},
  abstract = {Using the SIL-Icon compiler, whereby a general-purpose iconic system
	is specialized by expert provision of domain-specific grammar and
	semantics to define the individual visual language, a learning interface
	module (LIM) has been designed allowing the nonexpert user to utilize
	the system and define his or her own visual language providing only
	sample sentences from which the LIM autonomously constructs the domain-specific
	aspects of the language. A brief description of the SIL-Icon compiler
	is presented. An overview is given of the whole learning iconic system,
	and all the principal concepts and modules are detailed},
  doi = {10.1109/WVL.1989.77056},
  keywords = {LIM;SIL-Icon compiler;domain-specific grammar;intelligent iconic system;learning
	interface module;semantics;visual languages;high level languages;program
	compilers;user interfaces;}
}

@INPROCEEDINGS{4484888,
  author = {Cruz, F. and Barreto, R. and Cordeiro, L. and Maciel, P.},
  title = {ezRealtime: A Domain-Specific Modeling Tool for Embedded Hard Real-Time
	Software Synthesis},
  booktitle = {Design, Automation and Test in Europe, 2008. DATE '08},
  year = {2008},
  pages = {1510 -1515},
  month = {march},
  abstract = {In this paper, we introduce the ezRealtime project, which relies on
	the Time Petri Net (TPN) formalism and defines a Domain-Specific
	Modeling (DSM) tool to provide an easy- to-use environment for specifying
	Embedded Hard Real-Time (EHRT) systems and for synthesizing timely
	and predictable scheduled C code. Therefore, this paper presents
	a generative programming method in order to boost code quality and
	improve substantially developer productivity by making use of automated
	software synthesis. The ezRealtime tool reads and automatically translates
	the system's specification to a time Petri net model through composition
	of building blocks with the purpose of providing a complete model
	of all tasks in the system. Hence, this model is used to find a feasible
	schedule by applying a depth-first search algorithm. Finally, the
	scheduled code is generated by traversing the feasible schedule,
	and replacing transition's instances by the respective code segments.
	We also present the application of the proposed method in an expressive
	case study.},
  doi = {10.1109/DATE.2008.4484888},
  keywords = {Domain-Specific Modeling tool;Embedded Hard Real-Time systems;automated
	software synthesis;domain-specific modeling tool;embedded hard real-time
	software synthesis;ezRealtime;generative programming method;time
	Petri Net formalism;Petri nets;embedded systems;software engineering;specification
	languages;}
}

@ARTICLE{4782971,
  author = {Cuadrado, J.S. and Molina, J.G.},
  title = {A Model-Based Approach to Families of Embedded Domain-Specific Languages},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2009},
  volume = {35},
  pages = {825 -840},
  number = {6},
  month = {nov.-dec. },
  abstract = {With the emergence of model-driven engineering (MDE), the creation
	of domain-specific languages (DSLs) is becoming a fundamental part
	of language engineering. The development cost of a DSL should be
	modest compared to the cost of developing a general-purpose programming
	language. Reducing the implementation effort and providing reuse
	techniques are key aspects for DSL approaches to be really effective.
	In this paper, we present an approach to build embedded domain-specific
	languages applying the principles of model-driven engineering. On
	the basis of this approach, we will tackle reuse of DSLs by defining
	families of DSLs, addressing reuse both from the DSL developer and
	user point of views. A family of DSLs will be built up by composing
	several DSLs, so we will propose composition mechanisms for the abstract
	syntax, concrete syntax, and model transformation levels of a DSL's
	definition. Finally, we contribute a software framework to support
	our approach, and we illustrate the paper with a case study to demonstrate
	its practical applicability.},
  doi = {10.1109/TSE.2009.14},
  issn = {0098-5589},
  keywords = {abstract syntax;concrete syntax;embedded domain-specific languages;model
	transformation levels;model-driven engineering;programming language;computational
	linguistics;programming languages;software engineering;}
}

@ARTICLE{4302686,
  author = {Cuadrado, J.S. and Molina, J.G.},
  title = {Building Domain-Specific Languages for Model-Driven Development},
  journal = {Software, IEEE},
  year = {2007},
  volume = {24},
  pages = {48 -55},
  number = {5},
  month = {sept.-oct. },
  abstract = {Today, the popularity of dynamic languages such as Python and Ruby
	is growing beyond their use as scripting languages. In fact, Sun,
	Microsoft, and other companies are supporting some dynamic languages
	on their development platforms. Developers increasingly find that
	dynamic languages' features help them enhance their productivity,
	while common misconceptions about them, such as poor performance
	and reliability, are disappearing. Embedding domain-specific language
	in a dynamic language rather than constructing a compiler or interpreter
	can improve program readability and development time.},
  doi = {10.1109/MS.2007.135},
  issn = {0740-7459},
  keywords = {development platforms;domain-specific languages;dynamic languages;model-driven
	development;program readability;scripting languages;simulation languages;software
	engineering;}
}

@INPROCEEDINGS{6070396,
  author = {Cunha, Jacome and Mendes, Jorge and Saraiva, Joao and Fernandes,
	Joao Paulo},
  title = {Embedding and evolution of spreadsheet models in spreadsheet systems},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2011 IEEE
	Symposium on},
  year = {2011},
  pages = {179 -186},
  month = {sept.},
  abstract = {This paper describes the embedding of ClassSheet models in spreadsheet
	systems. ClassSheet models are well-known and describe the business
	logic of spreadsheet data. We embed this domain specific model representation
	on the (general purpose) spreadsheet system. By defining such an
	embedding, we provide end users a model-driven engineering spreadsheet
	developing environment. End users can interact with both the model
	and the spreadsheet data in the same environment. Moreover, we use
	advanced techniques to evolve spreadsheets and models and to have
	them synchronized. In this paper we present our work on extending
	a widely used spreadsheet system with such a model-driven spreadsheet
	engineering environment.},
  doi = {10.1109/VLHCC.2011.6070396},
  issn = {1943-6092}
}

@INPROCEEDINGS{4076961,
  author = {Matthew Curland and Terry Halpin},
  title = {Model Driven Development with NORMA},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {286a},
  month = {jan. },
  abstract = {Object-role modeling (OEM) is a fact-oriented approach for specifying,
	transforming, and querying information at a conceptual level. Unlike
	entity-relationship (ER) modeling and unified modeling language (UML)
	class diagrams, ORM is attribute-free, treating all elementary facts
	as relationships. For information modeling, fact-oriented graphical
	notations are typically far more expressive than other notations.
	Based on extensive industrial feedback, a second generation ORM (ORM
	2) was recently specified. This paper provides a detailed discussion
	of NORMA (Neumont ORM Architect), a software tool that facilitates
	entry, validation, and mapping of ORM 2 models. Building on Microsoft's
	domain specific language (DSL) technology, NORMA is implemented as
	an open-source plug-in to visual studio .NET. As well as supporting
	ORM 2, with automated verbalization and live error-handling, NORMA
	automatically generates code for relational database models, object
	models, and XML schemas},
  doi = {10.1109/HICSS.2007.384},
  issn = {1530-1605},
  keywords = {Microsoft domain specific language;Neumont ORM Architect software
	tool;UML class diagram;Unified Modeling Language;XML schema;automated
	verbalization;entity-relationship modeling;fact-oriented graphical
	notation;information modeling;live error-handling;model driven development;object
	model;object-role modeling;open-source plug-in to Visual Studio .NET;relational
	database model;Unified Modeling Language;XML;entity-relationship
	modelling;object-oriented programming;public domain software;relational
	databases;software architecture;software tools;visual languages;}
}

@INPROCEEDINGS{5532752,
  author = {Cur, Olivier and Forax, Rmi and Degenne, Pascal and Seen,
	Danny Lo and Parigot, Didier and Lahcen, Ayoub Ait},
  title = {Ocelet: An Ontology-Based Domain Specific Language to Model Complex
	Domains},
  booktitle = {Communication Theory, Reliability, and Quality of Service (CTRQ),
	2010 Third International Conference on},
  year = {2010},
  pages = {255 -260},
  month = {june},
  abstract = {In this work, we consider that the modeling of complex domains can
	be performed using Domain Specific Languages (DSL). The main principle
	of this approach consists in developing DSL primitives and to assemble
	them to model a certain domain. The ability to add new primitives
	into an existing model and to fine-tune it by replacing some of them
	provides a flexibility that is highly desirable in simulation intense
	fields. We have designed such a language, named textit{Ocelet}, which
	is tailored for dynamic landscape modeling. We consider that three
	important components may influence the adoption of this approach:
	a graphical user interface to build models in an efficient and user-friendly
	way, a solution to reason, e.g., consistency checking, about model
	primitives and a tool to facilitate the development of primitives
	repositories. In this paper, we emphasize that an ontology-based
	approach is adapted to design all these components. Moreover, a mapping
	between ontology and Ocelet elements is sufficient for its achievement
	and supports automatic transformations from one model to the other.},
  doi = {10.1109/CTRQ.2010.50}
}

@INPROCEEDINGS{1691609,
  author = {Czarnecki, K.},
  title = {Tutorial on Generative Software Development},
  booktitle = {Software Product Line Conference, 2006 10th International},
  year = {2006},
  pages = { 227},
  month = {aug.},
  abstract = { Software product line engineering (SPLE) [5] seeks to exploit the
	commonalities among systems from a given problem domain while managing
	the variabilities among them in a systematic way. In SPLE, new system
	variants can be rapidly created based on a set of reusable assets,
	such as a common architecture, components, and models. Generative
	software development [6] aims at modeling and implementing product
	lines in such a way that a given system can be automatically generated
	from a specification written in one or more textual or graphical
	domain-specific languages (DSLs) [13, 4, 15, 8, 3, 1, 12, 14].},
  doi = {10.1109/SPLINE.2006.1691609}
}

@INPROCEEDINGS{882405,
  author = {Czejdo, B. and Dinsmore, J. and Hwang, C.H. and Miller, R. and Rusinkiewicz,
	M.},
  title = {Automatic generation of ontology based annotations in XML and their
	use in retrieval systems},
  booktitle = {Web Information Systems Engineering, 2000. Proceedings of the First
	International Conference on},
  year = {2000},
  volume = {1},
  pages = {296 -300 vol.1},
  abstract = {The unprecedented growth in the volume of online information has led
	to an information explosion which has made producing queries over
	this information difficult. In this paper, we examine an automated
	mechanism which allows users to access this information in a structured
	manner by analyzing unstructured text by domain-specific ontologies,
	annotating these documents using XML tags and using specific query
	processing techniques. Our approach is to use focused linguistic
	techniques over a limited context of an ontology for the identification
	and annotation of basic concepts and relationships in textual documents.
	These annotated documents can then be related to other structured
	information sources specified over similar ontologies},
  doi = {10.1109/WISE.2000.882405},
  keywords = {XML tags;automatic annotation generation;domain-specific ontologies;focused
	linguistic techniques;information explosion;online information retrieval
	systems;ontology-based annotations;query processing techniques;structured
	information access;structured information sources;textual documents;unstructured
	text analysis;hypermedia markup languages;information retrieval systems;query
	processing;}
}

@INPROCEEDINGS{5619267,
  author = {Zhao Da-Zhe and Li Wei and Yang Jin-Zhu},
  title = {An XML-based process definition language for medical image understanding},
  booktitle = {Computer Application and System Modeling (ICCASM), 2010 International
	Conference on},
  year = {2010},
  volume = {1},
  pages = {V1-679 -V1-683},
  month = {oct.},
  abstract = {Medical image understanding is a computing process of object recognition
	in medical image using computer vision, applied mathematics, signal
	analysis and artificial intelligence. This paper provides a medical
	image understanding process definition language (MPDL) which aims
	at solving the problem of object recognition in medical image and
	satisfying the requisition of stable and high precision medical image
	analysis software engineering. MPDL supports coding, debugging, maintenance
	and share of medical image understanding algorithm. A software component
	model of medical image understanding algorithm and a mechanism of
	mapping and synchronization between component and process node in
	medical image understanding process are represented. The method of
	validating a medical image understanding process definition is showed
	in addition. The rationality and applicability of this language is
	validated by a platform of medical image understanding algorithm
	development which uses MPDL.},
  doi = {10.1109/ICCASM.2010.5619267},
  keywords = {MPDL;XML-based process definition language;applied mathematics;artificial
	intelligence;computer vision;medical image analysis software engineering;medical
	image understanding algorithm;medical image understanding process
	definition language;object recognition;signal analysis;software component
	model;XML;artificial intelligence;computer vision;medical image processing;object
	recognition;object-oriented programming;software engineering;specification
	languages;}
}

@INPROCEEDINGS{5158836,
  author = {Dabholkar, A. and Gokhale, A.},
  title = {An Approach to Middleware Specialization for Cyber Physical Systems},
  booktitle = {Distributed Computing Systems Workshops, 2009. ICDCS Workshops '09.
	29th IEEE International Conference on},
  year = {2009},
  pages = {73 -79},
  month = {june},
  abstract = {Contemporary computing infrastructure, such as networking stacks,
	OS and middleware, are made up of layers of software functionality
	that have evolved over decades to support the broadest range of applications.The
	feature-richness and the layers of functionality, however, tend to
	be excessive and a source of performance overhead for cyber-physical
	Systems (CPS). Yet it is necessary to leverage the decades of proven
	patterns and principles in these infrastructures. This paper presents
	an approach to systematically specialize general-purpose middleware
	used to host CPS. Our approach is based on the principles of feature-oriented
	software development (FOSD), which requires deducing an algebraic
	structure of contemporary middleware based on a higher level of abstraction
	of features. The paper showcase how Origami matrices and generative
	programming can play a key role in realizing the specializations.
	The paper concludes by delving in to future open areas of middleware
	specialization research.},
  doi = {10.1109/ICDCSW.2009.70},
  issn = {1545-0678},
  keywords = {Origami matrix;algebraic structure;cyber physical system;feature-oriented
	software development;middleware specialization;software functionality;middleware;object-oriented
	programming;software engineering;}
}

@INPROCEEDINGS{6037580,
  author = {Dahman, K. and Charoy, F. and Godart, C.},
  title = {Towards Consistency Management for a Business-Driven Development
	of SOA},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2011 15th
	IEEE International},
  year = {2011},
  pages = {267 -275},
  month = {29 2011-sept. 2},
  abstract = {The usage of the Service Oriented Architecture (SOA) along with the
	Business Process Management has emerged as a valuable solution for
	the complex (business process driven) system engineering. With a
	Model Driven Engineering where the business process models drive
	the supporting service component architectures, less effort is gone
	into the Business/IT alignment during the initial development activities,
	and the IT developers can rapidly proceed with the SOA implementation.
	However, the difference between the design principles of the emerging
	domain-specific languages imposes serious challenges in the following
	re-design phases. Moreover, enabling evolutions on the business process
	models while keeping them synchronized with the underlying software
	architecture models is of high relevance to the key elements of any
	Business Driven Development (BDD). Given a business process update,
	this paper introduces an incremental model transformation approach
	that propagates this update to the related service component configurations.
	It, therefore, supports the change propagation among heterogenous
	domain-specific languages, e.g., the BPMN and the SCA. As a major
	contribution, our approach makes model transformation more tractable
	to reconfigure system architecture without disrupting its structural
	consistency. We propose a synchronizer that provides the BPMN-to-SCA
	model synchronization with the help of the conditional graph rewriting.},
  doi = {10.1109/EDOC.2011.30},
  issn = {1541-7719},
  keywords = {BPMN-to-SCA model synchronization;SOA;business process management;business-driven
	development;consistency management;model driven engineering;service
	oriented architecture;software architecture models;system engineering;business
	data processing;service-oriented architecture;}
}

@INPROCEEDINGS{5479518,
  author = {Dalpez, S. and Passerone, R. and Cancila, D. and Terrier, F.},
  title = {An Industrial Case Study Using an MBE Approach: From Architecture
	to Safety Analysis},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	Workshops (ISORCW), 2010 13th IEEE International Symposium on},
  year = {2010},
  pages = {116 -122},
  month = {may},
  abstract = {We discuss the initial phases of software development of a real industrial
	safety-related device in the railway application domain. In particular,
	to achieve greater confidence in the system, we illustrate the development
	of the system architecture (using a standard model domain-specific
	language), the computation of the safety integrity level and the
	calculation of the reliability of the whole system. We reiterate
	the safety analysis on the sub-systems. The proposed methodology
	has found immediate industrial applications.},
  doi = {10.1109/ISORCW.2010.11},
  keywords = {MBE approach;domain-specific language standard model;industrial safety-related
	device;model-based engineering approach;railway application domain;safety
	integrity level;software development;subsystem safety analysis;system
	architecture;railways;safety devices;software architecture;}
}

@INPROCEEDINGS{1337574,
  author = {Damasevicius, R. and Stuikys, V.},
  title = {Application of UML for hardware design based on design process model},
  booktitle = {Design Automation Conference, 2004. Proceedings of the ASP-DAC 2004.
	Asia and South Pacific},
  year = {2004},
  pages = { 244 - 249},
  month = {jan.},
  abstract = { We address a problem of reusing and customizing soft IP components
	by introducing a concept of design process-a series of common, well-defined
	and well-proven domain-specific actions and methods performed to
	achieve a certain design aim. We especially examine system-level
	design processes that are aimed at designing a hardware system by
	integrating soft IPs at a high level of abstraction. We combine this
	concept with object-oriented hardware design using UML and metaprogramming
	paradigm for describing generation of domain code.},
  doi = {10.1109/ASPDAC.2004.1337574},
  issn = { },
  keywords = { UML; metaprogramming; object-oriented hardware design; soft IP components;
	system-level design process; hardware description languages; industrial
	property; object-oriented methods; specification languages;}
}

@INPROCEEDINGS{5463677,
  author = {Damjanovic, V. and Djuric, D.},
  title = {Functional Programming Way to Interact with Software Attacks and
	Vulnerabilities},
  booktitle = {Software Testing, Verification, and Validation Workshops (ICSTW),
	2010 Third International Conference on},
  year = {2010},
  pages = {388 -393},
  month = {april},
  abstract = {This paper proposes using functional programming style in a way to
	respond to detection of and interaction with the software attacks
	and vulnerabilities. Additionally, our approach considers involving
	Description Logics, as a basis for the use of the Semantic Web and
	meta-programming to produce executable ontologies and to enable semantic
	reasoning over behavior and interaction with software attacks and
	vulnerabilities. Accordingly, we introduce Magic Potion, a recently
	defined Domain Specific meta-Language that uses Modeling Spaces framework
	to study heterogeneous modeling and meta-modeling problems inspired
	by Model Driven Architecture. As an example of formalism for modeling
	software attacks and vulnerabilities, we explore Attack Tree, which
	provides a formal methodology for analyzing the security of the system.
	Based on Attack Tree, which is herein specified for a particular
	problem of dealing with known attacks and vulnerabilities of the
	security layer of the Wireless Application Protocol, and which is
	particularly built on top of Magic Potion specification, we define
	our specific Domain Specific Language that we call Attack Tree Domain
	Specific Language. It is envisioned as a tool for modeling and interacting
	with software attacks and vulnerabilities.},
  doi = {10.1109/ICSTW.2010.53},
  keywords = {Magic Potion language;attack tree;description logics;domain specific
	meta-language;executable ontologies;functional programming;meta-programming;semantic
	Web;semantic reasoning;software attacks;software vulnerability;wireless
	application protocol;formal logic;formal specification;functional
	programming;inference mechanisms;ontologies (artificial intelligence);security
	of data;semantic Web;software architecture;specification languages;}
}

@INPROCEEDINGS{5295308,
  author = {Dantra, R. and Grundy, J. and Hosking, J.},
  title = {A domain-specific visual language for report writing using Microsoft
	DSL tools},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {15 -22},
  month = {sept.},
  abstract = {Many domain specific textual languages have been developed for generating
	complex reports. These are challenging for novice users to learn,
	understand and use. We describe our work developing the prototype
	of a new visual language tool for a company to augment their textual
	report writing language. We describe key motivations for our visual
	language tool solution, its architecture, design and development
	using Microsoft DSL tools, and its evaluation by end-users.},
  doi = {10.1109/VLHCC.2009.5295308},
  issn = {1943-6092},
  keywords = {Microsoft DSL tools;domain specific textual languages;textual report
	writing language;visual language tool;software tools;specification
	languages;visual languages;}
}

@INPROCEEDINGS{1291158,
  author = {Dassen, W.R.M. and Gorgels, A.P.M. and Berendsen, A. and Dijk, W.A.
	and de Clercq, P.A. and Hasman, A. and Baljon, M.H.},
  title = {Guideline assessment and implementation in congestive heart failure},
  booktitle = {Computers in Cardiology, 2003},
  year = {2003},
  pages = { 331 - 334},
  month = {sept.},
  abstract = { In this study the feasibility of representing guidelines on the pharmacological
	management of heart failure was assessed using a toolbox specially
	designed to enter and during runtime manipulate guidelines. The toolbox
	distinguishes three layers to define guidelines. In the first layer,
	the flow charts derived from the guidelines are drawn, and annotated
	using natural language. Secondly, the respective rules are entered,
	using a domain specific vocabulary. The decision support system applies
	this guideline knowledge to the actual patient status and relevant
	laboratory results, entered by the physician or obtained from an
	existing data source, resulting in advice regarding this patient.
	In this limited study the selection and titration of three classes
	of drugs was modeled and for each category a prescription strategy
	was defined, based on severity of overfilling, co-medication and
	laboratory data. The decision support system selects, based on the
	current clinical state of the patient, the underlying pathophysiology
	and presence of comorbidity, the most appropriate drug strategy.},
  doi = {10.1109/CIC.2003.1291158},
  issn = {0276-6547 },
  keywords = { co-medication; comorbidity; congestive heart failure; decision support
	system; domain specific vocabulary; drugs; guideline assessment;
	guideline implementation; guideline knowledge; laboratory results;
	natural language; pathophysiology; patient status; pharmacological
	management; prescription strategy; cardiology; decision support systems;
	diseases; drugs; medical expert systems; medical information systems;
	natural languages;}
}

@ARTICLE{1234766,
  author = {Davalcu, H. and Vadrevu, S. and Nagarajan, S. and Ramakrishnan, I.V.},
  title = {OntoMiner: bootstrapping and populating ontologies from domain-specific
	Web sites},
  journal = {Intelligent Systems, IEEE},
  year = {2003},
  volume = {18},
  pages = { 24 - 33},
  number = {5},
  month = {sep/oct},
  abstract = { Key to the Semantic Web idea are ontologies that can transform legacy
	HTML documents into Semantic Web documents and encode domain knowledge
	to facilitate automated reasoning. The techniques presented in the
	paper can help bootstrap and populate specialized domain ontologies.},
  doi = {10.1109/MIS.2003.1234766},
  issn = {1541-1672},
  keywords = { OntoMiner; Semantic Web; automated reasoning; bootstrapping; domain
	knowledge; domain-specific Web sites; legacy HTML documents; metadata;
	ontologies; Internet; Web sites; data mining; hypermedia markup languages;
	inference mechanisms; learning (artificial intelligence); meta data;}
}

@INPROCEEDINGS{257490,
  author = {De Gloria, A. and Faraboschi, P. and Sensalari, G.},
  title = {VLSI design of domain specific architecture},
  booktitle = {CompEuro '91. Advanced Computer Technology, Reliable Systems and
	Applications. 5th Annual European Computer Conference. Proceedings.},
  year = {1991},
  pages = {786 -791},
  month = {may},
  abstract = {Novel methodologies for IC design need to be developed, to provide
	effective cost/performance tradeoffs. A structured approach to the
	design of domain specific integrated circuits is presented. Starting
	from a high-level language application description, the approach
	provides the designer with a set of tools to guide his choices at
	the different levels of detail of the design flow. Particular attention
	is focused on the layout phase, where a block standard cell technique
	is proposed. The technique can easily be integrated into the system
	and allows a good compromise between design time and chip area},
  doi = {10.1109/CMPEUR.1991.257490},
  keywords = {IC design;VLSI design;block standard cell;chip area;design time;domain
	specific integrated circuits;high-level language;layout;VLSI;circuit
	layout CAD;}
}

@INPROCEEDINGS{814270,
  author = {De Rammelaere, W. and Eckert, K. and Hilkens, E. and Lawell, T. and
	McGarity, R. and Le Moenner, P. and Steininger, F.},
  title = {Catalyst: a DSIP design flow development in industry},
  booktitle = {System Synthesis, 1999. Proceedings. 12th International Symposium
	on},
  year = {1999},
  pages = {122 -127},
  month = {nov},
  abstract = {The Motorola System on Chip Design Technologies (SoCDT) team aims
	at providing a system design environment for its customers. The Toulouse
	branch concentrates on design efforts incorporating DSP functionality.
	This is referred to as the Catalyst methodology. We found that in
	current systems, very often the software development cycle is longer
	than that of the silicon development. To ease the software burden,
	we have changed the silicon architecture and its flow to permit the
	DSP software to be written in the C language instead of assembler
	code, as is normally done. The resulting architecture is domain specific;
	it is smaller, has a reduced design cycle and is simpler to implement
	because it is tuned to the application software we are providing.
	The paper describes the methodology which we are developing to create
	domain specific architectures, it shows one example architecture
	and aspects which are critical for industry acceptance},
  doi = {10.1109/ISSS.1999.814270},
  keywords = {C language;Catalyst;DSIP design flow development;DSP functionality;DSP
	software;Motorola System on Chip Design Technologies;application
	software;domain specific architectures;industry acceptance;reduced
	design cycle;silicon architecture;silicon development;software burden;software
	development cycle;system design environment;C language;digital signal
	processing chips;hardware-software codesign;instruction sets;microprocessor
	chips;software engineering;}
}

@INPROCEEDINGS{899745,
  author = {De Swaan Arons, H. and Van Asperen, E.},
  title = {Computer assistance for model definition},
  booktitle = {Simulation Conference, 2000. Proceedings. Winter},
  year = {2000},
  volume = {1},
  pages = {399 -408 vol.1},
  abstract = {Modeling requires considerable knowledge of the various stages of
	the simulation process. The modeler needs to know a great deal of
	the system to be modeled (domain specific knowledge), the ins and
	outs of the modeling process itself (the degree of detail of the
	model) and how to implement the model in a simulation language. Each
	of these stages would benefit from some kind of knowledgeable support.
	In this article a decision-making process is described that supports
	the modeler to build a model step by step. As a vehicle the Arena
	simulation environment has been used. The support is based on information
	provided by the modeler and is essentially data-driven. It suggests
	which modules could be used best, which parameters need to be determined
	and helps to formulate route information. This research aims for
	an implementation of this support using a knowledge-based system},
  doi = {10.1109/WSC.2000.899745},
  keywords = {Arena simulation environment;data-driven;decision-making process;domain
	specific knowledge;knowledge-based system;model definition;modeling
	process;simulation language;simulation process;digital simulation;knowledge
	based systems;simulation languages;}
}

@INPROCEEDINGS{6076507,
  author = {Dede, Elif and Fadika, Zacharia and Gupta, Chaitali and Govindaraju,
	Madhusudhan},
  title = {Scalable and Distributed Processing of Scientific XML Data},
  booktitle = {Grid Computing (GRID), 2011 12th IEEE/ACM International Conference
	on},
  year = {2011},
  pages = {121 -128},
  month = {sept.},
  abstract = {A seamless and intuitive search capability for the vast amount of
	datasets generated by scientific experiments is critical to ensure
	effective use of such data by domain specific scientists. Currently,
	searches on enormous XML datasets is done manually via custom scripts
	or by using hard-to-customize queries developed by experts in complex
	and disparate XML query languages. Such approaches however do not
	provide acceptable performance for large-scale data since they are
	not based on a scalable distributed solution. Furthermore, it has
	been shown that databases are not optimized for queries on XML data
	generated by scientific experiments, as term kinship, range based
	queries, and constraints such as conjunction and negation need to
	be taken into account. There exists a critical need for an easy-to-use
	and scalable framework, specialized for scientific data, that provides
	natural-language-like syntax along with accurate results. As most
	existing search tools are designed for exact string matching, which
	is not adequate for scientific needs, we believe that such a framework
	will enhance the productivity and quality of scientific research
	by the data reduction capabilities it can provide. This paper presents
	how the MapReduce model should be used in XML metadata indexing for
	scientific datasets, specifically TeraGrid Information Services and
	the NeXus datasets generated by the Spallation Neutron Source (SNS)
	scientists. We present an indexing structure that scales well for
	large-scale MapReduce processing. We present performance results
	using two MapReduce implementations, Apache Hadoop and LEMO-MR, to
	emphasize the flexibility and adaptability of our framework in different
	MapReduce environments.},
  doi = {10.1109/Grid.2011.24},
  issn = {1550-5510}
}

@INPROCEEDINGS{4814172,
  author = {Deissenboeck, F. and Hummel, B. and Jurgens, E. and Schatz, B. and
	Wagner, S. and Girard, J.-F. and Teuchert, S.},
  title = {Clone detection in automotive model-based development},
  booktitle = {Software Engineering, 2008. ICSE '08. ACM/IEEE 30th International
	Conference on},
  year = {2008},
  pages = {603 -612},
  month = {may},
  abstract = {Model-based development is becoming an increasingly common development
	methodology. In important domains like embedded systems already major
	parts of the code are generated from models specified with domain-specific
	modelling languages. Hence, such models are nowadays an integral
	part of the software development and maintenance process and therefore
	have a major economic and strategic value for the software-developing
	organisations. Nevertheless almost no work has been done on a quality
	defect that is known to seriously hamper maintenance productivity
	in classic code-based development. This paper presents an approach
	for the automatic detection of clones in large models as they are
	used in model-based development of control systems. The approach
	is based on graph theory and hence can be applied to most graphical
	data-flow languages. An industrial case study demonstrates the applicability
	of our approach for the detection of clones in Matlab/Simulink models
	that are widely used in model-based development of embedded systems
	in the automotive domain.},
  doi = {10.1145/1368088.1368172},
  issn = {0270-5257},
  keywords = {Matlab/Simulink model;automatic clone detection;automotive model-based
	development;classic code-based development;control system;domain-specific
	modelling language;embedded system;graph theory;graphical data-flow
	language;quality defect;software development;software maintenance
	productivity;software reusability;software-development organisation;automotive
	engineering;embedded systems;graph theory;program diagnostics;software
	maintenance;software quality;software reusability;}
}

@INPROCEEDINGS{1182293,
  author = {van Delden, S.},
  title = {A hybrid approach to pre-conjunct identification},
  booktitle = {Language Engineering Conference, 2002. Proceedings},
  year = {2002},
  pages = { 72 - 76},
  month = {dec.},
  abstract = { An algorithm that identifies pre-conjuncts in natural language sentences
	is described. Syntactic and semantic information is combined to ascertain
	the size of the syntactic relation that precedes a coordinate conjunction.
	The approach is not domain specific and relies only on part-of-speech
	information. Performance results on several corpora are given.},
  doi = {10.1109/LEC.2002.1182293},
  issn = { },
  keywords = { coordinate conjunction; natural language sentences; part-of-speech
	information; pre-conjunct identification; semantic information; syntactic
	information; computational linguistics; grammars; natural languages;}
}

@INPROCEEDINGS{1180817,
  author = {van Delden, S. and Gomez, F.},
  title = {Combining finite state automata and a greedy learning algorithm to
	determine the syntactic roles of commas},
  booktitle = {Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.
	14th IEEE International Conference on},
  year = {2002},
  pages = { 293 - 300},
  abstract = { A method has been developed and implemented that assigns syntactic
	roles to commas. Text that has been tagged using a part-of-speech
	tagger serves as the input to the system. A set of Finite State Automata
	first assigns temporary syntactic roles to each comma in the sentence.
	A greedy learning algorithm is then used to determine the final syntactic
	roles of the commas. The system requires no training and is not domain
	specific. The performance of the system on numerous corpora is given.},
  doi = {10.1109/TAI.2002.1180817},
  issn = {1082-3409},
  keywords = { finite state automata; greedy learning; natural languages; part-of-speech
	tagger; syntactic roles; syntactic roles of commas; computational
	linguistics; finite state machines; learning (artificial intelligence);
	natural languages;}
}

@INPROCEEDINGS{5959795,
  author = {Demirli, E. and Tekinerdogan, B.},
  title = {SAVE: Software Architecture Environment for Modeling Views},
  booktitle = {Software Architecture (WICSA), 2011 9th Working IEEE/IFIP Conference
	on},
  year = {2011},
  pages = {355 -358},
  month = {june},
  abstract = {Currently, a common practice is to model and document architecture
	based on architectural views. Architectural views conform to viewpoints
	that represent the conventions for constructing and using architecture
	views. So far most architecture viewpoints seem to have been primarily
	used either to support the communication among stakeholders, or at
	the best to provide a blueprint for the detailed design. In this
	paper we introduce the eclipse plug-in tool, Software Architecture
	Environment for modeling Views (SAVE) tool that can be used to model
	software architecture based on viewpoints from existing viewpoint
	approaches. In the tool each viewpoint is modeled as a domain specific
	language which increases the formal precision of the derived views
	and as such enables model-driven development.},
  doi = {10.1109/WICSA.2011.57},
  keywords = {SAVE tool;architecture viewpoints;design blueprint;domain specific
	language;eclipse plug-in tool;model driven development;software architecture
	environment for modeling views tool;software architecture;software
	tools;specification languages;}
}

@INPROCEEDINGS{933658,
  author = {Demko, A.B. and Pizzi, N.J. and Somorjai, R.L.},
  title = {A Classification Canvas for the analysis of biomedical data},
  booktitle = {Electrical and Computer Engineering, 2001. Canadian Conference on},
  year = {2001},
  volume = {2},
  pages = {1391 -1396 vol.2},
  abstract = {With the rapid proliferation of complex high-dimensional biomedical
	data, an acute need exists for a comprehensive, knowledge-based,
	domain-specific, user-friendly software suite that allows investigators,
	in the health care disciplines, to classify their data through the
	detection of novel or discriminating features therein. The Classification
	Canvas is an attempt to achieve these goals in addition to providing
	intuitive visual computation and logic construction. In this paper
	we describe various design and implementation issues such as: balancing
	user (novice) friendliness and developer (experienced) utility, performance
	versus modularity trade-offs, C++ and Java data sharing responsibilities,
	and creating graphical interfaces for (user-supplied) algorithm control},
  doi = {10.1109/CCECE.2001.933658},
  keywords = {C++;CanClass;Classification Canvas;Java;complex high-dimensional biomedical
	data;comprehensive knowledge-based domain-specific user-friendly
	software suite;data sharing responsibilities;developer utility;graphical
	interfaces;health care;logic construction;modularity;performance;user
	friendliness;user-supplied algorithm control;visual computation;C++
	language;Java;knowledge based systems;medical signal processing;signal
	classification;software packages;tree data structures;}
}

@INPROCEEDINGS{1174250,
  author = {Demner-Fushman, D. and Oard, D.W.},
  title = {The effect of bilingual term list size on dictionary-based cross-language
	information retrieval},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { Bilingual term lists are extensively used as a resource for dictionary-based
	cross-language information retrieval (CLIR), in which the goal is
	to find documents written in one natural language based on queries
	that are expressed in another. This paper identifies eight types
	of terms that affect retrieval effectiveness in CLIR applications
	through their coverage by general-purpose bilingual term lists, and
	reports results from an experimental evaluation of the coverage of
	35 bilingual term lists in news retrieval application. Retrieval
	effectiveness was found to be strongly influenced by term list size
	for lists that contain between 3,000 and 30,000 unique terms per
	language. Supplemental techniques for named entity translation were
	found to be useful with even the largest lexicons. The contribution
	of named-entity translation was evaluated in a cross-language experiment
	involving English and Chinese. Smaller effects were observed from
	deficiencies in the coverage of domain-specific terminology when
	searching news stories.},
  doi = {10.1109/HICSS.2003.1174250},
  issn = { },
  keywords = { Chinese language; English language; bilingual term list; cross-language
	information retrieval; dictionary-based information retrieval; named-entity
	translation; natural languages; dictionaries; information retrieval;
	language translation; natural languages;}
}

@INPROCEEDINGS{4539324,
  author = {Deneke, W. and Eno, J. and Wingning Li and Thompson, C. and Talburt,
	J. and Loghry, J. and Nash, D. and Stires, J.},
  title = {Towards a Domain-Specific Modeling Language for Customer Data Integration
	Workflow},
  booktitle = {Grid and Pervasive Computing Workshops, 2008. GPC Workshops '08.
	The 3rd International Conference on},
  year = {2008},
  pages = {49 -56},
  month = {may},
  abstract = {This paper describes the workflow specification problem, how workflows
	are specified today, requirements for improved workflow specification,
	and begins to sketch a new domain-specific modeling language (DSML)
	approach for specifying intent that can be used to constructively
	generate a complete workflow meeting a collection of intent requirements.
	This is an interim report on work in progress.},
  doi = {10.1109/GPC.WORKSHOPS.2008.49},
  keywords = {customer data integration workflow;domain-specific modeling language;workflow
	specification problem;formal specification;marketing data processing;workflow
	management software;}
}

@INPROCEEDINGS{1630497,
  author = {Gan Deng and Schmidt, D.C. and Gokhale, A. and Nechypurenko, A.},
  title = {Modularizing variability and scalability concerns in distributed
	real-time and embedded systems with modeling tools and component
	middleware},
  booktitle = {Object and Component-Oriented Real-Time Distributed Computing, 2006.
	ISORC 2006. Ninth IEEE International Symposium on},
  year = {2006},
  pages = {8 pp.},
  month = {april},
  abstract = {Developing real-time software for large-scale distributed real-time
	and embedded (DRE) systems is hard due to variabilities that arise
	from (I) integration with various subsystems based on different programming
	languages and hardware, OS, middleware platforms, (2) fine tuning
	the system to satisfy a range of customer requirements, such as various
	quality-of-service (QoS) properties, and (3) changing functional
	and QoS properties of the system based on available system resources.
	This paper describes our experience applying model-driven development
	(MDD) tools and QoS-enabled component middleware technologies to
	address domain- and middleware-specific variability challenges in
	an inventory tracking system, which manages the storage and flow
	of items in warehouses. Our results show that (I) coherent integration
	of MDD tools and component middleware can provide a productive software
	process for developing DRE systems by modularizing and composing
	variability concerns and (2) significant challenges remain that must
	be overcome to apply these technologies to a broader range of DRE
	systems},
  doi = {10.1109/ISORC.2006.57},
  keywords = {component middleware;distributed real-time systems;domain-specific
	modeling languages;domain-specific variability;embedded systems;large-scale
	systems;middleware-specific variability;model-driven development;quality
	of service;requirement satisfaction;warehouse inventory tracking
	system;embedded systems;formal specification;inventory management;middleware;object-oriented
	programming;warehouse automation;}
}

@ARTICLE{991381,
  author = {van Deursen, A.},
  title = {Generative programming [Book Review]},
  journal = {Software, IEEE},
  year = {2002},
  volume = {19},
  pages = {107 -108},
  number = {2},
  month = {mar/apr},
  abstract = {Not available},
  doi = {10.1109/MS.2002.991381},
  issn = {0740-7459}
}

@INPROCEEDINGS{885018,
  author = {Deva, D. and Sprinkle, J. and Nordstrom, G. and Maroti, M.},
  title = {Towards a standard for model specification and storage},
  booktitle = {Systems, Man, and Cybernetics, 2000 IEEE International Conference
	on},
  year = {2000},
  volume = {1},
  pages = {364 -369 vol.1},
  abstract = {Software production has become an industrial task usually involving
	teams of programmers working on complex problems to produce large,
	even huge software systems. Globally distributed teams are doing
	a growing share of all software development work. The management
	of software engineering teamwork, especially of a temporally and/or
	spatially distributed team, presents an enormous organizational challenge
	as well as an intricate technical problem, as such distributed teamwork
	requires tool support for coordination of cooperative activities,
	maintenance of project control, and sharing of information. Domain-specific
	Model Integrated Program Synthesis environments are created according
	to a modeling paradigm: a description of the class of models that
	can be created using the system. Just as model integrated computing
	applications are executable instances of domain models, domain models
	can be viewed as instances of metamodels. The representation of these
	models and the modeling paradigm is unique to the specific modeling
	environment. This poses a major problem for portability of models
	from one modeling environment to another. The purpose of the paper
	is to explore the possibility of a common standard for the storage
	of models, in what framework the standard should exist, and who should
	define the standard},
  doi = {10.1109/ICSMC.2000.885018},
  issn = {1062-922X},
  keywords = {Domain-specific Model Integrated Program Synthesis environments ;common
	standard;cooperative activities;distributed teamwork;domain models;executable
	instances;globally distributed teams;huge software systems;industrial
	task;model integrated computing applications;model specification
	standard;modeling environment;modeling paradigm;organizational challenge;programmers;project
	control;software development work;software engineering teamwork management;software
	production;spatially distributed team;technical problem;tool support;formal
	specification;hypermedia markup languages;project management;software
	development management;software standards;}
}

@ARTICLE{846299,
  author = {Devanbu, P.T. and Perry, D.E. and Poulin, J.S.},
  title = {Guest editors introduction: next generation software reuse},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2000},
  volume = {26},
  pages = {423 -424},
  number = {5},
  month = {may},
  abstract = {Not available},
  doi = {10.1109/TSE.2000.846299},
  issn = {0098-5589}
}

@INPROCEEDINGS{5945382,
  author = {Dharavath, K. and Saritha, S.K.},
  title = {Organizing Extracted Data: Using Topic Maps},
  booktitle = {Information Technology: New Generations (ITNG), 2011 Eighth International
	Conference on},
  year = {2011},
  pages = {1048 -1049},
  month = {april},
  abstract = {Topic maps offer unprecedented power when it comes to searching, like
	for full-text searching, for complex queries, and also provide an
	excellent basis for natural language querying. The objective of this
	paper is to crawl portions of domain specific Web, extracting surface
	content based on the requirements of a particular search. Further,
	we aim to organize the extracted data by developing Topic Maps using
	pattern matching, so as to make search navigation directed, easier
	and less complex. It returns the best match for the search, together
	with additional information.},
  doi = {10.1109/ITNG.2011.181},
  keywords = {domain specific Web;extracted data organization;full-text searching;natural
	language querying;pattern matching;surface content extraction;topic
	maps;Web sites;natural language processing;pattern matching;query
	processing;}
}

@INPROCEEDINGS{1428481,
  author = {Di Santo, M. and Ranaldo, N. and Zimeo, E.},
  title = {Metacomputing through the enactment of a BPEL4WS workflow in a grid
	environment},
  booktitle = {Information Technology: Coding and Computing, 2005. ITCC 2005. International
	Conference on},
  year = {2005},
  volume = {1},
  pages = { 316 - 321 Vol. 1},
  month = {april},
  abstract = { The paper presents a Web Services-based platform for the definition
	and execution of Grid applications described through a workflow language,
	fulfilling QoS requirements, and implemented by basic and Internet-accessible
	services delivered by service providers. A first implementation of
	the platform employs a computation service developed on HiMM, a QoS-based,
	hierarchical middleware for Grid computing, extended to support the
	integration of domain specific Web Services invocations in the code
	of Grid applications. The platform was tested for the prediction
	of the effects of possible contingencies that compromise the operation
	of large electrical networks.},
  doi = {10.1109/ITCC.2005.192},
  keywords = { BPEL4WS workflow enactment; HiMM; Internet-accessible services; QoS
	requirements; QoS-based hierarchical middleware; computation service;
	domain specific Web service invocations; electrical networks; grid
	applications; grid computing; grid environment; metacomputing; workflow
	language; Internet; grid computing; middleware; power engineering
	computing; quality of service;}
}

@INPROCEEDINGS{5089305,
  author = {Dias, A. and Amaral, V. and Araujo, J.},
  title = {Towards a Domain Specific Language for a Goal-Oriented approach based
	on KAOS},
  booktitle = {Research Challenges in Information Science, 2009. RCIS 2009. Third
	International Conference on},
  year = {2009},
  pages = {409 -420},
  month = {april},
  abstract = {Requirements Engineering (RE) is the branch of Software Engineering
	dealing with requirements for software systems. A software requirement
	is a property which must be exhibited by software developed or adapted
	to solve a particular problem. Within RE, there are several branches
	of methodologies for obtaining requirements, among which we have
	Goal-Oriented Requirements Engineering (GORE), that uses goals to
	treat requirements.},
  doi = {10.1109/RCIS.2009.5089305},
  keywords = {KAOS;domain specific language;encapsulation technique;goal-oriented
	requirements engineering;software requirement;data encapsulation;software
	architecture;specification languages;systems analysis;}
}

@INPROCEEDINGS{4639087,
  author = {Diaz, P. and Aedo, I. and Sanz, D. and Malizia, A.},
  title = {A model-driven approach for the visual specification of Role-Based
	Access Control policies in web systems},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {203 -210},
  month = {sept.},
  abstract = {Specifying the access policy of a Web system is a relevant design
	concern that is often dismissed or postponed until the implementation.
	ADM-RBAC (Ariadne development method with role-based access control)
	is a model-driven approach for Web systems that supports the specification
	of access control policies in an integrated way and at two abstraction
	levels. At the conceptual level a number of visual models specify
	the access policy in a way that is similar to the userspsila point
	of view. At the detailed level, models are oriented towards providing
	enough details to generate prototypes in an automatic or semiautomatic
	way. In this paper we describe the visual models of ADM-RBAC and
	their empirical evaluation.},
  doi = {10.1109/VLHCC.2008.4639087},
  issn = {1943-6092},
  keywords = {Ariadne development method;Web system;access policy;role-based access
	control;visual model;visual specification;Internet;authorisation;formal
	specification;}
}

@INPROCEEDINGS{4530342,
  author = {Dib, A.A. and Feraud, L. and Ober, I. and Percebois, C.},
  title = {Towards a rigorous framework for dealing with domain specific language
	families},
  booktitle = {Information and Communication Technologies: From Theory to Applications,
	2008. ICTTA 2008. 3rd International Conference on},
  year = {2008},
  pages = {1 -6},
  month = {april},
  abstract = {In this paper we present our approach to rigorously handle variation
	within a family of languages. Our starting point is a case study
	that we developed with industrial partners, where a major difficulty
	arised from the need to work with a set of domain specific languages
	(DSLs). Our solution is based on using the category theory. We consider
	the category of algebraic specifications implementing the semantics
	of the DSLs and we calculate the unifying language of the family.},
  doi = {10.1109/ICTTA.2008.4530342},
  keywords = {algebraic specifications;category theory;domain specific language
	families;formal semantics;incremental verification;set theory;software
	engineering;specification languages;}
}

@INPROCEEDINGS{4053396,
  author = {Dibowski, H. and Oezluek, C. and Ploennigs, J. and Kabitzsch, K.},
  title = {Realizing the Automated Design of Building Automation Systems},
  booktitle = {Industrial Informatics, 2006 IEEE International Conference on},
  year = {2006},
  pages = {251 -256},
  month = {aug.},
  abstract = {The future design of large and complex building automation systems
	(BAS) needs to be increasingly efficient. The usage of prefabricated
	devices and design patterns alone is insufficient to face complex
	demands. New automated design approaches not only need to take over
	recurrent tasks, they also have to integrate more direct and smoother
	methods into the overall design process. This paper addresses that
	broad scope by introducing an automated functional design concept
	for BAS. Following a continuous top-down design starting at a platform-independent
	functional level, a semiautomatic composition over different levels
	of abstraction towards a full-developed and industry-spanning BAS
	network is accomplished. Here, devices from different manufacturers
	are integrated into a properly operating system by incorporating
	formal interoperability checks. The predominant technologies of the
	proposed automated design approach are ontologies, generative programming
	and evolutionary algorithms.},
  doi = {10.1109/INDIN.2006.275789},
  keywords = {automated design;building automation systems;design process;evolutionary
	algorithms;generative programming;ontologies;building management
	systems;evolutionary computation;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{4578313,
  author = {Weilong Ding and Jing Cheng and Kaiyuan Qi and Yan Li and Zhuofeng
	Zhao and Jun Fang},
  title = {A Domain-Specific Query Language for Information Services Mash-up},
  booktitle = {Services - Part I, 2008. IEEE Congress on},
  year = {2008},
  pages = {113 -119},
  month = {july},
  abstract = {With the prevalence of Web Services technology, more and more information
	in the web are provided through Web Services. In certain domains,
	information can be queried by web services, but building a complete
	and exact query from these services is always time-consuming and
	less convenient for end users, especially domain users. Meanwhile,
	mash-up brings a new way for end-users to build personal view of
	data and many mash-up tools like Yahoo Pipes, Popfly etc are provided
	to construct mash-up application. Many of these tools rely on a graphical
	user interface for ease of use. However, for domain users, they can
	not help to get the desired outcome quickly. Domain users are familiar
	with the domain knowledge, and yearn for a dialect using this available
	domain knowledge to express their query requirements precisely and
	concisely.In this paper, a domain-specific query language (DSQL)
	for services mash-up is proposed. We first abstract the domain knowledge
	model as components of the DSQL, and propose the definition of DSQL
	to express advanced query requirements. Meanwhile, correlated services
	could be recommended via business process in domain after the execution
	of the DSQL. We build a portal site as a case study, featuring the
	central idea of domain-specific query language (DSQL), according
	to the application of National Scientific Information System (NSIS)
	in scientific information domain. The portal site is an interactive
	platform to receive and respond to userspsila requests by the DSQL.},
  doi = {10.1109/SERVICES-1.2008.59},
  keywords = {DSQL;NSIS;National Scientific Information System;Popfly;Web services;Yahoo
	Pipes;domain-specific query language;graphical user interface;information
	services mash-up;Web services;graphical user interfaces;query languages;}
}

@INPROCEEDINGS{6078174,
  author = {Dinkelaker, Tom and Erradi, Mohammed},
  title = {Using Aspect-Oriented state machines for resolving feature interactions},
  booktitle = {Computer Science and Information Systems (FedCSIS), 2011 Federated
	Conference on},
  year = {2011},
  pages = {809 -816},
  month = {sept.},
  abstract = {Composing different features in a software system may lead to conflicting
	situations. The presence of one feature may interfere with the correct
	functionality of another feature, resulting in an incorrect behavior
	of the system. In this work we present an approach to manage feature
	interactions. A formal model, using Finite State Machines (FSM) and
	Aspect-Oriented (AO) technology, is used to specify, detect and resolve
	features interactions. In fact aspects can resolve interactions by
	intercepting the events which causes troubleshoot. Also a Domain-Specific
	Language (DSL) was developed to handle Finite State Machines using
	a pattern matching technique.}
}

@INPROCEEDINGS{6078294,
  author = {Djukic, Verislav and Lukovic, Ivan and Popovic, Aleksandar},
  title = {Domain-specific modeling in document engineering},
  booktitle = {Computer Science and Information Systems (FedCSIS), 2011 Federated
	Conference on},
  year = {2011},
  pages = {817 -824},
  month = {sept.},
  abstract = {Specification languages play a central role in supporting document
	engineering. We describe in this paper how domain-specific languages,
	along with domain-specific frameworks and generators, can support
	formal specification and document rendering in directory publishing.
	With flexible metamodel-based tools we have developed four languages
	for the modeling of: (i) small advertisements, (ii) appropriate documents,
	(iii) workflow control and (iv) layout patterns. The paper provides
	a more detailed description of the first and the third language,
	including a brief account of the language interpreter, as well as
	code, document and application generators. The presented approach
	enables, in a typical document-centric system, specification of both
	static and dynamic characteristics of the system on a high abstraction
	level with domain-specific concepts. The concepts of incremental
	document specification and incremental document rendering have been
	introduced, in order to address the problem of very frequent specification(s)
	refinements. The expression power of the created languages is demonstrated
	with a representative examples of document engineering covering document
	content specification, workflow control and application generation.
	All of the aforementioned languages are integrated into a single
	meta-model, under the name of DVDocLang which is, due to its simplicity,
	highly applicable for user-driven conceptual modeling.}
}

@ARTICLE{5473201,
  author = {Djuric, D. and Devedzic, V.},
  title = {Magic Potion: Incorporating New Development Paradigms through Metaprogramming},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {38 -44},
  number = {5},
  month = {sept.-oct. },
  abstract = {Software environments are typically based on a single programming
	paradigm, such as ontologies, functions, objects, or concurrency.
	This can limit what developers can represent and how elegant their
	solutions can be, so today's applications usually involve mixing
	and matching languages, platforms, and paradigms. However, cross-mapping
	multiple paradigms and platforms generates an impedance mismatch
	that increases a solution's complexity. Metaprogramming supports
	a lightweight process to incorporate different programming paradigms
	in a single development environment that's suitable for small development
	teams.},
  doi = {10.1109/MS.2010.90},
  issn = {0740-7459},
  keywords = {Magic Potion;metaprogramming;ontologies;software development;multiprogramming;object-oriented
	methods;software engineering;}
}

@INPROCEEDINGS{4670433,
  author = {Dobrev, M. and Gocheva, D. and Batchkova, I.},
  title = {An ontological approach for planning and scheduling in primary steel
	production},
  booktitle = {Intelligent Systems, 2008. IS '08. 4th International IEEE Conference},
  year = {2008},
  volume = {1},
  pages = {6-14 -6-19},
  month = {sept.},
  abstract = {Effective integration between manufacturing execution systems and
	business and logistics systems has been of great interest for a long
	time. Essential part in enterprise integration is the task of optimal
	planning and scheduling. An ontological approach consisting of integrated
	ontologies: a core meta-ontology, according to ANSI/ISA S95 standard
	and different domain specific ontologies for the primary steelmaking
	is proposed. For the purposes of planning and scheduling the integration
	between the core meta-ontology and a hot strip mill domain ontology
	is achieved and a case study is presented. Web Ontology Language
	(OWL) and Protege as an editor and knowledge acquisition tool are
	used.},
  doi = {10.1109/IS.2008.4670433},
  keywords = {ANSI/ISA S95 standard;Protege;Web Ontology Language;business systems;core
	meta-ontology;hot strip mill domain ontology;knowledge acquisition
	tool;logistics systems;manufacturing execution systems;optimal planning;primary
	steel production scheduling;steelmaking;knowledge acquisition;knowledge
	representation languages;milling;ontologies (artificial intelligence);production
	engineering computing;production planning;scheduling;steel manufacture;}
}

@INPROCEEDINGS{4222954,
  author = {Hai Dong and Hussain, F.K. and Chang, E.},
  title = {Project Track and Trace Ontology},
  booktitle = {Internet and Web Applications and Services, 2007. ICIW '07. Second
	International Conference on},
  year = {2007},
  pages = {52},
  month = {may},
  abstract = {It is well-known that ontology is utilized as an effective methodology
	to share domain-specific knowledge in multidisciplinary fields. In
	the field of project management, due to the characteristic of project
	organizations in which project members are geographically dispersed
	and from different cultural background, senior management would feel
	difficulty when they attempt to know about the detailed project completion
	status from dispersed project groups. Thus, the objective of this
	paper is to propose an automated project track and trace methodology
	through the use of ontology technology, to challenge the knowledge
	sharing issues in project organizations. By means of extending CCCI
	metrics into the field of project management and introducing a new
	ontological notation system, we deliver the project track and trace
	ontology.},
  doi = {10.1109/ICIW.2007.48},
  keywords = {domain-specific knowledge sharing;ontological notation system;project
	management;project track-trace ontology;senior management;ontologies
	(artificial intelligence);peer-to-peer computing;project management;software
	metrics;}
}

@INPROCEEDINGS{4031236,
  author = {Wen-Li Dong and Hang Yu and Yu-Bing Zhang},
  title = {Testing BPEL-based Web Service Composition Using High-level Petri
	Nets},
  booktitle = {Enterprise Distributed Object Computing Conference, 2006. EDOC '06.
	10th IEEE International},
  year = {2006},
  pages = {441 -444},
  month = {oct. },
  abstract = {This paper proposes a technique for analysis and testing BPEL-based
	Web service composition using high-level Petri nets. To illustrate
	how these compositions are verified, the relationships between BPEL-based
	Web service composition and high-level Petri nets is constructed.
	By analyzing the structure of Web service composition based on BPEL,
	the corresponding HPN is constructed. The dynamism and occurrence
	are presented in HPN with guard expression with coloured token. After
	translation, the equivalent HPN of the Web service composition based
	on BPEL can be verified on existing mature tool, and the related
	researches on HPN, e.g. testing coverage and reduction techniques
	that have been studied deeply, can be employed in testing of Web
	service composition based on BPEL, optimized test case can be generated
	based on the HPN translated. An example is provided to illustrate
	the translation ruled and the automatic verify progress},
  doi = {10.1109/EDOC.2006.59},
  issn = {1541-7719},
  keywords = {BPEL-based Web service composition;Web service composition analysis;Web
	service composition testing;high-level Petri nets;Petri nets;Web
	services;program diagnostics;program testing;program verification;}
}

@INPROCEEDINGS{1488779,
  author = {Draheim, D. and Lutteroth, C. and Weber, G.},
  title = {Integrating code generators into the CCHARP language},
  booktitle = {Information Technology and Applications, 2005. ICITA 2005. Third
	International Conference on},
  year = {2005},
  volume = {1},
  pages = { 107 - 110 vol.1},
  month = {july},
  abstract = { In this paper, we show how the concept of code generators can be
	safely implemented into an object oriented language. Modern languages
	like Java and C# begin to offer advanced features for generative
	programming, like generic types. Our own extension of C# generalizes
	the concept of generic types by combining it with reflection. With
	reflection, many code generation tasks can be accomplished for which
	generic types are insufficient. By balancing the availability of
	code generation features with their safety, we are able to detect
	potential generation errors statically.},
  doi = {10.1109/ICITA.2005.161},
  keywords = { C# language; Java; code generators; generation errors; generative
	programming; object oriented language; C language; Java; program
	compilers;}
}

@INPROCEEDINGS{5470549,
  author = {Drey, Z. and Consel, C.},
  title = {A visual, open-ended approach to prototyping ubiquitous computing
	applications},
  booktitle = {Pervasive Computing and Communications Workshops (PERCOM Workshops),
	2010 8th IEEE International Conference on},
  year = {2010},
  pages = {817 -819},
  month = {29 2010-april 2},
  abstract = {By nature, ubiquitous computing applications are intimately intertwined
	with users' everyday life. This situation is challenging because
	it requires to make the development of applications accessible to
	end-users. Furthermore, ubiquitous computing consists of a variety
	of areas, including home automation and assisted living, raising
	a need for an open-ended approach. We present Pantagruel, a visual
	programming language that is end-user oriented. Our approach is open-ended
	in that Pantagruel integrates a language to describe a ubiquitous
	computing environment. Such description takes the form of a taxonomy,
	defining the entities relevant to a given ubiquitous computing area.
	This description serves as a parameter to a sensor-controller-actuator
	development paradigm. The orchestration of area-specific entities
	is supported by high-level constructs, customized with respect to
	taxonomical information. We have implemented a visual environment
	to prototyping ubiquitous computing applications. Furthermore, we
	have developed a compiler for Pantagruel that targets a domain-specific
	middleware. Our environment leverages a 2D Tenderer to enable the
	simulation and of applications. We successfully simulated a range
	of applications in various ubiquitous computing areas, such as home
	automation, assisted living and building management.},
  doi = {10.1109/PERCOMW.2010.5470549},
  keywords = {2D Tenderer;Pantagruel language;area-specific entity;assisted living;building
	management;compiler;domain-specific middleware;end-user;home automation;open-ended
	approach;sensor-controller-actuator development paradigm;taxonomical
	information;ubiquitous computing;visual environment;visual programming
	language;middleware;personal computing;program compilers;ubiquitous
	computing;visual languages;visual programming;}
}

@INPROCEEDINGS{626593,
  author = {Duecker, M. and Geiger, C. and Lehrenfeld, G. and Mueller, W. and
	Tahedl, C.},
  title = {A visual programming language for qualitative data},
  booktitle = {Visual Languages, 1997. Proceedings. 1997 IEEE Symposium on},
  year = {1997},
  pages = {268 -269},
  month = {sep},
  abstract = {Modeling of human knowledge and reasoning requires the formulation
	of uncertainty in its various forms. Fuzzy logic was introduced to
	directly support these applications (H. Zimmermann, 1991). Fuzzy
	control (FC) which is based on fuzzy logic allows one to control
	complex systems based on qualitative information like human knowledge
	(C. Geiger and G. Lehrenfeld, 1994). In fuzzy logic, fuzzy sets are
	usually defined and manipulated by means of complex mathematics,
	whereas the fuzzy control process is frequently outlined by visual
	sketches based on set diagrams in order to enhance the comprehension
	of the inference process. The rule based execution of this process
	usually follows the lines of rule based visual programming languages
	(VPLs), i.e., languages comparable to Agentsheets and ChemTrains.
	This strongly indicates that VPLs are thus well applicable for this
	use. We first outline the basic concepts of fuzzy logic and fuzzy
	control. Thereafter, we sketch a visual language which integrates
	fuzzy set diagrams in the visual representation of rules. The basic
	concepts are inherited from the complete visual programming language,
	Pictorial Janus (PJ). However, we significantly simplify PJ's visual
	concepts in order to adapt it for our purpose},
  doi = {10.1109/VL.1997.626593},
  keywords = {Agentsheets;ChemTrains;Pictorial Janus;VPLs;fuzzy control process;fuzzy
	logic;fuzzy sets;human knowledge;inference process;qualitative data;qualitative
	information;reasoning;rule based execution;rule based visual programming
	languages;set diagrams;uncertainty;visual programming language;visual
	representation;visual sketches;fuzzy control;fuzzy logic;fuzzy set
	theory;inference mechanisms;uncertainty handling;visual languages;visual
	programming;}
}

@INPROCEEDINGS{848774,
  author = {Dujmovic, S.},
  title = {An understandable and configurable domain-specific framework for
	industrial automation applications},
  booktitle = {Technology of Object-Oriented Languages, 2000. TOOLS 33. Proceedings.
	33rd International Conference on},
  year = {2000},
  pages = {348 -358},
  abstract = {Modern industrial automation software is growing more and more complex.
	At the same time development time and costs have to be reduced. In
	order to reconcile these opposing demands a new approach is needed
	based on domain-specific architectures with appropriate plug-in components.
	Object-oriented frameworks offer a reuse efficiency not achievable
	with other techniques. Unfortunately, framework comprehension and
	use is normally difficult due to a steep learning curve and the lack
	of tool support. This work proposes the use of component models for
	structuring frameworks in order to simultaneously improve usability
	and comprehension of object-oriented frameworks. The approach is
	illustrated with a framework for elevator control software complying
	to the JavaBeans component model. This paper discusses the structure
	and the advantages of the approach but also reveals problematic points
	which should be considered in similar projects},
  doi = {10.1109/TOOLS.2000.848774},
  keywords = {JavaBeans;costs;domain-specific framework;elevator control;industrial
	automation software;object-oriented frameworks;plug-in components;software
	development time;software reuse;usability;computerised control;lifts;object-oriented
	programming;software architecture;software reusability;}
}

@INPROCEEDINGS{4632373,
  author = {Dura, E. and Gawronska, B. and Gawronska, B.},
  title = {Natural language processing in information fusion terminology management},
  booktitle = {Information Fusion, 2008 11th International Conference on},
  year = {2008},
  pages = {1 -8},
  month = {30 2008-july 3},
  abstract = {The dynamic development of information fusion research implies introduction
	of new terms and concepts, which in turn requires tools and methods
	for terminology organization and standardization, as well as tools
	for creating domain-specific ontology. In this paper, we show how
	natural language processing and corpus technology tools applied for
	term extraction from texts in biomedicine can successfully be used
	for the field of information fusion. We demonstrate term and information
	extraction from a corpus of research articles in information fusion,
	showing how a vision of a combined text retrieval and information
	extraction service can be made real.},
  keywords = {biomedicine;corpus technology tools;domain-specific ontology;dynamic
	development;information extraction service;information fusion terminology
	management;natural language processing;term extraction;terminology
	organization;text retrieval;information retrieval;natural language
	processing;text analysis;}
}

@INPROCEEDINGS{1312885,
  author = {Dursan, T. and Orencik, B.},
  title = {POLICE distributed conflict detection architecture},
  booktitle = {Communications, 2004 IEEE International Conference on},
  year = {2004},
  volume = {4},
  pages = { 2081 - 2085 Vol.4},
  month = {june},
  abstract = { POLICE [T. Dursun and B. Orencik, 2003] is a novel policy based management
	framework composed of a policy specification language and its deployment
	models for distribution of policies. In this paper we present distributed
	policy conflict detection model employed in POLICE policy framework.
	POLICE model is designed to handle application domain-specific conflicts
	[E. Lupu and M. Sloman, 1999] only, whereas the modality conflicts
	[E. Lupu and M. Sloman, 1999] are already eliminated. The POLICE
	policy specification language does not include negative policy concept.},
  doi = {10.1109/ICC.2004.1312885},
  keywords = { POLICE distributed conflict detection architecture; application domain-specific
	conflicts; deployment models; modality conflicts; policy specification
	language; specification languages; telecommunication computing; telecommunication
	network management;}
}

@INPROCEEDINGS{5447839,
  author = {Dyreson, C. and Bhowmick, S. and Jannu, A.R. and Mallampalli, K.
	and Shuohao Zhang},
  title = {XMorph: A shape-polymorphic, domain-specific XML data transformation
	language},
  booktitle = {Data Engineering (ICDE), 2010 IEEE 26th International Conference
	on},
  year = {2010},
  pages = {844 -847},
  month = {march},
  abstract = {By imposing a single hierarchy on data, XML makes queries brittle
	in the sense that a query might fail to produce the desired result
	if it is executed on the same data organized in a different hierarchy,
	or if the hierarchy evolves during the lifetime of an application.
	This paper presents a new transformation language, called XMorph,
	which supports more flexible querying. XMorph is a shape polymorphic
	language, that is, a single XMorph query can extract and transform
	data from differently-shaped hierarchies. The XMorph data shredder
	distills XML data into a graph of closest relationships, which are
	exploited by the query evaluation engine to produce a result in the
	shape specified by an XMorph query.},
  doi = {10.1109/ICDE.2010.5447839},
  keywords = {XML data;XMorph;data transformation language;query evaluation engine;shape
	polymorphic language;XML;graph theory;query processing;}
}

@INPROCEEDINGS{479343,
  author = {Edwards, M.L. and Flanzer, M. and Terry, M. and Landa, J.},
  title = {RECAP: a requirements elicitation, capture and analysis process prototype
	tool for large complex systems},
  booktitle = {Engineering of Complex Computer Systems, 1995. Held jointly with
	5th CSESAW, 3rd IEEE RTAW and 20th IFAC/IFIP WRTP, Proceedings.,
	First IEEE International Conference on},
  year = {1995},
  pages = {278 -281},
  month = {nov},
  abstract = {Complete, correct, and consistent requirements are essential to the
	development of large complex systems. The Requirements Elicitation,
	Capture and Analysis Process prototype tool (RECAP) provides automated
	assistance in identifying, capturing, analyzing, and using requirements.
	RECAP combines recent advances in natural language parsing, requirements
	specification, and knowledge-based rules to support semiautomatic
	capture elicitation analysis and the use of requirements data. The
	process begins with unformatted natural language text which is inherently
	ambiguous, incomplete and cumbersome. RECAP assists the user in translating
	requirements from natural language text into concise requirements
	data which can be organized and accessed by user defined views. Requirements
	and domain specific rules are used to help the user analyze and maintain
	the requirements data throughout the system life-cycle},
  doi = {10.1109/ICECCS.1995.479343},
  keywords = { RECAP; automated assistance; complete requirements; concise requirements
	data; consistent requirements; correct requirements; domain specific
	rules; knowledge-based rules; large complex system development; natural
	language parsing; requirement analysis; requirement capture; requirement
	identification; requirement translation; requirement use; requirements
	elicitation capture and analysis process prototype tool; requirements
	specification; semiautomatic capture elicitation analysis; system
	life-cycle; unformatted natural language text; user defined views;
	formal specification; knowledge based systems; natural languages;
	software tools; systems analysis;}
}

@INPROCEEDINGS{5954390,
  author = {Efkemann, C. and Peleska, J.},
  title = {Model-Based Testing for the Second Generation of Integrated Modular
	Avionics},
  booktitle = {Software Testing, Verification and Validation Workshops (ICSTW),
	2011 IEEE Fourth International Conference on},
  year = {2011},
  pages = {55 -62},
  month = {march},
  abstract = {In this paper the authors present the current research and development
	activities regarding automated testing of Integrated Modular Avionics
	controllers in the European research project SCARLETT. The authors
	describe the goals of the SCARLETT project and explain its background
	of Integrated Modular Avionics. Furthermore, they explain different
	levels of testing of components required for certification. A domain-specific
	modelling language designed for the IMA platform is presented. This
	language is used to create models from which tests of different levels
	can be generated automatically. The authors expect significant improvements
	in terms of effort to create and maintain test procedures compared
	to conventional test creation.},
  doi = {10.1109/ICSTW.2011.72},
  keywords = {SCARLETT research project;domain-specific modelling language;integrated
	modular avionics;model-based testing;aerospace computing;avionics;program
	testing;simulation languages;}
}

@INPROCEEDINGS{581969,
  author = {Ehlmann, B.K. and Riccardi, G.A.},
  title = {Object Relater Plus: a practical tool for developing enhanced object
	databases},
  booktitle = {Data Engineering, 1997. Proceedings. 13th International Conference
	on},
  year = {1997},
  pages = {412 -421},
  month = {apr},
  abstract = {Object Relater Plus is a practical tool currently being used for research
	and development of enhanced object databases (ODBs). The tool, which
	is a prototype object database management system (ODBMS), provides
	two languages that are compatible with the ODMG-93 ODBMS standard
	yet enhance it in some significant ways. The Object Database Definition
	Language (ODDL) allows object relationships to be better defined
	and supported; provides for the specification and separation of external,
	conceptual, and internal views; and facilitates the implementation
	of domain specific ODB extensions. The Object Database Manipulation
	Language (ODML) augments ODDL by providing a C++ interface for database
	creation, access, and manipulation based on an ODDL specification.
	We give an overview of Object Relater Plus, emphasizing its salient
	features. We also briefly discuss its architecture and implementation
	and its use in developing scientific databases},
  doi = {10.1109/ICDE.1997.581969},
  keywords = {C++ interface;ODMG-93 ODBMS standard;ODML;Object Database Definition
	Language;Object Database Manipulation Language;Object Relater Plus;database
	creation;domain specific ODB extensions;enhanced object database
	development;object relationships;practical tool;prototype object
	database management system;scientific databases;database languages;object-oriented
	databases;object-oriented languages;software tools;}
}

@INPROCEEDINGS{5678951,
  author = {Ehm, H. and Heilmayer, S. and Ponsignon, T. and Russland, T.},
  title = {A discussion of object-oriented process modeling approaches for discrete
	manufacturing on the example of the semiconductor industry},
  booktitle = {Winter Simulation Conference (WSC), Proceedings of the 2010},
  year = {2010},
  pages = {2553 -2562},
  month = {dec.},
  abstract = {We introduce a domain specific object-oriented data model for the
	high-tech discrete manufacturing on the example of the semiconductor
	company Infineon Technologies AG. This model is needed to describe
	the complex supply chain of a global company in the competitive semiconductor
	arena with frequent product changes. However, the data model alone
	is not solving all problems. For this we need e.g. event-driven internet-based
	workflows. To get those in a structured way, we show possibilities
	to come from an object-oriented data model to object-oriented business
	processes based on existing process models. Two ways - one with SysML
	and one with ARIS - are shown conceptually and are discussed. An
	outlook is given on how this approach will provide internet-based
	workflows on the one hand, and it also shows up process improvement
	potentials on the other hand.},
  doi = {10.1109/WSC.2010.5678951},
  issn = {0891-7736},
  keywords = {discrete manufacturing approaches;internet-based workflows;object
	oriented process modeling approaches;object-oriented business processes;semiconductor
	company Infineon technologies;semiconductor industry;supply chain;Internet;manufacturing
	systems;object-oriented programming;semiconductor industry;}
}

@INPROCEEDINGS{1007969,
  author = {Eide, E. and Reid, A. and Regehr, J. and Lepreau, J.},
  title = {Static and dynamic structure in design patterns},
  booktitle = {Software Engineering, 2002. ICSE 2002. Proceedings of the 24rd International
	Conference on},
  year = {2002},
  pages = {208 -218},
  month = {may},
  abstract = {Design patterns are a valuable mechanism for emphasizing structure,
	capturing design expertise, and facilitating restructuring of software
	systems. Patterns are typically applied in the context of an object-oriented
	language and are implemented so that the pattern participants correspond
	to object instances that are created and connected at run-time. The
	paper describes a complementary realization of design patterns, in
	which many pattern participants correspond to statically instantiated
	and connected components. Our approach separates the static parts
	of the software design from the dynamic parts of the system behavior.
	This separation makes the software design more amenable to analysis,
	thus enabling more effective and domain-specific detection of system
	design errors, prediction of run-time behavior, and more effective
	optimization. This technique is applicable to imperative, functional,
	and object-oriented languages: we have extended C, Scheme, and Java
	with our component model. We illustrate our approach in the context
	of the OSKit, a collection of operating system components written
	in C.},
  keywords = {C;Java;OSKit;Scheme;design expertise;design patterns;domain-specific
	detection;dynamic structure;object instances;object-oriented language;operating
	system components;pattern participants;run-time behavior;software
	systems restructuring;static structure;statically instantiated components;system
	behavior;system design errors;object-oriented languages;object-oriented
	programming;operating systems (computers);software reusability;}
}

@ARTICLE{683735,
  author = {Eigenmann, R. and Kale, L.V. and Padua, D.A.},
  title = {Guest Editor's Introduction: Languages For Computational Science
	and Engineering},
  journal = {Computational Science Engineering, IEEE},
  year = {1998},
  volume = {5},
  pages = {16 -17},
  number = {2},
  month = {apr-jun},
  abstract = {Not available},
  doi = {10.1109/MCSE.1998.683735},
  issn = {1070-9924}
}

@INPROCEEDINGS{6037578,
  author = {El Kharbili, M. and Qin Ma and Kelsen, P. and Pulvermueller, E.},
  title = {CoReL: Policy-Based and Model-Driven Regulatory Compliance Management},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2011 15th
	IEEE International},
  year = {2011},
  pages = {247 -256},
  month = {29 2011-sept. 2},
  abstract = {Regulatory compliance management is now widely recognized as one of
	the main challenges still to be efficiently dealt with in information
	systems. In the discipline of business process management in particular,
	compliance is considered as an important driver of the efficiency,
	reliability and market value of companies. It consists of ensuring
	that enterprise systems behave according to some guidance provided
	in the form of regulations. This paper gives a definition of the
	research problem of regulatory compliance. We show why we expect
	a formal policy-based and model-driven approach to provide significant
	advantages in allowing enterprises to flexibly manage decision-making
	related to regulatory compliance. For this purpose, we contribute
	CoReL, a domain-specific modeling language for representing compliance
	requirements that has a graphical concrete syntax. Informal semantics
	of CoReL are introduced and its use is illustrated on an example.
	CoReL allows to leverage business process compliance modeling and
	checking, enhancing it with regard to, among other dimensions, user-friendliness,
	genericity, and traceability.},
  doi = {10.1109/EDOC.2011.23},
  issn = {1541-7719},
  keywords = {CoReL;business process compliance checking;business process compliance
	modeling;business process management;decision-making management;domain-specific
	modeling language;enterprise systems;information systems;model-driven
	regulatory compliance management;policy-based regulatory compliance
	management;business data processing;decision making;information systems;simulation
	languages;}
}

@INPROCEEDINGS{6046972,
  author = {El Kharbili, M. and Ma, Q. and Kelsen, P. and Pulvermueller, E.},
  title = {Enterprise Regulatory Compliance Modeling Using CoReL: An Illustrative
	Example},
  booktitle = {Commerce and Enterprise Computing (CEC), 2011 IEEE 13th Conference
	on},
  year = {2011},
  pages = {185 -190},
  month = {sept.},
  abstract = {Regulatory compliance management is a critical and challenging task,
	especially in the context of Business Process Management. It requires
	a comprehensive framework for dealing with compliance requirements:
	elicitation, modeling, static and dynamic checking and reporting.
	We previously defined CoReL, a domain specific language for the domain
	of compliance decision-making. This paper shows how CoReL can be
	used to model compliance requirements using an illustrative example.
	In particular, we show how CoReL's agnosticism of logical formalisms
	and coverage of enterprise business aspects leverages the task of
	compliance modeling to the business user level.},
  doi = {10.1109/CEC.2011.39},
  keywords = {CoReL;business process management;compliance decision-making;domain
	specific language;enterprise regulatory compliance modeling;business
	data processing;corporate modelling;decision making;specification
	languages;}
}

@INPROCEEDINGS{4281123,
  author = {El-Kechai, H. and Choquet, C.},
  title = {Reusing Pedagogical Scenarios at a Knowledge Level: a Model Driven
	Approach},
  booktitle = {Advanced Learning Technologies, 2007. ICALT 2007. Seventh IEEE International
	Conference on},
  year = {2007},
  pages = {670 -674},
  month = {july},
  abstract = {The aim of this paper is to present a model driven approach for reusing
	pedagogical scenarios. In the framework of different pedagogical
	situations, we show how the design context should be expressed in
	scenarios and how these scenarios could be reused. This approach
	enhance the teaching expertise capitalization by defining contextualized
	pedagogical models and by allowing the reuse and the customization
	of these models and the relevant scenarios.},
  doi = {10.1109/ICALT.2007.217},
  keywords = {contextualized pedagogical models;model driven approach;pedagogical
	scenarios reuse;teaching expertise capitalization;computer aided
	instruction;teaching;}
}

@ARTICLE{798320,
  author = {Elliott, C.},
  title = {An embedded modeling language approach to interactive 3D and multimedia
	animation},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {291 -308},
  number = {3},
  month = {may/jun},
  abstract = {While interactive multimedia animation is a very compelling medium,
	few people are able to express themselves in it. There are too many
	low-level details that have to do not with the desired content-e.g.,
	shapes, appearance and behavior-but rather how to get a computer
	to present the content. For instance, behavior such as motion and
	growth are generally gradual, continuous phenomena. Moreover, many
	such behaviors go on simultaneously. Computers, on the other hand,
	cannot directly accommodate either of these basic properties, because
	they do their work in discrete steps rather than continuously, and
	they only do one thing at a time. Graphics programmers have to spend
	much of their effort bridging the gap between what an animation is
	and how to present it on a computer. We propose that this situation
	can be improved by a change of language, and present Fran, synthesized
	by complementing an existing declarative host language, Haskell,
	with an embedded domain-specific vocabulary for modeled animation.
	As demonstrated in a collection of examples, the resulting animation
	descriptions are not only relatively easy to write, but also highly
	composable},
  doi = {10.1109/32.798320},
  issn = {0098-5589},
  keywords = {Fran;Haskell;declarative host language;embedded domain-specific vocabulary;embedded
	modeling language approach;growth;interactive 3D animation;interactive
	multimedia animation;modeled animation;motion;computer animation;multimedia
	computing;simulation languages;}
}

@INPROCEEDINGS{6068333,
  author = {Elsner, Christoph and Lohmann, Daniel and Schroder-Preikschat, Wolfgang},
  title = {Fixing Configuration Inconsistencies across File Type Boundaries},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2011 37th
	EUROMICRO Conference on},
  year = {2011},
  pages = {116 -123},
  month = {30 2011-sept. 2},
  abstract = {Creating a valid software configuration often involves multiple configuration
	file types, such as feature models, domain-specific languages, or
	C header files with preprocessor defines. Enforcing constraints across
	file types boundaries already at configuration is necessary to prevent
	inconsistencies, which otherwise are costly to discover and resolve
	later on. We present a pragmatic framework to specify and apply inconsistency-resolving
	fixes on configuration files of arbitrary types. The framework converts
	each configuration file to a model, checks it for consistency, applies
	fixes, and serializes it back again. We argue that conventionally
	programmed fixes and round-trip mechanisms (i.e., converters and
	serializers) are indispensable for practical applicability and can
	provide sufficient reliability when following usual development practices.
	We have developed round-trip mechanisms for seven different configuration
	file types and two fixing mechanisms. One fixing mechanism extends
	previous work by combining automatic detection of correct fix locations
	with a marker mechanism that reduces the number of locations. A tool-supported
	process for applying the fixes provides user guidance and integrates
	additional semantic validity checks on serialized configuration files
	of complex types (e.g., feature models). Evaluations reveal a speed
	up in inconsistency fixing and that the performance of the currently
	integrated round-tripping and fixing mechanisms is competitive.},
  doi = {10.1109/SEAA.2011.26}
}

@INPROCEEDINGS{5295302,
  author = {Engels, G. and Fisseler, D. and Soltenborn, C.},
  title = {Improving reusability of dynamic meta modeling specifications with
	rule overriding},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {39 -46},
  month = {sept.},
  abstract = {Dynamic meta modeling (DMM) is a visual semantics specification technique
	targeted at languages equipped with a metamodel. In DMM, the metamodel
	of a language is mapped into a runtime metamodel able to express
	runtime states of instances of that language. In addition, graph
	transformation rules are defined which describe how these runtime
	states change in time. Given an instance of the runtime metamodel
	and a set of rules typed over that metamodel, a transition system
	can be computed which represents the semantics of the model instance
	under investigation. To be easily understandable by language engineers,
	DMM resembles a couple of well-known object-oriented concepts. Part
	of this is the fact that a DMM rule has many similarities to a method
	in an object-oriented language. In this paper, we enhance DMM such
	that DMM rules can "override" other DMM rules, similar to a method
	being overridden in a subclass. We argue that this does not only
	have positive impact on reusability of DMM specifications, but also
	improves the intuitive understandability of DMM rules.},
  doi = {10.1109/VLHCC.2009.5295302},
  issn = {1943-6092},
  keywords = {dynamic meta modeling specification;graph transformation rule;object-oriented
	concept;object-oriented language;reusability;runtime metamodel;visual
	semantics specification technique;formal specification;graph grammars;object-oriented
	languages;software reusability;visual languages;}
}

@INPROCEEDINGS{555339,
  author = {Englehart, M.},
  title = {ACSL Code: a high quality code generator for control applications
	},
  booktitle = {Computer-Aided Control System Design, 1996., Proceedings of the 1996
	IEEE International Symposium on},
  year = {1996},
  pages = {477 -482},
  month = {sep},
  abstract = {Honeywell and MGA Software have collaborated to produce ACSL Code,
	a high-quality C code generator for control applications. ACSL Code
	was designed using the lessons learned by Honeywell when they developed
	the language and code generator ControlH, under an ARPA (Advanced
	Research Projects Agency) DSSA (Domain-Specific Software Architecture)
	contract. We describe the language that the algorithm definitions
	are captured in by the ACSL Code tool, before being translated to
	C code. The language is designed to capture control algorithms concisely
	and rigorously. The language has both a textual and a graphical syntax.
	The textual syntax is a variant of MGA's language ACSL (Advanced
	Continuous Simulation Language), while the graphical syntax is derived
	from MGA's graphic modeling language. Highlights of the language
	include the integration between the textual and the graphical syntaxes,
	the rich base of presupplied blocks, and strong support for vectors
	and matrices. We describe our philosophy for code generation, and
	the ways in which ACSL Code implements that philosophy. The goal
	of the code generator is to produce software whose quality and structure
	are comparable to that produced by an experienced software engineer.
	One important characteristic is that the code produced preserves
	the structure of the original algorithm definition. Another characteristic
	is that the generated code is very portable and may be easily inserted
	into legacy environments. Finally, we present an example of a representative
	control law definition, and excerpts from the corresponding generated
	code},
  doi = {10.1109/CACSD.1996.555339},
  keywords = {ACSL Code;ARPA DSSA contract;Advanced Continuous Simulation Language;ControlH;Domain-Specific
	Software Architecture;Honeywell;MGA Software;code generation philosophy;control
	algorithm definitions capture;control applications;control law definition;graphic
	modeling language;graphical syntax;high-quality C code generator;legacy
	environments;matrices;portable code;presupplied blocks;software quality;software
	structure;textual syntax;vectors;C language;application generators;computer
	aided software engineering;control engineering;control engineering
	computing;matrix algebra;simulation languages;software portability;software
	quality;vectors;}
}

@INPROCEEDINGS{288905,
  author = {Englehart, M.},
  title = {High quality automatic code generation for control applications},
  booktitle = {Computer-Aided Control System Design, 1994. Proceedings., IEEE/IFAC
	Joint Symposium on},
  year = {1994},
  pages = {363 -367},
  month = {mar},
  abstract = {Automatic code generation has become a popular technique for generating
	software for control applications. The popularity of the technique
	is justified by the convenience of programming in a domain-specific
	specification language, and the elimination of communication errors
	between the control-law designer and the software engineer. Unfortunately,
	in the rapid rush toward the marketing and adoption of this technique,
	efforts directed toward generating high-quality code have often come
	half-heartedly as an afterthought. The author proposes some guidelines
	to be used to judge the quality of code generated for control applications.
	Many of these guidelines are directed toward the efficient use of
	time and space resources. Others impact the maintainability of the
	code, as well as the viewpoint of validation and verification. The
	author also introduces ControlH, and its corresponding code generator.
	ControlH is a language developed at Honeywell for specification of
	control applications. Its code generator translates applications
	specified in ControlH into applications implemented in Ada. These
	applications meet the author's guidelines for quality code generation},
  doi = {10.1109/CACSD.1994.288905},
  keywords = {Ada;ControlH;Honeywell;automatic code generation;control applications;control-law
	designer;domain-specific specification language;software engineer;validation;verification;automatic
	programming;control engineering;control engineering computing;software
	engineering;specification languages;}
}

@INPROCEEDINGS{288920,
  author = {Englehart, M. and Jackson, M.},
  title = {ControlH: a fourth generation language for real-time GN amp;C applications},
  booktitle = {Computer-Aided Control System Design, 1994. Proceedings., IEEE/IFAC
	Joint Symposium on},
  year = {1994},
  pages = {261 -270},
  month = {mar},
  abstract = {Describes the defining characteristics of the language ControlH. ControlH
	is being designed under the ARPA DSSA (Domain Specific Software Architectures)
	Program. ControlH is designed for describing guidance, navigation
	and control (GN amp;C) algorithms in a concise yet rigorous manner.
	The language objects, structure, data types, and operations have
	been tailored to the domain of GN amp;C algorithm specification.
	The language also provides hooks for optimization of the software
	produced from the specification. A translator has been developed
	which generates high-quality, modular Ada, based on ControlH specifications.
	The language and the translator are used to provide software reuse
	and configuration at the high level of GN amp;C algorithm specification},
  doi = {10.1109/CACSD.1994.288920},
  keywords = {ControlH;control;fourth generation language;guidance;high-quality,
	modular Ada;navigation;software configuration;software reuse;translator;computerised
	navigation;high level languages;program interpreters;software reusability;}
}

@INPROCEEDINGS{5283916,
  author = {Englmeier, K. and Koinig, R.},
  title = {Domain-Specific Deployment and Configuration Language for Composition
	and Adaptation of Coarse-Grained Services},
  booktitle = {Services Computing, 2009. SCC '09. IEEE International Conference
	on},
  year = {2009},
  pages = {490 -493},
  month = {sept.},
  abstract = {Domain-specific languages promise an unprecedented integration of
	business and IT aspects in software development. This translates
	into a stronger focus on user requirements, higher adaptability,
	and shorter time-to-market. DSLs provide the opportunity to bring
	business actors and IT experts closer together by raising the mutual
	understanding of the models underpinning software development. A
	closer cooperation in modeling improves the understanding of systems
	and allows for experimentation. Business actors can identify their
	business processes and resources in the models and can experiment
	with them. This paper presents work-in-progress addressing a model
	layer for the dynamic composition and adaptation of coarse-grained
	web services through configuration information. Our domain-specific
	configuration language (DSCL) enables IT experts and business actors
	to concentrate on model representations that reflect individually
	tailored compositions of generic portal services. Our approach fosters
	modeling on two different levels of abstraction. Business actors
	define high-level models focusing on the definition of processes
	across coarse-grained services. Low-level concepts complete technical
	aspects that are abstracted away in high-level concepts.},
  doi = {10.1109/SCC.2009.74},
  keywords = {business process;coarse-grained web services;configuration information;domain-specific
	configuration language;generic portal services;high-level models;software
	development;Web services;software engineering;specification languages;}
}

@INPROCEEDINGS{1314746,
  author = {Englmeier, K. and Mothe, J. and Murtagh, F. and Stempihuber, M.},
  title = {Adapting the communication of Web services to the language of their
	user community},
  booktitle = {Web Services, 2004. Proceedings. IEEE International Conference on},
  year = {2004},
  pages = { 252 - 259},
  month = {july},
  abstract = { This paper presents the WS-Talk interface layer, a concept for a
	structured natural language interface which virtualizes the composition
	of Web services at the end-user's side. As the number of available
	Web services grows, the representation of the context in which they
	are used becomes more and more an issue for both service providers
	and service consumers. While providers concentrate more on the technical
	levels of activation and communication within a service network,
	the users, i.e. the service consumers, want to form ad-hoc collaborations
	between Web services. To shield users of Web services from the technical
	details of finding and combining them, a semantic level has to be
	found that suits their specific needs and expertise. Through a semantic
	layer, WS Talk tries to create a level of abstraction that enables
	views on services expressed in natural language. To reduce the complexity
	of natural language, domain-specific languages are defined and their
	semantics are mapped onto Web services and their attributes. The
	domain of economics is currently used for testing architecture and
	usage aspects of the WS-Talk interface layer.},
  doi = {10.1109/ICWS.2004.1314746},
  issn = { },
  keywords = { Internet; WS-Talk interface layer; Web service communication; domain-specific
	languages; semantic Web; semantic layer; service consumers; service
	network; service providers; structured natural language interface;
	user community language; groupware; knowledge representation; natural
	language interfaces; semantic Web; user modelling;}
}

@INPROCEEDINGS{4031218,
  author = {Susan Entwisle and Heinz Schmidt and Ian Peake and Elizabeth Kendall},
  title = {A Model Driven Exception Management Framework for Developing Reliable
	Software Systems},
  booktitle = {Enterprise Distributed Object Computing Conference, 2006. EDOC '06.
	10th IEEE International},
  year = {2006},
  pages = {307 -318},
  month = {oct. },
  abstract = {Programming languages provide exception handling mechanisms to structure
	fault tolerant activities into software systems. However, the use
	of exceptions at this low level of abstraction can be error-prone
	and complex leading to new programming errors. In this paper, we
	present a model-driven framework to support the iterative development
	of reliable software systems. This framework is comprised of UML-based
	modeling notations and a transformation engine that supports the
	automated generation of exception management features for a software
	system. It leverages domain specific exception modeling languages,
	patterns, modeling tools and framework libraries. The feasibility
	of this approach is demonstrated through the development of a case
	study business application, known as Project Tracker},
  doi = {10.1109/EDOC.2006.10},
  issn = {1541-7719},
  keywords = {Project Tracker;UML-based modeling;exception handling;fault tolerant
	software;iterative software development;model driven exception management
	framework;model-driven framework;programming languages;reliable software
	systems;transformation engine;Unified Modeling Language;exception
	handling;formal specification;software reliability;}
}

@ARTICLE{4420058,
  author = {Erdogmus, Hakan},
  title = {So Many Languages, So Little Time},
  journal = {Software, IEEE},
  year = {2008},
  volume = {25},
  pages = {4 -6},
  number = {1},
  month = {jan.-feb. },
  abstract = {What's up and coming in the programming language arena? A rudimentary
	analysis of the 200+ sessions' titles and abstracts at OOPSLA 07
	(22nd Int'l Conf. Object-Oriented Programming, Systems, Languages,
	and Applications) provides a rough idea of what's happening with
	object-oriented, functional, dynamic, and domain-specific languages.},
  doi = {10.1109/MS.2008.20},
  issn = {0740-7459}
}

@INPROCEEDINGS{1607398,
  author = {Erfurth, I.},
  title = {Customer-oriented development of complex distributed systems},
  booktitle = {Engineering of Computer Based Systems, 2006. ECBS 2006. 13th Annual
	IEEE International Symposium and Workshop on},
  year = {2006},
  pages = {6 pp. -476},
  month = {march},
  abstract = {Complex and distributed systems are more and more common. Hardware
	is going from strength to strength and is embedded in high performance
	peer-to-peer networks mostly. The task of a software engineer is
	to develop software systems which are able to take part in these
	new possibilities. Hereby, the drawback is the simple fact that such
	software systems and their modeling are getting more and more complex.
	If we are looking at the difficulties between customer and developer
	teams, especially misunderstandings between both, then the challenge
	to model customer oriented systems is even higher. For customers,
	it is hard to understand the frequently used terms, process models,
	and technological concepts. Developers have a hard time to understand
	domain specific processes and structures, and exhibit a tendency
	to abstract concrete examples to higher level constructs. These problems
	are especially hard to avoid during the development of dynamic, distributed
	systems with multiple nodes and possibly asynchronous behavior. In
	our research, we develop a customer-friendly reference model to demonstrate
	the aspects of dynamic distributed systems understandable to the
	customer. This model presents and simulates the dynamic aspects of
	(distributed) systems without immediate abstraction from examples
	and allows for a stepwise generalization and evaluation with help
	of the customer team. In its final version the reference model serves
	as a requirements statement for the professional developer},
  doi = {10.1109/ECBS.2006.33},
  keywords = {asynchronous behavior;complex distributed system;customer oriented
	systems;customer-friendly reference model;customer-oriented development;dynamic
	distributed systems;higher level constructs;peer-to-peer network;software
	engineer;software systems;stepwise generalization;customer relationship
	management;peer-to-peer computing;software engineering;}
}

@INPROCEEDINGS{5351121,
  author = {Erkok, L. and Carlsson, M. and Wick, A.},
  title = {Hardware/software co-verification of cryptographic algorithms using
	Cryptol},
  booktitle = {Formal Methods in Computer-Aided Design, 2009. FMCAD 2009},
  year = {2009},
  pages = {188 -191},
  month = {nov.},
  abstract = {Cryptol is a programming language designed for specifying cryptographic
	algorithms. Despite its high-level modeling nature, Cryptol programs
	are fully executable. Further, a large subset of Cryptol can be automatically
	synthesized to hardware. To meet the inherent high-assurance requirements
	of cryptographic systems, Cryptol comes with a suite of formal-methods
	based tools that enable users to perform various program verification
	tasks. In this paper, we provide an overview of Cryptol and its verification
	toolset, especially focusing on the co-verification of third-party
	VHDL implementations against highlevel Cryptol specifications. As
	a case study, we demonstrate the technique on two hand-written VHDL
	implementations of the Skein hash algorithm.},
  doi = {10.1109/FMCAD.2009.5351121},
  keywords = {Skein hash algorithm;cryptographic algorithms;formal-methods based
	tool;hardware-software co-verification;high-level Cryptol specification;program
	verification;programming language;third-party VHDL implementations;cryptography;hardware
	description languages;program verification;programming languages;}
}

@INPROCEEDINGS{1509496,
  author = {Ermel, C. and Holscher, K. and Kuske, S. and Ziemann, P.},
  title = {Animated simulation of integrated UML behavioral models based on
	graph transformation},
  booktitle = {Visual Languages and Human-Centric Computing, 2005 IEEE Symposium
	on},
  year = {2005},
  pages = { 125 - 133},
  month = {sept.},
  abstract = { This paper shows how integrated UML models combining class, object,
	use-case, collaboration and state diagrams can be animated in a domain-specific
	layout. The presented approach is based on graph transformation,
	i.e., UML model diagrams are translated to a graph transformation
	system and the behavior of the integrated model is simulated by applications
	of graph transformation rules. For model validation, users may prefer
	to see the behavior of selected model aspects as scenarios presented
	in the layout of the application domain. We propose to integrate
	animation views with the model's graph transformation system. A prototypical
	validation system has been implemented recently supporting the automatic
	translation of a UML model into a graph transformation system, and
	the interactive execution and simulation of the model behavior. We
	sketch the tool interconnection to GenGED, a visual language environment
	which allows to enrich graph transformation systems for model simulation
	by features for animation.},
  doi = {10.1109/VLHCC.2005.18},
  keywords = { GenGED; UML class; UML model translation; UML object; UML state diagrams;
	UML use-case; animated simulation; domain-specific layout; graph
	transformation; integrated UML behavioral models; visual language
	environment; Unified Modeling Language; computer animation; formal
	specification; graph grammars; program visualisation;}
}

@INPROCEEDINGS{520825,
  author = {Erwig, M. and Meyer, B.},
  title = {Heterogeneous visual languages-integrating visual and textual programming},
  booktitle = {Visual Languages, Proceedings., 11th IEEE International Symposium
	on},
  year = {1995},
  pages = {318 -325},
  month = {sep},
  abstract = {After more than a decade of research, visual languages have still
	not become everyday programming tools. In the short term, an integration
	of visual languages with well-established (textual) programming languages
	may be more likely to meet the actual requirements of practical software
	development than the highly ambitious goal of creating purely visual
	languages. In such an integration, each paradigm can support the
	other where it is superior. Particularly attractive is the use of
	visual expressions for the description of domain-specific data structures
	in combination with textual notations for abstract control structures.
	In addition to a basic framework for heterogeneous languages, we
	outline the design of a development system that allows rapid prototyping
	of implementations of heterogeneous languages. Examples are presented
	from the domains of logical, functional and procedural languages},
  doi = {10.1109/VL.1995.520825},
  keywords = { abstract control structures; development system; domain-specific
	data structures; functional languages; heterogeneous visual languages;
	logical languages; practical software development requirements; procedural
	languages; rapid prototyping; textual notations; textual programming
	languages; visual expressions; visual programming; computer aided
	software engineering; data structures; development systems; functional
	languages; logic programming languages; software prototyping; visual
	languages;}
}

@INPROCEEDINGS{5635201,
  author = {Erwig, M. and Walkingshaw, E.},
  title = {Causal Reasoning with Neuron Diagrams},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2010 IEEE
	Symposium on},
  year = {2010},
  pages = {101 -108},
  month = {sept.},
  abstract = {The principle of causation is fundamental to science and society and
	has remained an active topic of discourse in philosophy for over
	two millennia. Modern philosophers often rely on ``neuron diagrams'',
	a domain-specific visual language for discussing and reasoning about
	causal relationships and the concept of causation itself. In this
	paper we formalize the syntax and semantics of neuron diagrams. We
	discuss existing algorithms for identifying causes in neuron diagrams,
	show how these approaches are flawed, and propose solutions to these
	problems. We separate the standard representation of a dynamic execution
	of a neuron diagram from its static definition and define two separate,
	but related semantics, one for the causal effects of neuron diagrams
	and one for the identification of causes themselves. Most significantly,
	we propose a simple language extension that supports a clear, consistent,
	and comprehensive algorithm for automatic causal inference.},
  doi = {10.1109/VLHCC.2010.23},
  issn = {1943-6092},
  keywords = {causal effect;causal inference;causal reasoning;causal relationships;causation;cause
	identification;domain-specific visual language;dynamic execution;neuron
	diagrams;semantics;static definition;syntax;causality;diagrams;inference
	mechanisms;programming language semantics;visual languages;}
}

@INPROCEEDINGS{5295309,
  author = {Erwig, M. and Walkingshaw, E.},
  title = {Visual explanations of probabilistic reasoning},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {23 -27},
  month = {sept.},
  abstract = {Continuing our research in explanation-oriented language design, we
	present a domain-specific visual language for explaining probabilistic
	reasoning. Programs in this language, called explanation objects,
	can be manipulated according to a set of laws to automatically generate
	many equivalent explanation instances. We argue that this increases
	the explanatory power of our language by allowing a user to view
	a problem from many different perspectives.},
  doi = {10.1109/VLHCC.2009.5295309},
  issn = {1943-6092},
  keywords = {domain-specific visual language;explanation objects;explanation-oriented
	language design;probabilistic reasoning;inference mechanisms;visual
	languages;}
}

@INPROCEEDINGS{5328553,
  author = {Esmaeilsabzali, S. and Day, N.A. and Atlee, J.M. and Jianwei Niu},
  title = {Semantic Criteria for Choosing a Language for Big-Step Models},
  booktitle = {Requirements Engineering Conference, 2009. RE '09. 17th IEEE International},
  year = {2009},
  pages = {181 -190},
  month = {31 2009-sept. 4},
  abstract = {With the popularity of model-driven methodologies, and the abundance
	of modelling languages, a major question for a requirements engineer
	is: which language is suitable for modelling a system under study?
	We address this question from a semantic point-of-view for big-step
	modelling languages (BSMLs). BSMLs are a class of popular behavioural
	modelling languages in which a model can respond to an input by executing
	multiple, possibly concurrent, transitions. We deconstruct the operational
	semantics of a large class of BSMLs into high-level, orthogonal semantic
	aspects, and analyze the relative advantages and disadvantages of
	the common semantic options for each of these aspects. Our goal is
	to empower a requirements engineer to compare and choose an appropriate
	BSML.},
  doi = {10.1109/RE.2009.29},
  issn = {1090-705X},
  keywords = {behavioural modelling language;big-step model;model-driven methodology;operational
	semantic criteria;requirements engineer;computational linguistics;formal
	specification;program diagnostics;specification languages;}
}

@ARTICLE{6065752,
  author = {Estevez, E. and Marcos, M.},
  title = {Model based Validation of Industrial Control Systems},
  journal = {Industrial Informatics, IEEE Transactions on},
  year = {2011},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {Current industrial applications demand the design of more and more
	complex, safe and trustworthy control systems exhibiting a high degree
	of flexibility and reutilization. To achieve this, the engineering
	process should be improved by making the engineering tools involved
	in the development process to collaborate during the design. This
	paper presents a modelbased approach for designing complex automation
	applications. The core of the approach is constituted by a set of
	domain specific models that depend on the application field and whose
	elements, syntax and semantics are defined from the point of view
	of the experts that participate in the design of the system. The
	domain models are defined using engineering tools as the design progresses
	and they can be used to achieve tool integration through model collaboration.
	This can be achieved following the Model Driven Engineering approach
	by means of model transformations. This paper specifically focuses
	on the first step of this paradigm: the definition of domain languages,
	in this case for industrial control systems, as well as validation
	mechanisms of application designs coming from different domain tools.
	Three well known and widely used industrial standards have been used:
	CAEX (Computer Aided Engineering eXchange), PLCopen (a representation
	format for the IEC 61131-3 standard) and MathML (a language for defining
	mathematical constraints). Using model checking it is possible to
	assure the correctness of the control system specification and using
	model transformation it is possible to detect design errors in early
	stages of the design.},
  doi = {10.1109/TII.2011.2174248},
  issn = {1551-3203}
}

@INPROCEEDINGS{5195900,
  author = {Estevez, E. and Marcos, M. and Sarachaga, I. and Lopez, F. and Burgos,
	A. and Perez, F. and Orive, D.},
  title = {Model based documentation of automation applications},
  booktitle = {Industrial Informatics, 2009. INDIN 2009. 7th IEEE International
	Conference on},
  year = {2009},
  pages = {768 -774},
  month = {june},
  abstract = {Automation applications consists of software projects that are developed
	using specific standard programming languages, such as the IEC 61131-3
	standard that provides 5 programming languages, two textual and three
	graphical. They are also executed on specific hardware platforms,
	mainly Programmable Logic Controllers. Thus, they can be viewed as
	special type of software projects. As such, their design must follow
	the different phases of the life cycle process: development, exploitation
	and maintenance. This is even more important and necessary in the
	case of current complex automation applications. There are different
	international standards that define the information that must document
	all the phases of the life cycle. This paper deals with the automatic
	generation of the documentation of some of the phases or at least,
	the automatic generation of templates to be filled by the designers.
	The proposal is based on the Model based Engineering paradigm, in
	which the application is defined by domain specific models that are
	being transformed along the development phases until the application
	code is obtained.},
  doi = {10.1109/INDIN.2009.5195900},
  issn = {1935-4576},
  keywords = {IEC 61131-3 programming standard;automatic generation of templates;automation
	application documentation;factory automation;markup languages;model
	based engineering paradigm;programmable logic controller;software
	project development;XML;control engineering computing;factory automation;programmable
	controllers;programming languages;system documentation;}
}

@INPROCEEDINGS{1652242,
  author = {Etter, R. and Costa, P.D. and Broens, T.},
  title = {A Rule-Based Approach Towards Context-Aware User Notification Services},
  booktitle = {Pervasive Services, 2006 ACS/IEEE International Conference on},
  year = {2006},
  pages = {281 -284},
  month = {june},
  abstract = {Pervasive computing is the vision of technology that is invisibly
	embedded in our natural surroundings. Users are offered unobtrusive
	services that require minimal attention. In this paper the Awareness
	and Notification Service (ANS) is presented that enables to rapidly
	build applications that inform users about their environment. Additionally,
	the service offers notifications tailored to the user's preferences
	and current context. ANS takes a rule based approach based on the
	event-condition-action pattern. Users specify when and what should
	be notified to them by using a convenient ANS rule language. This
	flexible mechanism allows to rapidly develop applications that provide
	context-aware notifications without the need to write programming
	code to activate rules, nor to implement personalized notifications},
  doi = {10.1109/PERSER.2006.1652242},
  keywords = {ANS rule language;Awareness and Notification Service;context-aware
	user notification services;event-condition-action pattern;pervasive
	computing;rule-based approach;unobtrusive services;knowledge based
	systems;knowledge representation languages;ubiquitous computing;}
}

@INPROCEEDINGS{5718862,
  author = {Evensen, K.D.},
  title = {Reducing Uncertainty in Architectural Decisions with AADL},
  booktitle = {System Sciences (HICSS), 2011 44th Hawaii International Conference
	on},
  year = {2011},
  pages = {1 -9},
  month = {jan.},
  abstract = {A model-driven approach to real-time software systems development
	enables the conceptualization of software, fostering a more thorough
	understanding of its often complex architecture and behavior and
	promoting the documentation and analysis of concerns common to real-time
	embedded systems such as scheduling, resource allocation, and performance.
	Key architectural decisions can be made early in the development
	lifecycle by analyzing quantifiable quality attributes related to
	these concerns. Two modeling notations, the Architectural Analysis
	and Design Language (AADL) and the Unified Modeling Language Profile
	for Modeling and Analysis of Real-Time Embedded Systems (MARTE),
	are domain specific notations with the capacity to analyze these
	concerns. However, MARTE is not mature, to the point where its existing
	formalisms have been adopted into wide tool support. Furthermore,
	the inherent ambiguity of UML makes analysis in MARTE difficult.
	As a declarative language, AADL provides an adopted, formal analysis
	framework that meets this need. AADL can be used to augment MARTE
	in modeling software systems and provide the formal mechanisms for
	conducting quality analyses, helping to reduce uncertainty in architectural
	decisions.},
  doi = {10.1109/HICSS.2011.358},
  issn = {1530-1605},
  keywords = {AADL;architectural analysis;architectural decisions;design language;formal
	analysis framework;real-time embedded systems;uncertainty reduction;unified
	modeling language profile;Unified Modeling Language;embedded systems;software
	architecture;}
}

@ARTICLE{1392718,
  author = {Evermann, J. and Wand, Y.},
  title = {Toward formalizing domain modeling semantics in language syntax},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2005},
  volume = {31},
  pages = { 21 - 37},
  number = {1},
  month = {jan.},
  abstract = { Information systems are situated in and are representations of some
	business or organizational domain. Hence, understanding the application
	domain is critical to the success of information systems development.
	To support domain understanding, the application domain is represented
	in conceptual models. The correctness of conceptual models can affect
	the development outcome and prevent costly rework during later development
	stages. This paper proposes a method to restrict the syntax of a
	modeling language to ensure that only possible configurations of
	a domain can be modeled, thus increasing the likelihood of creating
	correct domain models. The proposed method, based on domain ontologies,
	captures relationships among domain elements via constraints on the
	language metamodel, thus restricting the set of statements about
	the domain that can be generated with the language. In effect, this
	method creates domain specific modeling languages from more generic
	ones. The method is demonstrated using the Unified Modeling Language
	(UML). Specifically, it is applied to the subset of UML dealing with
	object behavior and its applicability is demonstrated on a specific
	modeling example.},
  doi = {10.1109/TSE.2005.15},
  issn = {0098-5589},
  keywords = { CASE; UML; Unified Modeling Language; conceptual models; domain modeling
	semantics; domain ontologies; information systems development; language
	metamodel; language syntax; object-oriented design methods; Unified
	Modeling Language; computer aided software engineering; formal specification;
	information systems; object-oriented methods; ontologies (artificial
	intelligence); programming language semantics;}
}

@ARTICLE{5035599,
  author = {Fabry, J. and Tanter, E. and D'Hondt, T.},
  title = {Infrastructure for domain-specific aspect languages: the relax case
	study},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {238 -254},
  number = {3},
  month = {june },
  abstract = {Domain-specific aspect languages (DSALs) bring the well-known advantages
	of domain specificity to the level of aspect code. However, DSALs
	incur the significant cost of implementing or extending a language
	processor or weaver. Furthermore, this weaver typically operates
	blindly, making detection of interactions with aspects written in
	other languages impossible. This raises the necessity of an appropriate
	infrastructure for DSALs. The case study we present here illustrates
	how the reflex kernel for multi-language AOP addresses these issues,
	by considering the implementation of a DSAL for advanced transaction
	management, KALA. We first detail the implementation of KALA in reflex,
	called relax, illustrating the ease of implementation of runtime
	semantics, syntax, and language translation. We then show a straightforward
	and modular extension to KALA at all these levels, and demonstrate
	how reflex helps in dealing with interactions between KALA and another
	DSAL for concurrency management. These invaluable assets enable faster
	development of DSALs as well as their ability to coexist within one
	application, thereby removing the most important impediments to their
	re-emergence in the aspect community.},
  doi = {10.1049/iet-sen.2007.0120},
  issn = {1751-8806},
  keywords = {advanced transaction management;domain-specific aspect languages;language
	processor;language translation;multilanguage AOP address;reflex kernel;language
	translation;object-oriented languages;}
}

@INPROCEEDINGS{5328834,
  author = {Biao Fan and Guangqiang Liu and Tao Liu and He Hu and Xiaoyong Du},
  title = {KeyOnto: A Hybrid Knowledge Retrieval Model in Law Semantic Web},
  booktitle = {ChinaGrid Annual Conference, 2009. ChinaGrid '09. Fourth},
  year = {2009},
  pages = {179 -184},
  month = {aug.},
  abstract = {This paper proposes a hybrid knowledge retrieval model KeyOnto, which
	combines ontology based knowledge retrieval model with traditional
	Vector Space Model (VSM). KeyOnto model makes use of domain ontology
	to organize and structure knowledge resources. Documents and queries
	are represented by concepts and term vectors respectively. Furthermore,
	ontology based query expansion called K2CM, is introduced to get
	expanded concepts of a query. Domain specific terms are used to form
	a term vector for queries and documents. Basing on these vectors,
	we can evaluate term similarity and concept similarity respectively,
	and integrate them together. Domain specific thesaurus is used to
	assist knowledge retrieval. Experiments show that compared with each
	single model, KeyOnto model improves precision of query result.},
  doi = {10.1109/ChinaGrid.2009.19},
  keywords = {KeyOnto model;domain ontology;hybrid knowledge retrieval model;law
	semantic Web;ontology based query expansion;vector space model;law;ontologies
	(artificial intelligence);query processing;semantic Web;}
}

@ARTICLE{5306048,
  author = {Fan, J and Chen, J and Tian, B and Yan, D and Cheng, G and Cui, P
	and Zhang, W},
  title = {Rapid Assessment of the Secondary Disasters Induced by the Wenchuan
	Earthquake},
  journal = {Computing in Science Engineering},
  year = {2009},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {The Wenchuan earthquake on May 12, 2008 led to large quantities of
	secondary mountain disasters. When the river channels were blocked
	by masses from the earthquake, the barrier lakes were formed. They
	were hazardous when the river inundated the farmlands and forests.
	If the damming objects collapsed, the flood could lead to massive
	casualties and property losses. Exploiting the real-time multi-platform
	remote sensing imagery data, we have discovered the barrier lakes
	induced by the earthquake, acquired the distribution information
	of the barrier lakes, and assessed the fatalness of them to prevent
	possible disastrous consequences. Also, after combining the multi-platform
	remote sensing data after the earthquake with that before the earthquake,
	we have assessed the farmland and forest losses. This work has been
	carried out within two weeks right after the earthquake, which helps
	the government visualize the disaster and plan emergency responses.},
  doi = {10.1109/MCSE.2009.162},
  issn = {1521-9615}
}

@INPROCEEDINGS{6032618,
  author = {Fant, J.S.},
  title = {Building domain specific software architectures from software architectural
	design patterns},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {1152 -1154},
  month = {may},
  abstract = {Software design patterns are best practice solutions to common software
	problems. However, applying design patterns in practice can be difficult
	since design pattern descriptions are general and can be applied
	at multiple levels of abstraction. In order to address the aforementioned
	issue, this research focuses on creating a systematic approach to
	designing domain specific distributed, real-time and embedded (DRE)
	software from software architectural design patterns. To address
	variability across a DRE domain, software product line concepts are
	used to categorize and organize the features and design patterns.
	The software architectures produced are also validated through design
	time simulation. This research is applied and validated using the
	space flight software (FSW) domain.},
  doi = {10.1145/1985793.1986026},
  issn = {0270-5257},
  keywords = {DRE software;FSW domain;design time simulation;distributed software;domain
	specific software architecture;embedded software;real-time software;software
	architectural design pattern;software product line concept;space
	flight software;object-oriented programming;software architecture;}
}

@INPROCEEDINGS{6044766,
  author = {Farag, M.M. and Lerner, L.W. and Patterson, C.D.},
  title = {Thwarting Software Attacks on Data-Intensive Platforms with Configurable
	Hardware-Assisted Application Rule Enforcement},
  booktitle = {Field Programmable Logic and Applications (FPL), 2011 International
	Conference on},
  year = {2011},
  pages = {207 -212},
  month = {sept.},
  abstract = {Security is difficult to achieve on general-purpose computing platforms
	due to their complexity, excess functionality, and resource sharing.
	An alternative is the creation of a Tailored Trustworthy Space for
	the system or application class of interest. We focus on data-intensive
	computing systems using reconfigurable hardware to implement streaming
	operations, and provide security assurances that are independent
	of application software, middleware, or operating system integrity
	and correctness. All interaction between software and the dataflow
	hardware passes through an automatically synthesized and formally
	verified hardware controller incorporating enforcement and real-time
	monitoring of application-specific rules. Abstractions provided by
	the Blue spec high-level language assist in the translation of domain-specific
	policy rules to synthesized logic. For the cognitive radio example
	used, hardware-enforced policies include physical layer rules such
	as sanctioned spectrum usage. Policy changes cause the secure generation
	and transfer of a new controller-wrapped datapath hardware plug-in.
	Datapath dynamic block swaps and cryptographic operations are managed
	entirely by the hardware controller rather than software drivers.
	Design for performance and design for security are therefore simultaneously
	addressed since the datapath is configured and monitored at hardware
	speeds, and software has no access to datapath configurations and
	cryptographic keys.},
  doi = {10.1109/FPL.2011.45},
  keywords = {configurable hardware-assisted application rule enforcement;controller-wrapped
	datapath hardware plug-in;cryptographic keys;cryptographic operations;data-intensive
	computing systems;data-intensive platforms;formally verified hardware
	controller;real-time monitoring;reconfigurable hardware;resource
	sharing;software attacks;streaming operations;tailored trustworthy
	space;cryptography;reconfigurable architectures;}
}

@INPROCEEDINGS{5254079,
  author = {Farkas, T. and Neumann, C. and Hinnerichs, A.},
  title = {An Integrative Approach for Embedded Software Design with UML and
	Simulink},
  booktitle = {Computer Software and Applications Conference, 2009. COMPSAC '09.
	33rd Annual IEEE International},
  year = {2009},
  volume = {2},
  pages = {516 -521},
  month = {july},
  abstract = {The increased amount of software in automotive embedded systems has
	challenged its C code development to successfully manage software
	design, reuse, flexibility and efficient implementation. Model-based
	methods help to address such challenges with more abstract specification,
	code generation and simulation to determine if software design will
	meet requirements. However, in todaypsilas development processes
	lots of different legacy artifacts are involved, that hamper a frictionless
	migration from C code to model-based design. Therefore, migration
	concepts and adequate domain-specific methods with adoption of modeling
	languages and their tools in established embedded coding environments
	are needed. In our approach we present a novel migration concept
	considering the integration of two different modeling languages UML
	and Simulink in a traditional automotive software engineering process.
	The proof is demonstrated within the software development of a real
	automotive car door-controller ECU.},
  doi = {10.1109/COMPSAC.2009.185},
  issn = {0730-3157},
  keywords = {C code generation;UML modeling language;abstract specification;automotive
	car door-controller ECU;automotive embedded software system design;automotive
	industry;automotive software engineering process;domain-specific
	method;embedded code-for-control unit;integrative approach;migration
	concept;object-based method;simulink control design;software component
	architecture;software development management process;software flexibility;software
	requirement;software reuse;C language;Unified Modeling Language;automobile
	industry;automobiles;automotive components;control system synthesis;digital
	simulation;embedded systems;object-oriented methods;object-oriented
	programming;program compilers;software architecture;software development
	management;software reusability;}
}

@ARTICLE{5353438,
  author = {Favre, Jean-Marie and Gasevic, Dragan and Lammel, Ralf and Winter,
	Andreas},
  title = {Guest Editors' Introduction to the Special Section on Software Language
	Engineering},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2009},
  volume = {35},
  pages = {737 -741},
  number = {6},
  month = {nov.-dec. },
  abstract = {The six articles in this special section are devoted to software language
	engineering.},
  doi = {10.1109/TSE.2009.78},
  issn = {0098-5589}
}

@ARTICLE{4543983,
  author = {Favre, J.-M. and Gasevic, D. and Lammel, R. and Winter, A.},
  title = {Editorial - Software language engineering},
  journal = {Software, IET},
  year = {2008},
  volume = {2},
  pages = {161 -164},
  number = {3},
  month = {june },
  abstract = {Software languages play an important role in software development.
	Software languages are the artificial languages that are used to
	describe software systems at various abstraction levels. They are
	applied to describe requirements and designs for software, definitions
	of software architectures, and implementations of software systems.
	A huge variety of different technological spaces exist to describe
	languages: programming languages, software modeling languages, data
	modeling languages, domain-specific languages, ontology language,
	and others.},
  doi = {10.1049/iet-sen:20089010},
  issn = {1751-8806},
  keywords = {data modeling language;domain-specific language;ontology language;programming
	language;software development;software language engineering;software
	modeling language;software system;formal languages;software engineering;}
}

@INPROCEEDINGS{342683,
  author = {Feather, M.S. and Fickas, S.},
  title = {A framework for distributed system designs},
  booktitle = {Knowledge-Based Software Engineering Conference, 1994. Proceedings.,
	Ninth},
  year = {1994},
  pages = {6 -13},
  month = {sep},
  abstract = {We present a framework to structure the space of designs for a class
	of distributed systems. The purpose of this framework is to codify
	known design knowledge, and thus, when given the task of developing
	a new system in this class, to facilitate: navigation-finding designs
	applicable to the task; evaluation-identifying the strengths of weaknesses
	of a given design comparing alternative designs; and reification-realizing
	an abstract design as the solution to a concrete task. This is illustrated
	on the class of resource control systems operating in a distributed
	setting},
  doi = {10.1109/KBSE.1994.342683},
  keywords = { abstract design; distributed setting; distributed system designs;
	evaluation; idealized requirements; known design knowledge; navigation;
	prior knowledge; reification; resource control systems; distributed
	processing; knowledge based systems; resource allocation; systems
	analysis;}
}

@INPROCEEDINGS{858487,
  author = {Felfernig, A. and Friedrich, G. and Jannach, D. and Zanker, M.},
  title = {Cooperating configuration agents supporting supply chain integration
	of customizable products},
  booktitle = {MultiAgent Systems, 2000. Proceedings. Fourth International Conference
	on},
  year = {2000},
  pages = {385 -386},
  abstract = {The integration of configuration systems for supporting the supply-chain
	management of configurable products and services is still an open
	research issue. Current configurator approaches are designed for
	solving local configuration problems, but there is still no support
	for the integration of different configuration systems. In order
	to facilitate cooperative configuration, we employ configuration
	agents that are capable of managing requests and posting configuration
	subtasks to remote agents. For integrating the different knowledge
	representation formalisms of the configuration agents, we construct
	common ontologies by employing broadly-used configuration domain-specific
	modelling concepts},
  doi = {10.1109/ICMAS.2000.858487},
  keywords = {common ontologies;configurable services;configuration domain-specific
	modelling concepts;configuration subtask posting;configuration systems
	integration;configurator approaches;cooperating configuration agents;cooperative
	configuration;customizable products;knowledge representation formalisms;remote
	agents;request management;supply chain integration;supply-chain management;integrated
	software;knowledge representation;logistics data processing;multi-agent
	systems;}
}

@INPROCEEDINGS{5588261,
  author = {Lei Feng and DeJiu Chen and Lnn, H. and Trngren, M.},
  title = {Verifying system behaviors in EAST-ADL2 with the SPIN model checker},
  booktitle = {Mechatronics and Automation (ICMA), 2010 International Conference
	on},
  year = {2010},
  pages = {144 -149},
  month = {aug.},
  abstract = {EAST-ADL2 is a domain-specific architecture description language to
	support the model-based development of automotive embedded systems.
	It emerged to manage the complexity of software and electronics in
	advanced automotive applications. The language focuses on the structural
	definition for functional specifications. Behavior is defined only
	on the component level, in terms of functional blocks and their triggers
	and interfaces. The behavioral definition inside each functional
	block is not prescribed. This paper shows one approach to augment
	the language with precise syntax and semantics for behavior, and
	develops a procedure that transforms the composed behavioral model
	to the SPIN model for logic model checking. The contribution improves
	the modeling and verification capability of EAST-ADL2.},
  doi = {10.1109/ICMA.2010.5588261},
  issn = {2152-7431},
  keywords = {EAST-ADL2;SPIN model checker;automotive embedded systems;domain specific
	architecture description language;logic model checking;software complexity;system
	behaviors verification;automotive engineering;embedded systems;program
	verification;specification languages;}
}

@INPROCEEDINGS{5501574,
  author = {Fernandes, D.D. and Cardoso, F. and Montini, D.A. and Supino, F.M.
	and Tasinaffo, P.M. and Dias, L.},
  title = {An Algorithm Model to Mapping Mealy Machines for a Software Manufacture
	Cell Petri Net},
  booktitle = {Information Technology: New Generations (ITNG), 2010 Seventh International
	Conference on},
  year = {2010},
  pages = {1306 -1308},
  month = {april},
  abstract = {This paper shows how to use the state machines and systematic approaches
	for modeling software to help improve the consistency of the model,
	verification and validation of an analysis area through a Domain
	Specific Language (Domain Specifical Language - DSL), in addition
	to the refinement of the business process (since the mapping of domain
	analysis and process of business development). The main objective
	of this approach is how to obtain systematically a DSL from a domain
	analysis that may be using the system code compliance to all Business
	Rules outlined, and no documents and settings very complex. Many
	problems of Systems Computer Software Computer Software Systems-CSS)
	are derived from a specification with and without its behavior defined.
	To resolve these problems, business rules will be treated since its
	formalization to its construction and testing. In this context, Petri
	Nets provide a graphical description technique easy to understand
	and, closed to state-transition diagrams. Parallelism, concurrency
	and sincronization are easy to model in a Petri net. Add to this,
	many techniques and tools (in software) are available for the analysis
	of Petri nets. However, too much formalization can bring problems
	to software development and the time and cost grow. Furthermore it
	is suggested that formal methods have fewer errors than the heuristic
	methods.},
  doi = {10.1109/ITNG.2010.83},
  keywords = {algorithm model;business process;business rules;computer software
	computer;domain specific language;formal methods;graphical description
	technique;mealy machines;software development;software manufacture
	cell Petri net;state machines;state-transition diagrams;Petri nets;business
	process re-engineering;finite state machines;formal verification;}
}

@INPROCEEDINGS{5195889,
  author = {Ferrarini, L. and Dede, A. and Salaun, P. and Tuan Dang and Fogliazza,
	G.},
  title = {Domain specific views in model-driven embedded systems design in
	industrial automation},
  booktitle = {Industrial Informatics, 2009. INDIN 2009. 7th IEEE International
	Conference on},
  year = {2009},
  pages = {702 -707},
  month = {june},
  abstract = {The work presented in this paper describes the concept of domain specific
	views (DSVs) according to which the specification of the control
	behavior is provided to the control developer. The basic idea is
	to investigate different application fields, to try to find common
	rules and models in different fields, and finally to build around
	this an automatic or semi-automatic transformation into executable
	code. The original idea is defined in a bigger aim that is the definition
	of a model oriented to the so-called ldquoautomation componentrdquo,
	used to standardize the design of an automation system. In the paper,
	the investigation is discussed for the discrete manufacturing and
	energy production fields, while in the main project (MEDEIA FP7-2007-211448)
	other fields are investigated and many languages and methodologies
	to develop control, diagnosis and simulation of automatic systems
	have been analyzed.},
  doi = {10.1109/INDIN.2009.5195889},
  issn = {1935-4576},
  keywords = {MEDEIA FP7-2007-211448;automation component;discrete manufacturing;domain
	specific views;energy production;industrial automation;model-driven
	embedded systems;embedded systems;industrial control;production engineering
	computing;}
}

@INPROCEEDINGS{5695617,
  author = {Ferreira, H. and Duarte, S. and Preguia, N.},
  title = {4Sensing -- Decentralized Processing for Participatory Sensing Data},
  booktitle = {Parallel and Distributed Systems (ICPADS), 2010 IEEE 16th International
	Conference on},
  year = {2010},
  pages = {306 -313},
  month = {dec.},
  abstract = {Participatory Sensing is an emerging application paradigm that leverages
	the growing ubiquity of sensor-capable smart phones to allow communities
	carry out wide-area sensing tasks, as a side-effect of people's everyday
	lives and movements. This paper proposes a decentralized infrastructure
	for supporting Participatory Sensing applications. It describes an
	architecture and a domain specific programming language for modeling,
	prototyping and developing the distributed processing of participatory
	sensing data with the goal of allowing faster and easier development
	of these applications. Moreover, a case-study application is also
	presented as the basis for an experimental evaluation.},
  doi = {10.1109/ICPADS.2010.20},
  issn = {1521-9097},
  keywords = {4Sensing;decentralized processing;distributed processing development;distributed
	processing modeling;distributed processing prototyping;domain specific
	programming language;mobile computing;participatory sensing;sensor-capable
	smart phones;wide-area sensing tasks;mobile computing;}
}

@INPROCEEDINGS{5195843,
  author = {Ferreira, J.},
  title = {MDAI: Model based design in automobile industry},
  booktitle = {Industrial Informatics, 2009. INDIN 2009. 7th IEEE International
	Conference on},
  year = {2009},
  pages = {434 -439},
  month = {june},
  abstract = {It is proposed a new approach based on a methodology, assisted by
	a tool, to create new products in the automobile industry based on
	previous defined processes and experiences inspired on a set of best
	practices or principles: it is based on high-level models or specifications;
	it is component-based architecture centric; it is based on generative
	programming techniques. This approach follows in essence the MDA
	(Model Driven Architecture) philosophy with some specific characteristics.
	We propose a repository that keeps related information, such as models,
	applications, design information, generated artifacts and even information
	concerning the development process itself (e.g., generation steps,
	tests and integration milestones). Generically, this methodology
	receives the users' requirements to a new product (e.g., functional,
	non-functional, product specification) as its main inputs and produces
	a set of artifacts (e.g., design parts, process validation output)
	as its main output, that will be integrated in the engineer design
	tool (e.g. CAD system) facilitating the work.},
  doi = {10.1109/INDIN.2009.5195843},
  issn = {1935-4576},
  keywords = {automobile industry;engineer design tool;generative programming techniques;model
	based design;model driven architecture;product design;automobile
	industry;design engineering;product design;}
}

@INPROCEEDINGS{65345,
  author = {Fertig, S. and Gelernter, D.H.},
  title = {FGP: a virtual machine for database-driven expert systems},
  booktitle = {Tools for Artificial Intelligence, 1989. Architectures, Languages
	and Algorithms, IEEE International Workshop on},
  year = {1989},
  pages = {388 -392},
  month = {oct},
  abstract = {The authors describe FGP, a specification for a virtual machine that
	makes domain-specific knowledge explicit and brings it to bear on
	classification and identification problems. The model provides much
	of the functionality of traditional expert systems without requiring
	the system builder to preprocess the database into rules, frames,
	or heuristics. The authors explain the model, describe a particular
	implementation of it, and present some intriguing test results},
  doi = {10.1109/TAI.1989.65345},
  keywords = {FGP;classification;database-driven expert systems;domain-specific
	knowledge;identification;virtual machine;database management systems;expert
	systems;knowledge engineering;virtual machines;}
}

@ARTICLE{4804670,
  author = {Filho, J.O. and Masekowsky, S. and Schweizer, T. and Rosenstiel,
	W.},
  title = {CGADL: An Architecture Description Language for Coarse-Grained Reconfigurable
	Arrays},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2009},
  volume = {17},
  pages = {1247 -1259},
  number = {9},
  month = {sept. },
  abstract = {The high degree of freedom in the design of coarse-grained reconfigurable
	arrays imposes new challenges on their description and modeling.
	In this paper, we introduce an architecture description language
	targeted to describe coarse-grained reconfigurable architecture templates.
	It comprises innovative key features to allow fast modeling and analysis
	of such architectures, i.e.: representation of processing element
	array (ir)regularities, and flexible and concise description of interconnection
	network. We demonstrate that the proposed language enables a formal
	validation of the described template, and it eases the analysis and
	estimation of hardware costs earlier in the design phase. Finally,
	we show how we automatically generate a SystemC-based simulator of
	the described architecture. Our results suggest that the semantic
	and technical innovations of the proposed architecture description
	language may have a positive impact on the productivity of the design
	phase.},
  doi = {10.1109/TVLSI.2008.2002429},
  issn = {1063-8210},
  keywords = {SystemC-based simulator;architecture description language;coarse-grained
	reconfigurable architecture;coarse-grained reconfigurable arrays;degree
	of freedom;interconnection network;processing element array irregularity
	representation;hardware description languages;reconfigurable architectures;}
}

@ARTICLE{4336299,
  author = {Fischmeister, S. and Sokolsky, O. and Insup Lee},
  title = {A Verifiable Language for Programming Real-Time Communication Schedules},
  journal = {Computers, IEEE Transactions on},
  year = {2007},
  volume = {56},
  pages = {1505 -1519},
  number = {11},
  month = {nov. },
  abstract = {Distributed hard real-time systems require predictable communication
	at the network level and verifiable communication behavior at the
	application level. At the network level, communication between nodes
	must be guaranteed to happen within bounded time and one common approach
	is to restrict the network access by enforcing a time-division multiple
	access (TDMA) schedule. At the application level, the application's
	communication behavior should be verified to ensure that the application
	uses the predictable communication in the intended way. Network code
	is a domain-specific programming language to write a predictable
	verifiable distributed communication for distributed real-time applications.
	In this paper, we present the syntax and semantics of network code,
	how we can implement different scheduling policies, and how we can
	use tools such as model checking to formally verify the properties
	of network code programs. We also present an implementation of a
	runtime system for executing network code on top of RTLinux and measure
	the overhead incurred from the runtime system.},
  doi = {10.1109/TC.2007.70747},
  issn = {0018-9340},
  keywords = {RTLinux;domain-specific programming language;network code;network
	code programs;network code semantics;network code syntax;real-time
	communication;time-division multiple access;verifiable distributed
	communication;verifiable language;computer networks;program verification;programming
	languages;real-time systems;telecommunication computing;time division
	multiple access;}
}

@INPROCEEDINGS{4351317,
  author = {Fish, Andrew and Knapp, Alexander},
  title = {Layout of (Software) Engineering Diagrams},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {4},
  month = {sept.},
  abstract = {Traditionally, diagrams play an important role in many disciplines
	such as electrical engineering (e.g. Karnaugh-diagrams), civil and
	mechanical engineering (construction plans), geography (maps), etc.
	In Software Engineering today, diagrammatic languages like IDEF,
	UML or ARIS are commonplace, and with the rise of model driven development
	and domain specific languages, the use of such languages will become
	even more widespread in the future. All in all, diagrams play an
	important role in communication between engineers.},
  doi = {10.1109/VLHCC.2007.57}
}

@INPROCEEDINGS{4639046,
  author = {Fish, Andrew and Storrle, Harald},
  title = {Second international workshop on Layout of (Software) Engineering
	diagrams (LED CHARPx2019;08)},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {3},
  month = {sept.},
  abstract = {Traditionally, diagrams play an important role in many disciplines
	from electrical engineering (e.g. Karnaugh diagrams), civil and mechanical
	engineering (construction plans), geography (maps), and so on. In
	Software Engineering, diagrammatic languages like IDEF, UML or ARIS
	are commonplace today. With the rise of model driven development
	and domain specific languages, such languages will be even more widespread
	in the future. All in all, diagrams play an important role in the
	communication among engineers.},
  doi = {10.1109/VLHCC.2008.4639046},
  issn = {1943-6092}
}

@INPROCEEDINGS{1553562,
  author = {Fisler, K. and Krishnamurthi, S. and Meyerovich, L.A. and Tschantz,
	M.C.},
  title = {Verification and change-impact analysis of access-control policies},
  booktitle = {Software Engineering, 2005. ICSE 2005. Proceedings. 27th International
	Conference on},
  year = {2005},
  pages = { 196 - 205},
  month = {may},
  abstract = { Sensitive data are increasingly available on-line through the Web
	and other distributed protocols. This heightens the need to carefully
	control access to data. Control means not only preventing the leakage
	of data but also permitting access to necessary information. Indeed,
	the same datum is often treated differently depending on context.
	System designers create policies to express conditions on the access
	to data. To reduce source clutter and improve maintenance, developers
	increasingly use domain-specific, declarative languages to express
	these policies. In turn, administrators need to analyze policies
	relative to properties, and to understand the effect of policy changes
	even in the absence of properties. This paper presents Margrave,
	a software suite for analyzing role-based access-control policies.
	Margrave includes a verifier that analyzes policies written in the
	XACML language, translating them into a form of decision-diagram
	to answer queries. It also provides semantic differencing information
	between versions of policies. We have implemented these techniques
	and applied them to policies from a working software application.},
  doi = {10.1109/ICSE.2005.1553562},
  keywords = { Margrave language; XACML language; access-control policy verification;
	change-impact analysis; data access; decision diagram; role-based
	access-control; semantic differencing information; authorisation;
	decision diagrams; formal verification; protocols;}
}

@INPROCEEDINGS{6078287,
  author = {Fister, Iztok and Mernik, Marjan and Fister, Iztok and Hrncic, Dejan},
  title = {Implementation of the domain-specific language EasyTime using a LISA
	compiler generator},
  booktitle = {Computer Science and Information Systems (FedCSIS), 2011 Federated
	Conference on},
  year = {2011},
  pages = {801 -808},
  month = {sept.},
  abstract = {A manually time-measuring tool in mass sporting competitions cannot
	be imagined nowadays because many modern disciplines, such as IronMan,
	take a long time and, therefore, demand additional reliability. Moreover,
	automatic timing devices, based on RFID technology, have become cheaper.
	However, these devices cannot operate stand-alone because they need
	a computer measuring system that is capable of processing the incoming
	events, encoding the results, assigning them to the correct competitor,
	sorting the results according to the achieved times, and then providing
	a printout of the results. In this article, the domain-specific language
	EasyTime is presented, which enables the controlling of an agent
	by writing the events in a database. In particular, we are focused
	on the implementation of EasyTime with a LISA tool that enables the
	automatic construction of compilers from language specifications
	using Attribute Grammars. By using of EasyTime, we can also decrease
	the number of measuring devices. Furthermore, EasyTime is universal
	and can be applied to many different sporting competitions in practice.}
}

@INPROCEEDINGS{4123400,
  author = {Fodor, P. and Huerta, J.M.},
  title = {PLANNING AND LOGIC PROGRAMMING FOR DIALOG MANAGEMENT},
  booktitle = {Spoken Language Technology Workshop, 2006. IEEE},
  year = {2006},
  pages = {214 -217},
  month = {dec.},
  abstract = {Dialog interaction in conversational applications is subordinated
	to the goal of completing a domain-specific task. In this paper we
	present a basic architecture, a knowledge representation system,
	and a planning algorithm for dialogue management that decouples the
	interaction process from the planning task. In our system, the interaction
	is driven by the planner. We use logic programming, automatic planning
	and problem solving algorithms for representing information states
	and performing interaction management in the dialogue system. Our
	approach leverages recent advances in formalisms, inference engines,
	planning and problem solving, and is particularly suitable when implementing
	negotiation-intensive conversational applications.},
  doi = {10.1109/SLT.2006.326793},
  keywords = {automatic planning algorithm;dialog interaction;dialogue management;inference
	engines;knowledge representation system;logic programming;negotiation-intensive
	conversational applications;problem solving algorithm;inference mechanisms;interactive
	systems;knowledge representation;logic programming;natural languages;problem
	solving;speech processing;}
}

@INPROCEEDINGS{5546775,
  author = {Foley, Christopher and Power, Gemma and Griffin, Leigh and Chen Chen
	and Donnelly, Niall and de Leastar, Eamonn},
  title = {Service Group Management facilitated by DSL driven policies in embedded
	middleware},
  booktitle = {Computers and Communications (ISCC), 2010 IEEE Symposium on},
  year = {2010},
  pages = {483 -488},
  month = {june},
  abstract = {Middleware by its very nature is fundamental to the functioning of
	systems as it provides the communication between software components.
	It is very much an underlying technology and is rarely visible to
	end users. As systems develop, certain domain semantics, provided
	by the domain experts, need to be injected into the behaviour of
	the underlying middleware, but in a controlled manner. The methods
	used to achieve this are often static in nature, wholly dependent
	on how they are implemented, deployed and managed. An increasingly
	popular way to manage this behaviour injection is through the use
	of policies, a technique used to govern defined rules, triggered
	by associated events, resulting in specific actions when certain
	conditions are encountered. Strong efforts have been made throughout
	the evolution of software development methods and programming languages
	to solve the lack of dynamicity which can arise through poor practices.
	Successive language based attempts to attain a higher level of abstraction
	in the notations used and techniques deployed have resulted in the
	re-discovery of Domain Specific Languages (DSL). This paper looks
	at injecting the dynamicity required in the management of service
	groups through a policy based DSL.},
  doi = {10.1109/ISCC.2010.5546775},
  issn = {1530-1346}
}

@ARTICLE{902347,
  author = {Fontoura, M. and Braga, C. and Moura, L. and Lucena, C.},
  title = {Using domain specific languages to instantiate object-oriented frameworks},
  journal = {Software, IEE Proceedings -},
  year = {2000},
  volume = {147},
  pages = {109 -116},
  number = {4},
  month = {aug},
  abstract = {Prior research has shown that high levels of software reuse can be
	achieved through the use of object-oriented frameworks. An object-oriented
	framework captures the common aspects of a family of applications,
	thus allowing the designers and implementers to reuse this experience
	at the design and code levels. In spite of being a powerful design
	solution, frameworks are not always easy to use. A technique is described
	that uses domain specific languages (DSL) to describe the framework
	variation points and therefore syntactically assure the creation
	of valid framework instances. This approach allows framework users
	to develop applications without the worry of framework implementation,
	so allowing them to remain focused on the problem domain. In addition,
	the use of DSLs allows for better error handling, when compared to
	the standard approach of adapting frameworks by directly adding subclasses.
	The DSL programs are translated to the framework instantiation code
	using a transformational system. The approach is illustrated through
	two real-world frameworks},
  doi = {10.1049/ip-sen:20000791},
  issn = {1462-5970},
  keywords = {domain specific languages;error handling;framework variation points;object-oriented
	frameworks;software reuse;object-oriented methods;software reusability;specification
	languages;}
}

@INPROCEEDINGS{4469160,
  author = {Forgac, M. and Kollar, J. and Poruban, J.},
  title = {Reflection as a tool for adaptability of software systems},
  booktitle = {Applied Machine Intelligence and Informatics, 2008. SAMI 2008. 6th
	International Symposium on},
  year = {2008},
  pages = {179 -182},
  month = {jan.},
  abstract = {Effective software evolution needs to be supported by appropriate
	execution environment. We state that adaptiveness can be also achieved
	using appropriate adaptive language. Runtime adaptability of aspect-oriented
	language can be one of the solutions which can help in software evolution,
	but there are some obstacles which do not allow to easy create a
	desired solution. Applied language or more languages should be minimal
	and strongly associated with properties of the software system in
	any point of its implementation, thus there is the need to change
	language when it is useful in order to language and program would
	be evolved together. From the viewpoint of adaptability, we classify
	software systems as being nonreflexive, introspective and adaptive.
	Adaptive execution can be supported by adaptive language and by environment,
	which offers reflective possibilities. One of the candidates for
	this purpose should be Smalltalk-like environment Squeak, which offers
	several reflective possibilities.},
  doi = {10.1109/SAMI.2008.4469160},
  keywords = {Smalltalk-like Squeak platform;aspect-oriented language;runtime adaptability;software
	evolution system;Smalltalk;object-oriented programming;software prototyping;}
}

@INPROCEEDINGS{4239964,
  author = {Forster, A. and Engels, G. and Schattkowsky, T. and Van Der Straeten,
	R.},
  title = {Verification of Business Process Quality Constraints Based on Visual
	Process Patterns},
  booktitle = {Theoretical Aspects of Software Engineering, 2007. TASE '07. First
	Joint IEEE/IFIP Symposium on},
  year = {2007},
  pages = {197 -208},
  month = {june},
  abstract = {Business processes usually have to consider certain constraints like
	domain specific and quality requirements. The automated formal verification
	of these constraints is desirable, but requires the user to provide
	an unambiguous formal specification. In particular since the notations
	for business process modeling are usually visual flow-oriented languages,
	the notational gap to the languages usually employed for the formal
	specification of constraints, e.g., temporal logic, is significant
	and hard to bridge. Thus, our approach relies on UML Activities as
	a single language for the specification of both business processes
	and the corresponding constraints. For the expression of such constraints,
	we have provided a process pattern definition language based on specialized
	Activities. In this paper, we describe how model checking can be
	employed for formal verification of business processes against such
	patterns. For this, we present an automated transformation of the
	business process and the corresponding patterns into a transition
	system and temporal logic, respectively.},
  doi = {10.1109/TASE.2007.56},
  keywords = {UML activity;automated formal verification;business process modeling;business
	process quality constraint verification;formal specification;model
	checking;temporal logic;transition system;visual flow-oriented language;visual
	process pattern;Unified Modeling Language;business data processing;formal
	specification;formal verification;temporal logic;}
}

@INPROCEEDINGS{1698775,
  author = {Forster, A. and Engels, G. and Schattkowsky, T. and Van Der Straeten,
	R.},
  title = {A Pattern-driven Development Process for Quality Standard-conforming
	Business Process Models},
  booktitle = {Visual Languages and Human-Centric Computing, 2006. VL/HCC 2006.
	IEEE Symposium on},
  year = {2006},
  pages = {135 -142},
  month = {sept.},
  abstract = {Quality management is a hot issue in most organisations and must be
	considered in the business processes of the organisation. Existing
	approaches on business process modelling provide neither explicit
	strategy to model quality requirements on business processes nor
	do they provide explicit support for the construction of business
	processes satisfying such quality requirements. In this paper, we
	present a pattern-driven development process for modelling business
	processes with respect to given quality constraints. We introduce
	a visual pattern specification language based on UML Activity Diagrams
	that enables the expression of quality constraints as patterns. These
	patterns can be used in a forward-engineering development process
	which supports the business process designer in constructing business
	processes by applying patterns. Thereby, quality constraints can
	be integrated into the design of business processes seamlessly},
  doi = {10.1109/VLHCC.2006.5},
  keywords = {UML Activity Diagrams;business process modelling;forward-engineering
	development;pattern-driven development process;quality management;visual
	pattern specification language;corporate modelling;object-oriented
	programming;quality management;specification languages;visual languages;}
}

@INPROCEEDINGS{1604764,
  author = {Forstner, B. and Lengyel, L. and Levendovszky, T. and Mezei, G. and
	Kelenyi, I. and Charaf, H.},
  title = {Model-based system development for embedded mobile platforms},
  booktitle = {Model-Based Development of Computer-Based Systems and Model-Based
	Methodologies for Pervasive and Embedded Software, 2006. MBD/MOMPES
	2006. Fourth and Third International Workshop on},
  year = {2006},
  pages = {10 pp. -52},
  month = {march},
  abstract = {With the introduction and popularity of wireless devices, the diversity
	of the platforms has also been increased. There are different platforms
	and tools from different vendors such as Microsoft, Sun, Nokia, SonyEricsson
	and many more. Because of the relatively low-level programming interface,
	software development for Symbian platform is a tiresome and error
	prone task, whereas .NET CF contains higher level structures. This
	paper introduces the problem of the software development for incompatible
	mobile platforms, moreover, it provides a model-driven architecture
	(MDA) and Domain Specific Modeling Language (DSML)-based solution.
	We also discuss the relevance of the model-based approach that facilitates
	a more efficient software development, because the reuse and the
	generative techniques are key characteristics of model-based computing.
	In the presented approach, the platform-independence lies in the
	graph rewriting-driven visual model transformation. This paper illustrates
	the creation of model compilers on a metamodeling basis by a software
	package called Visual Modeling and Transformation System (VMTS),
	which is an n-layer multipurpose modeling and metamodel-based transformation
	system. A case study is also presented how model compilers can be
	used to generate user interface handler code for different mobile
	platforms from the same platform-independent input models},
  doi = {10.1109/MBD-MOMPES.2006.20},
  keywords = {Domain Specific Modeling Language;Visual Modeling and Transformation
	System software package;embedded mobile platforms;graph rewriting-driven
	visual model transformation;metamodeling;model compilers;model-based
	system development;model-driven architecture;software reuse;embedded
	systems;mobile computing;program compilers;rewriting systems;software
	architecture;software packages;software reusability;specification
	languages;}
}

@INPROCEEDINGS{5346155,
  author = {Fortino, G. and Guerrieri, A. and Bellifemine, F. and Giannantonio,
	R.},
  title = {Platform-independent development of collaborative wireless body sensor
	network applications: SPINE2},
  booktitle = {Systems, Man and Cybernetics, 2009. SMC 2009. IEEE International
	Conference on},
  year = {2009},
  pages = {3144 -3150},
  month = {oct.},
  abstract = {Rapid development of wireless body sensor network (WBSN) applications
	can be enabled by suitable domain-specific frameworks which are usually
	organized in two parts: a base-station-side (or coordinator) and
	a sensor-node-side. While the former can be based on the Java language
	so being highly portable, the latter is usually highly dependent
	on the exploited sensor platform. Available state of the art frameworks
	follow such an organization and, in particular, the current version
	of SPINE is based on TinyOS and can be only used to effectively develop
	collaborative WBSN applications for TinyOS-based sensor platforms.
	To develop SPINE-based applications for new sensor platforms, the
	SPINE framework should be re-implemented for each new sensor platform
	to be exploited. This not only increases development efforts but
	also enforces SPINE-oriented developers to become skilled on the
	low-level programming abstractions provided by a new employed sensor
	platform. In this paper we discuss issues related to platform-independent
	development of collaborative WBSN applications and, specifically,
	describe the requirements, architecture and first implementation
	experiences of SPINE2 which aims at reaching a very high platform
	independency and raising the level of the used programming abstractions
	by providing a task-oriented programming model. The paper also discusses
	how such a task-oriented model enables dynamic task assignment and
	holistic collaborative task execution also for resource-constrained
	environments such as tiny sensor nodes.},
  doi = {10.1109/ICSMC.2009.5346155},
  issn = {1062-922X},
  keywords = {Java language;SPINE2;TinyOS;base-station-side;dynamic task assignment;holistic
	collaborative task execution;low-level programming abstractions;platform-independent
	development;resource-constrained environments;sensor-node-side;signal
	processing in node environment;task-oriented programming model;tiny
	sensor nodes;wireless body sensor network;Java;computerised instrumentation;operating
	systems (computers);wireless sensor networks;}
}

@ARTICLE{5076452,
  author = {Fowler, M.},
  title = {A Pedagogical Framework for Domain-Specific Languages},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {13 -14},
  number = {4},
  month = {july-aug. },
  abstract = {A framework for thinking about domain-specific languages (DSLs) divides
	them into internal DSLs, external DSLs, and language workbenches.
	In all cases, it's important to have an explicit semantic model so
	that they form a veneer over an underlying library. DSLs are valuable
	for increasing programmer productivity and improving communication
	with domain experts.},
  doi = {10.1109/MS.2009.85},
  issn = {0740-7459},
  keywords = {DSL;Unix;domain-specific languages;pedagogical framework;programmer
	productivity;semantic model;Unix;specification languages;}
}

@INPROCEEDINGS{667291,
  author = {Franke, H. and Sztipanovits, J. and Karsai, G.},
  title = {Model-integrated programming},
  booktitle = {System Sciences, 1997, Proceedings of the Thirtieth Hawaii International
	Conference on},
  year = {1997},
  volume = {1},
  pages = {415 -422 vol.1},
  month = {jan},
  abstract = {The building of complex, large-scale industrial applications necessitates
	the integration of models into the problem-solving process. This
	paper describes a model-integrated program synthesis (MIPS) environment
	for computer-based system applications. In MIPS, domain-specific,
	multiple-view models represent the software, its environment and
	relationships. Generic model interpreters translate the models into
	the input languages of static and dynamic analysis tools, and application-specific
	model interpreters synthesize software applications. The components
	of the system are built in the framework of the layered Multigraph
	architecture, which separates the generic and domain/application-specific
	components, and which defines interfaces for expandability},
  doi = {10.1109/HICSS.1997.667291},
  issn = {1060-3425},
  keywords = {application-specific components;computer-based system applications;domain-specific
	components;domain-specific multiple-view models;dynamic analysis
	tools;expandability;generic components;input languages;interfaces;large-scale
	industrial applications;layered Multigraph architecture;model interpreters;model-integrated
	program synthesis;problem-solving process;software application synthesis;software
	environment;static analysis tools;application program interfaces;large-scale
	systems;modelling;problem solving;program diagnostics;program interpreters;programming;}
}

@INPROCEEDINGS{4577867,
  author = {Freudenstein, P. and Nussbaumer, M.},
  title = {Constructing Advanced Web-Based Dialog Components with Stakeholders
	CHARP150; A DSL Approach},
  booktitle = {Web Engineering, 2008. ICWE '08. Eighth International Conference
	on},
  year = {2008},
  pages = {38 -44},
  month = {july},
  abstract = {Complex dialogs with comprehensive underlying data models are gaining
	increasing importance in todaypsilas Web applications. This in turn
	accelerates the need for highly dynamic dialogs offering guidance
	to the users and reducing cognitive overload. Beyond that, requirements
	from the fields of Web accessibility, platform-independence and Web
	service integration arise. Considering the resulting complexity,
	a systematic engineering approach becomes important. Besides addressing
	the specific characteristics of these dialogs, key success factors
	from a communication perspective like strong user involvement and
	clear business objectives must be taken into account. To this end,
	we present an evolutionary, extensible approach for the model-driven
	construction of advanced dialogs which is based on a Domain-specific
	Language (DSL). We introduce a modeling notation based on Petri net
	constructs and XForms as well as a supporting Web-based editor, both
	focusing on simplicity and fostering communications. The technical
	framework allows for quick prototyping and flexible changes. In conclusion,
	complex, device-independent dialogs with rich behavior and appearance
	can be constructed and evolved with intense stakeholder collaboration.},
  doi = {10.1109/ICWE.2008.39},
  keywords = {Petri net;Web accessibility;Web service integration;Web-based dialog
	component;Web-based editor;XForms;domain-specific language;Internet;Petri
	nets;specification languages;}
}

@ARTICLE{5420799,
  author = {Freudenthal, M.},
  title = {Domain-Specific Languages in a Customs Information System},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {65 -71},
  number = {2},
  month = {march-april },
  abstract = {Cybernetica AS's Customs Engine comprises several subsystems built
	on a componentized platform via domain-specific languages, enabling
	a highly iterative and reuse-oriented development method.},
  doi = {10.1109/MS.2010.41},
  issn = {0740-7459},
  keywords = {Cybernetica AS customs engine;componentized platform;customs information
	system;domain-specific languages;reuse-oriented development;software
	reusability;specification languages;}
}

@ARTICLE{5232805,
  author = {Freudenthal, M},
  title = {Domain Specific Languages in a Customs Information System},
  journal = {Software, IEEE},
  year = {2009},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {This paper presents a case study in applying the domain-specific languages
	(DSL) approach for building a configurable customs information system,
	namely the Customs Engine. The Customs Engine consists of several
	subsystems built on top of a componentized platform. Each component
	of the platform is divided into two layers: a formal specification
	of the component written in a DSL specific to that component, and
	an implementation of the DSL in question. The use of DSLs allowed
	us to follow a highly iterative and reuse-oriented development method.
	Our analysts benefited from the ability to specify the behaviour
	of the system directly, instead of relaying this information to programmers
	using lengthy human-language documentation. However, we encountered
	problems when trying to enable non-technical domain experts to write
	or modify DSL programs by themselves.},
  doi = {10.1109/MS.2009.152},
  issn = {0740-7459}
}

@INPROCEEDINGS{4273313,
  author = {Fritsch, S. and Senart, A. and Clarke, S.},
  title = {Addressing Dynamic Contextual Adaptation with a Domain-Specific Language},
  booktitle = {Software Engineering for Pervasive Computing Applications, Systems,
	and Environments, 2007. SEPCASE '07. First International Workshop
	on},
  year = {2007},
  pages = {2},
  month = {may},
  abstract = {The increasing number of mobile devices and sensors equipped with
	wireless networking capabilities enable a new generation of pro-active
	applications. These applications make use of context to adapt their
	behaviour to better fit their current situation. To support unanticipated
	changes to application behaviour, mechanisms are needed to specify
	when and how to adapt an application during its runtime. Many dynamic
	platforms exist that achieve this to some extent, and that are built
	on general-purpose languages (GPLs). However, these approaches suffer
	from standard difficulties of GPLs relating to the lack of semantic
	expressiveness of their constructs. In this paper, we describe high-level
	declarative constructs that can be used to specify the adaptation
	of application behaviour to specific situations. The language is
	supported by a framework that enables the exchange and merge of behaviours
	on-the-fly. Our approach is evaluated against application scenarios
	in the domain of autonomous vehicles.},
  doi = {10.1109/SEPCASE.2007.1},
  keywords = {autonomous vehicles;domain-specific language;dynamic contextual adaptation;general-purpose
	languages;mobile devices;mobile sensors;pro-active applications;wireless
	networking capabilities;programming language semantics;remotely operated
	vehicles;}
}

@INPROCEEDINGS{555251,
  author = {Fuchs, M.},
  title = {Let's talk: extending the Web to support collaboration},
  booktitle = {Enabling Technologies: Infrastructure for Collaborative Enterprises,
	1996. Proceedings of the 5th Workshop on},
  year = {1996},
  pages = {316 -321},
  month = {jun},
  abstract = {The current Web architecture is insufficient to support collaboration
	among independent human and computational agents. Communication,
	other than between browser and server such as among servers, or between
	a browser and local applications, requires exiting the Web's suite
	of technologies. HTML and Java are not particularly good languages
	for communicating among agents. We propose extending the Web to allow
	a variety of domain-specific little languages. Messages would be
	in these languages, and cognizant agents can manipulate them freely,
	such as display them to humans or otherwise process them. Semantics
	for these languages can be retrieved dynamically over the Web, providing
	scalable intelligence. SGML and IDL are two current systems capable
	of providing such languages. An SGML based Web would remain upwardly
	compatible with the current one},
  doi = {10.1109/ENABL.1996.555251},
  keywords = {IDL;SGML;Web extension;World Wide Web;browser;cognizant agents;collaboration
	support;communication;domain-specific little languages;independent
	computational agents;independent human agents;language semantics;local
	applications;messages;scalable intelligence;servers;Internet;cooperative
	systems;groupware;page description languages;software agents;}
}

@INPROCEEDINGS{4313226,
  author = {Fukuda, T. and Izumi, M. and Miura, T.},
  title = {Domain Dependent Word Segmentation Based on Conditional Random Fields},
  booktitle = {Communications, Computers and Signal Processing, 2007. PacRim 2007.
	IEEE Pacific Rim Conference on},
  year = {2007},
  pages = {268 -271},
  month = {aug.},
  abstract = {In this investigation, we propose an experimental approach for word
	segmentation in Japanese under domain-dependent situation. We apply
	Conditional Random Fields (CRF) to our issue. CRF learns several
	probabilistic parameters from training data with specific feature
	functions dependent on domains. Here we propose how to define domain
	specific feature functions.},
  doi = {10.1109/PACRIM.2007.4313226},
  keywords = {Japanese;conditional random fields;domain dependent word segmentation;probabilistic
	parameters;training data;character recognition;probability;}
}

@INPROCEEDINGS{4410418,
  author = {Fukuda, T. and Izumi, M. and Miura, T.},
  title = {Word Segmentation Using Domain Knowledge Based on Conditional Random
	Fields},
  booktitle = {Tools with Artificial Intelligence, 2007. ICTAI 2007. 19th IEEE International
	Conference on},
  year = {2007},
  volume = {2},
  pages = {436 -439},
  month = {oct.},
  abstract = {In this investigation, we propose an experimental approach for word
	segmentation in Japanese under domain-dependent situation. We apply
	Conditional Random Fields (CRF) to our issue. CRF learns several
	probabilistic parameters from training data with specific feature
	functions dependent on domains. Here we propose how to define domain
	specific feature functions.},
  doi = {10.1109/ICTAI.2007.93},
  issn = {1082-3409},
  keywords = {Japanese word segmentation;conditional random field;domain knowledge;domain
	specific feature function;probabilistic parameter;text processing;training
	data;learning (artificial intelligence);natural language processing;probability;random
	processes;text analysis;}
}

@INPROCEEDINGS{5954454,
  author = {Funke, H.},
  title = {Model Based Test Specifications: Developing of Test Specifications
	in a Semi Automatic Model Based Way},
  booktitle = {Software Testing, Verification and Validation Workshops (ICSTW),
	2011 IEEE Fourth International Conference on},
  year = {2011},
  pages = {496 -500},
  month = {march},
  abstract = {Developing and implementing conformity tests is a time-consuming and
	fault-prone task. To reduce these efforts a new route must be tackled.
	The current way of specifying tests and implementing them includes
	too many manual parts. Based on the experience of testing electronic
	smart cards in ID documents like passports or ID cards the author
	describes a new way of saving time to write new test specifications
	and to get test cases based on these specifications. With new technologies
	like model based testing (MBT) and domain specific languages (DSL)
	it is possible to improve the specification and implementation of
	tests significantly. The author describes his experience in using
	a DSL to define a new language for testing smart cards and to use
	this language to generate both documents and test cases that can
	be run in several test tools.},
  doi = {10.1109/ICSTW.2011.15},
  keywords = {ID cards;ID documents;domain specific languages;electronic smart cards
	testing;model based test specifications;passports;semiautomatic model
	based way;computer testing;software fault tolerance;}
}

@ARTICLE{4435106,
  author = {Furtado, A.W.B. and Santos, A.L.M. and Ramalho, G.L.},
  title = {Computer games software factory and edutainment platform for microsoft
	.NET},
  journal = {Software, IET},
  year = {2007},
  volume = {1},
  pages = {280 -293},
  number = {6},
  month = {december },
  abstract = {Abstract: An environment targeted at computer games development industrialisation
	in the .NET platform is presented. A computer game product line definition
	and its architecture are specified and implemented by means of software
	factory assets, such as a visual designer based on a domain- specific
	language, semantic validators and code generators. The proposed approach
	is then illustrated and empirically validated by the creation of
	real world case studies. Finally, it is investigated how the proposed
	factory can be used as an edutainment platform for Computer Science
	1 and 2 courses. The final intention is to empower game developers
	and designers to work more productively, with a higher level of abstraction
	and closer to their application domain.},
  doi = {10.1049/iet-sen:20070023},
  issn = {1751-8806},
  keywords = {Microsoft .NET;code generators;computer games development;computer
	games software factory;computer science courses;domain- specific
	language;edutainment platform;semantic validators;visual designer;computer
	games;program compilers;software engineering;}
}

@ARTICLE{5984795,
  author = {Furtado, A.W.B. and Santos, A.L.M. and Ramalho, G.L. and de Almeida,
	E.S.},
  title = {Improving Digital Game Development with Software Product Lines},
  journal = {Software, IEEE},
  year = {2011},
  volume = {28},
  pages = {30 -37},
  number = {5},
  month = {sept.-oct. },
  abstract = {Introducing reuse and software product line (SPL) concepts into digital
	game-development processes isn't a straightforward task. This work
	presents a systematic process for bridging SPLs to game development,
	culminating with domain-specific languages and generators streamlined
	for game subdomains. The authors present a game SPL for arcade games
	as a case study to illustrate and evaluate their proposed guidelines.
	This article is part of a special issue on games.},
  doi = {10.1109/MS.2011.101},
  issn = {0740-7459},
  keywords = {arcade games;digital game development;domain-specific languages;software
	product lines;software reusability;computer games;software development
	management;software reusability;specification languages;}
}

@INPROCEEDINGS{5332110,
  author = {Futrelle, R.P. and Satterley, J. and McCormack, T.},
  title = {NLP-NG CHARPx2014; A new NLP system for biomedical text analysis},
  booktitle = {Bioinformatics and Biomedicine Workshop, 2009. BIBMW 2009. IEEE International
	Conference on},
  year = {2009},
  pages = {296 -301},
  month = {nov.},
  abstract = {NLP-NG is a new NLP system consisting of three components: NG-CORE
	(language processing), NG-DB (database management), and NG-SEE (interactive
	visualization and entry). The ultimate goal of NLP-NG is to produce
	information retrieval systems in which users can choose full-text
	schema, adding specific items to focus their queries. Schema are
	created by a normalization process which elides adjunctive constructions
	as well as replacing items by prototypes. Biomedical text contains
	domain-specific constructions which are revealed by normalization.
	NLP-NG is based on Construction Grammar. Computationally, all representations
	are integer-based, allowing efficient storage, indexing, and retrieval.
	SEE, an Ajax web browser client, allows developers, linguists, and
	users to view a corpus and modify its properties. NLP-NG uses a 300
	million word BioMed Central corpus. NLP-NG does not focus on specific
	strategies to extract limited classes of information from papers.
	Instead, it is a universal approach that can codify a wide variety
	of text in papers.},
  doi = {10.1109/BIBMW.2009.5332110},
  keywords = {Ajax;NG-CORE;NG-DB;NG-SEE;NLP system;NLP-NG;Web browser client;biomedical
	text analysis;construction grammar;corpus;database management;information
	retrieval systems;interactive visualization and entry;linguists;natural
	language processing;normalization process;full-text databases;grammars;medical
	computing;natural language processing;text analysis;}
}

@INPROCEEDINGS{5479516,
  author = {Ganik, J.},
  title = {Providing Guidance in an Interdisciplinary Model-Based Design Process},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	Workshops (ISORCW), 2010 13th IEEE International Symposium on},
  year = {2010},
  pages = {130 -137},
  month = {may},
  abstract = {When developing real-time embedded systems, various professional disciplines
	are involved. Concerning AAS (assistance and automotive systems)
	in the automotive domain, the project DeSCAS (Design of Safety-Critical
	Automotive Systems) has identified the design streams functional
	development and architecture, safety measures and human factors.
	What has been proposed are an interwoven development process and
	related methodologies to cope with these different design streams
	and their domain specific terminology, models, methods and tools.
	A key aspect in the proposed methodology is formalizing domain knowledge
	using OWL (Web Ontology Language) [4] ontologies. Reasoning is applied
	to support analysis steps (impact analysis as well as hazard and
	risk analysis) and infer consequences of design decisions for a single
	stream or for the entire development process. This paper describes
	a toolchain prototype implementation which is used to summarize lessons
	learned from practical insights. The toolchain currently interweaves
	two development streams: functional development and architecture
	activities with management of safety measures. A simple emergency
	braking system is modeled as an example application of an assistance
	and automation system to illustrate the proposed proceeding.},
  doi = {10.1109/ISORCW.2010.29},
  keywords = {OWL;Web Ontology Language;automation system;emergency braking system;human
	factor;human factors;real-time embedded system;safety measure;safety-critical
	automotive system;automotive engineering;embedded systems;human factors;inference
	mechanisms;knowledge representation languages;ontologies (artificial
	intelligence);safety;}
}

@INPROCEEDINGS{5679044,
  author = {Gallant, S. and Gaughan, C.},
  title = {Systems engineering for distributed live, virtual, and constructive
	(LVC) simulation},
  booktitle = {Winter Simulation Conference (WSC), Proceedings of the 2010},
  year = {2010},
  pages = {1501 -1511},
  month = {dec.},
  abstract = {Designing a distributed simulation environment across multiple domains
	that typically have disparate middleware transport protocols, data
	exchange formats and applications increases the difficulty of capturing
	and linking system design decisions to the resultant implementation.
	Systems engineering efforts for distributed simulation environments
	are typically based on the middleware transport used, the applications
	available and the constraints placed on the technical team including
	network, computer and personnel limitations. To facilitate community
	re-use, systems engineering should focus on integrated operational
	function decomposition. This links data elements produced within
	the simulation to the functional capabilities required by the user.
	The system design should be captured at a functional level and subsequently
	linked to the technical design. Doing this within a data-driven systems
	engineering infrastructure allows generative programming techniques
	to assist accurate, flexible and rapid architecture development.
	This paper describes the MATREX program systems engineering process,
	infrastructure and path forward.},
  doi = {10.1109/WSC.2010.5679044},
  issn = {0891-7736},
  keywords = {LVC simulation;MATREX program;data driven system engineering;data
	exchange format;distributed simulation environment;generative programming
	technique;live virtual constructive simulation;middleware transport
	protocol;system design;digital simulation;electronic data interchange;middleware;military
	computing;systems engineering;transport protocols;}
}

@INPROCEEDINGS{1639574,
  author = {Gao, G.R. and Sterling, T. and Stevens, R. and Hereld, M. and Weirong
	Zhu},
  title = {Hierarchical multithreading: programming model and system software},
  booktitle = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006.
	20th International},
  year = {2006},
  pages = {8 pp.},
  month = {april},
  abstract = {This paper addresses the underlying sources of performance degradation
	(e.g. latency, overhead, and starvation) and the difficulties of
	programmer productivity (e.g. explicit locality management and scheduling,
	performance tuning, fragmented memory, and synchronous global barriers)
	to dramatically enhance the broad effectiveness of parallel processing
	for high end computing. We are developing a hierarchical threaded
	virtual machine (HTVM) that defines a dynamic, multithreaded execution
	model and programming model, providing an architecture abstraction
	for HEC system software and tools development. We are working on
	a prototype language, LITL-X (pronounced "little-X") for latency
	intrinsic-tolerant language, which provides the application programmers
	with a powerful set of semantic constructs to organize parallel computations
	in a way that hides/manages latency and limits the effects of overhead.
	This is quite different from locality management, although the intent
	of both strategies is to minimize the effect of latency on the efficiency
	of computation. We work on a dynamic compilation and runtime model
	to achieve efficient LITL-X program execution. Several adaptive optimizations
	were studied. A methodology of incorporating domain-specific knowledge
	in program optimization was studied. Finally, we plan to implement
	our method in an experimental testbed for a HEC architecture and
	perform a qualitative and quantitative evaluation on selected applications},
  doi = {10.1109/IPDPS.2006.1639574},
  keywords = {LITL-X;architecture abstraction;domain-specific knowledge;dynamic
	compilation;explicit locality management;fragmented memory;hierarchical
	multithreading;hierarchical threaded virtual machine;high end computing;latency
	intrinsic-tolerant language;overhead effect;parallel processing;performance
	degradation;performance tuning;program adaptive optimization;programmer
	productivity;programming model;prototype language;runtime model;synchronous
	global barrier;system software;multi-threading;optimisation;parallel
	processing;software architecture;systems software;virtual machines;}
}

@INPROCEEDINGS{6009573,
  author = {Garredu, S. and Vittori, E. and Santucci, J. and Urbani, D.},
  title = {A methodology to specify DEVS domain specific profiles and create
	profile-based models},
  booktitle = {Information Reuse and Integration (IRI), 2011 IEEE International
	Conference on},
  year = {2011},
  pages = {353 -359},
  month = {aug.},
  abstract = {Discrete EVent System Specification (DEVS) is a popular formalism
	which allows specifying and simulating models. Its main drawback
	is that its implementation is simulator-specific, i.e. models have
	to be programmed using an Object-Oriented Language (OOL). In this
	paper, we introduce DEVS profiles, which are specializations/restrictions
	of DEVS meta-model, and we explain how to create a DEVS Profile for
	non-computer scientists. We distinguish two kinds of users: the metamodelers
	and the modelers. Models designed with DEVS profiles can be mapped
	onto platform specific models and to object code using a Model Driven
	Architecture (MDA) approach. DEVS profiles improve the reusability
	of models.},
  doi = {10.1109/IRI.2011.6009573},
  keywords = {DEVS domain specific profiles;DEVS meta-model;OOL;discrete event system
	specification;model driven architecture;object-oriented language;profile-based
	models;discrete event simulation;object-oriented languages;software
	engineering;}
}

@INPROCEEDINGS{884355,
  author = {Garrett, J.T. and Ledeczi, A. and DeCaria, F.},
  title = {Towards a paradigm for activity modeling},
  booktitle = {Systems, Man, and Cybernetics, 2000 IEEE International Conference
	on},
  year = {2000},
  volume = {4},
  pages = {2425 -2430 vol.4},
  abstract = {Model Integrated Computing (J. Sztipanovits and G. Karsai, 1997) involves
	defining a domain-specific language that allows for someone to effectively
	program an environment at whatever level the modeling environment
	creator deems appropriate. We have taken several complex, heterogeneous
	systems that posed problems of needing integration and requiring
	frequent reconfiguration at very high levels. The paper discusses:
	gathering the specifications for these systems, how the systems can
	be represented at these high levels in a paradigm also capturing
	the specifications, and then how reconfiguring this group can proceed.
	All of the activities constituting monitoring functionality and the
	resulting decision making exposed by the information system are shown
	to have been solved through utilization of Model Integrated Computing
	techniques},
  doi = {10.1109/ICSMC.2000.884355},
  issn = {1062-922X},
  keywords = {Model Integrated Computing;activity modeling paradigm;decision making;domain-specific
	language;frequent reconfiguration;heterogeneous systems;information
	system;modeling environment creator;monitoring functionality;system
	specification;configuration management;formal specification;specification
	languages;virtual machines;}
}

@INPROCEEDINGS{5976550,
  author = {Gartner, J. and Musliu, N. and Schafhauser, W. and Slany, W.},
  title = {TEMPLE - A domain specific language for modeling and solving staff
	scheduling problems},
  booktitle = {Computational Intelligence in Scheduling (SCIS), 2011 IEEE Symposium
	on},
  year = {2011},
  pages = {58 -64},
  month = {april},
  abstract = {We present TEMPLE, a domain specific language for modeling and solving
	staff scheduling problems. TEMPLE provides a set of intuitive abstractions
	and notations allowing to formulate the constraints of a particular
	problem in a very compact and natural way. After modeling a staff
	scheduling problem in TEMPLE, three generic local search algorithms
	can immediately be applied to the corresponding optimization problem.
	We show how real-life staff scheduling problems can be both effectively
	modeled as well as efficiently solved using our approach. Finally,
	we report on a practical application of TEMPLE in a commercial staff
	scheduling software.},
  doi = {10.1109/SCIS.2011.5976550},
  keywords = {TEMPLE language;domain specific language;local search algorithm;optimization
	problem;staff scheduling problem modeling;staff scheduling problem
	solving;staff scheduling software;optimisation;personnel;search problems;simulation
	languages;specification languages;}
}

@INPROCEEDINGS{4656410,
  author = {de Geest, G. and Vermolen, S. and van Deursen, A. and Visser, E.},
  title = {Generating Version Convertors for Domain-Specific Languages},
  booktitle = {Reverse Engineering, 2008. WCRE '08. 15th Working Conference on},
  year = {2008},
  pages = {197 -201},
  month = {oct.},
  abstract = {Domain-specific languages (DSLs) improve programmer productivity by
	providing high-level abstractions for the development of applications
	in a particular domain. However,the smaller distance to the application
	domain entails more frequent changes to the language. As a result,
	existing DSL models need to be converted to the new version. Manual
	conversion is tedious and error prone.This paper presents an approach
	to support DSL evolution by generation of convertors between DSLs.
	By analyzing the differences between DSL meta-models, a mapping is
	reverse engineered which can be used to generate reengineering tools
	to automatically convert models between different versions of a DSL.
	The approach has been implemented for the Microsoft DSL Tools infrastructure
	in two tools called DSLCompare and ConverterGenerator. The approach
	has been evaluated by means of three case studies taken from the
	software development practice at the company Avanade.},
  doi = {10.1109/WCRE.2008.50},
  issn = {1095-1350},
  keywords = {ConverterGenerator;DSLCompare;application domain;domain-specific language
	meta-models;high-level abstractions;programmer productivity;reengineering
	tools;software development;version convertors;formal specification;programming
	languages;specification languages;}
}

@ARTICLE{4907692,
  author = {Ghosh, D. and Vinoski, S.},
  title = {Scala and Lift Functional Recipes for the Web},
  journal = {Internet Computing, IEEE},
  year = {2009},
  volume = {13},
  pages = {88 -92},
  number = {3},
  month = {may-june },
  abstract = {Today, there's significant interest in functional languages and frameworks
	that fit the Web better than imperative languages. We explore Scala,
	an OO-functional language on the Java virtual machine, and Lift,
	a framework implemented on Scala's functional features. The Scala
	language offers functional programming features and asynchronous
	message-passing concurrency alongside a statically typed model. Lift
	exploits this model to offer secure, higher-level abstractions to
	Web developers.},
  doi = {10.1109/MIC.2009.68},
  issn = {1089-7801},
  keywords = {Java virtual machine;Lift;Scala;World Wide Web;asynchronous message-passing
	concurrency;higher-level abstractions;object-oriented-functional
	language;Internet;Java;concurrency control;functional languages;functional
	programming;message passing;object-oriented languages;object-oriented
	programming;virtual machines;}
}

@INPROCEEDINGS{5089302,
  author = {Giachetti, G. and Marin, B. and Pastor, O.},
  title = {Using UML profiles to interchange DSML and UML models},
  booktitle = {Research Challenges in Information Science, 2009. RCIS 2009. Third
	International Conference on},
  year = {2009},
  pages = {385 -394},
  month = {april},
  abstract = {A key requirement for MDD solutions is to have a modeling language
	that allows the correct representation of conceptual models. Nowadays,
	there are two options that are the most widely used for the definition
	of these modeling languages: 1) the specification of a domain-specific
	modeling language (DSML) or 2) the customization of UML. In practice,
	these two modeling alternatives are viewed as opposite solutions.
	However, since both alternatives provide benefits for the application
	of MDD solutions, in this paper, we present a proposal that uses
	UML profile extension mechanisms to interchange modeling information
	between DSML-based models and UML models. This proposal shows how
	these two modeling alternatives can be integrated in a unique MDD
	solution.},
  doi = {10.1109/RCIS.2009.5089302},
  keywords = {DSML models;MDD solutions;UML profile extension mechanisms;domain-specific
	modeling language;formal specification;specification languages;}
}

@INPROCEEDINGS{5359586,
  author = {Gibbs, C. and Coady, Y.},
  title = {Joining Forces: A RIPPL Effect? A Constraint-Oriented Perspective
	on a Pervasive Pattern Language},
  booktitle = {Future Computing, Service Computation, Cognitive, Adaptive, Content,
	Patterns, 2009. COMPUTATIONWORLD '09. Computation World:},
  year = {2009},
  pages = {214 -219},
  month = {nov.},
  abstract = {Creating a unified catalogue of patterns is challenging in any domain.
	Difficultly lies in representing relationships between patterns,
	compounded by natural growth as new patterns are discovered. Existing
	pattern languages successfully describe relationships in small collections
	of patterns, but this approach lacks a systematic process that will
	scale to a growing catalogue of patterns. RIPPL (relationship initiated
	pervasive pattern language) structures patterns and tensions in their
	tradeoffs and facilitates comparison and composition in terms of
	domain specific constraints. A case study applying the proposed methodology
	to two existing pervasive pattern languages reveals the ability to
	represent pattern relationships in a structured, systematic form
	that can scale across individual pattern languages.},
  doi = {10.1109/ComputationWorld.2009.94},
  keywords = {RIPPL effect;domain specific constraints;parallel programing;pervasive
	pattern language;relationship initiated pervasive pattern language;object-oriented
	languages;parallel programming;ubiquitous computing;}
}

@INPROCEEDINGS{1385803,
  author = { Gibbs, C. and Coady, Y.},
  title = {Aspects of Memory Management},
  booktitle = {System Sciences, 2005. HICSS '05. Proceedings of the 38th Annual
	Hawaii International Conference on},
  year = {2005},
  pages = { 275b},
  month = {jan.},
  abstract = { With the constant demand for system change and upgrades comes the
	need to simplify and ensure accuracy in this process. As structural
	boundaries decay, non-local modifications compound the costs of system
	evolution and adaptation. Aspect-Oriented Programming (AOP) aims
	to improve structural boundaries for concerns that are inherently
	crosscutting - no single hierarchical decomposition can localize
	both the crosscutting concern and the concerns it crosscuts. This
	paper provides a case study of three crosscutting concerns within
	a rapidly evolving memory management subsystem of a JVM. The study
	shows how aspects can be structured as a natural locus of control,
	and how this new modularity provides leverage for system evolution
	and adaptation. Demonstrated benefits include enhanced extensibility
	for a dynamic analysis tool, centralized configurability for a subsystem-wide
	synchronization mechanism, and increased verifiability for a domain-specific
	design pattern.},
  doi = {10.1109/HICSS.2005.102},
  issn = {1530-1605 }
}

@INPROCEEDINGS{5479575,
  author = {Gilles, O. and Hugues, J.},
  title = {A MDE-Based Optimisation Process for Real-Time Systems},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	(ISORC), 2010 13th IEEE International Symposium on},
  year = {2010},
  pages = {50 -57},
  month = {may},
  abstract = {The design and implementation of Real-Time Embedded Systems is now
	heavily relying on Model-Driven Engineering (MDE) as a central place
	to define and then analyze or implement a system. MDE toolchains
	are taking a key role as to gather most of functional and not functional
	properties in a central framework, and then exploit this information.
	Such toolchain is based on both 1) a modeling notation, and 2) companion
	tools to transform or analyse models. In this paper, we present a
	MDE-based process for system optimisation based on an architectural
	description. We first define a generic evaluation pipeline, define
	a library of elementary transformations and then shows how to use
	it through Domain-Specific Language to evaluate and then transform
	models. We illustrate this process on an AADL case study modeling
	a Generic Avionics Platform.},
  doi = {10.1109/ISORC.2010.38},
  issn = {1555-0885},
  keywords = {AADL case study;MDE-based optimisation process;companion tools;domain
	specific language;generic avionics platform;generic evaluation pipeline;model
	driven engineering;modeling notation;real-time systems;toolchain;real-time
	systems;software architecture;}
}

@INPROCEEDINGS{656141,
  author = {Glass, R.L. and Vessey, I.},
  title = {Focusing on the application domain: everyone agrees it's vital, but
	who's doing anything about it?},
  booktitle = {System Sciences, 1998., Proceedings of the Thirty-First Hawaii International
	Conference on},
  year = {1998},
  volume = {3},
  pages = {187 -196 vol.3},
  abstract = {A great deal of progress has been made in the computing field by studying
	solution approaches that are independent of the problem to be solved.
	But there is now general agreement that further such progress is
	becoming increasingly difficult; it is now vital to begin considering
	solution approaches that address the particular problem at hand.
	Such approaches are called ldquo;application-domain specific rdquo;.
	Many communities within the field are addressing this problem. But
	how much progress are they making in coming up with domain-focused
	approaches? The authors address the status of each of several of
	those communities-domain analysis, patterns, architecture, method
	engineering, languages, and more. The paper shows that, while some
	progress is being made, much more is needed before the field can
	say it has made worthwhile inroads into the domain-focused approach},
  doi = {10.1109/HICSS.1998.656141},
  keywords = {application-domain specific approaches;architecture;domain analysis;languages;method
	engineering;patterns;software engineering;}
}

@INPROCEEDINGS{5587801,
  author = {Hui-Ngo Goh and Ching-Chieh Kiu},
  title = {Context-based term identification and extraction for ontology construction},
  booktitle = {Natural Language Processing and Knowledge Engineering (NLP-KE), 2010
	International Conference on},
  year = {2010},
  pages = {1 -7},
  month = {aug.},
  abstract = {Ontology construction often requires a domain specific corpus in conceptualizing
	the domain knowledge; specifically, it is an association of terms,
	relation between terms and related instances. It is a vital task
	to identify a list of significant term for constructing a practical
	ontology. In this paper, we present the use of a context-based term
	identification and extraction methodology for ontology construction
	from text document. The methodology is using a taxonomy and Wikipedia
	to support automatic term identification and extraction from structured
	documents with an assumption of candidate terms for a topic are often
	associated with its topic-specific keywords. A hierarchical relationship
	of super-topics and sub-topics is defined by a taxonomy, meanwhile,
	Wikipedia is used to provide context and background knowledge for
	topics that defined in the taxonomy to guide the term identification
	and extraction. The experimental results have shown the context-based
	term identification and extraction methodology is viable in defining
	topic concepts and its sub-concepts for constructing ontology. The
	experimental results have also proven its viability to be applied
	in a small corpus / text size environment in supporting ontology
	construction.},
  doi = {10.1109/NLPKE.2010.5587801},
  keywords = {Wikipedia;automatic term identification;context-based term identification;extraction
	methodology;ontology construction;text document;Web sites;document
	handling;ontologies (artificial intelligence);text analysis;}
}

@INPROCEEDINGS{479592,
  author = {Goldberg, J.L.},
  title = {CDM: an approach to learning in text categorization},
  booktitle = {Tools with Artificial Intelligence, 1995. Proceedings., Seventh International
	Conference on},
  year = {1995},
  pages = {258 -265},
  month = {nov},
  abstract = {The category discrimination method (CDM) is a new learning algorithm
	designed for text categorization. The motivation is that there are
	statistical problems associated with natural language text when it
	is applied as input to existing machine learning algorithms (too
	much noise, too many features, skewed distribution). The bases of
	the CDM are research results about the way that humans learn categories
	and concepts vis-a-vis contrasting concepts. The essential formula
	is cue validity borrowed from cognitive psychology, and used to select
	from all possible single word-based features the `best' predictors
	of a given category. The hypothesis that CDM's performance exceeds
	two non-domain specific algorithms, Bayesian classification and decision
	tree learners, is empirically tested},
  doi = {10.1109/TAI.1995.479592},
  keywords = {Bayesian classification;algorithm performance;best category predictors;category
	discrimination method;cognitive psychology;cue validity;decision
	tree learners;human category learning;human concept learning;learning
	algorithm;machine learning algorithms;natural language text;nondomain
	specific algorithms;single word-based features;statistical problems;text
	categorization;Bayes methods;algorithm theory;category theory;decision
	theory;document handling;knowledge engineering;learning (artificial
	intelligence);natural languages;pattern classification;statistical
	analysis;trees (mathematics);}
}

@INPROCEEDINGS{795871,
  author = {Goldman, N.M. and Balzer, R.M.},
  title = {The ISI visual design editor generator},
  booktitle = {Visual Languages, 1999. Proceedings. 1999 IEEE Symposium on},
  year = {1999},
  pages = {20 -27},
  abstract = {The benefits of ldquo;domain specific rdquo; languages and development
	environments are widely recognized. Constructing an environment for
	a new domain, however, remains a costly activity, requiring expertise
	in several areas of software development as well as in the targeted
	domain. The ISI design editor generator and design environment comprises
	novel infrastructure that simplifies this task, producing visual
	domain-specific design environments. This paper presents the runtime
	architecture of these environments, a visual ldquo;specify-by-example
	rdquo; capability that deals with a major portion of editor generation,
	and an implementation that uses a COTS product (Microsoft PowerPoint)
	as both graphic middleware and end-user GUI},
  doi = {10.1109/VL.1999.795871},
  keywords = {end-user GUI;graphic middleware;runtime architecture;software development;visual
	design editor generator;visual domain-specific design environments;automatic
	programming;client-server systems;graphical user interfaces;programming
	environments;}
}

@INPROCEEDINGS{6032553,
  author = {Golra, F.R. and Dagnat, F.},
  title = {The lazy initialization multilayered modeling framework: NIER track},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {924 -927},
  month = {may},
  abstract = {Lazy Initialization Multilayer Modeling (LIMM) is an object oriented
	modeling language targeted to the declarative definition of Domain
	Specific Languages (DSLs) for Model Driven Engineering. It focuses
	on the precise definition of modeling frameworks spanning over multiple
	layers. In particular, it follows a two dimensional architecture
	instead of the linear architecture followed by many other modeling
	frameworks. The novelty of our approach is to use lazy initialization
	for the definition of mapping between different modeling abstractions,
	within and across multiple layers, hence providing the basis for
	exploiting the potential of metamodeling.},
  doi = {10.1145/1985793.1985947},
  issn = {0270-5257},
  keywords = {2D architecture;NIER track;domain specific languages;lazy initialization
	multilayered modeling framework;linear architecture;metamodeling;model
	driven engineering;object oriented modeling language;object-oriented
	languages;software architecture;specification languages;}
}

@INPROCEEDINGS{4670174,
  author = {Gomadam, K. and Ranabahu, A. and Nagarajan, M. and Sheth, A.P. and
	Verma, K.},
  title = {A Faceted Classification Based Approach to Search and Rank Web APIs},
  booktitle = {Web Services, 2008. ICWS '08. IEEE International Conference on},
  year = {2008},
  pages = {177 -184},
  month = {sept.},
  abstract = {Web application hybrids, popularly known as mashups, are created by
	integrating services on the Web using their APIs. Support for finding
	an API is currently provided by generic search engines or domain
	specific solutions such as Google and ProgrammableWeb. Shortcomings
	of both these solutions in terms of and reliance on user tags make
	the task of identifying an API challenging. Since these APIs are
	described in HTML documents, it is essential to look beyond the boundaries
	of current approaches to Web service discovery that rely on formal
	descriptions. In this work, we present a faceted approach to searching
	and ranking Web APIs that takes into consideration attributes or
	facets of the APIs as found in their HTML descriptions. Our method
	adopts current research in document classification and faceted search
	and introduces the serviut score to rank APIs based on their utilization
	and popularity. We evaluate classification, search accuracy and ranking
	effectiveness using available APIs while contrasting our solution
	with existing ones.},
  doi = {10.1109/ICWS.2008.105},
  keywords = {Google;HTML documents;ProgrammableWeb;Web API;Web service discovery;application
	program interfaces;document classification;generic search engines;Web
	services;application program interfaces;hypermedia markup languages;pattern
	classification;search engines;}
}

@INPROCEEDINGS{5431728,
  author = {Gonzalez, O. and Casallas, R. and Deridder, D.},
  title = {Automating the Implementation of Analysis Concerns in Workflow Applications},
  booktitle = {Automated Software Engineering, 2009. ASE '09. 24th IEEE/ACM International
	Conference on},
  year = {2009},
  pages = {585 -589},
  month = {nov.},
  abstract = {In workflow management systems, analysis concerns related to monitoring,
	measurement, and control aim at identifying potential improvements
	of workflow applications. However, the specification of analysis
	concerns is done using a specific workflow language and engine, producing
	entangled code which is detrimental to their maintainability. The
	purpose of this paper is twofold. First, it presents briefly a domain-specific
	language to specify analysis concerns, independently of any workflow
	technology and in a modularized way. Second, it shows a strategy
	to assist developers to enhance a given workflow technology to support
	the automated implementation of analysis concerns into its workflow
	applications. Thus, given a workflow application and its analysis
	concerns, they are automatically integrated producing an enhanced
	executable workflow application.},
  doi = {10.1109/ASE.2009.29},
  issn = {1527-1366},
  keywords = {domain-specific language;workflow language;workflow management systems;programming
	languages;workflow management software;}
}

@INPROCEEDINGS{5934785,
  author = {Gopinath, V.S. and Sprinkle, J. and Lysecky, R.},
  title = {Modeling of Data Adaptable Reconfigurable Embedded Systems},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {276 -283},
  month = {april},
  abstract = {Many applications require high flexibility, high configurability and
	high processing speeds. The physical constraints of a highly flexible
	system's hardware implementation preclude a hardware solution that
	satisfies all configuration options. Similarly for pure software
	implementations, even if configurability is satisfied, process efficiency
	will be sacrificed. Thus for applications of any significant size,
	there can be no single hardware or software configuration that can
	efficiently support all the configurability options of the applications.
	The Data-Adaptable Reconfigurable Embedded System (DARES) approach
	tackles this problem through combination of the hardware-software
	co-design and reconfigurable computing methodologies. Data-adaptability
	means that as data streams change, the system is reconfigured along
	the baselines defined within the system's specifications. In this
	project we use the concepts of Model-Integrated Computing to implement
	a domain-specific modeling language for the DARES approach. The language
	captures all the configurability options of the application task(s),
	performs design-space exploration, and provides a template for source
	code generation.},
  doi = {10.1109/ECBS.2011.31},
  keywords = {DARES approach;data adaptable reconfigurable embedded system;design-space
	exploration;domain-specific modeling language;hardware-software codesign;model-integrated
	computing;physical constraint;reconfigurable computing;source code
	generation;embedded systems;hardware-software codesign;reconfigurable
	architectures;}
}

@INPROCEEDINGS{5635209,
  author = {Gordon, P.M.K. and Barker, K. and Sensen, C.W.},
  title = {Programming-by-Example Meets the Semantic Web: Using Ontologies and
	Web Services to Close the Semantic Gap},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2010 IEEE
	Symposium on},
  year = {2010},
  pages = {133 -140},
  month = {sept.},
  abstract = {Programming-by-example (PbE) as a technique can only make substantial
	progress by addressing the open problem commonly known as the Semantic
	Gap. This gap is the discrepancy between what the user intended by
	their actions, and how the inference engine generalizes the actions
	for reuse in a program. The Seahawk/Daggoo PbE system attempts to
	close this gap in a novel way: by using Semantic Web and Web Service
	resources as the building blocks the user manipulates during the
	demonstration stage of PbE. This eliminates the need for PbE inference
	by leveraging domain-specific Web Services and community-based agreements
	about data meanings. The semantic gap in PbE is closed a priori because
	of semantic agreement between users and service providers within
	a domain. The new PbE system has been tested in the domain of Bioinformatics
	to create Web Service workflows, with justification and results reported
	here. Fundamentally, Seahawk/Daggoo is domain-agnostics because semantics
	are defined in Web-based ontologies rather than by the PbE system
	itself. Novel action-to-workflow concept mappings are also introduced.},
  doi = {10.1109/VLHCC.2010.27},
  issn = {1943-6092},
  keywords = {Seahawk/Daggoo PbE system;Web service workflow;Web-based ontology;action-to-workflow
	concept mapping;bioinformatics;community-based agreement;data meaning;domain-specific
	Web service;inference engine;programming-by-example;semantic Web;semantic
	agreement;semantic gap;Web services;automatic programming;ontologies
	(artificial intelligence);semantic Web;workflow management software;}
}

@INPROCEEDINGS{4278667,
  author = {Goudos, S.K. and Loutas, N. and Peristeras, V. and Tarabanis, K.},
  title = {Public Administration Domain Ontology for a Semantic Web Services
	EGovernment Framework},
  booktitle = {Services Computing, 2007. SCC 2007. IEEE International Conference
	on},
  year = {2007},
  pages = {270 -277},
  month = {july},
  abstract = {In this paper we present a generic public administration (PA) domain
	ontology. We define a formal model for a public administration service
	on the basis of the Web service modeling ontology (WSMO). For this
	purpose we employ the generic public service object model of the
	governance enterprise architecture (GEA) providing PA domain specific
	semantics. We describe the ontology using the Web service modeling
	language (WSML). This domain ontology is implemented in order to
	be used in semantic Web services architecture for e-government.},
  doi = {10.1109/SCC.2007.89},
  keywords = {Web service modeling language;Web service modeling ontology;e-government;governance
	enterprise architecture;public administration domain ontology;semantic
	Web services architecture;Web services;programming languages;public
	administration;software architecture;}
}

@INPROCEEDINGS{4444233,
  author = {Goudos, S.K. and Peristeras, V. and Loutas, N. and Tarabanis, K.},
  title = {A public administration domain ontology for semantic discovery of
	eGovernment services},
  booktitle = {Digital Information Management, 2007. ICDIM '07. 2nd International
	Conference on},
  year = {2007},
  volume = {1},
  pages = {260 -265},
  month = {oct.},
  abstract = {In this paper we present a top level public administration (PA) domain
	ontology. For this purpose we employ the generic public service object
	model of the governance enterprise architecture (GEA) providing PA
	domain specific semantics. We represent this service model using
	the Web ontology language (OWL). This domain ontology may serve as
	the knowledge base for eGovernment semantic Web applications. A sample
	application is presented that finds the public services that can
	be executed by a citizen based on his/hers profile using semantic
	discovery. The input to the application consists of the user profile,
	while the output returned consists of the public services that match
	the specified profile.},
  doi = {10.1109/ICDIM.2007.4444233},
  keywords = {Web ontology language;eGovernment services;governance enterprise architecture;public
	administration domain ontology;public service object model;semantic
	Web applications;semantic discovery;knowledge representation languages;ontologies
	(artificial intelligence);public administration;semantic Web;}
}

@INPROCEEDINGS{1540667,
  author = {Governatori, G. and Milosevic, Z.},
  title = {Dealing with contract violations: formalism and domain specific language},
  booktitle = {EDOC Enterprise Computing Conference, 2005 Ninth IEEE International},
  year = {2005},
  pages = { 46 - 57},
  month = {sept.},
  abstract = { This paper presents a formal system for reasoning about violations
	of obligations in contracts. The system is based on the formalism
	for the representation of contrary-to-duty obligations. These are
	the obligations that take place when other obligations are violated
	as typically applied to penalties in contracts. The paper shows how
	this formalism can be mapped onto the key policy concepts of a contract
	specification language. This language, called Business Contract Language
	(BCL) was previously developed to express contract conditions of
	relevance for run time contract monitoring. The aim of this mapping
	is to establish a formal underpinning for this key subset of BCL.},
  doi = {10.1109/EDOC.2005.13},
  keywords = { Business Contract Language; contract specification language; contract
	violations; contrary-to-duty obligations; run time contract monitoring;
	contracts; formal specification; specification languages;}
}

@INPROCEEDINGS{4148977,
  author = {Graaf, Bas and van Deursen, Arie},
  title = {Visualisation of Domain-Specific Modelling Languages Using UML},
  booktitle = {Engineering of Computer-Based Systems, 2007. ECBS '07. 14th Annual
	IEEE International Conference and Workshops on the},
  year = {2007},
  pages = {586 -595},
  month = {march},
  abstract = {Currently, general-purpose modelling tools are often only used to
	draw diagrams for the documentation. The introduction of model-driven
	software development approaches involves the definition of domain-specific
	modelling languages that allow code generation. Although graphical
	representations of the involved models are important for documentation,
	the development of required visualisations and editors is cumbersome.
	In this paper we propose to extend the typical model-driven approach
	with the automatic generation of diagrams for documentation. We illustrate
	the approach using the model driven architecture in the domains of
	software architecture and control systems},
  doi = {10.1109/ECBS.2007.77},
  keywords = {UML;data visualisation;domain-specific modelling languages;graphical
	representation;model driven architecture;model-driven software development;Unified
	Modeling Language;data visualisation;software architecture;}
}

@INPROCEEDINGS{1630743,
  author = {Graf, P. and Muller-Glaser, K.D.},
  title = {Dynamic Mapping of Runtime Information Models for Debugging Embedded
	Software},
  booktitle = {Rapid System Prototyping, 2006. Seventeenth IEEE International Workshop
	on},
  year = {2006},
  pages = {3 -9},
  month = {june},
  abstract = {Model based development based on different domain specific tools and
	graphical notations gains increasing importance in system design
	of embedded electronic systems allowing fast concept-oriented prototyping
	from model to code. This paper describes an extension to our seamless
	model based development approach: An architecture for debugging models
	that are executed on target systems or in dedicated rapid-prototyping
	environments. We discuss the advantages of such an approach as opposed
	to simulation and describe our universal architecture. We focus on
	the definition of MOF-based runtime models and their synchronisation
	with the runtime target state. An example of debugging state-charts
	shows the feasibility of the approach},
  doi = {10.1109/RSP.2006.15},
  issn = {1074-6005},
  keywords = {dynamic mapping;embedded electronic systems;embedded software debugging;fast
	concept-oriented prototyping;rapid-prototyping environments;runtime
	information models;state-charts;embedded systems;program debugging;software
	prototyping;}
}

@INPROCEEDINGS{1174890,
  author = {Granicz, A. and Hickey, J.},
  title = {Phobos: a front-end approach to extensible compilers},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { This paper describes a practical approach for implementing domain-specific
	languages with extensible compilers. Given a compiler with one or
	more front-end languages, we introduce the idea of a "generic" front-end
	that allows the syntactic and semantic specification of domain-specific
	languages. Phobos, our generic front-end, offers modular language
	specification, allowing the programmer to define new syntax and semantics
	incrementally. A key feature of our approach is the use of an open
	term language that can be used to describe arbitrary syntax, and
	the use of a term rewriting engine to encode semantic actions. The
	term language is expressive. Scoping can be defined explicitly, and
	term rewrites use second-order substitution, allowing the use of
	higher-order abstract syntax if needed. Given a language specification
	and a source string, the generic front-end constructs a push-down
	automaton (PDA) based on the supplied grammar, lexes the source string,
	and simulates the constructed PDA with the stream of tokens obtained.
	During parsing, rewrite rules associated, with grammar productions
	are executed, producing a single term when the PDA accepts. This
	term is then converted, via further rewriting into a compiler representation
	and compilation proceeds to generate executable code.},
  doi = {10.1109/HICSS.2003.1174890},
  issn = { },
  keywords = { Phobos; arbitrary syntax; domain-specific languages; executable code
	generation; extensible compilers; front-end languages; grammar; higher-order
	abstract syntax; modular language specification; open term language;
	parsing; push-down automaton; second-order substitution; semantic
	actions encoding; semantic specification; source string; syntactic
	specification; term rewriting engine; computational linguistics;
	formal specification; high level languages; program compilers; rewriting
	systems; specification languages;}
}

@INPROCEEDINGS{989515,
  author = {Graubitz, H. and Spiliopoulou, M. and Winkler, K.},
  title = {The DIAsDEM framework for converting domain-specific texts into XML
	documents with data mining techniques},
  booktitle = {Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference
	on},
  year = {2001},
  pages = {171 -178},
  abstract = {Modern organizations are accumulating huge volumes of textual documents.
	To turn archives into valuable knowledge sources, textual content
	must become explicit and able to be queried. Semantic tagging with
	markup languages such as XML satisfies both requirements. We thus
	introduce the DIAsDEM* framework for extracting semantics from structural
	text units (e.g., sentences), assigning XML tags to them and deriving
	a flat XML DTD for the archive. DIAsDEM focuses on archives characterized
	by a peculiar terminology and by an implicit structure such as court
	filings and company reports. In the knowledge discovery phase, text
	units are iteratively clustered by similarity of their content. Each
	iteration outputs clusters satisfying a set of quality criteria.
	Text units contained in these clusters are tagged with semiautomatically
	determined cluster labels and XML tags respectively. Additionally,
	extracted named entities (e.g., persons) serve as attributes of XML
	tags. We apply the framework in a case study on the German Commercial
	Register},
  doi = {10.1109/ICDM.2001.989515},
  keywords = {DIAsDEM framework;German Commercial Register;XML documents;archive;company
	reports;content similarity;court filings;data mining;domain-specific
	text conversion;flat XML DTD;iterative clustering;knowledge discovery;markup
	languages;quality criteria;semantic tagging;semiautomatically determined
	cluster labels;structural text units;terminology;data mining;data
	warehouses;hypermedia markup languages;}
}

@INPROCEEDINGS{1174892,
  author = {Gray, J. and Karsai, G.},
  title = {An examination of DSLs for concisely representing model traversals
	and transformations},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { A key advantage for the use of a domain-specific language (DSL) is
	the leverage that can be captured from a concise representation of
	a programmer's intention. This paper reports on three different DSLs
	that were developed for two different projects. Two of the DSLs assisted
	in the specification of various modeling tool ontologies, and the
	integration of models across these tools. On another project, a different
	DSL has been applied as a language to assist in aspect-oriented modeling.
	Each of these three languages was converted to C++ using different
	code generators. These DSLs were concerned with issues of traversing
	a model and performing transformations. The paper also provides quantitative
	data on the relative sizes of the intention (as expressed in the
	DSL) and the generated C++ code. Observations are made regarding
	the nature of the benefits and the manner in which the conciseness
	of the DSL is best leveraged.},
  doi = {10.1109/HICSS.2003.1174892},
  issn = { },
  keywords = { C++ code generation; aspect-oriented modeling; domain-specific language;
	executable specification language; model transformations; model traversals;
	modeling tool ontologies; programming language; C++ language; formal
	specification; object-oriented programming; program compilers; specification
	languages;}
}

@INPROCEEDINGS{5503306,
  author = {Gringinger, E. and Eier, D. and Merkl, D.},
  title = {Ontology-based CNS software development},
  booktitle = {Integrated Communications Navigation and Surveillance Conference
	(ICNS), 2010},
  year = {2010},
  pages = {C3-1 -C3-13},
  month = {may},
  abstract = {The heart of Air Traffic Control (ATC) lays in the Control Room (CR)
	in the ATC en route center, Terminal Radar Approach Control (TRACON),
	and ATC Tower (ATCT) facilities. However, CRs are also used in other
	mission critical domains such as 911, or Emergency control centers.
	In the past this led to the development of domain specific control
	rooms resulting in different solutions for each specific environment.
	This raises the cost for efficient software development and increases
	the time-to-market. A modern Ontology-Based Control Room Framework
	(ONTOCOR) could dramatically improve this Air Traffic Management
	(ATM) situation. Uniform and open standards build up ontologies described
	by the Web Ontology Language (OWL). Information Management (IM) and
	the development of uniform and open standards are key components
	of the Next Generation Air Transportation System (NextGen) and Europe's
	SESAR Program. ONTOCOR increases productive code usage and reduces
	software development. It focuses on improving efficiency and gain
	effort by code reusability, thus contributing to reduction of deployment
	cost of such solutions. This paper analyzes and compares different
	ontology languages as well as relevant semantic tools for ontology
	development and management. The present paper will also give a brief
	survey on ontology-based software engineering, before the ongoing
	research of ONTOCOR is introduced.},
  doi = {10.1109/ICNSURV.2010.5503306},
  issn = {2155-4943},
  keywords = {Europe SESAR Program;Next Generation Air Transportation System;Single
	European Sky ATM Research;Terminal Radar Approach Control;Web ontology
	language;air traffic control;air traffic management;information management;ontology-based
	CNS software development;aerospace computing;air traffic;information
	management;knowledge representation languages;software engineering;traffic
	engineering computing;}
}

@INPROCEEDINGS{365797,
  author = {Griss, M.L.},
  title = {Architecting kits for reuse},
  booktitle = {Software Reuse: Advances in Software Reusability, 1994. Proceedings.,
	Third International Conference on},
  year = {1994},
  pages = {216 -217},
  month = {nov},
  abstract = {Our work on systematic domain-specific reuse has led to the notion
	of domain-specific kits, comprised of compatible, domain-specific
	components, frameworks and glue languages, and supported by a variety
	of tools. A key element of a successful kit is a flexible, open architecture.
	We discuss guidelines, methods and technologies that lead to more
	effective kits},
  doi = {10.1109/ICSR.1994.365797},
  keywords = { domain-specific components; domain-specific kits; glue languages;
	open architecture; systematic domain-specific reuse; software reusability;
	software tools;}
}

@ARTICLE{5167269,
  author = {Grobelnik, M. and Mladenic, D. and Fortuna, B.},
  title = {Semantic Technology for Capturing Communication Inside an Organization},
  journal = {Internet Computing, IEEE},
  year = {2009},
  volume = {13},
  pages = {59 -67},
  number = {4},
  month = {july-aug. },
  abstract = {Capturing information about employees can give organizations insight
	into underlying knowledge processes. Analyzing email communications,
	for example, can produce an informal structure that's flexible and
	that organizations can recalculate regularly to capture information
	flow among employees. The structure could also help institutions
	to both identify collaboration patterns and predict changes in those
	patterns. Using semantic technologies in various domains, researchers
	have developed domain-specific ontologies to capture knowledge and
	enable reasoning. Organizations can support such knowledge management
	by capturing knowledge of their own people and their communication
	records, including email exchanges. The proposed approach analyzes
	an internal social network and uses the resulting information to
	produce an informal organizational structure. The authors evaluated
	their approach using a mid-size organization's actual data and compared
	the informal structure they obtained with the formal organizational
	structure. As the results show, the approach proved useful for modeling
	social structures based on real-world communication records.},
  doi = {10.1109/MIC.2009.88},
  issn = {1089-7801},
  keywords = {collaboration pattern identification;domain-specific ontology;email
	communication;email exchange;employees;informal organizational structure;knowledge
	management;knowledge process;reasoning;semantic technology;social
	network analysis;electronic mail;knowledge management;knowledge representation
	languages;network theory (graphs);ontologies (artificial intelligence);semantic
	Web;}
}

@INPROCEEDINGS{4747227,
  author = {Grobelny, P.},
  title = {The expert system approach in development of loosely coupled software
	with use of Domain Specific Language},
  booktitle = {Computer Science and Information Technology, 2008. IMCSIT 2008. International
	Multiconference on},
  year = {2008},
  pages = {119 -123},
  month = {oct.},
  abstract = {This paper addresses the problem of supporting the software development
	process through the artificial intelligence. The expert systems could
	advise the domain engineer in programming without the detailed experience
	in programming languages. He will use and integrate, with the help
	of deductive database and domain knowledge, the previously developed
	software components to new complex functionalities. The service oriented
	architecture (SOA) and loosely coupled software allow to fulfill
	these requirements. The objective of this document is to provide
	the knowledge representation of atomic Web services which will be
	registered as the facts in the deductive database as well as the
	inferring techniques. Also, the use of domain specific language (DSL)
	for modeling domain engineerpsilas requests to the expert system
	will be considered within this document.},
  doi = {10.1109/IMCSIT.2008.4747227},
  keywords = {artificial intelligence;atomic Web services;deductive database;domain
	knowledge;domain specific language;expert system;knowledge representation;loosely
	coupled software;programming languages;service oriented architecture;software
	components;software development process;Web services;deductive databases;expert
	systems;knowledge representation;programming languages;software architecture;}
}

@ARTICLE{5473205,
  author = {Groenewegen, D. and Hemel, Z. and Visser, E.},
  title = {Separation of Concerns and Linguistic Integration in WebDSL},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {31 -37},
  number = {5},
  month = {sept.-oct. },
  abstract = {WebDSL is a domain-specific language for Web information systems that
	maintains separation of concerns while integrating its sublanguages,
	enabling consistency checking and reusing common language concepts.},
  doi = {10.1109/MS.2010.92},
  issn = {0740-7459},
  keywords = {Web information systems;WebDSL;domain specific language;linguistic
	integration;Web sites;computational linguistics;information systems;specification
	languages;}
}

@INPROCEEDINGS{4577881,
  author = {Groenewegen, D. and Visser, E.},
  title = {Declarative Access Control for WebDSL: Combining Language Integration
	and Separation of Concerns},
  booktitle = {Web Engineering, 2008. ICWE '08. Eighth International Conference
	on},
  year = {2008},
  pages = {175 -188},
  month = {july},
  abstract = {In this paper, we present the extension of WebDSL, a domain-specific
	language for web application development, with abstractions for declarative
	definition of access control. The extension supports the definition
	of a wide range of access control policies concisely and transparently
	as a separate concern. In addition to regulating the access to pages
	and actions, access control rules are used to infer navigation options
	not accessible to the current user, preventing the presentation of
	inaccessible links. The extension is an illustration of a general
	approach to the design of domain-specific languages for different
	technical domains to support separation of concerns in application
	development, while preserving linguistic integration. This approach
	is realized by means of a transformational semantics that weaves
	separately defined aspects into an integrated implementation.},
  doi = {10.1109/ICWE.2008.15},
  keywords = {WebDSL;access control;domain-specific language;Internet;authorisation;specification
	languages;}
}

@INPROCEEDINGS{4626872,
  author = {Groher, I. and Volter, M. and Schwanninger, C.},
  title = {Integrating Models and Aspects into Product Line Engineering},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {355},
  month = {sept.},
  abstract = {This demonstration presents an approach that facilitates variability
	implementation, management, and tracing from architectural modeling
	to implementation. A tool suite is provided that integrates aspect-oriented
	and model-driven software development into product line engineering.},
  doi = {10.1109/SPLC.2008.52},
  keywords = {architectural modeling;aspect-oriented development;model-driven software
	development;product line engineering;variability implementation;object-oriented
	programming;product development;software architecture;software development
	management;software reusability;}
}

@INPROCEEDINGS{1509533,
  author = {Gross, J.B.},
  title = {pProgramming for artists: a visual language for expressive lighting
	design},
  booktitle = {Visual Languages and Human-Centric Computing, 2005 IEEE Symposium
	on},
  year = {2005},
  pages = { 331 - 332},
  month = {sept.},
  abstract = { Programming is a process of formalizing and codifying knowledge,
	and, as a result, programming languages are designed for generalists
	trained in this process of formalization. Artists, whose training
	focuses on skill and tacit knowledge, are marginalized by existing
	tools. By designing visual languages that take advantage of an artist's
	skills in visual perception and expression, we can allow that artist
	to take advantage of the expressive potential that modern computing
	offers. In particular, this paper will look at lighting design for
	interactive, virtual environments, and augmenting an existing programming
	language to allow artists to leverage their skills in the pragmatics
	of that medium.},
  doi = {10.1109/VLHCC.2005.54},
  keywords = { artists; domain specific language; expressive lighting design; interactive
	virtual environments; pragmatics; programming languages; semiotics;
	visual expression; visual language; visual perception; art; computational
	linguistics; lighting; virtual reality; visual languages;}
}

@INPROCEEDINGS{4721477,
  author = {Grund, M. and Krueger, J. and Schaffner, J. and Schapranow, M. and
	Bog, A.},
  title = {Operational Reporting Using Navigational SQL},
  booktitle = {Advanced Management of Information for Globalized Enterprises, 2008.
	AMIGE 2008. IEEE Symposium on},
  year = {2008},
  pages = {1 -5},
  month = {sept.},
  abstract = {Ad-hoc analytics on top of OLTP data provides direct business value
	in that business users can directly interact with the system to gather
	information for decision-making. From a business user perspective,
	SQL requires - for a schema that is never changed - deep schema knowledge
	in order to express navigation along join paths even for simple queries.
	In this paper we introduce navigational SQL, a language that provides
	constructs specifically tailored for data retrieval in the context
	of operational reporting, such as a navigation operator that has
	explicit knowledge about join paths in a particular schema and gives
	the user an "application level view" of the tables. We motivate NSQL
	using examples from operational reporting and outline its benefits
	over materialized views and domain specific languages, which could
	also be used to address the issues with SQL described above fully
	or in part.},
  doi = {10.1109/AMIGE.2008.ECP.35},
  keywords = {ad-hoc analytics;business users;data retrieval;decision making;navigational
	SQL;operational reporting;SQL;business data processing;data handling;decision
	making;}
}

@INPROCEEDINGS{4222590,
  author = {Grundy, J. and Hosking, J.},
  title = {Supporting Generic Sketching-Based Input of Diagrams in a Domain-Specific
	Visual Language Meta-Tool},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {282 -291},
  month = {may},
  abstract = {Software engineers often use hand-drawn diagrams as preliminary design
	artefacts and as annotations during reviews. We describe the addition
	of sketching support to a domain-specific visual language meta-tool
	enabling a wide range of diagram-based design tools to leverage this
	human-centric interaction support. Our approach allows visual design
	tools generated from high-level specifications to incorporate a range
	of sketching-based functionality including both eager and lazy recognition,
	moving from sketch to formalized content and back and using sketches
	for secondary annotation and collaborative design review. We illustrate
	the use of our sketching extension for an example domain-specific
	visual design tool and describe the architecture and implementation
	of the extension as a plug-in for our Eclipse-based meta-tool.},
  doi = {10.1109/ICSE.2007.81},
  issn = {0270-5257},
  keywords = {Eclipse-based meta-tool;collaborative design;diagram-based design
	tools;domain-specific visual language meta-tool;generic sketching-based
	input;high-level specifications;software engineering;data visualisation;software
	engineering;}
}

@INPROCEEDINGS{4814200,
  author = {Grundy, J. and Hosking, J. and Huh, J. and Na-Liu Li, K.},
  title = {Marama},
  booktitle = {Software Engineering, 2008. ICSE '08. ACM/IEEE 30th International
	Conference on},
  year = {2008},
  pages = {819 -822},
  month = {may},
  abstract = {We describe the Marama suite of meta-tools. This Eclipse-based toolset
	permits rapid specification of notational elements, meta-models,
	view editors and view-model mappings. It has a novel set of behavioural
	specification tools for both visual and model level behaviours. An
	integrated mapping tool provides model transformation and code generation
	support. The toolset has been applied to several significant application
	development tasks and has undergone a variety of evaluations.},
  doi = {10.1145/1368088.1368210},
  issn = {0270-5257},
  keywords = {Marama Eclipse meta-toolset;behavioural visual language specification
	tool;code generation;code transformation;meta-model;multiview environment
	generation;notational element specification;view editor;view-model
	mapping;formal specification;program compilers;programming environments;software
	tools;visual languages;}
}

@INPROCEEDINGS{4019559,
  author = {Grundy, J. and Hosking, J. and Nianping Zhu and Na Liu},
  title = {Generating Domain-Specific Visual Language Editors from High-level
	Tool Specifications},
  booktitle = {Automated Software Engineering, 2006. ASE '06. 21st IEEE/ACM International
	Conference on},
  year = {2006},
  pages = {25 -36},
  month = {sept.},
  abstract = {Domain-specific visual language editors are useful in many areas of
	software engineering but developing such editors is challenging and
	time-consuming. We describe an approach to generating a wide range
	of these graphical editors for use as plug-ins to the Eclipse environment.
	Tool specifications from an existing meta-tool, Pounamu, are interpreted
	to produce dynamic, multi-view, multiuser Eclipse graphical editors.
	We describe the architecture and implementation of our approach,
	examples of its use realizing domain-specific modelling tools, and
	strengths and limitations of the approach},
  doi = {10.1109/ASE.2006.39},
  issn = {1527-1366},
  keywords = {Eclipse environment;Pounamu metatool;domain-specific modelling;domain-specific
	visual language editor;graphical editors;high-level tool specifications;software
	architecture;software engineering;formal specification;graphical
	user interfaces;programming environments;software architecture;software
	tools;visual programming;}
}

@INPROCEEDINGS{989788,
  author = {Grundy, J. and Mugridge, R. and Hosking, J. and Kendall, P.},
  title = {Generating EDI message translations from visual specifications},
  booktitle = {Automated Software Engineering, 2001. (ASE 2001). Proceedings. 16th
	Annual International Conference on},
  year = {2001},
  pages = { 35 - 42},
  month = {nov.},
  abstract = { Electronic data interchange (EDI) systems are used in many domains
	to support inter-organisational information exchange. To get systems
	using different EDI message formats to communicate, complex message
	translations (where data must be transformed from one EDI message
	format into another), are required. We describe a visual language
	and support environment which greatly simplify the task of the systems
	integrator by using a domain-specific visual language to express
	data formats and format translations. Complex message translations
	are automated by an underlying transformation engine. We describe
	the motivation for this system, its key visual language and transformation
	engine features, a prototype environment, and experience translating
	it into a commercial product.},
  doi = {10.1109/ASE.2001.989788},
  issn = {1527-1366},
  keywords = { EDI message formats; EDI message translation generation; EDI systems;
	XML; commercial product; complex message translations; data formats;
	domain-specific visual language; electronic data interchange systems;
	format translations; inter-organisational information exchange; key
	visual language; prototype environment; support environment; systems
	integrator; transformation engine; transformation engine features;
	visual language; visual specifications; electronic data interchange;
	electronic messaging; formal specification; hypermedia markup languages;
	visual languages;}
}

@INPROCEEDINGS{995282,
  author = {Grundy, J. and Mugridge, R. and Hosking, J. and Kendall, P.},
  title = {A visual language and environment for EDI message translation},
  booktitle = {Human-Centric Computing Languages and Environments, 2001. Proceedings
	IEEE Symposia on},
  year = {2001},
  pages = {330 -331},
  abstract = {Electronic data interchange (EDI) systems are used in many domains
	to support inter-organisational information exchange. These systems
	require complex message translation, where data must be transformed
	from one EDI message format into another. We describe a visual language
	and support environment which greatly simplify the task of the systems
	integrator by using a domain-specific visual language to express
	translations},
  doi = {10.1109/HCC.2001.995282},
  keywords = {EDI;XML;domain-specific visual language;electronic data interchange;inter-organisational
	information exchange;message mapping specification;message translation;business
	data processing;electronic data interchange;hypermedia markup languages;visual
	languages;}
}

@INPROCEEDINGS{5775109,
  author = {Gruttner, Kim and Hylla, Kai and Rosinger, Sven and Nebel, Wolfgang},
  title = {Towards an ESL framework for timing and power aware rapid prototyping
	of HW/SW systems},
  booktitle = {Specification Design Languages, 2010. IC 2010. Forum on},
  year = {2010},
  pages = {1 -6},
  month = {sept.},
  abstract = {Consideration of an embedded system's timing behaviour and power consumption
	at system-level is an ambitious task. Sophisticated tools and techniques
	exist for power and timing estimations of individual components such
	as custom hard-and software as well as IP components. But prediction
	of the composed system behaviour can hardly be made. In this paper
	we present the concept of an ESL framework for timing and power aware
	rapid virtual system prototyping of embedded HW/SW systems. Our proposed
	flow combines system-level timing and power estimation techniques
	available in commercial tools with platform-based rapid prototyping.
	Our proposal aims at the generation of executable virtual prototypes
	from a functional C/C #x002B; #x002B; specification. These prototypes
	are enriched by static and dynamic power values as well as execution
	times. They allow a trade-off between different platforms, mapping
	alternatives, and optimization techniques, based on domain-specific
	workload scenarios. The proposed flow will be implemented in the
	COMPLEX FP7 European integrated project.},
  doi = {10.1049/ic.2010.0129}
}

@INPROCEEDINGS{1281728,
  author = {Zonghua Gu and Shige Wang and Kodase, S. and Shin, K.G.},
  title = {Multi-view modeling and analysis of embedded real-time software with
	meta-modeling and model transformation},
  booktitle = {High Assurance Systems Engineering, 2004. Proceedings. Eighth IEEE
	International Symposium on},
  year = {2004},
  pages = { 32 - 41},
  month = {march},
  abstract = { We present an end-to-end tool-chain for model-based design and analysis
	of component-based embedded real-time software, with Avionics Mission
	Computing as an application domain. The tool-chain covers the entire
	system development life-cycle including modeling, analysis, code
	generation, and runtime instrumentation. Emphasis is placed on integration
	of tools developed by multiple institutions via standardized interface
	format definitions in XML. All aspects of an embedded real-time system
	are captured in domain-specific models, including software components
	and architecture, timing and resource constraints, processes and
	threads, execution platforms, etc. Configuration code generation
	allows automated building of the application executable. Instrumentation
	of the application running on a target platform is used to collect
	run-time statistics that are fedback into the models. Analysis tools
	perform various static analyses based on the models, including system-level
	dependency analysis, execution-rate assignment to component ports,
	real-time and schedulability analysis, and automated allocation of
	components to processors. By capturing all relevant information explicitly
	in models at the design level, and performing analysis that provides
	insight into non-functional aspects of the system, we can raise the
	level of abstraction for the designer, and facilitate rapid system
	prototyping.},
  doi = {10.1109/HASE.2004.1281728},
  issn = {1530-2059 },
  keywords = { XML; automated allocation; avionics mission computing; code generation;
	component-based embedded real-time software; eXtensible Markup Language;
	end-to-end tool-chain; interface format definitions; meta-modeling;
	model transformation; model-based design; multiview modeling; run
	time instrumentation; schedulability analysis; system development
	life-cycle; system prototyping; embedded systems; metacomputing;
	object-oriented programming; program compilers; real-time systems;
	software development management; software prototyping; software tools;}
}

@INPROCEEDINGS{685749,
  author = {Guerrieri, E.},
  title = {Software document reuse with XML},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {246 -254},
  month = {jun},
  abstract = {Looks into a generalized approach to reusing software artifacts (i.e.
	requirements, functional specifications, plans, designs, code, build
	scripts, test cases, installation scripts, user manuals, etc.). If
	we consider each software artifact as a ldquo;document rdquo;, we
	can then provide the appropriate structure to the documents from
	which we can perform the appropriate processing and reuse of the
	software artifacts (without limiting it to just code reuse). This
	approach takes advantage of the W3C Recommendation for structured
	documents called XML (eXtensible Markup Language). This paper also
	shows several uses of this approach applied to software development},
  doi = {10.1109/ICSR.1998.685749},
  issn = {1085-9098},
  keywords = {Extensible Markup Language;W3C Recommendation;XML;build scripts;code
	reuse;document structure;domain-specific toolkits;functional specifications;installation
	scripts;software artifact reuse;software designs;software development;software
	document reuse;software plans;software requirements;software test
	cases;structured documents;user manuals;page description languages;software
	reusability;system documentation;user manuals;}
}

@ARTICLE{311068,
  author = {Guerrieri, E.},
  title = {Case study: Digital's application generator},
  journal = {Software, IEEE},
  year = {1994},
  volume = {11},
  pages = {95 -96},
  number = {5},
  month = {sept. },
  abstract = {One approach to software reuse is to generate an application from
	a domain specific specification. The specification describes the
	application's problem or task, and it might take diverse forms, such
	as an interactive dialogue or a formal language. DECAdmire, an application
	generator provided by Digital Equipment, is an interactive application
	development tool that provides code generation for interactive management
	information-systems that involve relational database updates. DECAdmire
	provides 80 to 95 percent code reuse.<>},
  doi = {10.1109/52.311068},
  issn = {0740-7459},
  keywords = {DECAdmire;application development tool;application generator;code
	generation;domain specific specification;formal language;interactive
	dialogue;interactive management information-systems;relational database
	updates;software reuse;application generators;management information
	systems;relational databases;software reusability;software tools;}
}

@INPROCEEDINGS{4839226,
  author = {Gulotta, J. and Diyang Chu and Ximing Yu and Patki, H.A.-H.T. and
	Hansen, J. and Hudson, M. and Sprinkle, J.},
  title = {Using Integrative Models in an Advanced Heterogeneous System Simulation},
  booktitle = {Engineering of Computer Based Systems, 2009. ECBS 2009. 16th Annual
	IEEE International Conference and Workshop on the},
  year = {2009},
  pages = {3 -10},
  month = {april},
  abstract = {This paper is an academic experience report describing the use by
	researchers at the University of Arizona of a domain-specific language
	developed by the Institute for Software Integrated Systems (at Vanderbilt
	University). The domain in question is heterogeneous, distributed
	simulation of quad-rotor unmanned aerial vehicles (UAVs) as they
	respond to command and control requests from a human operator. We
	describe in detail how our individual designs of the controller and
	guidance laws for the UAV, its rendering and position updates, on-board
	sensors, and the various commands to delegate mission-critical behaviors,
	all interact using the ISIS-developed modeling language. We then
	discuss the outlook for this domain (heterogeneous system simulation
	and integration) for domain-specific languages and models, specifically
	for unmanned vehicle control and interaction.},
  doi = {10.1109/ECBS.2009.42},
  keywords = {advanced heterogeneous system simulation;domain-specific language;integrative
	models;quad-rotor unmanned aerial vehicles;unmanned vehicle control;aircraft;control
	engineering computing;control system CAD;remotely operated vehicles;}
}

@INPROCEEDINGS{5615143,
  author = {Gunther, S. and Haupt, M. and Splieth, M.},
  title = {Agile Engineering of Internal Domain-Specific Languages with Dynamic
	Programming Languages},
  booktitle = {Software Engineering Advances (ICSEA), 2010 Fifth International Conference
	on},
  year = {2010},
  pages = {162 -168},
  month = {aug.},
  abstract = {Domain-Specific Languages (DSL) abstract from the domain entities
	and operations to represent domain knowledge in the form of an executable
	language. While they solve many of the current software development
	challenges, related literature claims that DSLs usually have a flaw:
	The high effort required to implement and use them. However, internal
	DSLs are developed with less effort because they are built on top
	of an existing programming language and can use the whole language
	infrastructure consisting of interpreter, compiler, or editors. This
	article presents an engineering process for internal DSLs. An agile
	process leads from analysis to design and implementation. Expressions
	and language capabilities are implemented using tests and a set of
	patterns, which provide reusable knowledge how to properly structure
	and design the DSL implementation. As a case study, we show how to
	implement a software product line configuration DSL using Ruby and
	Python as host languages. In summary, the proposed process and patterns
	facilitate the successful planning and developing of internal DSLs
	using dynamic programming languages as the host.},
  doi = {10.1109/ICSEA.2010.32},
  keywords = {Python;Ruby;agile engineering;agile process;compiler;domain entities;domain
	knowledge;dynamic programming languages;editors;executable language;internal
	domain-specific languages;interpreter;reusable knowledge;software
	development;software product line configuration DSL;high level languages;software
	engineering;}
}

@INPROCEEDINGS{6008968,
  author = {Jing Guo and Rodrigues, W. and Thiyagalingam, J. and Guyomarc'h,
	F. and Boulet, P. and Scholz, S.},
  title = {Harnessing the Power of GPUs without Losing Abstractions in SAC and
	ArrayOL: A Comparative Study},
  booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW),
	2011 IEEE International Symposium on},
  year = {2011},
  pages = {1183 -1190},
  month = {may},
  abstract = {Over recent years, using Graphics Processing Units (GPUs) has become
	as an effective method for increasing the performance of many applications.
	However, these performance benefits from GPUs come at a price. Firstly
	extensive programming expertise and intimate knowledge of the underlying
	hardware are essential for gaining good speedups. Secondly, the expressibility
	of GPU-based programs are not powerful enough to retain the high-level
	abstractions of the solutions. Although the programming experience
	has been significantly improved by existing frameworks like CUDA
	and OpenCL, it is still a challenge to effectively utilise these
	devices while still retaining the programming abstractions. To this
	end, performing a source-to-source transformation, whereby a high-level
	language is mapped to CUDA or OpenCL, is an attractive option. In
	particular, it enables one to retain high-level abstractions and
	to harness the power of GPUs without any expertise on the GPGPU programming.
	In this paper, we compare and analyse two such schemes. One of them
	is a transformation mechanism for mapping a image/signal processing
	domain-specific language, ArrayOL, to OpenCL. The other one is a
	transformation route for mapping a high-level general purpose array
	processing language, Single Assignment C (SaC) to CUDA. Using a real-world
	image processing application as a running example, we demonstrate
	that albeit the fact of being general purpose, the array processing
	language be used to specify complex array access patterns generically.
	Performance of the generated CUDA code is comparable to the OpenCL
	code created from domain-specific language.},
  doi = {10.1109/IPDPS.2011.276},
  issn = {1530-2075},
  keywords = {ArrayOL;CUDA;GPGPU programming;GPU-based programs;OpenCL;SaC;Single
	Assignment C;graphics processing units;high-level abstractions;high-level
	general purpose array processing language;high-level language;image/signal
	processing domain-specific language;source-to-source transformation;computer
	graphic equipment;high level languages;image processing;}
}

@INPROCEEDINGS{4593456,
  author = {Qinglin Guo},
  title = {Question answering system based on Ontology},
  booktitle = {Intelligent Control and Automation, 2008. WCICA 2008. 7th World Congress
	on},
  year = {2008},
  pages = {3347 -3352},
  month = {june},
  abstract = {The background and system frame of the question answering system are
	presented. A new approach is explored in which questions are analyzed
	based on the detection of question focus chunk, semantic chunk and
	question template. Then vectors are used for the whole semantic representation.
	With the elicitation of ontology and semantic Web, a domain ontology
	is built up. Ontology and semantic Web are becoming the pivotal methodology
	to represent domain-specific conceptual knowledge in order to promote
	the semantic capability of a question answering system. In this paper
	we present a question answering system in which the domain knowledge
	is represented by means of ontology. And a Chinese natural language
	human-machine interface is implemented mainly through a NL parser
	in this system. An initial evaluation result shows the feasibility
	to build such a semantic QA system based on ontology, the extensibility
	of ontology, and the effectivity of personalized semantic QA.},
  doi = {10.1109/WCICA.2008.4593456},
  keywords = {Chinese natural language human-machine interface;domain ontology;domain-specific
	conceptual knowledge representation;natural language parser;ontology
	elicitation;question analysis;question answering system;question
	focus chunk detection;question template;semantic Web;semantic chunk
	detection;semantic representation;grammars;information retrieval;information
	retrieval systems;natural language interfaces;ontologies (artificial
	intelligence);semantic Web;}
}

@INPROCEEDINGS{5599823,
  author = {Yuhang Guo and Wanxiang Che and Ting Liu and Sheng Li},
  title = {Semi-supervised domain adaptation for WSD: Using a word-by-word model
	selection approach},
  booktitle = {Cognitive Informatics (ICCI), 2010 9th IEEE International Conference
	on},
  year = {2010},
  pages = {680 -687},
  month = {july},
  abstract = {This paper proposes a word-by-word model selection approach to domain
	adaptation for Word Sense Disambiguation. By this approach, the model
	for a target word is automatically selected from a candidate model
	set, which is comprised of improved self-training models and a supervised
	model. The improved self-training uses sense priors to prevent its
	iteration from converging into undesirable states. Experimental results
	on a domain-specific corpus show that: (1) our improved self-training
	model is effective for the words which have target domain linked
	senses; (2) the selected models obtain higher accuracies than each
	single model and effectively improve the performance compared to
	the state-of-the-art supervised model.},
  doi = {10.1109/COGINF.2010.5599823},
  keywords = {natural language processing;self-training models;semi-supervised domain
	adaptation;supervised model;word sense disambiguation;word-by-word
	model selection;learning (artificial intelligence);natural language
	processing;}
}

@INPROCEEDINGS{4803031,
  author = {Anand Gupta and Goyal, A. and Bindal, A. and Ankuj Gupta},
  title = {Meliorated approach for extracting bilingual terminology from Wikipedia},
  booktitle = {Computer and Information Technology, 2008. ICCIT 2008. 11th International
	Conference on},
  year = {2008},
  pages = {560 -565},
  month = {dec.},
  abstract = {With the demand of accurate and domain specific bilingual dictionaries,
	research in the field of automatic dictionary extraction has become
	popular. Due to lack of domain specific terminology in parallel corpora,
	extraction of bilingual terminology from Wikipedia (a corpus for
	knowledge extraction having a huge amount of articles, links within
	different languages, a dense link structure and a number of redirect
	pages) has taken up a new research in the field of bilingual dictionary
	creation. Our method not only analyzes interlanguage links along
	with redirect page titles and link text titles but also filters out
	inaccurate translation candidates using pattern matching. Score of
	each translation candidate is calculated using page parameters and
	then setting an appropriate threshold as compared to previous approach,
	which was solely, based on backward links. In our experiment, we
	proved the advantages of our approach compared to the traditional
	approach.},
  doi = {10.1109/ICCITECHN.2008.4803031},
  keywords = {Wikipedia;automatic dictionary extraction;bilingual terminology;domain
	specific bilingual dictionaries;knowledge extraction;pattern matching;dictionaries;information
	retrieval;natural languages;pattern matching;search engines;}
}

@INPROCEEDINGS{5493435,
  author = {Gupta, Chaitali and Govindaraju, Madhusudhan},
  title = {Framework for Efficient Indexing and Searching of Scientific Metadata},
  booktitle = {Cluster, Cloud and Grid Computing (CCGrid), 2010 10th IEEE/ACM International
	Conference on},
  year = {2010},
  pages = {553 -556},
  month = {may},
  abstract = {A seamless and intuitive data reduction capability for the vast amount
	of scientific metadata generated by experiments is critical to ensure
	effective use of the data by domain specific scientists. The portal
	environments and scientific gateways currently used by scientists
	provide search capability that is limited to the pre-defined pull-down
	menus and conditions set in the portal interface. Currently, data
	reduction can only be effectively achieved by scientists who have
	developed expertise in dealing with complex and disparate query languages.
	A common theme in our discussions with scientists is that data reduction
	capability, similar to web search in terms of ease-of-use, scalability,
	and freshness/accuracy of results, is a critical need that can greatly
	enhance the productivity and quality of scientific research. Most
	existing search tools are designed for exact string matching, but
	such matches are highly unlikely given the nature of metadata produced
	by instruments and a user #x02019;s inability to recall exact numbers
	to search in very large datasets. This paper presents research to
	locate metadata of interest within a range of values. To meet this
	goal, we leverage the use of XML in metadata description for scientific
	datasets, specifically the NeXus datasets generated by the SNS scientists.
	We have designed a scalable indexing structure for processing data
	reduction queries. Web semantics and ontology based methodologies
	are also employed to provide an elegant, intuitive, and powerful
	free-form query based data reduction interface to end users.},
  doi = {10.1109/CCGRID.2010.120}
}

@INPROCEEDINGS{895452,
  author = {Gupta, G.},
  title = {Reliable software construction: a logic programming based methodology},
  booktitle = {High Assurance Systems Engineering, 2000, Fifth IEEE International
	Symposim on. HASE 2000},
  year = {2000},
  pages = {140 -141},
  abstract = {In this position paper we investigate how logic programming technology
	can aid software development. The overall goal is to provide a framework
	for specification and verification that is ldquo;computational rdquo;
	in nature rather than being based on traditional, more complex formalisms
	such as theorem proving and term rewriting. Two approaches are discussed.
	In the first approach, given a program written in a traditional language
	(e.g., C), an equivalent logic program is automatically obtained.
	This equivalent logic program serves as a high level abstraction
	of the original program and can be put to a number of uses including
	verification, structured debugging and generation of provably correct
	target code. The second approach is centered around domain specific
	languages. Given a task for which a software system is to be developed,
	a high-level domain specific language (DSL) is first designed. Domain
	experts can use this DSL for writing programs at their level of abstraction.
	Logic programming provides a framework in which programs written
	in this DSL can be interpreted, compiled, debugged, verified, and
	profiled},
  doi = {10.1109/HASE.2000.895452},
  keywords = {high level abstraction;high-level domain specific language;logic programming
	based methodology;reliable software construction;software development;specification;structured
	debugging;term rewriting;theorem proving;verification;formal specification;formal
	verification;logic programming;program debugging;software reliability;theorem
	proving;}
}

@INPROCEEDINGS{4262592,
  author = {Gupta, R. and Shourya Roy and Bhide, M.},
  title = {Identity Delegation in Policy Based Systems},
  booktitle = {Policies for Distributed Systems and Networks, 2007. POLICY '07.
	Eighth IEEE International Workshop on},
  year = {2007},
  pages = {229 -240},
  month = {june},
  abstract = {Policy based systems have received considerable attention in the recent
	past from academia as well as the industry. Research on policy based
	systems encompasses a gamut of areas such as: models and languages
	for policy based systems, policy standards, domain specific implementations,
	policy tools etc. However an important issue, which did not receive
	much attention from researchers, is that of access control for policy
	execution. In this paper we present the concept of "identity delegation"
	which involves finding the 'correct' users/ identities, to whom task
	of policy execution can be delegated. Policies are generally defined
	by high level business executives (policy authors) and are implemented
	by policy enforcers who have sufficient access rights on the underlying
	systems. Given the increasing complexity of enterprise systems, we
	show in this paper that finding the right policy enforcers for a
	policy can be a fairly non-trivial task. We address this important
	problem by proposing a unique concept of 'implicit identity delegation',
	whereby an autonomic system automatically figures out the correct
	policy enforcers and implicitly delegates the task of policy execution.
	We present the Implicit Identity Delegation architecture which boasts
	of an efficient technique for performing implicit identity delegation
	and uses a plugin based architecture ensuring its applicability and
	use in diverse domains.},
  doi = {10.1109/POLICY.2007.26},
  keywords = {access control;enterprise systems;implicit identity delegation architecture;policy
	based systems;policy execution;policy standards;authorisation;}
}

@INPROCEEDINGS{1012749,
  author = {Gupta, S. and Kam, T. and Kishinevsky, M. and Rotem, S. and Savoiu,
	N. and Dutt, N. and Gupta, R. and Nicolau, A.},
  title = {Coordinated transformations for high-level synthesis of high performance
	microprocessor blocks},
  booktitle = {Design Automation Conference, 2002. Proceedings. 39th},
  year = {2002},
  pages = { 898 - 903},
  abstract = {High performance microprocessor designs are partially characterized
	by functional blocks consisting of a large number of operations that
	are packed into very few cycles (often single-cycle) with little
	or no resource constraints but tight bounds on the cycle time. Extreme
	parallelization, conditional and speculative execution of operations
	is essential to meet the processor performance goals. However, this
	is a tedious task for which classical high-level synthesis (HLS)
	formulations are inadequate and thus rarely used. In this paper,
	we present a new methodology for application of HLS targeted to such
	microprocessor functional blocks that can potentially speed up the
	design space exploration for microprocessor designs. Our methodology
	consists of a coordinated set of source-level and fine-grain parallelizing
	compiler transformations that targets these behavioral descriptions,
	specifically loop constructs in them and enables efficient chaining
	of operations and high-level synthesis of the functional blocks.
	As a case study in understanding the complexity and challenges in
	the use of HLS, we walk the reader through the detailed design of
	an instruction length decoder drawn from the Pentium reg;-family
	of processors. The chief contribution of this paper is formulation
	of a domain-specific methodology for application of high-level synthesis
	techniques to a domain that rarely, if ever, finds use for it.},
  doi = {10.1109/DAC.2002.1012749},
  issn = {0738-100X },
  keywords = { HLS; Pentium processors; behavioral descriptions; coordinated transformations;
	cycle time bounds; design space exploration; domain-specific methodology;
	extreme parallelization; functional blocks; high performance microprocessor
	blocks; high-level synthesis; instruction length decoder; loop constructs;
	microprocessor designs; operation chaining; processor performance
	goals; resource constraints; source-level fine-grain parallelizing
	compiler transformations; hardware description languages; high level
	synthesis; integrated circuit design; microprocessor chips; parallel
	architectures;}
}

@INPROCEEDINGS{5349852,
  author = {Guta, G. and Schreiner, W. and Draheim, D.},
  title = {A Lightweight MDSD Process Applied in Small Projects},
  booktitle = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th
	Euromicro Conference on},
  year = {2009},
  pages = {255 -258},
  month = {aug.},
  abstract = {Model driven software development, domain specific languages and other
	generative programming approaches have gained much attention. There
	is a large number of tools available, starting from simple code generators
	to full-blown tool suites. There are several success stories and
	process frameworks about applying full scale MDSD approaches, but
	there is no or little help for small-size projects. To fill this
	gap, we designed a new development process which targets small and
	middle size projects with limited resources. According to our experience
	the process is working efficiently in small projects even if they
	cannot afford bigger initial resource investment. In this experience
	report we document an industrial project in which the process was
	used. We present this process in the form which is applied in the
	first project and also discuss our experience and the limitations
	of the process.},
  doi = {10.1109/SEAA.2009.63},
  issn = {1089-6503},
  keywords = {MDSD process;code generators;development process;domain specific languages;generative
	programming;industrial project;model driven software development;resource
	investment;small-size projects;object-oriented programming;program
	compilers;software engineering;}
}

@ARTICLE{5473204,
  author = {Gnther, S.},
  title = {Multi-DSL Applications with Ruby},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {25 -30},
  number = {5},
  month = {sept.-oct. },
  abstract = {Exploiting Ruby's support for the imperative, functional, and object-oriented
	paradigms, several DSLs' integrated and interwoven multiparadigm
	expressions can express all concerns, application layers, and artifacts
	of an application.},
  doi = {10.1109/MS.2010.91},
  issn = {0740-7459},
  keywords = {Ruby;application layers;domain-specific languages;multiDSL applications;object-oriented
	paradigms;object-oriented languages;}
}

@INPROCEEDINGS{4161032,
  author = {de Haan, G. and Koutek, M. and Post, F.H.},
  title = {Flexible Abstraction Layers for VR Application Development},
  booktitle = {Virtual Reality Conference, 2007. VR '07. IEEE},
  year = {2007},
  pages = {239 -242},
  month = {march},
  abstract = {The development of domain-specific virtual reality applications is
	often a slow and laborious process. The integration of the domain-specific
	functionality in an interactive Virtual Environment requires close
	collaboration between domain expert and VR developer, as well as
	the integration of domain-specific data and software in a VR application.
	The software environment needs to support the entire development
	process and software life cycle, from the early stages of iterative,
	rapid prototyping to a final end-user application. In this paper,
	we propose the use of flexible abstraction layers in the form of
	a dynamic scripting language, which act as the glue between VR system
	components and external software libraries and applications. First,
	we discuss the motivation and potential of our approach, after which
	we overview related approaches. Then, we describe the integration
	of a Python interpreter in our VR toolkit. The potential of our integration
	approach is demonstrated by rapid prototyping features, the flexible
	extension of core functionality and the integration of an external
	toolkit. We conclude with an overview of implications our approach
	has for the future development of new framework features and application
	integration},
  doi = {10.1109/VR.2007.352490},
  keywords = {Python interpreter;dynamic scripting language;flexible abstraction
	layers;interactive virtual environment;rapid prototyping;virtual
	reality;authoring languages;software prototyping;virtual reality;}
}

@INPROCEEDINGS{545289,
  author = {Haarslev, V. and Wessel, M.},
  title = {GenEd-an editor with generic semantics for formal reasoning about
	visual notations},
  booktitle = {Visual Languages, 1996. Proceedings., IEEE Symposium on},
  year = {1996},
  pages = {204 -211},
  month = {sep},
  abstract = {We describe the object-oriented editor GenEd supporting the design
	of specifications for visual notations. Prominent features of GenEd
	are: it is generic, i.e. domain-specific syntax and semantics are
	specified by users; built-in parser for actual drawings, driven by
	formal specifications; powerful reasoning capabilities about diagrams
	and their specification. GenEd's specification language is based
	on a fully formalized theory for describing visual notations. Three
	examples, place-transition Petri nets, entity-relationship diagrams,
	and a small GIS application are presented},
  doi = {10.1109/VL.1996.545289},
  keywords = {GenEd;built-in parser;diagrams;domain-specific syntax;drawings;entity-relationship
	diagrams;formal specifications;formal visual reasoning;generic semantics;geographic
	information system;object-oriented editor;place-transition Petri
	nets;semantics;specification language;specifications;visual notations;visual
	programming;Petri nets;diagrams;entity-relationship modelling;formal
	specification;geographic information systems;object-oriented programming;program
	compilers;spatial reasoning;specification languages;visual programming;}
}

@INPROCEEDINGS{4279746,
  author = {Hadar, E. and Perreira, M.},
  title = {Web Services Variation Faade - Domain Specific Reference Architecture
	for Increasing Integration Usability},
  booktitle = {Web Services, 2007. ICWS 2007. IEEE International Conference on},
  year = {2007},
  pages = {1207 -1211},
  month = {july},
  abstract = {Providing a usable service is a well known requirement. However, integration
	use-cases variations and evolution needs usually result in the construction
	of a generic, coarse-grained service. Such services tend to be complicated
	in comparison to specific, fine grained, highly usable ones. The
	differences between complex and simple integration points should
	not make them mutually exclusive, but rather mutually supported according
	to practical use cases. This paper describes an architectural reference
	solution implemented as a Web service variation facade. The architecture
	defines two types of services: a simple, open access, domain specific
	business language type; and an access-controlled, complex, coarse
	type, based on licensing permits. Both types of services are constructed
	in accordance with relevant publishing policies. The first is ready
	to integrate with a simple contract, and the second requires preliminary
	integration negotiations with a detailed and usually complex contract.
	The solution differentiates between the variations in integration
	scenarios and the core product functionality, while using single
	deployment protocol that utilizes a licensing policy. It provides
	highly usable services as well as complex generic ones, while limiting
	changes from the external to the internal domain, and vice versa.},
  doi = {10.1109/ICWS.2007.193},
  keywords = {Web service variation facade;deployment protocol;Web services;software
	architecture;}
}

@INPROCEEDINGS{4815033,
  author = {Hahn, C. and Nesbigall, S. and Warwas, S. and Zinnikus, I. and Klusch,
	M. and Fischer, K.},
  title = {Model-driven Approach to the Integration of Multiagent Systems and
	Semantic Web Services},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops, 2008
	12th},
  year = {2008},
  pages = {317 -324},
  month = {sept.},
  abstract = {This paper discusses an innovative mean on model-driven agent-based
	coordination of semantic Web services. The general idea is to define
	a platform independent meta-model for semantic Web services and integrate
	it into a platform independent metamodel for multiagent systems.
	A model-driven semantic Web services matchmaker agent in combination
	with model transformations between the platform independent metamodel
	and existing semantic Web service formats like OWL-S allow the seamless
	integration of semantic Web services into multiagent systems.},
  doi = {10.1109/EDOCW.2008.43},
  keywords = {OWL-S;agent-oriented software engineering;domain specific modeling
	language;model transformation;model-driven agent-based coordination
	approach;multiagent system;platform independent metamodel;semantic
	Web service matchmaker agent;Web services;knowledge representation
	languages;multi-agent systems;semantic Web;software agents;}
}

@INPROCEEDINGS{5691237,
  author = {Haleplidis, E. and Tranoris, C. and Denazis, S. and Koufopavlou,
	O.},
  title = {Adopting software engineering practices to network processor devices
	introducing the Domain Specific Modeling paradigm to the ForCES Framework},
  booktitle = {Network and Service Management (CNSM), 2010 International Conference
	on},
  year = {2010},
  pages = {366 -369},
  month = {oct.},
  abstract = {IETF's new Forwarding and Control Element Separation (ForCES) architecture
	specifies the ForCES model providing an accurate description of the
	Forwarding Plane in an Object-Oriented fashion. However, the model
	is described totally in an XML Schema Definition (XSD): it is well-defined
	but purely machine oriented, being readable and usable, thus not
	human-friendly and difficult extending itself in the future. We argue
	that the ForCES model is actually a meta-model that is used to model
	ForCES components, e.g. Logical Function Blocks (LFBs), that later
	are used in ForCES applications. This paper presents a methodology
	based on a case study on how to automate the process of configuring
	the forwarding plane of network devices using state-of-the-art model-driven
	techniques in a tangible way while specifying a tool supported by
	a Domain Specific Language (DSL) for ForCES. We first consider describing
	the ForCES XSD based meta-model to a more manageable Ecore (MOF)
	based meta-model and then we create a DSL based on this Ecore meta-model.
	Then we target to transform automatically a Platform Independent
	ForCES model specified in the DSL to an executable target source
	code (Platform Specific: XML-ForCES compliant, C++, Java) able to
	communicate with the ForCES protocol.},
  doi = {10.1109/CNSM.2010.5691237},
  keywords = {C++;Domain Specific Language;Ecore meta-model;ForCES XSD based meta
	model;ForCES framework;Forwarding and Control Element Separation;Java;Logical
	Function Blocks;Platform Independent ForCES model;XML ForCES compliant;XML
	Schema Definition;manageable Ecore based meta model;meta-model;model
	driven techniques;network processor devices;object oriented fashion;software
	engineering;specific modeling paradigm;XML;object-oriented programming;software
	engineering;}
}

@INPROCEEDINGS{1508576,
  author = {Hamada, T. and Nakasato, N.},
  title = {Massively parallel processors generator for reconfigurable system},
  booktitle = {Field-Programmable Custom Computing Machines, 2005. FCCM 2005. 13th
	Annual IEEE Symposium on},
  year = {2005},
  pages = { 329 - 330},
  month = {april},
  abstract = { We have developed PGR (processors generator for reconfigurable system)
	package which generate (a) a suitable configuration file for the
	FPGAs, (b) the C source code for interfacing with an FPGA-based accelerator,
	and (c) a software emulator from a high-level domain specific language.
	Using PGR package, we can easily produce high performance implementations
	for the particle-based simulation.},
  doi = {10.1109/FCCM.2005.45},
  keywords = { C source code; FPGA-based accelerator; field programmable gate array;
	massively parallel processor generator; particle-based simulation;
	reconfigurable system; software emulator; astronomy computing; field
	programmable gate arrays; logic CAD; parallel architectures; software
	packages;}
}

@INPROCEEDINGS{4438539,
  author = {Hamdeh, N.A. and Shilong Ma},
  title = {OWL-based Ontology for Secure and Adaptable Ubiquitous Environment},
  booktitle = {Semantics, Knowledge and Grid, Third International Conference on},
  year = {2007},
  pages = {230 -235},
  month = {oct.},
  abstract = {Computing is moving towards ubiquitous vision where applications have
	to be context-aware and adapt their behavior without explicit interaction
	with the user. We claim that context-aware ubiquitous services browsers
	(CAUSB) which utilize the reasoning for classifying the services
	based on the context and security will become an essential part of
	these environments. UPnP is an appropriate technology, but still
	not suitable for ubiquitous vision, in particular CAUSB, since the
	control point such as a mobile phone or a PDA provides the user with
	all the services in the surrounding without taking into account the
	context information, it only relieves him from having the physical
	contact with the device, and it doesn't support any adaptation. Lots
	of work has been done for context modeling, however, adaptability
	and security still remaining among other challenges. Therefore, in
	this paper, we propose a reusable ubiquitous OWL-based ontology for
	a smart home. Our ontology is modeled into two levels: a generic
	core level and a domain specific level. We also propose a novel integration
	of the context-based adaptation and security policies into that ontology
	along with the domain and its context knowledge representation.},
  doi = {10.1109/SKG.2007.152},
  keywords = {context-aware adaptable ubiquitous services browser;knowledge representation;ontology
	Web language;security policy;smart home;Internet;home automation;mobile
	computing;ontologies (artificial intelligence);security of data;}
}

@INPROCEEDINGS{5325382,
  author = {Hamdi, H. and Mosbah, M.},
  title = {A DSL Framework for Policy-Based Security of Distributed Systems},
  booktitle = {Secure Software Integration and Reliability Improvement, 2009. SSIRI
	2009. Third IEEE International Conference on},
  year = {2009},
  pages = {150 -158},
  month = {july},
  abstract = {Securing distributed systems remains a significant challenge for several
	reasons. First, the security features required in an application
	may depend on the environment in which the application is operating,
	the type of data exchanged, and the capability of the end-points
	of communication. Second, the security mechanisms deployed could
	apply to both communication and application layers in the system,
	making it difficult to understand and manage overall system security.
	This paper presents a policy-based approach to meeting these needs.
	We propose a framework based on a domain-specific language for the
	specification, verification and implementation of security policies
	for distributed systems. Based on a set of abstractions, this framework
	allows to develop modular security policies and independent of the
	underlying system. Thus, security policies can be developed by a
	developer who is not necessarily computer security expert.},
  doi = {10.1109/SSIRI.2009.43},
  keywords = {data exchange;distributed systems;domain-specific language;policy-based
	security;specification;system security;verification;formal specification;program
	verification;programming languages;security of data;}
}

@INPROCEEDINGS{4300048,
  author = {Hamdi, H. and Mosbah, M. and Bouhoula, A.},
  title = {A Domain Specific Language for Securing Distributed Systems},
  booktitle = {Systems and Networks Communications, 2007. ICSNC 2007. Second International
	Conference on},
  year = {2007},
  pages = {76},
  month = {aug.},
  abstract = {Distributed applications are becoming increasingly common. However,
	incorporating security in them remains a major challenge. There are
	currently few choices to express and enforce security in distributed
	systems. We can either use a special-purpose language which may be
	too limited to express security requirements, or use a general purpose
	language that provides the ability to make complicated security policy
	but makes us reimplement infrastructure code for authorization, interdiction,
	obligation and so on with each new security policy. In this paper,
	we introduce a domain-specific language approach that takes the middle
	road, giving a way to reuse security infrastructure for new policies
	while also allowing the expression of complicated security policy
	easily. We present our DSL approach and and apply it to a real-world
	scenario: specification and implementation of security policy.},
  doi = {10.1109/ICSNC.2007.2},
  keywords = {DSL approach;distributed system security;domain-specific language
	approach;general purpose language;infrastructure code;security policy;special-purpose
	language;distributed processing;programming languages;security of
	data;}
}

@INPROCEEDINGS{4426843,
  author = {Hamey, Leonard G. C.},
  title = {Efficient Image Processing with the Apply Language},
  booktitle = {Digital Image Computing Techniques and Applications, 9th Biennial
	Conference of the Australian Pattern Recognition Society on},
  year = {2007},
  pages = {533 -540},
  month = {dec.},
  abstract = {Apply is a Domain-Specific Language for image processing and low-level
	computer vision. Apply allows programmers to write kernel operations
	that focus on the computation for a single pixel location. The compiler
	generates code to perform the kernel computation over entire images.
	The original Apply implementation was developed 20 years ago for
	efficient processing on parallel architectures. The current-generation
	Apply compiler targets efficient code generation for general-purpose
	computers, typically outperforming handwritten code, while maintaining
	the simplicity of the original language. The use of modern compiler
	writing tools, specifically Stratego/XT, has facilitated improvements
	in the language design and made it easy to target the compiler to
	different environments. A large number of computer vision and image
	processing operations can be expressed in Apply. However, some algorithms
	require additional features. To motivate future language development,
	we analyse the requirements of the algorithms provided in a commercial
	machine vision library.},
  doi = {10.1109/DICTA.2007.4426843}
}

@INPROCEEDINGS{5583629,
  author = {Lei Han and DongHong Ji and Han Ren},
  title = {Statistics-based Semantic Role Labeling for Chinese menu corpus},
  booktitle = {Natural Computation (ICNC), 2010 Sixth International Conference on},
  year = {2010},
  volume = {8},
  pages = {4291 -4295},
  month = {aug.},
  abstract = {This paper describes our attempt at automatic Semantic Role Labeling
	for a Chinese specific corpus. We present a method to construct a
	domain specific corpus, a set of semantic roles that we will use,
	and their annotation. Additionally, we construct an ontology that
	we use in order to improve the accuracy of Chinese word segmentation
	and unknown word recognition. We propose two statistical methods
	for Semantic Role labeling and carry out experiments to compare them,
	and experimental results show that the methods achieve good performance.},
  doi = {10.1109/ICNC.2010.5583629},
  keywords = {Chinese menu corpus;Chinese word segmentation;ontology;statistics-based
	semantic role labeling;unknown word recognition;natural language
	processing;ontologies (artificial intelligence);statistical analysis;}
}

@INPROCEEDINGS{4730787,
  author = {Yu Han and Shufen Liu and Xiaoyan Wang and Bin Li},
  title = {Design of a metamodel-based telecoms modelling language},
  booktitle = {Computer-Aided Industrial Design and Conceptual Design, 2008. CAID/CD
	2008. 9th International Conference on},
  year = {2008},
  pages = {1235 -1238},
  month = {nov.},
  abstract = {Along with the evolution of computer technology, language oriented
	programming came out as a revolutionary progress which beyond the
	object-oriented programming. In current object-oriented programming,
	the general modeling language UML lacks of rich syntax and semantics
	in the specific domain. Developing and using domain specific language
	model in the progress of language oriented programming can solve
	this problem well. By constructing a executable model based on the
	MOF, defining an abstract syntax model and an concrete syntax model
	and extending semantics, this paper designs a telecommunication topology
	modelling language TML which based on the MOF metamodel .TML contains
	rich syntax, semantics and constraints which are telecommunication
	domain specific, and has the executable feature, thus it greatly
	simplifies the complexity of system modelling in domain and enhances
	the efficiency of software production.},
  doi = {10.1109/CAIDCD.2008.4730787},
  keywords = {UML;abstract syntax model;concrete syntax model;language oriented
	programming;metamodel-based telecoms modelling language;object-oriented
	programming;Unified Modeling Language;computational linguistics;object-oriented
	programming;}
}

@INPROCEEDINGS{5486602,
  author = {Handurukande, S. and Wallin, S. and Jonsson, A.},
  title = {IPTV service modeling in Magneto networks},
  booktitle = {Network Operations and Management Symposium Workshops (NOMS Wksps),
	2010 IEEE/IFIP},
  year = {2010},
  pages = {51 -54},
  month = {april},
  abstract = {One of the main steps of service assurance is service monitoring using
	Key Performance Indicators (KPIs) and Service Level Agreements (SLAs).
	We show an approach for service modeling, first starting with an
	abstract service model that depends on the network. And then, we
	show how a corresponding model can be realized using a domain specific
	language. This solution is able to condense various sources of service
	model requirements into a condense formal and executable model including
	service decomposition and KPI aggregation. We have described this
	solution in the context of Magneto project and uses IPTV as a service
	in our description.},
  doi = {10.1109/NOMSW.2010.5486602},
  keywords = {IPTV;Magneto networks;abstract service model;condense formal;domain
	specific language;executable model;key performance indicators;service
	assurance;service decomposition;service level agreements;service
	monitoring;IPTV;quality of service;}
}

@INPROCEEDINGS{1547464,
  author = {Haney, M.J. and Ahuja, S. and Bapty, G. and Cheung, H. and Kalbarczyk,
	Z. and Khanna, A. and Kowalkowski, J. and Messie, D. and Mosse, D.
	and Neema, S. and Nordstrom, S. and Jae Oh and Sheldon, P. and Shetty,
	S. and Volper, D. and Long Wang and Di Yao},
  title = {The RTES project - BTeV, and beyond},
  booktitle = {Real Time Conference, 2005. 14th IEEE-NPSS},
  year = {2005},
  pages = {4 pp.},
  month = {june},
  abstract = {The real time embedded systems (RTES) project was created to study
	the design and implementation of high-performance, heterogeneous,
	and fault-adaptive real time embedded systems. The driving application
	for this research was the proposed BTeV high energy physics experiment,
	which called for large farms of embedded computational elements (DSPs),
	as well as a large farm of conventional high-performance processors
	to implement its Level 1 and Level 2/3 triggers. At the time of BTeV's
	termination early in 2005, the RTES project was within days of completing
	a prototype implementation for providing a reliable and fault-adaptive
	infrastructure to the L2/3 farm; a prototype for the L1 farm had
	been completed in 2003. This paper documents the conclusion of the
	RTES focus on BTeV, and provides an evaluation of the applicability
	of the RTES concepts to other systems},
  doi = {10.1109/RTC.2005.1547464},
  keywords = {BTeV high energy physics experiment;DSP;Level 1 triggers;Level 2/3
	triggers;RTES project;computer reliability;conventional high-performance
	processors;embedded computational elements;fault-adaptive infrastructure;fault-adaptive
	real time embedded systems;heterogeneous embedded systems;high-performance
	embedded systems;large-scale systems;real time embedded systems project;real
	time systems;reliability modeling;reliable infrastructure;embedded
	systems;high energy physics instrumentation computing;large-scale
	systems;reliability;trigger circuits;}
}

@INPROCEEDINGS{5628930,
  author = {Harbach, M. and Dornemann, T. and Juhnke, E. and Freisleben, B.},
  title = {Semantic Validation of BPEL Fragment Compositions},
  booktitle = {Semantic Computing (ICSC), 2010 IEEE Fourth International Conference
	on},
  year = {2010},
  pages = {176 -183},
  month = {sept.},
  abstract = {We present an approach to improve design-time user support during
	the composition of web services into BPEL processes. The approach
	is based on adding semantic service descriptions to SimpleBPEL, a
	framework that supports web service novices in designing workflows.
	Experienced BPEL developers define domain specific profiles, containing
	parts of a BPEL process called SimpleBPEL fragments. The application
	domain specialists then use these fragments to intuitively create
	workflows. Semantic service descriptions are leveraged to support
	workflow design, including data mediation, automated workflow completion
	and compatibility checks as well as validation based on domain and
	data semantics. A prototypical implementation demonstrates the feasibility
	of our approach.},
  doi = {10.1109/ICSC.2010.84},
  keywords = {BPEL fragment composition;SimpleBPEL framework;Web services composition;business
	process execution language;design-time user support;semantic service
	descriptions;semantic validation;Web services;formal specification;formal
	verification;specification languages;user interfaces;}
}

@INPROCEEDINGS{4149167,
  author = {Hardebolle, Cecile and Boulanger, Frederic and Marcadet, Dominique
	and Vidal-Naquet, Guy},
  title = {A Generic Execution Framework for Models of Computation},
  booktitle = {Model-Based Methodologies for Pervasive and Embedded Software, 2007.
	MOMPES '07. Fourth International Workshop on},
  year = {2007},
  pages = {45 -54},
  month = {march},
  abstract = {The model driven engineering approach has had an important impact
	on the methods used for the conception of systems. However, some
	important difficult points remain in this domain. In this paper,
	we focus on problems related to the heterogeneity of the computation
	models (and therefore of the modeling techniques) used for the different
	aspects of a system and to the validation and the execution of a
	model. We present here a language for describing computation models,
	coupled with a generic execution platform where different computation
	models as well as their composition can be interpreted. Our goal
	is to be able to describe precisely the semantics of the computation
	models underlying domain specific languages, and to allow the interpretation
	of these models within our platform. This provides for a non ambiguous
	definition of the behavior of heterogeneous models of a system, which
	is essential for validation, simulation and code generation},
  doi = {10.1109/MOMPES.2007.1},
  keywords = {computation models;domain specific languages;generic execution;heterogeneous
	models;model driven engineering;formal specification;specification
	languages;}
}

@INPROCEEDINGS{4539573,
  author = {Hargassner, W. and Hofer, T. and Klammer, C. and Pichler, J. and
	Reisinger, G.},
  title = {A Script-Based Testbed for Mobile Software Frameworks},
  booktitle = {Software Testing, Verification, and Validation, 2008 1st International
	Conference on},
  year = {2008},
  pages = {448 -457},
  month = {april},
  abstract = {Software testing is essential and takes a large part of resources
	during software development. This motivates automating software testing
	as far as possible. Frameworks for automating unit testing are approved
	and applied for a plethora of programming languages to write tests
	for small units in the same programming language. Both constraints,
	unit size and programming language, inhibit automation of software
	testing in domain of mobile software frameworks. This circumstance
	has motivated the development of a new testbed for a framework in
	the domain of mobile systems. In this paper, we describe requirements
	and challenges in testing mobile software frameworks in general and
	present a novel testbed for the APOXI framework that addresses these
	requirements. The main ideas behind this testbed are the usage of
	a scripting language to specify test cases and to incorporate domain-specific
	aspects on the language level. The testbed facilitates component
	and system testing but can be used for unit testing as well.},
  doi = {10.1109/ICST.2008.51},
  keywords = {mobile software;programming languages;script-based testbed;software
	development;software testing;mobile computing;program testing;software
	engineering;}
}

@INPROCEEDINGS{5381641,
  author = {Hartmann, T.},
  title = {Model Based Testing of End-to-End Chains Using Domain Specific Languages},
  booktitle = {Testing: Academic and Industrial Conference - Practice and Research
	Techniques, 2009. TAIC PART '09.},
  year = {2009},
  pages = {82 -91},
  month = {sept.},
  abstract = {In this paper, the author explains a new approach of model based end-to-end
	chain testing using scenarios with original and simulated equipment.
	The first goal is to automatically derive test data and test cases
	from the model, which is defined by a domain specific language. Several
	solvers can be attached to the conversion to quickly create a wide
	variety of stimuli for the system(s) under test. Furthermore, the
	system under test can be stimulated by either original equipment
	- which is connected to the test bench - or the test bench can simulate
	equipment and create inputs for the tested systems. Any mixture of
	simulated and original equipment is possible and can be changed on
	the fly. In the end, the results from the system under test are collected.
	These results can then be displayed back in the model. This method
	is currently used and improved in the project "E-Cab" in which the
	author is involved. Passengers travelling by plane are in the focus
	of this project. Complete services and service chains - from the
	booking at home up to leaving the destination airport - are created
	and used by many systems communicating with each other. The author
	expects advantages from testing these end-to-end chains with this
	approach.},
  doi = {10.1109/TAICPART.2009.25},
  keywords = {E-Cab;automatic test case generation;domain specific language;end-to-end
	chain;model based testing;system under test;automatic test pattern
	generation;}
}

@INPROCEEDINGS{5350025,
  author = {Haschemi, S. and Wider, A.},
  title = {An Extensible Language for Service Dependency Management},
  booktitle = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th
	Euromicro Conference on},
  year = {2009},
  pages = {470 -473},
  month = {aug.},
  abstract = {Service dependency management in service-oriented component platforms
	is described with languages, which cannot be easily adapted to domain-specific
	requirements. This prevents the development of language concepts,
	which are more suitable for the cognitive space and intuition of
	domain experts than general-purpose languages. We use a model-driven
	approach to create an extensible language supporting the creation
	of new language concepts. With this approach, languages for service
	dependency management can be extended to have the desired concepts
	and expressiveness.},
  doi = {10.1109/SEAA.2009.62},
  issn = {1089-6503},
  keywords = {cognitive space;domain experts;domain-specific requirements;extensible
	language;general-purpose languages;model-driven approach;service
	dependency management;service-oriented component platforms;cognitive
	systems;expert systems;object-oriented programming;software engineering;specification
	languages;}
}

@INPROCEEDINGS{4626848,
  author = {Haugen, O. and Moller-Pedersen, B. and Oldevik, J. and Olsen, G.K.
	and Svendsen, A.},
  title = {Adding Standardized Variability to Domain Specific Languages},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {139 -148},
  month = {sept.},
  abstract = {We show how a common language of variability can be used to enhance
	the expressiveness of a domain specific language (DSL). DSLs have
	been proposed as a mechanism for expressing variability. Variability
	between models in a given domain or of a family of systems is captured
	by language constructs, implying that all possible models in this
	language are the allowed variations. We explore the possibility of
	expressing variability in a language independently of the base modeling
	language. We explore how this works for small DSLs as well as for
	general purpose languages like UML. Implications of this approach
	are that the variability language can be standardized, and that DSLs
	do not have to include variability mechanisms.},
  doi = {10.1109/SPLC.2008.25},
  keywords = {UML;domain specific languages;standardized variability;variability
	language;Unified Modeling Language;}
}

@INPROCEEDINGS{1631061,
  author = {Hause, M.},
  title = {Domain Specific Process Modelling - Making UML Accessible},
  booktitle = {Process Modelling Using UML, 2006. The IEE Seminar on (Ref. No. 2006/11432)},
  year = {2006},
  pages = {65 -83},
  month = {march},
  abstract = {A collection of slides from author's conference presentation is given.},
  issn = {0537-9989},
  keywords = {SysML;UML;architectural framework modelling;business process modelling;domain
	specific process modelling;Unified Modeling Language;business data
	processing;software architecture;}
}

@INPROCEEDINGS{995242,
  author = {Hausmann, J.H. and Heckel, R. and Sauer, S.},
  title = {Towards dynamic meta modeling of UML extensions: an extensible semantics
	for UML sequence diagrams},
  booktitle = {Human-Centric Computing Languages and Environments, 2001. Proceedings
	IEEE Symposia on},
  year = {2001},
  pages = { 80 - 87},
  abstract = { The unified modeling language (UML) still lacks a formal and commonly
	agreed specification of its semantics that also accounts for UML's
	built-in semantic variation points and extension mechanisms. The
	semantic specification of such extensions must be formally integrated
	and consistent with the standard UML semantics without changing the
	latter. Feasible semantic approaches must thus allow advanced UML
	modelers to define domain-specific language extensions in a precise,
	yet usable manner. We proposed dynamic meta modeling for specifying
	operational semantics of UML behavioral diagrams based on UML collaboration
	diagrams that are interpreted as graph transformation rules. Herein
	we show how this approach can be advanced to specify the semantics
	of UML extensions. As a case study we specify the operational semantics
	of UML sequence diagrams and extend this specification to include
	features for modeling multimedia applications.},
  doi = {10.1109/HCC.2001.995242},
  issn = { },
  keywords = { UML semantics; domain-specific language; dynamic meta modeling; graph
	transformation; graph transformation rules; multimedia; semantics
	specification; sequence diagrams; synchronization; unified modeling
	language; formal specification; multimedia computing; programming
	language semantics; simulation languages; specification languages;
	synchronisation;}
}

@ARTICLE{940728,
  author = {Havelund, K. and Lowry, M. and Penix, J.},
  title = {Formal analysis of a space-craft controller using SPIN},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2001},
  volume = {27},
  pages = {749 -765},
  number = {8},
  month = {aug},
  abstract = {The paper documents an application of the finite state model checker
	SPIN to formally analyze a multithreaded plan execution module. The
	plan execution module is one component of NASA's New Millennium Remote
	Agent, an artificial intelligence-based spacecraft control system
	architecture which launched in October of 1998 as part of the DEEP
	SPACE 1 mission. The bottom layer of the plan execution module architecture
	is a domain specific language, named ESL (Executive Support Language),
	implemented as an extension to multithreaded COMMON LISP. ESL supports
	the construction of reactive control mechanisms for autonomous robots
	and spacecraft. For the case study, we translated the ESL services
	for managing interacting parallel goal-and-event driven processes
	into the PROMELA input language of SPIN. A total of five previously
	undiscovered concurrency errors were identified within the implementation
	of ESL. According to the Remote Agent programming team, the effort
	has had a major impact, locating errors that would not have been
	located otherwise and, in one case, identifying a major design flaw.
	In fact, in a different part of the system, a concurrency bug identical
	to one discovered by this study escaped testing and caused a deadlock
	during an in-flight experiment, 96 million kilometers from Earth.
	The work additionally motivated the introduction of procedural abstraction
	in terms of inline procedures into SPIN},
  doi = {10.1109/32.940728},
  issn = {0098-5589},
  keywords = {DEEP SPACE 1 mission;ESL services;Executive Support Language;New Millennium
	Remote Agent;PROMELA input language;Remote Agent programming team;SPIN;artificial
	intelligence-based spacecraft control system architecture;autonomous
	robots;concurrency bug;concurrency errors;design flaw;domain specific
	language;finite state model checker;formal analysis;in-flight experiment;inline
	procedures;interacting parallel goal-and-event driven processes;multithreaded
	COMMON LISP;multithreaded plan execution module;plan execution module
	architecture;procedural abstraction;reactive control mechanisms;spacecraft
	controller;aerospace control;intelligent control;multi-threading;program
	verification;software agents;space vehicles;}
}

@INPROCEEDINGS{941665,
  author = {Hayase, T. and Ikeda, N. and Matsumoto, K.},
  title = {A three-view model for developing object-oriented frameworks},
  booktitle = {Technology of Object-Oriented Languages and Systems, 2001. TOOLS
	39. 39th International Conference and Exhibition on},
  year = {2001},
  pages = {108 -119},
  abstract = {This paper describes a three-view model for developing object-oriented
	frameworks. We propose a new methodology based on this model, and
	demonstrate its effectiveness using an example of practical industrial
	applications. This model can lead software engineers to a framework
	that has high reliability, portability, and maintainability. These
	quality factors of applications are especially important in a domain
	that has repeatability and changeability of hardware and software
	environment. The three-view model consists of a domain analysis view,
	a layer view, and a mechanism view. The domain analysis view is used
	to clarify all information and domain knowledge by using a new concept
	that we call Domain Reference Models (DRM), which reference models
	for modeling domain-specific objects, so that a framework has high
	reusability. The layer view is used to divide a framework into three
	layers that are piled up vertically: an infrastructure layer, a generic
	layer, and a domain layer. Because software engineers can replace
	a part of the framework for restriction on implementation, the framework
	has high portability. The mechanism view is used to decide which
	mechanism of whitebox frameworks or blackbox frameworks. By using
	this guideline, the framework has high maintainability. We applied
	our approach to the framework development for an industrial monitoring
	applications. By developing a prototype, we have a prospect of withdrawing
	the development costs of the framework by five or six times of application
	deployment. In this application, we estimate several ten times of
	application deployment. Therefore, it is effective for software engineers
	to develop a framework based on the three-view model},
  doi = {10.1109/TOOLS.2001.941665},
  keywords = {Domain Reference Models;domain analysis view;maintainability;object-oriented
	frameworks;portability;reliability;reuse technique;three-view model;object-oriented
	methods;software reusability;}
}

@INPROCEEDINGS{5741251,
  author = {Hayase, Y. and Kashima, Y. and Manabe, Y. and Inoue, K.},
  title = {Building Domain Specific Dictionaries of Verb-Object Relation from
	Source Code},
  booktitle = {Software Maintenance and Reengineering (CSMR), 2011 15th European
	Conference on},
  year = {2011},
  pages = {93 -100},
  month = {march},
  abstract = {An identifier is an important key in mapping program elements onto
	domain knowledge for the purpose of program comprehension. Therefore,
	if identifiers in a program have inappropriate names, developers
	can waste a lot of time trying to understand the program. This paper
	proposes a method for extracting and gathering verb-object (V-O)
	relations, as good examples of naming, from source code written in
	an object-oriented programming language. For each of several application
	domains, dictionaries containing the V-O relations are built and
	evaluated by software developers. The evaluation results confirm
	that the relations in the dictionaries are adequate in many cases.},
  doi = {10.1109/CSMR.2011.15},
  issn = {1534-5351},
  keywords = {domain specific dictionary;object-oriented programming language;program
	comprehension;source code;verb-object relation;object-oriented languages;programming
	languages;software maintenance;}
}

@INPROCEEDINGS{1022253,
  author = {He, D. and Muller, G. and Lawall, J.L.},
  title = {Distributing MPEG movies over the Internet using programmable networks},
  booktitle = {Distributed Computing Systems, 2002. Proceedings. 22nd International
	Conference on},
  year = {2002},
  pages = { 161 - 170},
  abstract = { Distributing video over the Internet is an increasingly important
	application. Nevertheless, the real-time and high bandwidth requirements
	of video make video distribution over today's Internet a challenge.
	Adaptive approaches can be used to respond to changes in bandwidth
	availability while limiting the effect of such changes on perceptual
	quality and resource consumption. Nevertheless, most existing adaptation
	mechanisms have limited scalability and do not effectively exploit
	the heterogeneity of the Internet. In this paper, we describe the
	design and implementation of a MPEG video broadcasting service based
	on active networks. In an active network, routers can be programmed
	to make routing decisions based on local conditions. Because decisions
	are made locally, adaptation reacts rapidly to changing conditions
	and is unaffected by conditions elsewhere in the network. Programmability
	allows the adaptation policy to be tuned to the structure of the
	transmitted data, and to the properties of local clients. We use
	the PLAN-P domain-specific language for programming active routers;
	this language provides high-level abstractions and safety guarantees
	that allow complex protocols to be developed rapidly and reliably.
	Our experiments show that our approach to video distribution permits
	the decoding of up to 9 times as many frames in a heavily loaded
	network as distribution using standard routers.},
  doi = {10.1109/ICDCS.2002.1022253},
  issn = {1063-6927 },
  keywords = { Internet; MPEG movie distribution; MPEG video broadcasting service;
	PLAN-P domain-specific language; active networks; adaptation policy;
	complex protocols; decoding; heavily loaded network; high-level abstractions;
	local clients; local conditions; programmability; programmable networks;
	real-time high bandwidth requirements; routers; routing decisions;
	safety guarantees; Internet; broadcasting; client-server systems;
	protocols; telecommunication network routing; video servers;}
}

@INPROCEEDINGS{4722390,
  author = {Yanxiang He and Liang Zhao and Fei Li and Zhao Wu},
  title = {Formal Modeling of Transaction Behavior in WS-BPEL},
  booktitle = {Computer Science and Software Engineering, 2008 International Conference
	on},
  year = {2008},
  volume = {3},
  pages = {490 -494},
  month = {dec.},
  abstract = {Web Services can be composed to build domain-specific application
	and solution. Some Web Services Composition (WSC) standards are proposed,
	for instance, WS-BPEL and WS-CDL. WS-BPEL consists of basic activity,
	structured activity and control-flow. It uses control-flow to construct
	sequence, branching, parallelism, synchronization, etc. This paper
	proposes an approach to formally analyze transaction behavior in
	WS-BPEL through General Stochastic High-level Petri Net (GSHLPN).
	Unlike other approaches based on Petri Net, our approach can simulate
	the interrupt behavior resulted from the trigger of failure and user
	exception operation in WS-BPEL, and accurately compute the time of
	process, whereas other approaches can not effectively do so.},
  doi = {10.1109/CSSE.2008.873},
  keywords = {WS-BPEL;WS-CDL;Web services composition;basic activity;control flow;domain-specific
	application;failure trigger;formal modeling;general stochastic high-level
	Petri net;structured activity;transaction behavior;user exception
	operation;Petri nets;Web services;high level languages;stochastic
	processes;}
}

@INPROCEEDINGS{6058739,
  author = {Headrick, W.J. and Davis, T.W. and Bodkin, M.A. and Dusch, K. and
	Fox, R.R. and Wolfe, D.},
  title = {Signal Based Domain Specific Language (SBDSL) a proposal for a next
	generation test},
  booktitle = {AUTOTESTCON, 2011 IEEE},
  year = {2011},
  pages = {240 -244},
  month = {sept.},
  abstract = {Signal Based Domain Specific Language (SBDSL) is a domain specific
	language which combines the use of ATLAS Signal statements with high-level
	programming language constructs. The goals of this new language are:
	facilitate the writing of concurrent test programs, provide a language
	that is easy to extend with new constructs, maintain backwards compatibility
	with ATLAS Family of languages, enable interoperability between test
	stations, and enable engineers' fresh out of college to quickly become
	productive with a test programming language. This paper will cover
	how the design of the SBDSL language, SBDSL Integrated Development
	Environment (IDE) and runtime executable will accomplish these goals
	and present results from the technology demonstration developed.},
  doi = {10.1109/AUTEST.2011.6058739},
  issn = {1088-7725},
  keywords = {ATLAS language family;ATLAS signal statements;SBDSL integrated development
	environment;backwards compatibility;concurrent test programs;high
	level programming language constructs;interoperability;next generation
	test;signal based domain specific language;test programming language;open
	systems;program testing;specification languages;}
}

@INPROCEEDINGS{5298551,
  author = {Hedegaard, S. and Houen, S. and Simonsen, J.G.},
  title = {LAIR: A Language for Automated Semantics-Aware Text Sanitization
	Based on Frame Semantics},
  booktitle = {Semantic Computing, 2009. ICSC '09. IEEE International Conference
	on},
  year = {2009},
  pages = {47 -52},
  month = {sept.},
  abstract = {We present LAIR: A domain-specific language that enables users to
	specify actions to be taken upon meeting specific semantic frames
	in a text, in particular to rephrase and redact the textual content.
	While LAIR presupposes superficial knowledge of frames and frame
	semantics, it requires only limited prior programming experience.
	It neither contain scripting or I/O primitives, nor does it contain
	general loop constructions and is not Turing-complete. We have implemented
	a LAIR compiler and integrated it in a pipeline for automated redaction
	of web pages. We detail our experience with automated redaction of
	web pages for subjectively undesirable content; initial experiments
	suggest that using a small language based on semantic recognition
	of undesirable terms can be highly useful as a supplement to traditional
	methods of text sanitization.},
  doi = {10.1109/ICSC.2009.79},
  keywords = {LAIR compiler;automated Web page redaction;automated semantics-aware
	text sanitization;domain-specific language;frame semantics;language
	for automatically inferred redaction;semantic recognition;textual
	content;Internet;computational linguistics;natural languages;program
	compilers;text analysis;}
}

@ARTICLE{5613453,
  author = {Heer, J. and Bostock, M.},
  title = {Declarative Language Design for Interactive Visualization},
  journal = {Visualization and Computer Graphics, IEEE Transactions on},
  year = {2010},
  volume = {16},
  pages = {1149 -1156},
  number = {6},
  month = {nov.-dec. },
  abstract = {We investigate the design of declarative, domain-specific languages
	for constructing interactive visualizations. By separating specification
	from execution, declarative languages can simplify development, enable
	unobtrusive optimization, and support retargeting across platforms.
	We describe the design of the Protovis specification language and
	its implementation within an object-oriented, statically-typed programming
	language (Java). We demonstrate how to support rich visualizations
	without requiring a toolkit-specific data model and extend Protovis
	to enable declarative specification of animated transitions. To support
	cross-platform deployment, we introduce rendering and event-handling
	infrastructures decoupled from the runtime platform, letting designers
	retarget visualization specifications (e.g., from desktop to mobile
	phone) with reduced effort. We also explore optimizations such as
	runtime compilation of visualization specifications, parallelized
	execution, and hardware-accelerated rendering. We present benchmark
	studies measuring the performance gains provided by these optimizations
	and compare performance to existing Java-based visualization tools,
	demonstrating scalability improvements exceeding an order of magnitude.},
  doi = {10.1109/TVCG.2010.144},
  issn = {1077-2626},
  keywords = {Java;Protovis specification language;animated transition;declarative
	domain-specific language;declarative language design;event handling;hardware-accelerated
	rendering;interactive visualization;object-oriented language;parallelized
	execution;runtime compilation;statically-typed programming language;unobtrusive
	optimization;visualization specification;Java;data visualisation;interactive
	programming;object-oriented programming;rendering (computer graphics);specification
	languages;}
}

@INPROCEEDINGS{994484,
  author = {Heering, J. and Mernik, M.},
  title = {Domain-specific languages for software engineering},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = { 3649 - 3650},
  month = {jan.},
  abstract = {Not available},
  doi = {10.1109/HICSS.2002.994484}
}

@INPROCEEDINGS{1174888,
  author = {Heering, J. and Mernik, M. and Sloane, A.M.},
  title = {Domain-specific languages},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 323},
  month = {jan.},
  abstract = {Not available},
  doi = {10.1109/HICSS.2003.1174888},
  issn = { }
}

@INPROCEEDINGS{6070373,
  author = {Hegedus, Abel and Horvath, Akos and Rath, Istvan and Branco, Moises
	Castelo and Varro, Daniel},
  title = {Quick fix generation for DSMLs},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2011 IEEE
	Symposium on},
  year = {2011},
  pages = {17 -24},
  month = {sept.},
  abstract = {Domain-specific modeling languages (DSML) proved to be an important
	asset in creating powerful design tools for domain experts. Although
	these tools are capable of preserving the syntax-correctness of models
	even during free-hand editing, they often lack the ability of maintaining
	model consistency for complex language-specific constraints. Hence,
	there is a need for a tool-level automatism to assist DSML users
	in resolving consistency violation problems. In this paper, we describe
	an approach for the automatic generation of quick fixes for DSMLs,
	taking a set of domain-specific constraints and model manipulation
	policies as input. The computation relies on statespace exploration
	techniques to find sequences of operations that decrease the number
	of inconsistencies. Our approach is illustrated using a BPMN case
	study, and it is evaluated by several experiments to show its feasibility
	and performance.},
  doi = {10.1109/VLHCC.2011.6070373},
  issn = {1943-6092}
}

@INPROCEEDINGS{5598116,
  author = {Heijstek, W. and Chaudron, M.R.V.},
  title = {The Impact of Model Driven Development on the Software Architecture
	Process},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2010 36th
	EUROMICRO Conference on},
  year = {2010},
  pages = {333 -341},
  month = {sept.},
  abstract = {While Model-Driven Development (MDD) is an increasingly popular software
	development approach, its impact on the development process in large-scale,
	industrial practice is not yet clear. For this study the application
	of MDD in a large-scale industrial software development project is
	analyzed over a period of two years. Applying a grounded theory approach
	we identified 14 factors which impact the architectural process.
	We found that scope creep is more likely to occur, late changes can
	imply more extensive rework and that business engineers need to be
	more aware of the technical impact of their decisions. In addition,
	the introduced Domain-Specific Language (DSL) provides a new common
	idiom that can be used by more team members and will ease communication
	among team members and with clients. Also, modelers need to be much
	more explicit and complete in their descriptions. Parallel development
	of a code generator and defining a proper meta-model require additional
	time investments. Lastly, the more central role of software architecture
	design documentation requires more structured, detailed and complete
	architectural information and consequently, more frequent reviews.},
  doi = {10.1109/SEAA.2010.63},
  issn = {1089-6503},
  keywords = {code generator;domain-specific language;large-scale industrial software
	development project;model driven development;software architecture
	process;software architecture;specification languages;}
}

@INPROCEEDINGS{5718627,
  author = {Heitkoetter, H.},
  title = {Transforming PICTURE to BPMN 2.0 as Part of the Model-Driven Development
	of Electronic Government Systems},
  booktitle = {System Sciences (HICSS), 2011 44th Hawaii International Conference
	on},
  year = {2011},
  pages = {1 -10},
  month = {jan.},
  abstract = {The support of public administration processes through information
	systems is a precondition for electronic government projects. Model-driven
	development is a promising concept to overcome the challenges associated
	with developing these systems, for example changing requirements,
	diverse stakeholders and evolving processes. This paper presents
	a model-to-model transformation between PICTURE, a domain-specific
	process modeling language for the public administration sector, and
	BPMN 2.0. The transformation can be part of the model-driven development
	of electronic government systems. With the building block-based language
	PICTURE, administration processes can be modeled in a domain-specific
	way under involvement of all relevant stakeholders. BPMN, as a generic
	process modeling language, enables the generation of detailed and
	executable models from PICTURE models. Besides the individual transformation
	rules, this paper includes an overview of the implementation with
	Eclipse and QVT Operational.},
  doi = {10.1109/HICSS.2011.457},
  issn = {1530-1605},
  keywords = {BPMN 2.0;PICTURE language;block-based language;domain-specific process
	modeling language;electronic government project;electronic government
	system;generic process modeling language;information system;model-driven
	development;model-to-model transformation;public administration;government
	data processing;public administration;specification languages;}
}

@ARTICLE{4602669,
  author = {Helsen, Simon and Ryman, Arthur and Spinellis, Diomidis},
  title = {Where's My Jetpack?},
  journal = {Software, IEEE},
  year = {2008},
  volume = {25},
  pages = {18 -21},
  number = {5},
  month = {sept.-oct. },
  abstract = {Software development tools often fail to deliver on inflated promises.
	Rather than the predicted progression toward ever-increasing levels
	of abstraction, two simple trends have driven the evolution of currently
	available software development tools: integration at the source-code
	level and a focus on quality. Thus source code has become the bus
	that tools tap into for communicating with other tools. Also, focus
	has shifted from defect removal in the later phases to defect prevention
	in the earlier phases. In the future, tools are likely to support
	higher levels of abstraction, perhaps in the form of domain-specific
	languages communicated using XML.},
  doi = {10.1109/MS.2008.138},
  issn = {0740-7459}
}

@INPROCEEDINGS{4291016,
  author = {Hemingway, G. and Hang Su and Kai Chen and Koo, T.J.},
  title = {A Semantic Anchoring Infrastructure for the Design of Embedded Systems},
  booktitle = {Computer Software and Applications Conference, 2007. COMPSAC 2007.
	31st Annual International},
  year = {2007},
  volume = {1},
  pages = {287 -294},
  month = {july},
  abstract = {Embedded systems are a key enabling technology for the recent vast
	increase in functionality of a huge list of critical infrastructures.
	Hybrid automata can be used to model system-level behaviors for the
	large category of systems that exhibit strong couplings between discrete
	and continuous dynamics. Many software tools have been developed
	for hybrid automata to enable model-based design of embedded systems
	and these software tools are constructed by using their own modeling
	languages. Model-based design frameworks, such as model-integrated
	computing (MIC), model driven architecture (MDA), and model driven
	design (MDD), have been advocated to raise the level of abstraction
	in software tool design by placing stronger emphasis on the use of
	software models in the software tool design process. In particular,
	MIC places strong emphasis on the use of domain specific modeling
	languages (DSMLs) and model transformations in design flows. Practical
	and effective development of formal specifications for DSML semantics
	within model-based tools can be challenging, but could positively
	impact adoption and reuse of these tools. The semantic anchoring
	methodology was developed to address this challenge by formally tying
	DSMLs to a "semantic unit", which is a formal specification that
	captures the operational semantics of a specific model of computation.
	Leveraging our prior work with semantic units, we develop a semantic
	unit for hybrid automata. In this paper, we explicitly specify the
	operational semantics of hybrid automata, and develop the corresponding
	semantic unit and model transformation rules. We demonstrate the
	effectiveness of the infrastructure in a practical case study involving
	the hybrid automata DSMLs, HyVisual and ReachLab.},
  doi = {10.1109/COMPSAC.2007.39},
  issn = {0730-3157},
  keywords = {domain specific modeling languages;embedded systems;formal specifications;hybrid
	automata;model driven architecture;model driven design;model-based
	design;model-integrated computing;semantic anchoring infrastructure;software
	tools;automata theory;embedded systems;formal specification;software
	tools;specification languages;systems analysis;}
}

@ARTICLE{5076457,
  author = {Hen-Tov, A. and Lorenz, D.H. and Pinhasi, A. and Schachter, L.},
  title = {ModelTalk: When Everything Is a Domain-Specific Language},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {39 -46},
  number = {4},
  month = {july-aug. },
  abstract = {Large-scale, complex, back-end business applications such as telecommunications
	software constitute a highly competitive and demanding market. These
	applications feature deep integration with other business and operational
	support systems. They must be tailored for each customer, and the
	customized systems must meet strict extrafunctional requirements,
	commonly called "telco's five 9s" (99.999 percent availability).
	All this, combined with the need for an agile development process
	with a short time to market, presents a software development and
	business challenge. ModelTalk, model- driven software development
	framework, supports the creation of product lines by using domain-specific
	languages pervasively at the core of the development process.},
  doi = {10.1109/MS.2009.97},
  issn = {0740-7459},
  keywords = {ModelTalk-model-driven software development;agile development process;back-end
	business application;domain-specific language;operational support
	system;reusable software component;software product lines;business
	data processing;object-oriented programming;product development;software
	reusability;specification languages;}
}

@INPROCEEDINGS{5561578,
  author = {Tan Lai Heng and Lim Tong Ming},
  title = {Using multi-coordinated views with agent communication protocol to
	detect and resolve inconsistent requirements to improve accuracy},
  booktitle = {Information Technology (ITSim), 2010 International Symposium in},
  year = {2010},
  volume = {2},
  pages = {1041 -1044},
  month = {june},
  abstract = {There are few challenges with eliciting requirements from stakeholders.
	Firstly, inconsistent requirements are usually not realized by stakeholders
	at the point of requirement specification compilation. Secondly,
	existing inconsistency checking techniques are usually mathematically
	or formally validated in a non-collaborative environment which is
	less practical and not productive for multiple stakeholders. In this
	research, a proposed requirements conflict detection and resolution
	solution is provided. The requirements from multiple stakeholders
	are captured as views on the functional requirements of a business
	domain using a proposed Semi-formal Ontology Driven Domain-Specific
	Requirements Specification Language. The proposed language will capture
	other relevant requirements includes domain, actors, relationships,
	processes, condition, and data fields to enable consistency checking.
	The proposed solution enables proper consistency checking in two
	ways: (1) Inter-Check (check within captured ontology), and (2) Intra-Check
	(check among stakeholders' views) so that stakeholders can resolve
	detected conflicts collaboratively through Agent Communication Protocol.
	Lab test will be conducted with a selected group of novice users
	to measure the learning curve (hours) for adopting the proposed solution
	as well as the accuracy and the consistency. A few set of requirements
	will be given to the group of selected users and a prototype for
	proposed solution will be produced. The values of this research are:
	(1) to improve the understanding of how much accuracy the inconsistent
	requirements detection is helps stakeholders and requirements analysts
	improve their effort in requirements conflicts detection, (2) to
	improve the understanding of what inconsistent requirements are detected
	during elicitation helps stakeholders improve the awareness of requirements
	conflicts.},
  doi = {10.1109/ITSIM.2010.5561578},
  issn = {2155-897},
  keywords = {agent communication protocol;conflict detection;consistency checking;formal
	validation;inconsistency checking techniques;learning curve;multicoordinated
	views;multiple stakeholder eliciting requirement;noncollaborative
	environment;resolution solution;semiformal ontology driven domain
	specific requirement specification language;formal specification;formal
	verification;groupware;knowledge acquisition;protocols;software engineering;specification
	languages;}
}

@INPROCEEDINGS{6034952,
  author = {Hennig, S. and Braune, A.},
  title = {Sustainable visualization solutions in industrial automation with
	Movisa CHARPx2014; A case study},
  booktitle = {Industrial Informatics (INDIN), 2011 9th IEEE International Conference
	on},
  year = {2011},
  pages = {634 -639},
  month = {july},
  abstract = {Current visualization systems in industrial automation mostly rely
	on dedicated runtime environments making the migration of visualization
	solutions from one platform to another almost impossible. On the
	other hand, an increasing number of different platforms has to be
	used also in industrial automation, to monitor or to operate the
	technical process. We propose with Movisa a model-driven approach
	to the development of sustainable visualization solutions in industrial
	automation. Movisa is a Domain Specific Language designed to capture
	only functional contents of visualization solutions. This paper presents
	Movisa by introducing its main concepts in a case study. It will
	also point out the potential of model-driven approaches concerning
	the sustainability of visualization solution in industrial automation.},
  doi = {10.1109/INDIN.2011.6034952},
  keywords = {Movisa language;domain specific language;industrial automation;sustainable
	visualization;data visualisation;manufacturing data processing;manufacturing
	systems;programming languages;}
}

@INPROCEEDINGS{5641320,
  author = {Hennig, S. and Braune, A. and Koycheva, E.},
  title = {Towards a model driven approach for development of visualization
	applications in industrial automation},
  booktitle = {Emerging Technologies and Factory Automation (ETFA), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {1 -6},
  month = {sept.},
  abstract = {The eXtensible Visualization Components Markup Language (XVCML) was
	developed in order to ensure the sustainability of visualization
	solutions. Therefore, XVCML follows the Model-Driven Software Development
	(MDSD) approach: It enables technology independent modeling of visualization
	solutions whereas those models are compliant to a formal metamodel.
	XVCML was enhanced by a mean to express arbitrary routines using
	Executable UML. Since we realized the latter aspect only as a first
	proof of concept, this part of XVCML lacks a formal metamodel-a requirement
	for MDSD. This paper seizes on this topic so much that a Domain Specific
	Language, which enables the creation of Executable UML models, will
	be worked out in the context of MDSD. Therefore, this paper explains
	the required foundations for the subject matter at hand before it
	presents the way of proceeding towards a model-driven approach with
	XVCML.},
  doi = {10.1109/ETFA.2010.5641320},
  issn = {1946-0740},
  keywords = {XVCML language;extensible visualization components markup language;formal
	metamodel;industrial automation;model-driven software development;visualization
	application;XML;data visualisation;formal specification;formal verification;production
	engineering computing;}
}

@ARTICLE{4543984,
  author = {Henriksson, J. and Heidenreich, F. and Johannes, J. and Zschaler,
	S. and Assmann, U.},
  title = {Extending grammars and metamodels for reuse: the Reuseware approach},
  journal = {Software, IET},
  year = {2008},
  volume = {2},
  pages = {165 -184},
  number = {3},
  month = {june },
  abstract = {The trend towards domain-specific languages leads to an ever-growing
	plethora of highly specialised languages. Developers of such languages
	focus on their specific domains rather than on the technical challenges
	of language design. The generic features of languages are rarely
	included in special-purpose languages. One very important feature
	is the ability to formulate partial programs in separate encapsulated
	entities, which can be composed into complete programs in a well-defined
	manner. A language-independent approach is presented that adds useful
	constructs for defining components. The authors discuss the underlying
	concepts and describe a composition environment and tool supporting
	these ideas-the reuseware composition framework. To evaluate this
	approach, the authors enrich the (Semantic) Web query language Xcerpt
	with an additional useful reuse concept - modules.},
  doi = {10.1049/iet-sen:20070060},
  issn = {1751-8806},
  keywords = {data encapsulation;domain-specific languages;grammar;metamodel;semantic
	Web query language;software component;software reuse;data encapsulation;grammars;object-oriented
	programming;software reusability;}
}

@INPROCEEDINGS{1225416,
  author = {Henriques, P.R. and Kosar, T. and Mernik, M. and Pereira, M.J.V.
	and Zumer, V.},
  title = {Grammatical approach to problem solving},
  booktitle = {Information Technology Interfaces, 2003. ITI 2003. Proceedings of
	the 25th International Conference on},
  year = {2003},
  pages = { 645 - 650},
  month = {june},
  abstract = { We present a grammatical approach to problem solving. It supports
	formal software specification using attribute grammars, from which
	a rapid prototype can be generated as well the incremental software
	development. Domain concepts and relationships among them have to
	be identified from a problem statement and represented as a context-free
	grammar. The obtained context-free grammar describes the syntax of
	a domain-specific language whose semantics is the same as the functionality
	of the system under implementation. The semantics of this language
	is then described using attribute grammars from which a compiler
	is automatically generated. The execution of a particular program
	written in a domain-specific language corresponds to the execution
	of a prototype of a system on a particular use-case.},
  doi = {10.1109/ITI.2003.1225416},
  issn = {1330-1012 },
  keywords = { attribute grammars; context-free grammar; domain- specific language;
	formal software specification; grammatical problem solving approach;
	program compiler; programming language semantics; rapid prototype
	generation; software design; software development; software modelling;
	attribute grammars; context-free grammars; formal specification;
	program compilers; programming language semantics; software prototyping;}
}

@INPROCEEDINGS{6005451,
  author = {van Heuven van Staereling, R. and Appuswamy, R. and van Moolenbroek,
	D.C. and Tanenbaum, A.S.},
  title = {Efficient, Modular Metadata Management with Loris},
  booktitle = {Networking, Architecture and Storage (NAS), 2011 6th IEEE International
	Conference on},
  year = {2011},
  pages = {278 -287},
  month = {july},
  abstract = {With the amount of data increasing at an alarming rate, domain-specific
	user-level metadata management systems have emerged in several application
	areas to compensate for the shortcomings of file systems. Such systems
	provide domain-specific storage formats for performance-optimized
	metadata storage, search-based access interfaces for enabling declarative
	queries, and type-specific indexing structures for performing scalable
	search over metadata. In this paper, we highlight several issues
	that plague these user-level systems. We then show how integrating
	metadata management into the Loris stack solves all these problems
	by design. In doing so, we show how the Loris stack provides a modular
	framework for implementing domain-specific solutions by presenting
	the design of our own Loris-based metadata management system that
	provides 1) LSM-tree-based metadata storage, 2) an indexing infrastructure
	that uses LSM-trees for maintaining real-time attribute indices,
	and 3) scalable metadata querying using an attribute-based query
	language.},
  doi = {10.1109/NAS.2011.32},
  keywords = {LSM-tree-based metadata storage;Loris stack;attribute-based query
	language;declarative queries;domain- specific storage formats;domain-specific
	user-level metadata management systems;metadata querying;performance-optimized
	metadata storage;search-based access interfaces;type-specific indexing
	structures;database indexing;meta data;query languages;query processing;storage
	management;tree data structures;}
}

@INPROCEEDINGS{5070990,
  author = {Hidaka, S. and Hu, Z. and Kato, H. and Nakano, K.},
  title = {A compositional approach to bidirectional model transformation},
  booktitle = {Software Engineering - Companion Volume, 2009. ICSE-Companion 2009.
	31st International Conference on},
  year = {2009},
  pages = {235 -238},
  month = {may},
  abstract = {Bidirectional model transformation plays an important role in maintaining
	consistency between two models, and has many potential applications
	in software development, including model synchronization, round-trip
	engineering, software evolution, multiple-view software development,
	and reverse engineering. However, unclear bidirectional semantics,
	domain-specific bidirectionalization method, and lack of systematic
	development framework are known problems that prevent it from being
	practically used. In this paper, we propose a novel compositional
	framework for bidirectional model transformation based on an existing
	graph querying language UnQL, so that one can develop various useful
	bidirectional model transformation by combination of a fixed number
	of primitive bidirectional model transformations. We have implemented
	a prototype system, and the experimental results show promise of
	the new approach.},
  doi = {10.1109/ICSE-COMPANION.2009.5070990},
  keywords = {UnQL;bidirectional model transformation;bidirectional semantics;compositional
	approach;domain-specific bidirectionalization method;graph querying
	language;model synchronization;multiple-view software development;reverse
	engineering;round-trip engineering;software evolution;query languages;reverse
	engineering;software engineering;}
}

@INPROCEEDINGS{336763,
  author = {Hildreth, H.},
  title = {Reverse engineering requirements for process-control software},
  booktitle = {Software Maintenance, 1994. Proceedings., International Conference
	on},
  year = {1994},
  pages = {316 -325},
  month = {sep},
  abstract = {A method of reverse engineering requirements for process-control system
	software is presented, along with a domain-specific functional structure.
	Techniques are demonstrated on the executable pseudocode of a commercial
	avionics control system. Resulting requirements are expressed as
	a state-based model of externally visible behavior specified completely
	in the language of process control},
  doi = {10.1109/ICSM.1994.336763},
  keywords = { commercial avionics control system; domain-specific functional structure;
	executable pseudocode; externally visible behavior; process-control
	software; reverse engineering requirements; state-based model; aerospace
	computer control; aerospace computing; aircraft instrumentation;
	process computer control; process control; software maintenance;}
}

@INPROCEEDINGS{5753594,
  author = {Hill, J.H.},
  title = {Modeling Interface Definition Language Extensions (IDL3+) Using Domain-Specific
	Modeling Languages},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	(ISORC), 2011 14th IEEE International Symposium on},
  year = {2011},
  pages = {75 -82},
  month = {march},
  abstract = {Model-driven engineering (MDE) of distributed real-time and embedded
	(DRE) systems built using distributed middleware technologies typically
	rely on interface definition language (IDL) to define interfaces
	and attributes of the system under development. Recent needs for
	using IDL to design and implement systems composed of heterogeneous
	communication architectures, however, has realized the limitations
	of IDL. To address these limitations, vendors have proposed several
	non-trivial extensions to IDL also known as IDL3+. In order to leverage
	such extensions in the modeling domain, it is necessary to update
	existing tools, e.g., domain-specific modeling languages) to support
	such extensions. This paper provides two contributions to MDE of
	DRE systems using domain-specific modeling languages (DSMLs). First,
	this paper highlights the technical challenges associated with modeling
	IDL3+. Secondly, this paper discusses how to overcome such challenges
	in the context of a representative DSML for modeling DRE systems
	designed and implemented using IDL3+. Experience gained from using
	DSMLs to model IDL3+ shows that DSML environments as is do not suffice
	and need improved application frameworks to support complex DSMLs,
	such as IDL3+.},
  doi = {10.1109/ISORC.2011.19},
  issn = {1555-0885},
  keywords = {DRE systems;DSML;IDL3+;distributed middleware technologies;distributed
	real-time systems;domain-specific modeling languages;embedded systems;interface
	definition language extensions;model-driven engineering;embedded
	systems;middleware;software engineering;specification languages;}
}

@INPROCEEDINGS{5934812,
  author = {Hill, J.H.},
  title = {Measuring and Reducing Modeling Effort in Domain-Specific Modeling
	Languages with Examples},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {120 -129},
  month = {april},
  abstract = {Domain-specific modeling languages (DSMLs) facilitate rapid and ``correct-by-construction''
	realization of concepts for the target domain. Although DSMLs provide
	such benefits, there is implied (or hidden) modeling effort---in
	terms of user actions---associated with using a DSML that can negatively
	impact its effectiveness. It is therefore critical that DSML developers
	understand the meaning of modeling effort and how to reduce it so
	their DSML is of high quality. This paper provides two contributions
	to research on developing DSMLs. First, the paper defines a metric
	for measuring model effort. Secondly, this paper discusses several
	techniques, with examples, reducing (or improving) modeling effort.
	The techniques discussed in the paper have been applied to an open-source
	DSML called the Platform Independent Component Modeling Language
	(PICML), which is currently used in both academic and industry settings
	for designing and implementing large-scale distributed systems. Finally,
	results show that it is possible to reduce modeling effort without
	requiring user studies to analyze such concerns.},
  doi = {10.1109/ECBS.2011.22},
  keywords = {DSML developers;correct-by-construction realization;domain specific
	modeling language;large-scale distributed system;modeling effort
	reduction;open source DSML;platform independent component modeling
	language;distributed processing;object-oriented languages;public
	domain software;simulation languages;}
}

@INPROCEEDINGS{4536573,
  author = {Hill, J.H. and Gokhale, A.},
  title = {Model-driven specification of component-based distributed real-time
	and embedded systems for verification of systemic QoS properties},
  booktitle = {Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International
	Symposium on},
  year = {2008},
  pages = {1 -8},
  month = {april},
  abstract = {The adage "the whole is not equal to the sum of its parts" is very
	appropriate in the context of verifying a range of systemic properties,
	such as deadlocks, correctness, and conformance to quality of service
	(QoS) requirements, for component-based distributed real-time and
	embedded (DRE) systems. For example, end-to-end worst case response
	time (WCRT) in component-based DRE systems is not as simple as accumulating
	WCRT for each individual component in the system because of inherent
	complexities introduced by the large solution space of possible deployment
	and configurations. This paper describes a novel process and tool-based
	artifacts that simplify the formal specification of component-based
	DRE systems for verification of systemic QoS properties. Our approach
	is based on the mathematical formalism of Timed Input/Output Automata
	and uses generative programming techniques for automating the verification
	of systemic QoS properties for component-based DRE systems.},
  doi = {10.1109/IPDPS.2008.4536573},
  issn = {1530-2075},
  keywords = {component-based distributed real-time systems;conformance;correctness;deadlock;embedded
	systems;end-to-end worst case response time;formal specification;generative
	programming;mathematical formalism;model-driven specification;systemic
	QoS property verification;timed input-output automata;distributed
	object management;finite automata;formal specification;formal verification;object-oriented
	programming;software quality;}
}

@ARTICLE{5306061,
  author = {Hill, J.H. and Schmidt, D.C. and Edmondson, J.R. and Gokhale, A.S.},
  title = {Tools for Continuously Evaluating Distributed System Qualities},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {65 -71},
  number = {4},
  month = {july-aug. },
  abstract = {Developers are increasingly using service-oriented middleware to develop
	distributed systems. This middleware raises the abstraction level
	for software so that distributed-system developers can focus more
	on application-level concerns (for instance, business logic) rather
	than wrestle with infrastructure-level concerns (such as software
	adaptation, context-awareness, and life-cycle management). Service-oriented
	middleware also promotes reuse of business logic and services across
	heterogeneous application domains, thus facilitating the development
	of larger, more complex systems.},
  doi = {10.1109/MS.2009.197},
  issn = {0740-7459},
  keywords = {application-level concern;business logic;complex system;distributed-system
	qualities;heterogeneous application domain;service-oriented middleware;software
	development;middleware;software engineering;}
}

@INPROCEEDINGS{4148946,
  author = {Hill, James H. and Tambe, Sumant and Gokhale, Aniruddha},
  title = {Model-Driven Engineering for Development-Time QoS Validation of Component-Based
	Software Systems},
  booktitle = {Engineering of Computer-Based Systems, 2007. ECBS '07. 14th Annual
	IEEE International Conference and Workshops on the},
  year = {2007},
  pages = {307 -316},
  month = {march},
  abstract = {Model-driven engineering (MDE) techniques are increasingly being used
	to address many of the development and operational lifecycle concerns
	of large-scale component-based systems. One such concern lacking
	significant research deals with the validation of quality-of-service
	(QoS) properties of component-based systems throughout their development
	lifecycle instead of waiting until system integration time, which
	is very late and can be detrimental to project schedules and costs.
	This paper describes our novel MDE-based solution to address this
	challenge. At the core of our solution approach are (1) a set of
	domain-specific modeling languages that allow us to mimic component
	"business logic," and (2) a generative programming framework that
	synthesizes empirical benchmarking code for system emulation and
	continuous QoS evaluation},
  doi = {10.1109/ECBS.2007.53},
  keywords = {business logic;component-based software system;continuous QoS validation;development
	lifecycle;development-time QoS validation;domain-specific modeling
	languages;generative programming;large-scale component-based system;model-driven
	engineering;quality-of-service;object-oriented programming;program
	verification;software quality;}
}

@INPROCEEDINGS{47330,
  author = {Hirschman, L. and Palmer, M. and Dowding, J. and Dahl, D. and Linebarger,
	M. and Passonneau, R. and Land, F.-M. and Ball, C. and Weir, C.},
  title = {The PUNDIT natural-language processing system},
  booktitle = {AI Systems in Government Conference, 1989.,Proceedings of the Annual},
  year = {1989},
  pages = {234 -243},
  month = {mar},
  abstract = {The authors describe the PUNDIT (Prolog Understanding of Integrated
	Text) text-understanding system, which is designed to analyze and
	construct representations of paragraph-length text. PUNDIT is implemented
	in Quintus Prolog, and consists of distinct lexical, syntactic, semantic,
	and pragmatic components. Each component draws on one or more sets
	of data, including a lexicon, a broad-coverage grammar of English,
	semantic verb decompositions, rules mapping between syntactic and
	semantic constituents, and a domain model. Modularity, careful separation
	of declarative and procedural information, and separation of domain-specific
	and domain-independent information all contribute to a system which
	is flexible, extensible and portable. Versions of PUNDIT are now
	running in five domains, including four military and one medical},
  doi = {10.1109/AISIG.1989.47330},
  keywords = {CASREP;PUNDIT natural-language processing system;Quintus Prolog;broad-coverage
	grammar of English;domain model;domain-independent information;lexicon;modularity;portability;semantic
	verb decompositions;text-understanding system;knowledge based systems;natural
	languages;software portability;user interfaces;}
}

@INPROCEEDINGS{6032245,
  author = {Hlaoui, Y.B. and Benayed, L.J.},
  title = {A Model Transformation Approach Based on Homomorphic Mappings between
	UML Activity Diagrams and BPEL4WS Specifications of Grid Service
	Workflows},
  booktitle = {Computer Software and Applications Conference Workshops (COMPSACW),
	2011 IEEE 35th Annual},
  year = {2011},
  pages = {243 -248},
  month = {july},
  abstract = {We use a Domain Specific Language (DSL) based on UML activity diagrams
	to specify and compose systematically workflow models from Grid services.
	To be executed, workflow activity diagram models should be translated
	into BPEL4WS models which will be executed by the BPEL4WS engine.
	To reach this objective, we propose a meta-model based transformation
	from UML activity diagrams to BPEL4WS language. To ensure the correctness
	and the completion of the transformation, we propose a graph homomorphic
	mapping between the activity diagram and BPEL4WS language elements.},
  doi = {10.1109/COMPSACW.2011.51},
  keywords = {BPEL4WS specification;UML activity diagram;business process execution
	language for Web services;domain specific language;graph homomorphic
	mapping;grid service workflow;homomorphic mapping;model transformation
	approach;workflow activity diagram model;Unified Modeling Language;formal
	specification;grid computing;}
}

@INPROCEEDINGS{5362477,
  author = {Hoefler, T. and Siebert, C. and Lumsdaine, A.},
  title = {Group Operation Assembly Language - A Flexible Way to Express Collective
	Communication},
  booktitle = {Parallel Processing, 2009. ICPP '09. International Conference on},
  year = {2009},
  pages = {574 -581},
  month = {sept.},
  abstract = {The implementation and optimization of collective communication operations
	is an important field of active research. Such operations directly
	influence application performance and need to map the communication
	requirements in an optimal way to steadily changing network architectures.
	In this work, we define an abstract domain-specific language to express
	arbitrary group communication operations. We show the universality
	of this language and how all existing collective operations can be
	implemented with it. By design, it readily lends itself to blocking
	and nonblocking execution, as well as to off-loaded execution of
	complex group communication operations. We also define several offline
	and online optimizations (compiler transformations and scheduling
	decisions, respectively) to improve the overall performance of the
	operation. Performance results show that the overhead to express
	current collective operations is negligible in comparison to the
	potential gains in a highly optimized implementation.},
  doi = {10.1109/ICPP.2009.70},
  issn = {0190-3918},
  keywords = {abstract domain-specific language;collective communication;compiler
	transformations;complex group communication operations;group operation
	assembly language;network architectures;scheduling decisions;assembly
	language;groupware;program compilers;scheduling;specification languages;}
}

@INPROCEEDINGS{4724587,
  author = {Hokamura, K. and Ubayashi, N. and Nakajima, S. and Iwai, A.},
  title = {Aspect-Oriented Programming for Web Controller Layer},
  booktitle = {Software Engineering Conference, 2008. APSEC '08. 15th Asia-Pacific},
  year = {2008},
  pages = {529 -536},
  month = {dec.},
  abstract = {We propose a new domain-specific aspect-oriented programming (AOP)
	mechanism for Web application development. A variety of crosscutting
	concerns such as access control and performance tuning are found
	in typical Web applications, but it is not easy to concisely modularize
	the concerns as aspects in current AOP languages because they do
	not provide pointcut mechanisms for directly handling events in the
	Web controller layer. To deal with this problem, we propose a Web-specific
	AOP mechanism called AOWP and a PHP-based AOWP framework. Using this
	framework, a programmer can easily address Web-specific crosscutting
	concerns triggered by Web-specific events, including page requests,
	page transitions, and session management events.},
  doi = {10.1109/APSEC.2008.69},
  issn = {1530-1362},
  keywords = {Web controller layer;Web-specific events;crosscutting concerns;domain-specific
	aspect-oriented programming;page requests;page transitions;session
	management events;Internet;object-oriented programming;}
}

@INPROCEEDINGS{5561453,
  author = {Lin Cheng Hong and Lim Tong Ming},
  title = {To enable formal verification of semi-formal requirements by using
	pre-defined template and mapping rules to map to Promela specification
	to reduce rework},
  booktitle = {Information Technology (ITSim), 2010 International Symposium in},
  year = {2010},
  volume = {3},
  pages = {1393 -1397},
  month = {june},
  abstract = {Gap has always been found between semi-formal requirements and formal
	specification. Semi-formal or informal requirements are not able
	to do formal verification as imprecise and ambiguity is always found.
	The proposed research is to carry out the mapping of semi-formal
	requirements to Promela (Process Meta Language) specification in
	order to enable early verification before the requirements analysis
	process take place and to obtain highly accurate and complete requirements
	specification. The proposed solutions include a set of pre-define
	requirements templates that helps analysts to collect requirements
	and a set of mapping rules to bridge semi-formal and formal specification.
	The inputs are a set of semi-formal requirements specifications called
	Swimlane Domain-specific Requirements Language based on business
	processes, business actor, flow of processes and simple formula or
	logics within a process. The target language is Promela, a Process
	Meta language that can be verified using SPIN (Simple Promela Interpreter)
	tool to perform formal verification. Inconsistency of requirements
	will be identified before the inputs are mapped to Promela language.
	The supporting tool will be included and tested with a group of novice
	users by applying different formalization strategies like generation
	of OCL (Object Constraint Language) specification from UML (Unified
	Modeling Language) diagram set, generation of Z specification from
	UML and the proposed solution to measure the hours required to finalized
	requirements, accuracy of the generated specification and the completeness
	of requirements comparing to the prepared requirements set for measuring
	purpose.},
  doi = {10.1109/ITSIM.2010.5561453},
  issn = {2155-897},
  keywords = {Promela specification;Swimlane domain-specific requirement language;UML
	diagram set;business processes;formal specification;formal verification;informal
	requirements;object constraint language specification;predefined
	template;process meta language specification;requirements analysis
	process;semiformal requirement mapping;semiformal requirement specification;simple
	Promela interpreter tool;unified modeling language diagram set;Unified
	Modeling Language;formal specification;formal verification;program
	interpreters;}
}

@INPROCEEDINGS{1508126,
  author = {Honkola, J. and Leppanen, S. and Tynjala, T.},
  title = {Modeling the SpaceWire architecture with Lyra},
  booktitle = {Application of Concurrency to System Design, 2005. ACSD 2005. Fifth
	International Conference on},
  year = {2005},
  pages = { 15 - 24},
  month = {june},
  abstract = { We use an application domain specific design method, Lyra, to model
	a network architecture defined in the SpaceWire standard. The Lyra
	development method provides service-oriented approach to development
	of distributed communicating systems. Structurization of behavior
	into internal computation and externally observable behavior, and
	further on into different types of communication allows more efficient
	use of advanced formal verification and testing methods. The Lyra
	method has been designed particularly for industrial-strength use,
	so strong emphasis is on early development of the system-level integration
	model. The SpaceWire standard defines OSI layers 1 and 2. In this
	paper we focus on modeling the functionality of layer 2. We also
	discuss on verification of Lyra models indicating the need for more
	advanced verification methods and tools.},
  doi = {10.1109/ACSD.2005.26},
  issn = {1550-4808 },
  keywords = { Lyra development; Lyra model; OSI layer; SpaceWire standard; behavior
	structurization; distributed communicating system; externally observable
	behavior; formal testing; formal verification; layer 2 functionality;
	network architecture; system-level integration model; verification
	tool; computer networks; formal verification;}
}

@INPROCEEDINGS{5331984,
  author = {Hosking, J.},
  title = {Supporting model driven engineering using the Marama meta toolset},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops, 2009.
	EDOCW 2009. 13th},
  year = {2009},
  pages = {300},
  month = {sept.},
  abstract = {The Marama project aims to develop an accessible and efficient meta
	toolset for rapidly constructing domain specific visual languages
	and environments to support integrated model driven engineering toolchains.
	Marama supports the rapid specification of visual notations and their
	underlying meta models, behavioural constraints, and model transformations.
	In this talk, I will describe the design and development of the open
	source Marama platform, together with a variety of enterprise system-oriented
	applications developed using Marama. These include a novel overlay
	based business service modelling environment and a dynamic supply
	chain modelling tool. In addition, I will describe plans for future
	development of Marama, together with related work that is applying
	some of the lessons we have gained from Marama to knowledge management
	oriented applications.},
  doi = {10.1109/EDOCW.2009.5331984},
  keywords = {Marama meta toolset;behavioural constraint;business service modelling
	environment;domain specific visual environment;domain specific visual
	languages;dynamic supply chain modelling;enterprise system-oriented
	application;knowledge management oriented application;meta model;model
	driven engineering;model transformation;rapid specification;visual
	notation;formal specification;visual languages;visual programming;}
}

@INPROCEEDINGS{4639068,
  author = {Hosking, J. and Mehandjiev, N. and Grundy, J.},
  title = {A domain specific visual language for design and coordination of
	supply networks},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {109 -112},
  month = {sept.},
  abstract = {We have developed a domain specific visual language (DSVL) and environment
	to support the modeling of small business-based dynamic supply networks.
	We describe our approach to the design of the DSVL, challenges faced,
	the implementation of a prototype environment, and preliminary evaluation.},
  doi = {10.1109/VLHCC.2008.4639068},
  issn = {1943-6092},
  keywords = {DSVL;domain specific visual language;preliminary evaluation;prototype
	environment;small business-based dynamic supply networks;business
	data processing;production engineering computing;systems analysis;visual
	languages;}
}

@INPROCEEDINGS{5967187,
  author = {Hrncic, D. and Mernik, M.},
  title = {Memetic grammatical inference approach for DSL embedding},
  booktitle = {MIPRO, 2011 Proceedings of the 34th International Convention},
  year = {2011},
  pages = {919 -924},
  month = {may},
  abstract = {Domain-specific languages (DSLs) are often designed by domain experts
	who have no knowledge about the syntax and semantics of programming
	languages. However, they are able to write sample programs to accomplish
	their goals and illustrate the features of their language. Grammatical
	inference is a technique for inferring a context-free grammar (CFG)
	from a set of positive (and negative) samples. This paper presents
	an improved memetic algorithm for grammatical inference that may
	assist domain experts and software language engineers in developing
	DSLs by automatically producing a grammar which describes a set of
	sample DSL programs. Our approach uses local search technique with
	improved generalization step and mutation operator. Negative samples
	are also introduced to overcome overgeneralization problem. The algorithm
	was tested on several DSLs and a case study of embedding a DSL is
	presented.},
  keywords = {DSL embedding;context-free grammar;domain-specific languages;improved
	generalization step;local search technique;memetic grammatical inference
	approach;mutation operator;context-free grammars;inference mechanisms;search
	problems;specification languages;}
}

@INPROCEEDINGS{4780813,
  author = {Changjun Hu and Feng Jiao and Chongchong Zhao and Huayu Li},
  title = {A Service-Oriented Modeling Technique for Domain-Specific Software},
  booktitle = {Asia-Pacific Services Computing Conference, 2008. APSCC '08. IEEE},
  year = {2008},
  pages = {1026 -1031},
  month = {dec.},
  abstract = {Existing software for oil-drilling engineering may be developed by
	different people using different languages and technologies, run
	on different hardware platforms, use different operating systems,
	and provide very different functionalities. Therefore, it is of practical
	significance and great value to find out a modeling technique for
	integrating the software. In this paper, a goal-tree based domain-specific
	model for oil-drilling engineering is suggested, which consists of
	objects wrapping business goal and business-logic. It offers the
	benefits of extensibility, modularity, and reusability. Furthermore,
	an abstract service model with reference to Service Component Architecture
	is proposed to mapping domain-specific model into real system. The
	specialties of Service Component Architecture contribute to the integration
	of heterogeneous modules and keep the consistency of business modules
	at various phases of development.},
  doi = {10.1109/APSCC.2008.35},
  keywords = {abstract service model;business logic wrapping;domain-specific software;goal
	tree;hardware platform;heterogeneous module;oil-drilling engineering;operating
	system;service component architecture;service-oriented modeling technique;Web
	services;business data processing;object-oriented programming;oil
	drilling;software architecture;software reusability;tree data structures;}
}

@INPROCEEDINGS{5662719,
  author = {Caihua Hu and RuiSheng Zhang and TongMing Wei and RuiPeng Wei and
	ShuPing Li and Yao Cheng},
  title = {Implementing the Compiler of UDLC},
  booktitle = {Grid and Cooperative Computing (GCC), 2010 9th International Conference
	on},
  year = {2010},
  pages = {383 -387},
  month = {nov.},
  abstract = {The increasing complexity of chemical problems often requiring multiple
	chemical software work together to complete. Most chemical software
	uses different script languages to describe jobs, chemists have to
	consume lots of time to learn them before work. Unified Job-Description
	Language on Chemical Grid (UDLC) is designed to solve the problem.
	It is a domain specific-language (DSL), aims at reducing the cost
	of chemical research substantially by providing a general-purpose
	chemical job description language standard on grid. Using UDLC to
	describe a job is simple and can be translate to other chemical software
	script automatically by computer. Many heterogeneous chemical resources
	integrated in the grid can be directly invoked using UDLC. In this
	article, we focus on the three main parts in implementing the compiler
	of UDLC. Firstly, use ANTLR to build Abstract Syntax tree (AST) for
	UDLC. The AST is an intermediate form not only records the content
	of the UDLC input, but also records the structure of it. With the
	help of AST, we can get the chemical information out conveniently.
	Secondly, traverse the AST for Semantic processing. The major task
	of this part is to map the AST into chemical markup language (CML)
	file and produce jobs written by specified chemical script languages.
	Lastly, generate the target code. In this part, we insert the jobs
	into flow control sentences to make the target JAVA code which will
	be executed in the dynamic runtime we built.},
  doi = {10.1109/GCC.2010.80},
  keywords = {JAVA code;UDLC;abstract syntax tree;chemical grid;chemical markup
	language;chemical problems;chemical script languages;chemical software;compiler;domain
	specific-language;unified job-description language;Java;authoring
	languages;chemical engineering computing;grid computing;hypermedia
	markup languages;program compilers;}
}

@INPROCEEDINGS{5622272,
  author = {Haitao Hu and Guangwei Yan},
  title = {Research on DSL-based composition language in service-oriented architecture},
  booktitle = {Computer Application and System Modeling (ICCASM), 2010 International
	Conference on},
  year = {2010},
  volume = {14},
  pages = {V14-357 -V14-360},
  month = {oct.},
  abstract = {Enterprise Service Bus is the request and convergence point of all
	the services for the large-scale distributed applications. How to
	compose the service in the dynamic and open network environment is
	a difficult problem. This paper proposes a service-oriented DSL composition
	language, which provides Web service semantic description of the
	composition process. The language uses top-down composition approach
	and considers the matching of heterogeneous messages. It can be properly
	resolved the composition process granularity control and heterogeneous
	messages matching. It has some application value for loosely coupled,
	cross-platform, heterogeneous environments composition and integration
	of Web service such as e-commerce, e-government, and workflow based
	on Web services and so on.},
  doi = {10.1109/ICCASM.2010.5622272},
  keywords = {DSL based composition language;Web service semantic description;composition
	process granularity control;convergence point;e-commerce;e-government;enterprise
	service bus;heterogeneous environment composition;heterogeneous message
	matching;large scale distributed application;open network environment;service
	oriented architecture;top-down composition approach;Web services;pattern
	matching;software architecture;specification languages;}
}

@INPROCEEDINGS{5616575,
  author = {Yuh-Jong Hu and Boley, H.},
  title = {SemPIF: A Semantic Meta-policy Interchange Format for Multiple Web
	Policies},
  booktitle = {Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010
	IEEE/WIC/ACM International Conference on},
  year = {2010},
  volume = {1},
  pages = {302 -307},
  month = {31 2010-sept. 3},
  abstract = {We propose a semantics-enabled layered policy architecture for the
	exchange and management of multiple policies created by different
	policy languages on the Web. This architecture consists of four layers:
	Unifying Logic (UNL), Policy Interchange Format (PIF), Privacy Protection/DRM
	(PPD), and Domain Specific Applications (DSA). A meta-Policy Interchange
	Format (meta-PIF) layer is also introduced, side by side with the
	corresponding PIF layer, allowing agents in the facilitator to provide
	uniform services of interchange, reconciliation, and combination
	of policies. This SemPIF architecture extends W3C's Semantic Web
	architecture to permit the reuse of earlier work. A scenario of agents
	in the facilitator employing SemPIF for Digital Rights Management
	(DRM) and privacy protection policies on digital library subscription
	services will be demonstrated.},
  doi = {10.1109/WI-IAT.2010.238},
  keywords = {SemPIF architecture;digital library;digital right management;domain
	specific application;multiple Web policy;policy language;privacy
	protection;semantic meta policy interchange format;unifying logic;copy
	protection;data privacy;digital rights management;electronic data
	interchange;ontologies (artificial intelligence);semantic Web;}
}

@INPROCEEDINGS{4420483,
  author = {Peng Huang and Jiajun Bu and Chun Chen and Guang Qiu and Lijun Zhang},
  title = {Learning a Flexible Question Classifier},
  booktitle = {Convergence Information Technology, 2007. International Conference
	on},
  year = {2007},
  pages = {1608 -1613},
  month = {nov.},
  abstract = {To generate high quality answers, many question taxonomies designed
	for modern question answering systems are getting more and more complex
	and fine-grained. Furthermore, without concrete context some questions
	are ambiguous and are difficult to be correctly labeled by question
	classifier, even by people manually. All above bring a big challenge
	to current question classifiers. However, previous research seldom
	pays attention to these situations above. In general, the labeled
	question dataset is usually small, so a feasible solution to these
	issues is to integrate new feedbacks and certain domain-specific
	knowledge into current model continuously. In this paper we explore
	the application of an online learning algorithm to question classification.
	The experimental results show that the performance of our approach
	is comparable to other popular learning algorithms: SVMs and SNoW.
	Furthermore, we evaluate our approach on ambiguous questions and
	the results prove its feasibility and efficiency.},
  doi = {10.1109/ICCIT.2007.56},
  keywords = {flexible question classifier;labeled question dataset;online learning
	algorithm;question answering systems;learning (artificial intelligence);pattern
	classification;}
}

@ARTICLE{677183,
  author = {Huang, Y.-M. and Ravishankar, C.V.},
  title = {Constructive protocol specification using Cicero},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1998},
  volume = {24},
  pages = {252 -267},
  number = {4},
  month = {apr},
  abstract = {This paper describes Cicero, a set of language constructs to allow
	constructive protocol specifications. Unlike other protocol specification
	languages, Cicero gives programmers explicit control over protocol
	execution, and facilitates both sequential and parallel implementations,
	especially for protocols above the transport-layer. It is intended
	to be used in conjunction with domain-specific libraries, and is
	quite different in philosophy and mode of use from existing protocol
	specification languages. A feature of Cicero is the use of event
	patterns to control synchrony, asynchrony, and concurrency in protocol
	execution, which helps programmers build robust protocol implementations.
	Event-pattern driven execution also enables implementers to exploit
	parallelism of varying grains in protocol execution. Event patterns
	can also be translated into other formal models, so that existing
	verification techniques may be used},
  doi = {10.1109/32.677183},
  issn = {0098-5589},
  keywords = {Cicero;asynchrony;concurrency;event patterns;language constructs;protocol
	execution;protocol specification;protocol specification languages;synchrony;distributed
	processing;protocols;specification languages;}
}

@INPROCEEDINGS{1173564,
  author = {Zhisheng Huang and Raje, R.R. and Olson, A.M. and Auguston, M.},
  title = {Unified approach for system-level generative programming},
  booktitle = {Algorithms and Architectures for Parallel Processing, 2002. Proceedings.
	Fifth International Conference on},
  year = {2002},
  pages = { 136 - 142},
  abstract = { Today's and future distributed software systems will certainly require
	combining heterogeneous software components that are geographically
	dispersed so that its realization not only meets the functional requirements,
	but also satisfies the nonfunctional criteria such as the desired
	quality of services (QoS). The unified approach (UA) incorporates
	the concepts of product line practice (PLP) and generative programming
	with the unified meta-component model (UMM) to achieve automatic
	development, maximal reuse and seamless interoperation. The creation
	of a software solution for a distributed computing system (DCS),
	using the UA has two levels, the component level and the system level.
	In this paper, the system-level generative programming of the UA
	is described.},
  doi = {10.1109/ICAPP.2002.1173564},
  issn = { },
  keywords = { DCS; PLP; QoS; UMM; distributed computing system; distributed software
	systems; functional requirements; heterogeneous software component
	combination; maximal reuse; nonfunctional criteria; product line
	practice; seamless interoperation; service quality; system-level
	generative programming; unified meta-component model; automatic programming;
	distributed object management; distributed programming; object-oriented
	programming;}
}

@INPROCEEDINGS{685738,
  author = {Hudak, P.},
  title = {Modular domain specific languages and tools},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {134 -142},
  month = {jun},
  abstract = {A domain specific language (DSL) allows one to develop software for
	a particular application domain quickly and effectively, yielding
	programs that are easy to understand, reason about, and maintain.
	On the other hand, there may be a significant overhead in creating
	the infrastructure needed to support a DSL. To solve this problem,
	a methodology is described for building domain specific embedded
	languages (DSELs), in which a DSL is designed within an existing,
	higher-order and typed, programming language such as Haskell or ML.
	In addition, techniques are described for building modular interpreters
	and tools for DSELs. The resulting methodology facilitates reuse
	of syntax semantics, implementation code, software tools, as well
	as look-and-feel},
  doi = {10.1109/ICSR.1998.685738},
  issn = {1085-9098},
  keywords = {Haskell;ML;domain specific embedded languages;functional languages;higher-order
	typed programming language;implementation code;methodology;modular
	domain specific languages;modular interpreters;program understanding;semantics;software
	maintenance;software reuse;software tools;syntax;functional languages;program
	interpreters;software reusability;software tools;}
}

@INPROCEEDINGS{5934827,
  author = {Hudson, M. and Sprinkle, J.},
  title = {Simplification of Semantically-Rich Model Transformations through
	Generated Transformation Blocks},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {260 -268},
  month = {april},
  abstract = {This paper demonstrates a novel concept for the simplification of
	model transformations in which composite or complex objects are inserted
	into an existing model through a well-defined interface. The technique
	utilizes a model transformation from the domain of the modeling language
	into the domain of model transformation languages. The user specifies
	these semantically rich blocks using the original domain-specific
	modeling language. Then, a transformation generates the necessary
	model transformation graph to create an instance of the semantically
	rich, user-defined pattern. Users insert these generated patterns
	into their customized transformations. The approach is helpful for
	endogenous transformations in which existing objects may be refactored.
	It will also serve as a teaching tool for users who are unfamiliar
	with model transformations: specifically how to represent a newly-created
	model in the transformation domain. Finally, the approach is designed
	to reduce specification errors of model transformations in which
	new (semantically rich) blocks are inserted at key points, as the
	correctness of the semantically rich blocks is guaranteed, based
	on their construction in the original domain.},
  doi = {10.1109/ECBS.2011.28},
  keywords = {domain-specific modeling language;generated transformation blocks;model
	transformation graph;semantically-rich model transformations;graph
	theory;simulation languages;software engineering;}
}

@INPROCEEDINGS{5076633,
  author = {Jun Huh and Grundy, J. and Hosking, J. and Liu, K. and Amor, R.},
  title = {Integrated Data Mapping for a Software Meta-tool},
  booktitle = {Software Engineering Conference, 2009. ASWEC '09. Australian},
  year = {2009},
  pages = {111 -120},
  month = {april},
  abstract = {Complex data mapping tasks often arise in software engineering, particularly
	in code generation and model transformation. We describe Marama Torua,
	a tool supporting high-level specification and implementation of
	complex data mappings. Marama Torua is embedded in, and provides
	model transformation support for, our Eclipse-based Marama domain-specific
	language meta-tool. Developers can quickly develop stand alone data
	mappers and model translation and code import-export components for
	their tools. Complex data schema and mapping relationships are represented
	in multiple, high-level notational forms and users are provided semi-automated
	mapping assistance for large models. MaramaTorua is a set of Eclipse
	plug-ins allowing close integration with other tools such as schema
	browsers, and with the Marama meta-tool itself.},
  doi = {10.1109/ASWEC.2009.21},
  issn = {1530-0803},
  keywords = {Eclipse-based Marama domain-specific language meta-tool;Marama Torua;code
	generation;code import-export component;complex data schema;integrated
	data mapping;model transformation;model translation;schema browsers;software
	engineering;software metatool;software tools;specification languages;visual
	languages;}
}

@INPROCEEDINGS{4736742,
  author = {Hulette, G.C. and Sottile, M.J. and Malony, A.D.},
  title = {WOOL: A Workflow Programming Language},
  booktitle = {eScience, 2008. eScience '08. IEEE Fourth International Conference
	on},
  year = {2008},
  pages = {71 -78},
  month = {dec.},
  abstract = {Workflows offer scientists a simple but flexible programming model
	at a level of abstraction closer to the domain-specific activities
	that they seek to perform. However, languages for describing workflows
	tend to be highly complex, or specialized towards a particular domain,
	or both. WOOL is an abstract workflow language with human-readable
	syntax, intuitive semantics, and a powerful abstract type system.
	WOOL workflows can be targeted to almost any kind of runtime system
	supporting data-flow computation. This paper describes the design
	of the WOOL language and the implementation of its compiler, along
	with a simple example runtime. We demonstrate its use in an image-processing
	workflow.},
  doi = {10.1109/eScience.2008.43},
  keywords = {WOOL language design;WOOL workflows;abstract type system;abstract
	workflow language;data flow computation;domain-specific activities;flexible
	programming model;human-readable syntax;intuitive semantics;runtime
	system;workflow programming language;abstract data types;high level
	languages;type theory;}
}

@ARTICLE{4602673,
  author = {Hummel, O. and Janjic, W. and Atkinson, C.},
  title = {Code Conjurer: Pulling Reusable Software out of Thin Air},
  journal = {Software, IEEE},
  year = {2008},
  volume = {25},
  pages = {45 -52},
  number = {5},
  month = {sept.-oct. },
  abstract = {For many years, the IT industry has sought to accelerate the software
	development process by assembling new applications from existing
	software assets. However, true component-based reuse of the form
	Douglas Mcllroy envisaged in the 1960s is still the exception rather
	than the rule, and most of the systematic software reuse practiced
	today uses heavyweight approaches such as product-line engineering
	or domain-specific frameworks. By component, we mean any cohesive
	and compact unit of software functionality with a well-defined interface
	- from simple programming language classes to more complex artifacts
	such as Web services and Enterprise JavaBeans.},
  doi = {10.1109/MS.2008.110},
  issn = {0740-7459},
  keywords = {IT industry;code conjurer;component-based reuse;programming language;reusable
	software;software assets;software development;software functionality;object-oriented
	programming;software reusability;}
}

@INPROCEEDINGS{1265100,
  author = {Hung, P.C.K. and Haifei Li and Jun-Jang Jeng},
  title = {WS-Negotiation: an overview of research issues},
  booktitle = {System Sciences, 2004. Proceedings of the 37th Annual Hawaii International
	Conference on},
  year = {2004},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { A Web service is defined as an autonomous unit of application logic
	that provides either some business functionality or information to
	other applications through an Internet connection. Web services are
	based on a set of XML standards such as Simple Object Access Protocol
	(SOAP), Universal Description, Discovery and Integration (UDDI) and
	Web Services Description Language (WSDL). In particular, Web services
	discovery is the process of finding most appropriate Web services
	providers needed by a Web services requestor. One of the important
	issues in the discovery process is for Web services providers and
	Web services requestors to negotiate and find a solution that is
	acceptable to both sides. Thus, a more sophisticated business model
	with negotiation feature is required for this challenging research
	area. As there are increasing demands for negotiation technologies
	in the context of Web services, this paper proposes an independent
	declarative XML language called WS-Negotiation for Web services providers
	and requestors. In general, WS-Negotiation contains three parts:
	negotiation message, which describes the format for messages exchanged
	among negotiation parties, negotiation protocol, which describes
	the mechanism and rules that negotiation parties should follow, and
	negotiation decision making, which is an internal and private decision
	process based on a cost-benefit model or other strategies. This paper
	also presents a service level agreement (SLA) template model with
	different domain specific vocabularies for supporting different types
	of business negotiations in WS-Negotiation.},
  doi = {10.1109/HICSS.2004.1265100},
  keywords = { WS-Negotiation; Web service; XML language; negotiation decision making;
	negotiation message; negotiation protocol; service level agreement
	template; Internet; XML; decision making; negotiation support systems;}
}

@INPROCEEDINGS{6030046,
  author = {Hutchesson, S. and McDermid, J.},
  title = {Towards Cost-Effective High-Assurance Software Product Lines: The
	Need for Property-Preserving Transformations},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {55 -64},
  month = {aug.},
  abstract = {Generative programming and model transformation techniques are becoming
	widely used for the development of software components for product
	lines. The ability to develop components with identified common and
	variable parts, and rapidly instantiate product-specific versions
	is key to many software product line approaches. However if this
	approach is to be truly cost effective for high assurance applications,
	the instantiation process must be property-preserving, any verification
	evidence acquired on the product-line component must be demonstrably
	applicable to the instantiated component. In this paper we outline
	an approach that uses static analysis techniques and the SPARK language
	that can potentially demonstrate the correctness of model transformations.},
  doi = {10.1109/SPLC.2011.32},
  keywords = {SPARK language;cost effective high assurance software product line
	component;generative programming;high assurance application;instantiation
	process;model transformation technique;product-specific version;property
	preserving transformation;software component;static analysis technique;verification
	evidence;software cost estimation;software reliability;}
}

@INPROCEEDINGS{4814994,
  author = {Iacob, M.-E. and Steen, M.W.A. and Heerink, L.},
  title = {Reusable Model Transformation Patterns},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops, 2008
	12th},
  year = {2008},
  pages = {1 -10},
  month = {sept.},
  abstract = {This paper is a reflection of our experience with the specification
	and subsequent execution of model transformations in the QVT core
	and Relations languages. Since this technology for executing transformations
	written in high-level, declarative specification languages is of
	very recent date, we observe that there is little knowledge available
	on how to write such declarative model transformations. Consequently,
	there is a need for a body of knowledge on transformation engineering.
	With this paper we intend to make an initial contribution to this
	emerging discipline. Based on our experiences we propose a number
	of useful design patterns for transformation specification. In addition
	we provide a method for specifying such transformation patterns in
	QVT, such that others can add their own patterns to a catalogue and
	the body of knowledge can grow as experience is built up. Finally,
	we illustrate how these patterns can be used in the specification
	of complex transformations.},
  doi = {10.1109/EDOCW.2008.51},
  keywords = {declarative specification language;design pattern;executing model
	transformation;query/view/transformation core;reusable model;object-oriented
	programming;software reusability;specification languages;}
}

@INPROCEEDINGS{970442,
  author = {Iba, T. and Takabe, Y. and Chubachi, Y. and Takefuji, Y.},
  title = {Boxed Economy Simulation Platform and foundation model},
  booktitle = {Computational Intelligence and Multimedia Applications, 2001. ICCIMA
	2001. Proceedings. Fourth International Conference on},
  year = {2001},
  pages = {34 -38},
  abstract = {The authors propose a "Boxed Economy Simulation Platform", which is
	a sharable basis for agent-based economic simulations. By providing
	the basic design of the social model, which we call "Boxed Economy
	Foundation Model", it enables collaborative research more efficiently.
	Sharing and cumulating the model components can be promoted by domain-specific
	design at the level of social model rather than the level of abstract
	general purpose model. It will be able to contribute to remove factors
	that have been making it difficult for social scientists to participate
	in and conduct agent-based research},
  doi = {10.1109/ICCIMA.2001.970442},
  keywords = {Boxed Economy Simulation Platform;agent-based economic simulations;agent-based
	research;collaborative research;domain-specific design;foundation
	model;model components;social model;social scientists;digital simulation;economics;software
	agents;specification languages;}
}

@INPROCEEDINGS{4221880,
  author = {Ignatova, T.},
  title = {Model-Driven Development of Content-Based Image Retrieval Systems},
  booktitle = {Digital Information Management, 2006 1st International Conference
	on},
  year = {2007},
  pages = {137 -144},
  month = {dec.},
  abstract = {Generic systems for content-based image retrieval (CBIR), such as
	QB1C [7] cannot be used to solve domain-specific retrieval problems,
	as for example, the identification of manuscript writers based in
	the visual characteristics of their handwriting. Domain-specific
	CBIR systems currently have to be implemented bottom up, i.e. almost
	from scratch, each time a new domain-specific solution is sought.
	Driven by the recognition that CBIR systems, although developed for
	different domain-problems, have similar building blocks and architecture,
	we propose a framework model for the development of CBIR systems.
	The aim of the framework model is to support the design of domain-
	specific CBIR-systems on a conceptual level by reusing architectural
	design and building blocks. The model-driven development of CBIR
	systems can thus be realized by software components for modeling
	the data structures and the update, storage and retrieval operations
	using the predefined framework model, and by components for mapping
	this model onto a specific implementation platform.},
  doi = {10.1109/ICDIM.2007.369343},
  keywords = {content-based image retrieval systems;domain-specific CBIR systems;domain-specific
	retrieval problems;model-driven development;content-based retrieval;image
	retrieval;}
}

@INPROCEEDINGS{952442,
  author = {Ihme, T.},
  title = {An architecture line structure for command and control software},
  booktitle = {Euromicro Conference, 2001. Proceedings. 27th},
  year = {2001},
  pages = {90 -96},
  abstract = {Product line software engineering should include an up-front and ongoing
	investment in a reusable architecture. Identification, selection,
	documentation and validation of general architectural patterns and
	their product line, organisation or domain specific variants allow
	designers to adopt approved patterns as a starting point. This paper
	discusses how to apply design patterns to the problems of product
	lines, including various and evolutionary design techniques of embedded
	software. The patterns are a precondition for the development of
	frameworks for designing and reusing architectures of the sub-domains
	that promise the highest benefits. The classification of core architectural
	assets helps to achieve a balance between discipline product line
	architecture and creativity forced by future needs that are not fully
	predictable},
  doi = {10.1109/EURMIC.2001.952442},
  keywords = {embedded software;evolutionary design techniques;reusable architecture;software
	engineering;software reuse;embedded systems;software architecture;software
	development management;software reusability;}
}

@INPROCEEDINGS{1391898,
  author = { Inkpen, D. and Kipp, D.},
  title = {A prototype natural language interface for animation systems},
  booktitle = {Haptic, Audio and Visual Environments and Their Applications, 2004.
	HAVE 2004. Proceedings. The 3rd IEEE International Workshop on},
  year = {2004},
  pages = { 153 - 157},
  month = {oct.},
  abstract = { We present a prototype implementation of a natural language interface
	to an animation system. The interface provides the means for a human
	user to issue commands in natural language to an avatar in a virtual
	reality environment. The purpose of our system is to convert the
	input text into commands in an animation script language and execute
	them. Our system uses a general-purpose parser and a domain-specific
	semantic interpreter based on pattern matching.},
  doi = {10.1109/HAVE.2004.1391898},
  issn = { },
  keywords = { animation script language; animation system; avatar; domain-specific
	semantic interpreter; input text; natural language interface; parser;
	pattern matching; virtual reality environment; avatars; computational
	linguistics; computer animation; grammars; natural language interfaces;}
}

@INPROCEEDINGS{5750504,
  author = {Irazbal, J. and Pons, C.},
  title = {Supporting Modularization in Textual DSL Development},
  booktitle = {Chilean Computer Science Society (SCCC), 2010 XXIX International
	Conference of the},
  year = {2010},
  pages = {124 -130},
  month = {nov.},
  abstract = {A domain-specific language DSL provides a notation tailored towards
	an application domain. The closer the language is to their target
	domain, the more useful the language is for developers. But this
	increment in the level of specificity raises the issue of duplication
	of concepts among different languages, and leads to the definition
	of a family of languages. Despite the advance in tool support for
	defining the abstract and concrete syntaxes of DSLs, developing a
	family of DSLs still requires a significant amount of duplicated
	effort. In this work we present an infrastructure for implementing
	families of textual DSLs. We introduce a technique based on XText
	that reduces the effort required to create editors and interpreters
	by enabling the modularization of the common features and the smooth
	specification of variability between the DSLs.},
  doi = {10.1109/SCCC.2010.52},
  issn = {1522-4902},
  keywords = {XTex;domain-specific language;modularization support;textual DSL development;specification
	languages;}
}

@INPROCEEDINGS{346044,
  author = {Iscoe, N.},
  title = {Domain modeling-overview and ongoing research at EDS},
  booktitle = {Software Engineering, 1993. Proceedings., 15th International Conference
	on},
  year = {1993},
  pages = {198 -200},
  month = {may},
  abstract = {An application domain model is defined as a formal representation
	of the knowledge necessary to support specific operational goals.
	EDS is a company which produces large business systems for a variety
	of industries. Domain modeling at EDS is discussed. First, domain-specific
	knowledge is acquired from various sources and it is synthesized
	into a domain model. At this stage, the focus is on digesting, understanding,
	and formalizing the gathered information. Second, to build specification
	models for various applications, relevant pieces of domain knowledge
	are selected from the domain model. In this phase, the focus is on
	tailoring general domain knowledge to the specific application. The
	approach to gathering domain knowledge is based on a combination
	of manual, semi-automated, and automated techniques. The knowledge
	acquisition synthesis system is described which was implemented using
	X Windows on a SPARC II client/server architecture. Domain experts
	interact with the system to store their knowledge into a domain model},
  doi = {10.1109/ICSE.1993.346044},
  issn = {0270-5257},
  keywords = { SPARC II client/server architecture; X Windows; application domain
	model; domain-specific knowledge; formal representation; knowledge
	acquisition synthesis system; large business systems; operational
	goals; specification models; commerce; formal specification; knowledge
	acquisition; knowledge based systems;}
}

@ARTICLE{5440163,
  author = {Izquierdo, J.L.C. and Molina, J.G.},
  title = {An Architecture-Driven Modernization Tool for Calculating Metrics},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {37 -43},
  number = {4},
  month = {july-aug. },
  abstract = {Model-driven software development (MDD) is gaining increasing acceptance,
	mainly because it can raise the level of abstraction and automation
	in software construction. MDD techniques (see the sidebar "MDD Basic
	Concepts"), such as metamodeling and model transformation, not only
	apply to the creation of new software systems but also can be used
	to evolve existing systems. These techniques can help reduce software
	evolution costs by automating many basic activities in software change
	processes, such as representing source code at a higher level of
	abstraction, providing information to analyze the impact of the changes,
	or automatically generating software artifacts of the evolved system.
	Several experiences of applying MDD in platform migration scenarios
	have recently been published,1'2 but they define ad hoc metamodels
	that hinder interoperability.},
  doi = {10.1109/MS.2010.61},
  issn = {0740-7459},
  keywords = {MDD;MDD basic concepts;ad hoc metamodels;architecture driven modernization
	tool;metrics calculation;model driven software development;model
	transformation;software artifacts;software construction;software
	architecture;}
}

@INPROCEEDINGS{4591712,
  author = {Jablonski, S. and Volz, B. and Dornstauder, S.},
  title = {A Meta Modeling Framework for Domain Specific Process Management},
  booktitle = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual
	IEEE International},
  year = {2008},
  pages = {1011 -1016},
  month = {28 2008-aug. 1},
  abstract = {Process Management has become an acknowledged technology for application
	integration. However, different applications leverage from different
	process modeling capabilities. Thus, domain specific process management
	becomes more and more relevant. In this paper we present our solution
	for an abstract process modeling method and language based on an
	extensible meta modeling framework which has two main advantages
	compared to standard MDA tools. First, we can easily implement modeling
	patterns (here: powertypes and type/usage concept). Second, we can
	use more than two meta layers which results in a more clear structure
	and in higher flexibility (here a separation between general process
	modeling principles and domain specific languages that can better
	express domain specific semantics).},
  doi = {10.1109/COMPSAC.2008.58},
  issn = {0730-3157},
  keywords = {MDA tools;abstract process modeling method;domain specific languages;domain
	specific process management;domain specific semantics;extensible
	meta modeling framework;general process modeling principles;process
	modeling capability;data models;software process improvement;specification
	languages;}
}

@INPROCEEDINGS{5614707,
  author = {Jacobi, S. and Hahn, C. and Raber, D.},
  title = {Integration of Multiagent Systems and Service Oriented Architectures
	in the Steel Industry},
  booktitle = {Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010
	IEEE/WIC/ACM International Conference on},
  year = {2010},
  volume = {2},
  pages = {479 -482},
  month = {31 2010-sept. 3},
  abstract = {In the course of globalization competitive pressure is rising in most
	industrial sectors. High quality products are basic prerequisites
	for companies of high wage countries to be present on the global
	market. Improved adherence to delivery date, increased flexibility
	despite to decreased production costs are some examples of the challenges
	to be managed just to keep current positions. In general, these requirements
	are mostly requirements on the processes - not on the actual products.
	Economic efficiency is not any longer just a property of products
	and quality, but more and more a property of processes. Thus, process
	capability is getting more important beside production capability.
	This paper shows how service-oriented architectures (SOA) and multi-agent
	systems (MAS) can be integrated using a model-driven approach. In
	fact, a model transformation from SoaML - a metamodel for SOA - to
	DSML4MAS - a domain-specific modeling language for MAS - is utilized
	for the integration. The relevance of this approach is proven by
	applying it to a real-world industry scenario. This includes modeling
	a segment of a production chain of Saarstahl AG - a global respected
	steel manufacturer. The presented approach helps to increase flexibility
	of mid and short term planning and scheduling along the chosen segment
	and thus improve processes.},
  doi = {10.1109/WI-IAT.2010.218},
  keywords = {DSML4MAS;Saarstahl AG;SoaML;domain-specific modeling language;economic
	efficiency;global market;globalization competitive pressure;high
	quality products;metamodel;model driven approach;multiagent systems;process
	capability;production chain;service oriented architectures;short
	term planning;short term scheduling;steel industry;multi-agent systems;production
	engineering computing;production planning;scheduling;software architecture;steel
	industry;}
}

@INPROCEEDINGS{6058985,
  author = {Jager, T. and Fay, A. and Wagner, T. and Lowen, U.},
  title = {Mining technical dependencies throughout engineering process knowledge},
  booktitle = {Emerging Technologies Factory Automation (ETFA), 2011 IEEE 16th Conference
	on},
  year = {2011},
  pages = {1 -7},
  month = {sept.},
  abstract = {Within the engineering of automated systems, different engineering
	disciplines are involved. Typically intermediate results from one
	discipline are handed over to another discipline. These results are
	refined throughout domain specific activities, and then handed over
	to other disciplines, incl. the originating one. This results in
	hidden dependencies between the involved disciplines, the planning
	assumptions as well as results, and the technical artefacts. This
	paper shows a method, proven in the engineering of automated plants
	in the metal industry, to gain explicit knowledge about the technical
	dependencies within the engineering of automated systems. Therefore
	the typical characteristics of the engineering process are described
	first, followed by a description how to capture the engineering process
	and a systematic approach to make these dependencies visible.},
  doi = {10.1109/ETFA.2011.6058985},
  issn = {1946-0740},
  keywords = {automated plants;automated systems;data mining;engineering process
	knowledge;hidden dependencies;metal industry;technical artefacts;technical
	dependencies;data mining;engineering computing;}
}

@INPROCEEDINGS{4781679,
  author = {Jalali, V. and Borujerdi, M.R.M.},
  title = {The effect of using domain specific ontologies in query expansion
	in medical field},
  booktitle = {Innovations in Information Technology, 2008. IIT 2008. International
	Conference on},
  year = {2008},
  pages = {277 -281},
  month = {dec.},
  abstract = {Domain specific ontologies can be used to improve both precision and
	recall of information retrieval systems. One approach in this regard
	is using query expansion techniques and the other would be introducing
	a semantic similarity measure for concepts in ontology. Although
	each approach has its own benefits and drawbacks, query expansion
	techniques are preferred when the corpus volume is so huge that examining
	concept pairs between query and documents is not reasonable. In this
	paper a semantic query expansion algorithm for medical information
	retrieval is introduced. Proposed approach consists of identifying
	MeSH (Medical Subject Headings) concepts in user's query and applying
	expansion algorithm to them. Expansion algorithm is based on the
	location of concepts in MeSH hierarchy, number of synonyms of each
	concept and number of terms the concept is made of. Results show
	improvements over classic method, query expansion using general purpose
	ontology and a number of other approaches.},
  doi = {10.1109/INNOVATIONS.2008.4781679},
  keywords = {domain specific ontologies;information retrieval systems;medical field;medical
	information retrieval;medical subject headings;query expansion techniques;semantic
	query expansion algorithm;semantic similarity measure;information
	retrieval;medical computing;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{5298652,
  author = {Janzen, S. and Maass, W.},
  title = {Ontology-Based Natural Language Processing for In-store Shopping
	Situations},
  booktitle = {Semantic Computing, 2009. ICSC '09. IEEE International Conference
	on},
  year = {2009},
  pages = {361 -366},
  month = {sept.},
  abstract = {Natural Language communication between customers and products within
	in-store shopping environments enables new forms of product interfaces
	and an improved filtering and intuitive presentation of product information.
	In this article, we describe how customer's access to product information
	at the point of sale can be improved through the use of dialogue
	systems and heterogeneous web-based representations of product information
	based on formal ontologies within in-store shopping environments.
	After considering specific requirements of in-store shopping environments
	on dialogue systems, we present the model of a Conversational Recommendation
	Agent (CoRA), a domain-specific dialogue system, which realizes an
	ontology-based Natural Language Processing system for shopping situations.},
  doi = {10.1109/ICSC.2009.44},
  keywords = {conversational recommendation agent;customer;dialogue systems;domain-specific
	dialogue;heterogeneous Web based representations;in-store shopping
	situations;natural language processing;ontology;product information;product
	interfaces;Web services;interactive systems;natural language processing;ontologies
	(artificial intelligence);retail data processing;}
}

@INPROCEEDINGS{5552781,
  author = {Jaroucheh, Z. and Xiaodong Liu and Smith, S.},
  title = {Apto: A MDD-Based Generic Framework for Context-Aware Deeply Adaptive
	Service-Based Processes},
  booktitle = {Web Services (ICWS), 2010 IEEE International Conference on},
  year = {2010},
  pages = {219 -226},
  month = {july},
  abstract = {Context-awareness and adaptability are important and desirable properties
	of service-based processes designed to provide personalized services.
	Most of the existing approaches focus on the adaptation at the process
	instance level which involves extending the standard Business Process
	Execution Language (BPEL) and its engine or creating their own process
	languages (e.g. However, the approach proposed here aims to apply
	an adaptation to processes modeled or developed without any adaptation
	possibility in mind and independently of specific usage contexts.
	In addition, most of the existing approaches tackle the adaptation
	on the process instance or definition levels by explicitly specifying
	some form of variation points. This, however, leads to a contradiction
	between how the architect logically views and interprets differences
	in the process family and the actual modeling constructs through
	which the logical differences must be expressed. We introduce the
	notion of an evolution fragment and evolution primitive to capture
	the variability in a more logical and independent way. Finally, the
	proposed approach intends to support the viewpoint of context-aware
	adaptation as a crosscutting concern with respect to the core #x201C;business
	logic #x201D; of the process. In this way, the design of the process
	core can be decoupled from the design of the adaptation logic. To
	this end, we leverage ideas from the domain of model-driven development
	(MDD) and generative programming.},
  doi = {10.1109/ICWS.2010.16},
  keywords = {Apto;MDD-based generic framework;business logic;business process execution
	language;context aware adaptation;context awareness;context-aware
	deeply adaptive service-based processes;crosscutting concern;definition
	levels;generative programming;model-driven development;personalized
	services;process family;process instance level;high level languages;software
	engineering;ubiquitous computing;}
}

@ARTICLE{912378,
  author = {Jarzabek, S. and Seviora, R.},
  title = {Engineering components for ease of customisation and evolution},
  journal = {Software, IEE Proceedings -},
  year = {2000},
  volume = {147},
  pages = {237 -248},
  number = {6},
  abstract = {Building software systems from prefabricated components is a very
	attractive vision. Distributed component platforms (DCP) and their
	visual development environments bring this vision closer. However,
	some experiences with component libraries warn us about potential
	problems that arise when software-system families or systems evolve
	over many years of changes. Indeed, implementation-level components,
	when affected by many independent changes, tend to grow in both size
	and number, impeding reuse. This unwanted effect is analysed in detail.
	It is argued that components affected by frequent unexpected changes
	require higher levels of flexibility than the `plug-and-play' paradigm
	is able to provide. A program construction environment is proposed,
	based on generative programming techniques, to help in customisation
	and evolution of components that require much flexibility. This solution
	allows the benefits of DCPs to be reaped during runtime and, at the
	same time, keeps components under control during system construction
	and evolution. Salient features of a construction environment for
	component based systems are discussed. Its implementation with commercial
	reuse technology FusionTM is described. The main lesson learnt from
	the project is that generative-programming techniques can extend
	the strengths of the component based approach in two important ways:
	1) generative-programming techniques automate routine component customisation
	and composition tasks and allow developers work more productively,
	at a higher abstraction level; 2) as custom components with required
	properties are generated on demand, it is not necessary to store
	and manage multiple versions of components},
  doi = {10.1049/ip-sen:20000914},
  issn = {1462-5970},
  keywords = {DCPs;Fusion;abstraction level;commercial reuse technology;component
	based approach;component based systems;component engineering;component
	libraries;construction environment;custom components;distributed
	component platforms;generative programming techniques;generative-programming
	techniques;implementation-level components;independent changes;prefabricated
	components;program construction environment;routine component customisation;software
	systems;software-system families;system construction;visual development
	environments;configuration management;distributed object management;management
	of change;software reusability;}
}

@INPROCEEDINGS{6032552,
  author = {Jarzabek, S. and Ha Duy Trung},
  title = {Flexible generators for software reuse and evolution: NIER Track},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {920 -923},
  month = {may},
  abstract = {Developers tend to use models and generators during initial development,
	but often abandon them later in software evolution and reuse. One
	reason for that is that code generated from models (e.g., UML) is
	often manually modified, and changes cannot be easily propagated
	back to models. Once models become out of sync with code, any future
	re-generation of code overrides manual modifications. We propose
	a flexible generator solution that alleviates the above problem.
	The idea is to let developers weave arbitrary manual modifications
	into the generation process, rather than modify already generated
	code. A flexible generator stores specifications of manual modifications
	in executable form, so that weaving can be automatically re-done
	any time code is regenerated from modified models. In that way, models
	and manual modification can evolve independently but in sync with
	each other, and the generated code never gets directly changed. As
	a proof of concept, we have already built a flexible generator prototype
	by a merger of conventional generation system and variability technique
	to handle manual modifications. We believe a flexible generator approach
	alleviates an important problem that hinders wide spread adoption
	of MDD in software practice.},
  doi = {10.1145/1985793.1985946},
  issn = {0270-5257},
  keywords = {NIER track;UML;code generation;flexible generators;software evolution;software
	reuse;Unified Modeling Language;software reusability;}
}

@INPROCEEDINGS{4578350,
  author = {Jennings, B. and Lei Xu and de Leastar, E.},
  title = {Specifying Flexible Charging Rules for Composable Services},
  booktitle = {Services - Part I, 2008. IEEE Congress on},
  year = {2008},
  pages = {376 -383},
  month = {july},
  abstract = {Where services are offered on a commercial basis, the manner in which
	charges for service usage are calculated is of key importance. Services
	typically have associated with them a charging scheme specifying
	the rules for charge calculation; schemes can range from the simple
	(such fixed charge per service invocation) to highly complex (where
	charges are calculated dynamically in order to influence customer
	demand and thereby optimise overall system performance). Typically,
	charging schemes are manually configured and verified prior to services
	being made available to customers - typically, a time consuming and
	expensive process. In environments where service compositions can
	be rapidly built and offered to customers, manual specification of
	a charging scheme for the service composition becomes untenable.
	In this paper we describe how charge modification rules associated
	with individual services can be used to flexibly govern how a service
	is charged for when it is used in the context of a composed service.},
  doi = {10.1109/SERVICES-1.2008.70},
  keywords = {charge calculation;charge per service invocation;charging scheme specification;customer
	demand;flexible charging rule specification;service composition;service
	usage;system performance;cost accounting;formal specification;pricing;software
	architecture;}
}

@INPROCEEDINGS{5313746,
  author = {Yoonjae Jeong and Youngho Kim and Seongchan Kim and Sung-Hyon Myaeng
	and Hyo-Jung Oh},
  title = {Generating and mixing feature sets from language models for sentiment
	classification},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2009. NLP-KE
	2009. International Conference on},
  year = {2009},
  pages = {1 -8},
  month = {sept.},
  abstract = {This paper presents methods for mixing feature sets in sentence-level
	sentiment analysis where a sentence is classified into one of three
	classes: positive, negative, and neutral. Motivated by the need to
	classify sentences in Korean whose sentiment-revealing expressions
	tend to have different effects according to their syntactic categories,
	we employed a language modeling (LM) approach with 162 different
	LMs based on syntactic categories that are effectively combined with
	a logistic regression classifier. The experimental results show that
	this approach significantly outperforms clue-based SVM classifiers.
	The enumeration of feature types arising from the LMs for the logistic
	regression classifier allowed us to show that domain specific models
	can be smoothed with a general model and that attaching a syntactic
	category to a feature helps improving effectiveness. The classification
	results are further improved by applying a clue-based classifier.
	The rationale behind this two-step process is to classify sentences
	with a relatively conservative classifier in picking positive and
	negative sentences and to apply a high-precision classifier to the
	sentences in the neutral class.},
  doi = {10.1109/NLPKE.2009.5313746},
  keywords = {clue-based classifier;language modeling;logistic regression classifier;sentence-level
	sentiment analysis;sentiment classification;syntactic category;classification;natural
	language processing;regression analysis;text analysis;}
}

@INPROCEEDINGS{1509453,
  author = {Jerad, C. and Barkaoui, K.},
  title = {On the use of rewriting logic for verification of distributed software
	architecture description based LfP},
  booktitle = {Rapid System Prototyping, 2005. (RSP 2005). The 16th IEEE International
	Workshop on},
  year = {2005},
  pages = { 202 - 208},
  month = {june},
  abstract = { Software architecture description languages (ADLs) allow software
	designers to focus on high level aspects of an application by abstracting
	from the details of the components that compose architecture. It
	is precisely this abstraction that makes ADLs suitable for verification
	using model checking techniques. ADLs are, in a way, domain-specific
	languages for aspects such as coordination and distribution. LfP
	(language for prototyping) is a formal approach for distributed software
	architectures that is based on RM-ODP and that can be linked to an
	UML methodology. We propose in this paper a rewriting of the LfP
	semantics, specified in rewriting logic which is well suited for
	axiomatization of concurrent languages. Using the Maude system, a
	high-performance interpreter based on rewriting logic, we illustrate
	through an example how this rewriting semantics can be exploited
	for verification aspects related to distributed object interactions.},
  doi = {10.1109/RSP.2005.34},
  issn = {1074-6005},
  keywords = { Maude system; UML methodology; concurrent language; formal verification;
	language for prototyping; model checking technique; software architecture
	description languages; Unified Modeling Language; distributed programming;
	program interpreters; program verification; rewriting systems; software
	architecture; software prototyping;}
}

@INPROCEEDINGS{1003612,
  author = {Jersak, M. and Richter, K. and Henia, R. and Ernst, R. and Slomka,
	F.},
  title = {Transformation of SDL specifications for system-level timing analysis},
  booktitle = {Hardware/Software Codesign, 2002. CODES 2002. Proceedings of the
	Tenth International Symposium on},
  year = {2002},
  pages = {121 -126},
  abstract = {Complex embedded systems are typically specified using multiple domain-specific
	languages. After code-generation, the implementation is simulated
	and tested. Validation of non-functional properties, in particular
	timing, remains a problem because full test coverage cannot be achieved
	for realistic designs. The alternative, formal timing analysis, requires
	a system representation based on key application and architecture
	properties. These properties must first be extracted from a system
	specification to enable analysis. In this paper we present a suitable
	transformation of SDL specifications for system-level timing analysis.
	We show ways to vary modeling accuracy in order to apply available
	formal techniques. A practical approach utilizing a recently developed
	system model is presented},
  doi = {10.1109/CODES.2002.1003612},
  keywords = {SDL specifications;SDL specifications transformation;complex embedded
	systems;formal timing analysis;multiple domain-specific languages;system
	model;system representation;system specification;system-level timing
	analysis;timing;formal specification;specification languages;systems
	analysis;timing;}
}

@INPROCEEDINGS{4460637,
  author = {Ji, Luning and Lu, Qin and Li, Wenjie and Chen, YiRong},
  title = {Automatic Construction of a Core Lexicon for Specific Domain},
  booktitle = {Advanced Language Processing and Web Information Technology, 2007.
	ALPIT 2007. Sixth International Conference on},
  year = {2007},
  pages = {183 -188},
  month = {aug.},
  abstract = {The rapid development of science and technology in different domains
	has created many new concepts and the domain lexicon must be updated
	timely to include the new terms as domain knowledge. However, automatic
	update of domain knowledge requires a core lexicon for bootstrapping
	purpose. The core lexicon should contain the fundamental terms used
	in a domain and from the core lexicon other concepts and terms can
	be built upon. In this paper we present an algorithm for extracting
	the core lexicon from some domain specific lexicons. Experiment on
	a large domain specific lexicon with 139,429 entries shows that only
	3,413 terms form the core lexicon with a high precision of 97 #x025;
	and a good coverage.},
  doi = {10.1109/ALPIT.2007.21}
}

@INPROCEEDINGS{1509246,
  author = {Jia, L. and Spalding, F. and Walker, D. and Glew, N.},
  title = {Certifying compilation for a language with stack allocation},
  booktitle = {Logic in Computer Science, 2005. LICS 2005. Proceedings. 20th Annual
	IEEE Symposium on},
  year = {2005},
  pages = { 407 - 416},
  month = {june},
  abstract = { This paper describes an assembly-language type system capable of
	ensuring memory safety in the presence of both heap and stack allocation.
	The type system uses linear logic and a set of domain-specific predicates
	to specify invariants about the shape of the store. Part of the model
	for our logic is a tree of "stack tags" that tracks the evolution
	of the stack over time. To demonstrate the expressiveness of the
	type system, we define Micro-CLI, a simple imperative language that
	captures the essence of stack allocation in the common language infrastructure.
	We show how to compile well-typed Micro-CLI into well-typed assembly.},
  doi = {10.1109/LICS.2005.9},
  issn = {1043-6871 },
  keywords = { MicroCLI; assembly-language type system; common language infrastructure;
	linear logic; stack allocation; tree data structures; assembly language;
	formal logic; program compilers; storage management; tree data structures;}
}

@INPROCEEDINGS{5689083,
  author = {Tao Jiang and Xin Wang and Yong Yu},
  title = {A formal definition of the structural semantics of Domain-Specific
	Modeling languages},
  booktitle = {Information Science and Engineering (ICISE), 2010 2nd International
	Conference on},
  year = {2010},
  pages = {1696 -1699},
  month = {dec.},
  abstract = {As a Model-Driven Development methodology (MDD) for the specific domain,
	Domain-Specific Modeling (DSM) has been widely and successfully used
	in system design and analysis of specific areas. In spite of its
	general important, due to informal definition of Domain-Specific
	Modeling Language (DSMLs), the structural semantics of DSMLs cannot
	be strictly described and the properties based on it also cannot
	be analyzed and validated. In response, the paper proposes a formal
	definition method of the structural semantics of DSMLs. Firstly,
	a formal definition of domain indicating structural semantics of
	DSMLs based on algebra is presented to unify DSMLs and its models
	in the domain, secondly, a mapping mechanism from domain to the corresponding
	first-order logic system is established to finish analysis and validation
	of properties of domain such as consistency based on first-order
	logical inference, based on this, the method of formalization and
	consistency analysis and validation of structural semantics of DSMLs
	based on first-order logic is presented, finally, the formalization
	automatic mapping engine for model and metamodel is introduced to
	show the application of formalization of structural semantics in
	analysis and validation of properties of models.},
  doi = {10.1109/ICISE.2010.5689083}
}

@INPROCEEDINGS{5952827,
  author = {Tao Jiang and WenYun Zheng},
  title = {Research on formalization of Domain-Specific Metamodeling Language
	based on first-order logic},
  booktitle = {Computer Science and Automation Engineering (CSAE), 2011 IEEE International
	Conference on},
  year = {2011},
  volume = {4},
  pages = {170 -174},
  month = {june},
  abstract = {Domain-Specific Modeling has been widely and successfully used in
	software system modeling of specific domains. In spite of its general
	important, due to its informal definition, Domain-Specific Metamodeling
	Language (DSMML) cannot strictly represent its structural semantics,
	so its properties such as consistency cannot be systematically verified.
	In response, the paper proposes a formal representation of the structural
	semantics of DSMML named XMML based on first-order logic. Firstly,
	XMML is introduced, secondly, we illustrate our approach by formalization
	of attachment relationship which is one of association meta-types
	based on first-order logic, based on this, the approach of consistency
	verification of XMML itself and metamodels built based on XMML is
	presented, finally, the formalization automatic mapping engine for
	metamodels is introduced to show the application of formalization
	of XMML.},
  doi = {10.1109/CSAE.2011.5952827},
  keywords = {DSMML;XMML consistency verification;association meta-types;domain-specific
	metamodeling language formalization;first-order logic;formal representation;formalization
	automatic mapping engine;software system modeling;XML;formal verification;simulation
	languages;}
}

@INPROCEEDINGS{1565752,
  author = {Xing Jiang and Ah-Hwee Tan},
  title = {Mining ontological knowledge from domain-specific text documents},
  booktitle = {Data Mining, Fifth IEEE International Conference on},
  year = {2005},
  pages = { 4 pp.},
  month = {nov.},
  abstract = { Traditional text mining systems employ shallow parsing techniques
	and focus on concept extraction and taxonomic relation extraction.
	This paper presents a novel system called CRCTOL for mining rich
	semantic knowledge in the form of ontology from domain-specific text
	documents. By using a full text parsing technique and incorporating
	both statistical and lexico-syntactic methods, the knowledge extracted
	by our system is more concise and contains a richer semantics compared
	with alternative systems. We conduct a case study wherein CRCTOL
	extracts ontological knowledge, specifically key concepts and semantic
	relations, from a terrorism domain text collection. Quantitative
	evaluation, by comparing with a state-of-the-art ontology learning
	system known as text-to-onto, has shown that CRCTOL produces much
	better precision and recall for both concept and relation extraction,
	especially from sentences with complex structures.},
  doi = {10.1109/ICDM.2005.97},
  issn = {1550-4786},
  keywords = { concept extraction; concept relation concept tuple; domain-specific
	text document; full text parsing; lexico-syntactic method; ontological
	knowledge mining; ontology learning; relation extraction; statistical
	method; text mining; data mining; grammars; learning systems; ontologies
	(artificial intelligence); statistical analysis; text analysis;}
}

@ARTICLE{5076456,
  author = {Jimenez, M. and Rosique, F. and Sanchez, P. and Alvarez, B. and Iborra,
	A.},
  title = {Habitation: A Domain-Specific Language for Home Automation},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {30 -38},
  number = {4},
  month = {july-aug. },
  abstract = {The appearance of model-driven engineering (MDE) has invigorated research
	on domain-specific languages (DSLs) and automatic code generation.
	MDE uses models to build software, thereby displacing source code
	as the development process's main feature. DSLs provide easy, intuitive
	descriptions of the system using graphic models. In this new context,
	DSLs facilitate work in the first design stages. In addition, MDE
	helps reduce DSL development costs. It therefore represents a synergistic
	union that can significantly improve software development.},
  doi = {10.1109/MS.2009.93},
  issn = {0740-7459},
  keywords = {Habitation-domain-specific language;automatic code generation;graphic
	model;home automation;model-driven engineering;software development;source
	code;computer graphics;home automation;object-oriented programming;program
	compilers;specification languages;}
}

@INPROCEEDINGS{5260866,
  author = {Li Jin and Zhan Dechen and Nie Lanshun and Xu Xiaofei},
  title = {Design and Implementation of a MOF Based Enterprise Modeling Tool},
  booktitle = {Interoperability for Enterprise Software and Applications China,
	2009. IESA '09. International Conference on},
  year = {2009},
  pages = {76 -81},
  month = {april},
  abstract = {A MOF based model extending technology and architecture for enterprise
	modeling tool is proposed to support building enterprise models,
	which are integrated, comprehensively, precise, domain specific,
	describing enterprise in suitable levels and granularity of abstraction,
	and covering each necessary aspect. The architecture includes four
	layers, i.e. ICE-M3 Layer, ICE-M2 Layer, ICE-M1 Layer and ICE-M0
	Layer. Abstract syntax of ICE-M3 Layer defines meta-models in ICE-M2
	Layer, and graphics notation of ICE-M2 Layer defines models in ICE-M1
	Layer, and models can describe the enterprise information. Modeling
	language can be defined and extended in the architecture. The tool
	implementing this architecture has the capability of defining enterprise
	modeling languages and building understandable, domain-specific enterprise
	model by combining several modeling languages. A body assembly model
	for ship-building industry has been built by this tool and proves
	the effectiveness of it.},
  doi = {10.1109/I-ESA.2009.23},
  keywords = {ICE-M3 layer;MOF based enterprise modeling tool;abstract syntax;domain-specific
	enterprise model;enterprise modeling language;graphics notation;understandable
	enterprise model;corporate modelling;}
}

@INPROCEEDINGS{5377693,
  author = {Qiwei Jin and Thomas, D.B. and Luk, W.},
  title = {Automated application acceleration using software to hardware transformation},
  booktitle = {Field-Programmable Technology, 2009. FPT 2009. International Conference
	on},
  year = {2009},
  pages = {411 -414},
  month = {dec.},
  abstract = {This paper describes an approach that allows applications to be developed
	in a software language, while taking advantage of hardware by facilities
	that automatically transform such software programs for hardware
	accelerators. A demonstration of this approach has been built for
	the C# language. Three case studies in numerical integration show
	that the automatically generated hardware accelerators can achieve
	similar speed-ups to manually optimised versions. In particular,
	the automatically generated accelerator running on an xc4vlx160 FPGA
	at 83 MHz with single precision arithmetic can be more than 18 times
	faster and up to 143 times more power efficient than a Pentium 4
	processor at 3.6 GHz, while the double precision accelerator running
	at 64 MHz is 7 times faster and 77 times more power efficient.},
  doi = {10.1109/FPT.2009.5377693},
  keywords = {C# language;FPGA;Pentium 4 processor;automated application acceleration;frequency
	3.6 GHz;frequency 64 MHz;frequency 83 MHz;hardware accelerators;software
	language;software programs;software-hardware transformation;C language;field
	programmable gate arrays;}
}

@INPROCEEDINGS{4639070,
  author = {Johnson, G.},
  title = {FlatCAD and FlatLang: Kits by code},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {117 -120},
  month = {sept.},
  abstract = {The FlatCAD system lets you create physical construction kits by coding
	in the LOGO-like FlatLang language. Designers often use structured
	CAD tools to specify physical form. Programming offers an alternative
	and powerful method for designing shapes. This paper describes our
	experimental domain-specific language used to program and manufacture
	physical shape in the domain of construction kits.},
  doi = {10.1109/VLHCC.2008.4639070},
  issn = {1943-6092},
  keywords = {FlatCAD system;LOGO-like FlatLang language;domain-specific language;physical
	construction kit;physical shape manufacturing;shape design;solid
	modelling;CAD/CAM;solid modelling;}
}

@INPROCEEDINGS{816822,
  author = {Johnson, G.D.},
  title = {Networked simulation with HLA and MODSIM III},
  booktitle = {Simulation Conference Proceedings, 1999 Winter},
  year = {1999},
  volume = {2},
  pages = {1065 -1070 vol.2},
  abstract = {The paper describes a networked simulation application using HLA (The
	High Level Architecture) and MODSIM III, a commercial off-the-shelf
	(COTS) object oriented simulation language. The Department of Defense
	(DoD) developed HLA for training simulation exercises, but HLA is
	applicable to a wide range of simulation work far beyond wargames.
	HLA is documented in terms of C++ and Java while discrete event simulations
	are often developed in a simulation language such as MODSIM III,
	SIMSCRIPT II.5, or SLX, or by using a graphical, domain-specific
	simulator such as COMNET III, SIMPROCESS or ProModel. The requirement
	addressed by the paper is to interface an existing discrete event
	simulation model to the HLA, in order to evaluate that task and to
	set directions for future work. To further direct focus on interfaces
	between HLA and a discrete event simulation, we deliberately chose
	a small simulation application developed in MODSIM III. Real simulations
	are, of course, more detailed, but will still use the same interfaces
	},
  doi = {10.1109/WSC.1999.816822},
  keywords = {C++;Department of Defense;HLA;High Level Architecture;Java;MODSIM
	III;commercial off-the-shelf object oriented simulation language
	;discrete event simulations;military simulation;networked simulation
	application;simulation work;small simulation application;training
	simulation exercises;computer networks;discrete event simulation;military
	computing;object-oriented languages;simulation languages;}
}

@INPROCEEDINGS{994495,
  author = {Johnstone, A. and Scott, E.},
  title = {Generalised reduction modified LR parsing for domain specific language
	prototyping},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = { 3666 - 3675},
  month = {jan.},
  abstract = { Domain specific languages should support syntax that is comfortable,for
	specialist users. We discuss the impact of the standard deterministic
	parsing techniques such as LALR(1) and LL(1) on the design of programming
	languages and the desirability of more flexible parsers in a development
	environment. We present a new bottom-up nondeterministic parsing
	algorithm (GRMLR) that combines a modified notion of reduction with
	a Tomita-style breadth-first search of parallel parsing stacks. We
	give experimental results,for standard programming language grammars
	and LR(0), SLR(1) and LR(1) tables; the weaker tables generate significant
	amounts of nondeterminism. We show that GRMLR parsing corrects errors
	in the standard Tomita algorithm without incurring the performance
	overheads associated with other published solutions. We also demonstrate
	that the performance of GRMLR is upper-bounded by the performance
	of Tomita's algorithm, and that,for once realistic language grammar
	GRMLR only needs to search around 74% of the nodes. Our heavily instrumented
	development version of the algorithm achieves parsing rates of around
	4000-10000 tokens per second on a 400 MHz Pentium II processor. Proof
	of correctness and details of our implementation are omitted here
	for space reasons but are available in an accompanying technical
	report.},
  doi = {10.1109/HICSS.2002.994495},
  keywords = { 400 MHz; GRMLR; LALR(1); LL(1); LR(0) tables; LR(1) tables; Pentium
	II processor; SLR(1) tables; bottom-up nondeterministic parsing algorithm;
	breadth-first search; correctness proof; design programming languages;
	deterministic parsing techniques; development environment; domain
	specific language prototyping; generalised reduction modified LR
	parsing; nondeterminism; parallel parsing stacks; reduction; syntax;
	context-free grammars; high level languages;}
}

@INPROCEEDINGS{6070372,
  author = {Jones, Michael and Scaffidi, Christopher},
  title = {Obstacles and opportunities with using visual and domain-specific
	languages in scientific programming},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2011 IEEE
	Symposium on},
  year = {2011},
  pages = {9 -16},
  month = {sept.},
  abstract = {Scientific discovery is the lifeblood of technological progress, and
	end-user programming in turn is increasingly essential to modern
	science. In order to uncover opportunities to facilitate scientific
	programming, we interviewed scientists about their choice of tools
	and languages, as well as the obstacles resulting from those choices.
	We focused on domain-specific languages (DSLs), particularly visual
	DSLs, because prior empirical studies had not explored scientists'
	DSL use in detail. We found that DSLs were indeed used by most of
	these scientists, and in fact it was typical for scientific projects
	to use an increasing number of DSLs over time. Our study extended
	some findings from related work, and it identified obstacles not
	previously uncovered. In particular, we found that scientists often
	struggled with managing data complexity, as well as with using version
	control systems. Our study revealed several opportunities to improve
	DSLs and related tools, such as for helping scientists to cope with
	data complexity and for helping them to foresee problems when choosing
	a language.},
  doi = {10.1109/VLHCC.2011.6070372},
  issn = {1943-6092}
}

@INPROCEEDINGS{1260212,
  author = {Jonsson, T. and Hamfelt, A.},
  title = {DAViLa - a Domain Adaptable Visual Language},
  booktitle = {Human Centric Computing Languages and Environments, 2003. Proceedings.
	2003 IEEE Symposium on},
  year = {2003},
  pages = { 114 - 116},
  month = {oct.},
  abstract = { We present a framework where different visual notations may be implemented
	to suit different domains. The major characteristic of DAViLa is
	that it provides a domain independent framework to programming, adding
	components of interest for specific domains. A domain adaptation
	is achieved through implementation of domain specific components,
	which are added to the framework. The semantics of the framework
	is based on Combilog, a compositional programming method for relational
	programs. In Combilog, programming semantically is composition of
	sets of tuples.},
  doi = {10.1109/HCC.2003.1260212},
  keywords = { Combilog; DAViLa; Domain Adaptable Visual Language; compositional
	programming; domain independent programming; framework semantics;
	relational programs; visual notations; programming language semantics;
	visual languages;}
}

@INPROCEEDINGS{1620098,
  author = {A. Joolia and T. Batista and G. Coulson and A.T.A. Gomes},
  title = {Mapping ADL Specifications to an Efficient and Reconfigurable Runtime
	Component Platform},
  booktitle = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference
	on},
  year = {2005},
  pages = {131 -140},
  month = { },
  abstract = {Recent research has recognised the potential of coupling ADLs with
	underlying runtime environments to support systematic and integrated
	"specification-todeployment" architectures. However, while some promising
	results have been obtained, much of this research has not considered
	the crucial issue of causally-connected dynamic reconfiguration and
	has considered only domain-specific areas. In this paper we discuss
	a specification-to-deployment architecture called Plastik that employs
	an extended generalpurpose ADL and is underpinned by an efficient
	runtime that is suited both for high-level application development
	and low-level systems development (e.g. embedded systems). Runtime
	reconfiguration is supported both at the ADL level and at the runtime
	level, and both programmed reconfiguration and adhoc reconfiguration
	are supported. The paper focuses on the mapping of ADL-level specifications
	to runtime instantiations and on the necessary runtime support for
	causally-connected dynamic reconfiguration.},
  doi = {10.1109/WICSA.2005.42}
}

@INPROCEEDINGS{4404742,
  author = {Joshi, A. and Heimdahl, M.P.E.},
  title = {Behavioral Fault Modeling for Model-based Safety Analysis},
  booktitle = {High Assurance Systems Engineering Symposium, 2007. HASE '07. 10th
	IEEE},
  year = {2007},
  pages = {199 -208},
  month = {nov.},
  abstract = {Recent work in the area of model-based safety analysis has demonstrated
	key advantages of this methodology over traditional approaches, for
	example, the capability of automatic generation of safety artifacts.
	Since safety analysis requires knowledge of the component faults
	and failure modes, one also needs to formalize and incorporate the
	system fault behavior into the nominal system model. Fault behaviors
	typically tend to be quite varied and complex, and incorporating
	them directly into the nominal system model can clutter it severely.
	This manual process is error-prone and also makes model evolution
	difficult. These issues can be resolved by separating the fault behavior
	from the nominal system model in the form of a "fault model", and
	providing a mechanism for automatically combining the two for analysis.
	Towards implementing this approach we identify key requirements for
	a flexible behavioral fault modeling notation. We formalize it as
	a domain-specific language based on Lustre, a textual synchronous
	dataflow language. The fault modeling extensions are designed to
	be amenable for automatic composition into the nominal system model.},
  doi = {10.1109/HASE.2007.58},
  issn = {1530-2059},
  keywords = {behavioral fault modeling;domain-specific language;model-based safety
	analysis;nominal system model;textual synchronous dataflow language;formal
	specification;safety-critical software;software fault tolerance;specification
	languages;}
}

@INPROCEEDINGS{4517402,
  author = {Jouve, W. and Lancia, J. and Palix, N. and Consel, C. and Lawall,
	J.},
  title = {High-level Programming Support for Robust Pervasive Computing Applications},
  booktitle = {Pervasive Computing and Communications, 2008. PerCom 2008. Sixth
	Annual IEEE International Conference on},
  year = {2008},
  pages = {252 -255},
  month = {march},
  abstract = {In this paper, we present a domain-specific interface definition language
	(IDL) and its compiler, dedicated to the development of pervasive
	computing applications. Our IDL provides declarative support for
	concisely characterizing a pervasive computing environment. This
	description is (1) to be used by programmers as a high-level reference
	to develop applications that coordinate entities of the target environment
	and (2) to be passed to a compiler that generates a programming framework
	dedicated to the target environment. This process enables verifications
	to be performed prior to runtime on both the declared environment
	and a given application. Furthermore, customized operations are automatically
	generated to support the development of pervasive computing activities,
	such as service discovery and session negotiation for stream-oriented
	devices.},
  doi = {10.1109/PERCOM.2008.9},
  keywords = {declarative support;high-level programming support;high-level reference;interface
	definition language;robust pervasive computing applications;session
	negotiation;stream-oriented devices;target environment;program compilers;ubiquitous
	computing;}
}

@INPROCEEDINGS{5692318,
  author = {Jurez-Martnez, U. and Alor-Hernndez, G.},
  title = {Advise Weaving in CHARPx0C9;nfasis},
  booktitle = {Electronics, Robotics and Automotive Mechanics Conference (CERMA),
	2010},
  year = {2010},
  pages = {90 -95},
  month = {28 2010-oct. 1},
  abstract = {This paper describes the implementation of advice weaving in E #x0301;NFASIS.
	E #x0301;nfasis is a domain-specific framework designed to program
	fine-grained aspects and apply crosscutting on local variables. Applications
	of fine-grained aspects include data flow analysis, program comprehension,
	assertions, code coverage, among others. The E #x0301;NFASIS framework
	uses byte code instrumentation to weave statically pieces of advice.
	We describe how E #x0301;NFASIS join points are mapped to specific
	regions of byte code, how to implement advice, and how to expose
	the point cut context without using arguments between point cuts
	and pieces of advice, a novel capability in our framework not available
	in AspectJ-like languages.},
  doi = {10.1109/CERMA.2010.92},
  keywords = {Enfasis;advice weaving;assertions;bytecode;code coverage;crosscutting;data
	flow analysis;domain-specific framework;fine-grained aspects;program
	comprehension;Java;aspect-oriented programming;weaving;}
}

@INPROCEEDINGS{1545465,
  author = {Jung, E. and Kapoor, C. and Batory, D.},
  title = {Automatic code generation for actuator interfacing from a declarative
	specification},
  booktitle = {Intelligent Robots and Systems, 2005. (IROS 2005). 2005 IEEE/RSJ
	International Conference on},
  year = {2005},
  pages = { 2839 - 2844},
  month = {aug.},
  abstract = { Common software design practices use object-oriented (OO) frameworks
	that structure software in terms of objects, classes, and packages;
	designers then create programs by inheritance and composition of
	classes and objects. Operational software components for advanced
	robotics (OSCAR) is one such framework for robot control software
	with abstractions for generalized kinematics, dynamics, performance
	criteria, decision making, and hardware interfacing. Even with OSCAR,
	writing new programs still requires a significant amount of manual
	labor. Feature-oriented programming (FOP) is method for software
	design that models and specifies programs in terms of features, where
	a feature encapsulates the common design decisions that occur in
	a domain. A set of features then forms a domain model for a product
	line architecture. Product variants in this product line can then
	be generated from a declarative specification. FOP and related technologies
	are emerging software engineering techniques for automatically generating
	programs. Our research applies FOP to robot controller software.
	As an example, the domain of hardware interfacing is analyzed and
	41 features identified. A GUI for specifying and generating programs
	is presented as well. Analysis of features shows 200 possible different
	programs could be generated.},
  doi = {10.1109/IROS.2005.1545465},
  keywords = { GUI; OSCAR; actuator interfacing; automatic code generation; class
	composition; decision making; declarative specification; feature-oriented
	programming; generative programming; graphical user interface; hardware
	interfacing; inheritance; object composition; object-oriented programming;
	operational software components for advanced robotics; product line
	architecture; program generation; robot control software; robot controller
	software; robot dynamics; robot kinematics; software design; software
	engineering; actuators; formal specification; graphical user interfaces;
	inheritance; object-oriented programming; program compilers; robot
	dynamics; robot kinematics; robot programming;}
}

@INPROCEEDINGS{366642,
  author = {Kabakc Cioglu, A.M. and Mazuera, O.L.},
  title = {DEVE: An expert system approach to hardware design verification},
  booktitle = {Artificial Intelligence for Applications, 1993. Proceedings., Ninth
	Conference on},
  year = {1993},
  pages = {375 -381},
  month = {mar},
  abstract = {Introduces a knowledge-based approach to hardware verification. A
	Prolog-based expert system, DEVE, has been developed for hardware
	design verification. A formal hardware description language for the
	implementation and for the specification is the input to the verification
	system. The verification is achieved by interpreting the specification
	to invoke proper domain specific methods on the implementation model
	and by reasoning from first principles. This expert system approach
	to hardware verification integrates formal and domain specific methods
	in a knowledge-based environment. DEVE attempts to provide a verification
	tool in a knowledge-based framework by guiding the theorem-proving
	component provided by the Prolog interpreter with domain specific
	knowledge and methods},
  doi = {10.1109/CAIA.1993.366642},
  keywords = {DEVE;Prolog-based expert system;domain specific methods;formal hardware
	description language;hardware design verification;implementation;implementation
	model;interpreter;knowledge-based approach;reasoning from first principles;specification;theorem-proving
	component;expert systems;formal verification;hardware description
	languages;theorem proving;}
}

@INPROCEEDINGS{246456,
  author = {Kabakcioglu, A.M. and Mazuera, O.L.},
  title = {DEVE: an expert system for hardware design verification},
  booktitle = {Tools with Artificial Intelligence, 1992. TAI '92, Proceedings.,
	Fourth International Conference on},
  year = {1992},
  pages = {469 -470},
  month = {nov},
  abstract = {An expert system approach to digital hardware design verification
	is described. Artificial-intelligence-based approaches typically
	use general-purpose theorem-proving to show that the design meets
	the formal specification. In contrast, the expert system DEVE interprets
	the specification to invoke proper domain-specific verification methods
	in a knowledge-based environment},
  doi = {10.1109/TAI.1992.246456},
  keywords = { DEVE; domain-specific verification methods; expert system; formal
	specification; hardware design verification; knowledge-based environment;
	circuit analysis computing; expert systems; formal verification;}
}

@INPROCEEDINGS{1579139,
  author = {Kaiya, H. and Saeki, M.},
  title = {Ontology based requirements analysis: lightweight semantic processing
	approach},
  booktitle = {Quality Software, 2005. (QSIC 2005). Fifth International Conference
	on},
  year = {2005},
  pages = { 223 - 230},
  month = {sept.},
  abstract = { We propose a software requirements analysis method based on domain
	ontology technique, where we can establish a mapping between a software
	requirements specification and the domain ontology that represents
	semantic components. Our ontology system consists of a thesaurus
	and inference rules and the thesaurus part comprises domain specific
	concepts and relationships suitable for semantic processing. It allows
	requirements engineers to analyze a requirements specification with
	respect to the semantics of the application domain. More concretely,
	we demonstrate following three kinds of semantic processing through
	a case study, (1) detecting incompleteness and inconsistency included
	in a requirements specification, (2) measuring the quality of a specification
	with respect to its meaning and (3) predicting requirements changes
	based on semantic analysis on a change history.},
  doi = {10.1109/QSIC.2005.46},
  issn = {1550-6002 },
  keywords = { domain ontology technique; lightweight semantic processing approach;
	ontology based requirements analysis; requirements specification;
	software requirements analysis method; formal specification; formal
	verification; ontologies (artificial intelligence);}
}

@ARTICLE{683736,
  author = {Kale, L.V.},
  title = {Programming languages for CSE: the state of the art},
  journal = {Computational Science Engineering, IEEE},
  year = {1998},
  volume = {5},
  pages = {18 -26},
  number = {2},
  month = {apr-jun},
  abstract = {To meet the diverse demands of building CSE applications, developers
	can choose from a multitude of programming languages. This survey
	offers an overview of available programming languages and the contexts
	for their use},
  doi = {10.1109/99.683736},
  issn = {1070-9924},
  keywords = {C++;CSE applications;Fortran;JAVA;domain specific langauges;integrated
	languages;parallel languages;programming languages;scripting languages;C
	language;FORTRAN;engineering computing;message passing;natural sciences
	computing;object-oriented languages;parallel languages;}
}

@INPROCEEDINGS{4406104,
  author = {Kai Kang and Kunhui Lin and Changle Zhou and Feng Guo},
  title = {Domain-Specific Information Retrieval Based on Improved Language
	Model},
  booktitle = {Fuzzy Systems and Knowledge Discovery, 2007. FSKD 2007. Fourth International
	Conference on},
  year = {2007},
  volume = {2},
  pages = {374 -378},
  month = {aug.},
  abstract = {There are two key ingredients in the general framework of language
	models used in information retrieval, one is importance weighting,
	the other is word relationship computing. A series of improvements
	are made to these ingredients of the general framework of language
	models which is used in domain-specific information retrieval. First,
	an EM algorithm is proposed to estimate the importance weights of
	query terms, and the Bayesian smoothing is used to compute the productive
	probabilities of important terms. Next, a new algorithm based on
	Dynamic Bayesian Network is proposed for obtaining the explanation
	probabilities between terms. Experiment shows that the improved model
	performs remarkably better for domain-specific information retrieval
	than some traditional retrieval techniques, and the extended framework
	has good expansibility.},
  doi = {10.1109/FSKD.2007.261},
  keywords = {Bayesian smoothing;domain-specific information retrieval;dynamic Bayesian
	network;word relationship computing;Bayes methods;information retrieval;}
}

@INPROCEEDINGS{5452700,
  author = {Kara, S. and Alan, O. and Sabuncu, O. and Akpinar, S. and Cicekli,
	N.K. and Alpaslan, F.N.},
  title = {An ontology-based retrieval system using semantic indexing},
  booktitle = {Data Engineering Workshops (ICDEW), 2010 IEEE 26th International
	Conference on},
  year = {2010},
  pages = {197 -202},
  month = {march},
  abstract = {In this paper, we present an ontology-based information extraction
	and retrieval system and its application to soccer domain. In general,
	we deal with three issues in semantic search, namely, usability,
	scalability and retrieval performance. We propose a keyword-based
	semantic retrieval approach. The performance of the system is improved
	considerably using domain-specific information extraction, inference
	and rules. Scalability is achieved by adapting a semantic indexing
	approach. We implement the system using the state-of-the-art technologies
	in Semantic Web and evaluate the performance against traditional
	systems. Further detailed evaluation is provided to observe the performance
	gain due to domain-specific information extraction and inference.},
  doi = {10.1109/ICDEW.2010.5452700},
  keywords = {domain-specific information extraction;inference;information retrieval
	system;keyword-based semantic retrieval;ontology-based information
	extraction;ontology-based retrieval system;retrieval performance;scalability;semantic
	Web;semantic indexing;semantic search;soccer domain;usability;indexing;information
	retrieval;ontologies (artificial intelligence);semantic Web;}
}

@INPROCEEDINGS{4222618,
  author = {Karaila, M. and Systa, T.},
  title = {Applying Template Meta-Programming Techniques for a Domain-Specific
	Visual Language--An Industrial Experience Report},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {571 -580},
  month = {may},
  abstract = {Template meta-programming techniques can be used to increase efficiency
	in software development. These techniques have traditionally been
	used with textual programming languages, such as C++. In this paper,
	we discuss how corresponding techniques can be used with visual languages.
	The visual language under study in this paper is function block language
	(FBL). FBL is used in Metso Automation for writing automation control
	programs that are executed in realtime distributed environments.
	Efficient development of high quality programs and easy customizability
	of existing programs are key requirements in practical customer projects.
	These requirements have been one of the main motivations to develop
	template meta-programming support in FBL discussed. In this paper,
	we focus both on the technical aspects and on the lessons learnt
	from programmers' experiences and ways to work with templates. FBL
	and the programming techniques proposed have been used in hundreds
	of real-world projects at Metso Automation.},
  doi = {10.1109/ICSE.2007.16},
  issn = {0270-5257},
  keywords = {Metso Automation;automation control programs;domain-specific visual
	language;function block language;template meta-programming techniques;control
	engineering computing;metacomputing;visual languages;visual programming;}
}

@INPROCEEDINGS{1402125,
  author = { Karaila, M. and Systa, T.},
  title = {Maintenance and Analysis of Visual Programs CHARP8212; An Industrial
	Case},
  booktitle = {Software Maintenance and Reengineering, 2005. CSMR 2005. Ninth European
	Conference on},
  year = {2005},
  pages = { 158 - 167},
  month = {march},
  abstract = { A domain-specific visual language, Function Block Language (FBL),
	is used in Metso Automation for writing automation control programs.
	The same engineering environment is used for both forward and reverse
	engineering activities, providing convenient support for the maintenance
	and evolution of FBL programs. Various data and program analysis
	methods are applied to study the FBL programs stored in project library
	archives. Metadata stored about the program allows various kinds
	of queries and enables focusing the analysis to certain kinds of
	programs. The application of the provided analysis methods further
	aids the maintenance and reuse activities. Software and data reverse
	engineering techniques are traditionally used to support program
	and data comprehension, respectively. In this paper we show how corresponding
	techniques can be used to analyze visual programs. The visual language
	under study in this paper is FBL. FBL and the analysis techniques
	proposed have been used in real-world projects at Metso Automation.},
  doi = {10.1109/CSMR.2005.36},
  issn = {1534-5351 },
  keywords = { Metso Automation; automation control program; domain-specific visual
	language; function block language; metadata; program analysis; project
	library archives; reverse engineering; software maintenance; software
	reuse; meta data; program diagnostics; reverse engineering; software
	maintenance; software reusability; visual languages; visual programming;}
}

@INPROCEEDINGS{1374329,
  author = {Karaila, M. and Systa, T.},
  title = {Maintenance and analysis of visual programs - an industrial case},
  booktitle = {Reverse Engineering, 2004. Proceedings. 11th Working Conference on},
  year = {2004},
  pages = { 294 - 295},
  month = {nov.},
  abstract = { A domain-specific visual language, Function Block Language (FBL),
	is used in Metso Automation for writing automation control programs.
	The same engineering environment is used for both forward and reverse
	engineering activities, providing convenient support for the maintenance
	and evolution of FBL programs. Various data and program analysis
	methods are applied to study the FBL programs stored in project library
	archives. From the large amount of programs, the analysis can be
	focused to certain kinds of programs. Using metadata stored in a
	database enables this. The application of the provided analysis methods
	further aids the reverse engineering, re-engineering, and reuse activities.
	Software and data reverse engineering techniques are traditionally
	used to support program and data comprehension, respectively. In
	our research, we have successfully used corresponding techniques
	to analyze visual programs written in FBL in real-world projects
	at Metso Automation.},
  doi = {10.1109/WCRE.2004.27},
  issn = {1095-1350},
  keywords = { FBL program evolution; Function Block Language; Metso Automation;
	automation control program writing; data analysis; data comprehension;
	domain-specific visual language; engineering environment; forward
	engineering; metadata; program analysis; program comprehension; project
	library archives; reverse engineering; software maintenance; software
	reuse; system reengineering; visual programs; meta data; program
	diagnostics; reverse engineering; software maintenance; software
	prototyping; software reusability; systems re-engineering; visual
	languages; visual programming;}
}

@INPROCEEDINGS{5298887,
  author = {Karakoidas, V. and Spinellis, D.},
  title = {J CHARPx025;: Integrating Domain-Specific Languages with Java},
  booktitle = {Informatics, 2009. PCI '09. 13th Panhellenic Conference on},
  year = {2009},
  pages = {109 -113},
  month = {sept.},
  abstract = {J% (J-mod), is a Java language extension that supports integration
	with domain-specific languages (DSLs). The integration is realized
	through an architecture that permits external modules to support
	DSLs. The DSL statements can be syntactically checked at compile-time.
	An additional facility allows the static type checking of Java variables
	that appear within DSL code. To support this process each DSL module
	comes as a library that is used both at compile time and during program
	execution.},
  doi = {10.1109/PCI.2009.26},
  keywords = {DSL code;J-mod;Java language extension;domain-specific languages;static
	type checking;Java;program compilers;program diagnostics;software
	architecture;specification languages;}
}

@INPROCEEDINGS{5764682,
  author = {Karrenberg, R. and Hack, S.},
  title = {Whole-function vectorization},
  booktitle = {Code Generation and Optimization (CGO), 2011 9th Annual IEEE/ACM
	International Symposium on},
  year = {2011},
  pages = {141 -150},
  month = {april},
  abstract = {Data-parallel programming languages are an important component in
	today's parallel computing landscape. Among those are domain-specific
	languages like shading languages in graphics (HLSL, GLSL, RenderMan,
	etc.) and #x201C;general-purpose #x201D; languages like CUDA or OpenCL.
	Current implementations of those languages on CPUs solely rely on
	multi-threading to implement parallelism and ignore the additional
	intra-core parallelism provided by the SIMD instruction set of those
	processors (like Intel's SSE and the upcoming AVX or Larrabee instruction
	sets). In this paper, we discuss several aspects of implementing
	dataparallel languages on machines with SIMD instruction sets. Our
	main contribution is a language- and platform-independent code transformation
	that performs whole-function vectorization on low-level intermediate
	code given by a control flow graph in SSA form. We evaluate our technique
	in two scenarios: First, incorporated in a compiler for a domain-specific
	language used in realtime ray tracing. Second, in a stand-alone OpenCL
	driver. We observe average speedup factors of 3.9 for the ray tracer
	and factors between 0.6 and 5.2 for different OpenCL kernels.},
  doi = {10.1109/CGO.2011.5764682},
  keywords = {AVX instruction set;CUDA;GLSL;HLSL;Intel SSE;Larrabee instruction
	set;OpenCL kernels;OpenGL;RenderMan;SIMD instruction set;compiler;control
	flow graph;data-parallel programming language;domain-specific language;general-purpose
	languages;intracore parallelism;language-independent code transformation;multithreading;parallel
	computing;platform-independent code transformation;realtime ray tracing;shading
	language;whole-function vectorization;data flow graphs;instruction
	sets;parallel languages;parallel programming;parallelising compilers;}
}

@ARTICLE{4629341,
  author = {Karuri, K. and Chattopadhyay, A. and Xiaolin Chen and Kammler, D.
	and Ling Hao and Leupers, R. and Meyr, H. and Ascheid, G.},
  title = {A Design Flow for Architecture Exploration and Implementation of
	Partially Reconfigurable Processors},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {2008},
  volume = {16},
  pages = {1281 -1294},
  number = {10},
  month = {oct. },
  abstract = {During the last years, the growing application complexity, design,
	and mask costs have compelled embedded system designers to increasingly
	consider partially reconfigurable application-specific instruction
	set processors (rASIPs) which combine a programmable base processor
	with a reconfigurable fabric. Although such processors promise to
	deliver excellent balance between performance and flexibility, their
	design remains a challenging task. The key to the successful design
	of a rASIP is combined architecture exploration of all the three
	major components: the programmable core, the reconfigurable fabric,
	and the interfaces between these two. This work presents a design
	flow that supports fast architecture exploration for rASIPs. The
	design flow is centered around a unified description of an entire
	rASIP in an architecture description language (ADL). This ADL description
	facilitates consistent modeling and exploration of all three components
	of a rASIP through automatic generation of the software tools (compiler
	tool chain and instruction set simulator) and the RTL hardware model.
	The generated software tools and the RTL model can be used either
	for final implementation of the rASIP or can serve as a preoptimized
	starting point for implementation that can be hand optimized afterward.
	The design flow is further enhanced by a number of automatic application
	analysis tools, including a fine-grained application profiler, an
	instruction set extension (ISE) generator, and a data path mapper
	for coarse grained reconfigurable architectures (CGRAs). We present
	some case studies on embedded benchmarks to show how the design space
	exploration process helps to efficiently design an application domain
	specific rASIP.},
  doi = {10.1109/TVLSI.2008.2002685},
  issn = {1063-8210},
  keywords = {RTL hardware model;architecture description language;architecture
	exploration;automatic application analysis tools;coarse grained reconfigurable
	architectures;compiler tool chain;data path mapper;design flow;embedded
	system;fine-grained application profiler;instruction set extension
	generator;instruction set simulator;partially reconfigurable processors;programmable
	base processor;rASIP;reconfigurable application-specific instruction
	set processors;reconfigurable fabric;software tools;instruction sets;integrated
	circuit design;microprocessor chips;reconfigurable architectures;}
}

@INPROCEEDINGS{607992,
  author = {Katae, N. and Kimura, S.},
  title = {Natural prosody generation for domain specific text-to-speech systems},
  booktitle = {Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International
	Conference on},
  year = {1996},
  volume = {3},
  pages = {1852 -1855 vol.3},
  month = {oct},
  abstract = {The paper proposes a method for generating natural prosody for text
	to speech systems. In this method, sentences are composed by inlaying
	a variable word into each slot in prepared sentence structures. This
	method can be used for domain specific text to speech applications
	that don't require so many sentence structures but many words. Important
	parameters for prosody are declination of F0 contour, accent strength
	of word, and position and duration of pause. So we construct a database
	containing these parameters, manually extracted from natural speech
	samples that have the sentence structures to be synthesized. In the
	process of prosody generation, these parameters are retrieved by
	the type of the sentence structure, and the other parameters are
	generated with rules. The F0 contour is generated by superposing
	these components on baseline frequency. Mean opinion score of prosody
	naturalness for the speech synthesized by the proposed method is
	3.6. This is 1.2 points better than that of the former method},
  doi = {10.1109/ICSLP.1996.607992},
  keywords = {F0 contour;accent strength;baseline frequency;database;domain specific
	text to speech applications;domain specific text to speech systems;mean
	opinion score;natural prosody generation;natural speech samples;prepared
	sentence structures;prosody naturalness;sentence structures;speech
	synthesis;variable word;database management systems;natural languages;speech
	synthesis;word processing;}
}

@INPROCEEDINGS{4566948,
  author = {Kavimandan, A. and Gokhale, A.},
  title = {Automated Middleware QoS Configuration Techniques using Model Transformations},
  booktitle = {EDOC Conference Workshop, 2007. EDOC '07. Eleventh International
	IEEE},
  year = {2007},
  pages = {20 -27},
  month = {oct.},
  abstract = {This paper provides following three contributions to the study of
	developing and applying model driven engineering (MDE) techniques
	to quality of service (QoS) configuration of distributed real-time
	and embedded (DRE) systems. First, we describe the challenges associated
	with mapping domain-level QoS policies of DRE systems to middleware
	configuration space. Second, we discuss a domain specific modeling
	language (DSML) to capture QoS requirements of DRE system at a higher
	level of abstraction, simplifying the system QoS specification process.
	Third, we describe model transformations to automate the mapping
	of domain-specific QoS requirements.Our results indicate that our
	approach provides significant benefits in terms of productivity,
	scalability, reusability and automation of middleware QoS mapping
	compared to traditional QoS configuration techniques for publish/subscribe-based
	DRE systems.},
  doi = {10.1109/EDOCW.2007.5},
  keywords = {automated middleware QoS configuration techniques;distributed real-time
	and embedded systems;domain specific modeling language;model driven
	engineering;model transformations;quality of service configuration;formal
	specification;middleware;quality of service;software engineering;}
}

@INPROCEEDINGS{1174891,
  author = {Kawashima, H. and Gondow, K.},
  title = {Experience with ANSI C markup language for a cross-referencer},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { The purpose of this paper is twofold: (1) to examine the properties
	of our ANSI C markup language (ACML) as a domain-specific language
	(DSL); and (2) to show that ACML is useful as a DSL by implementing
	an ANSI C cross-referencer using ACML. We have introduced ACML as
	a DSL for developing CASE tools. ACML is defined as a set of XML
	tags and attributes, and describes ANSI C program's syntax trees,
	types, symbol tables, and so on. That is, ACML is the DSL which plays
	the role of intermediate representation among CASE tools. ACML-tagged
	documents are automatically generated, from ANSI C programs, and
	then used as input of CASE tools. ACML is self-descriptive and has
	CASE-tool specific information, which results in high productivity
	of CASE tools. To show this, we experimentally implemented an ANSI
	C cross-referencer based on ACML. In the implementation, we had a
	good result; it took only 0.5 man-month.},
  doi = {10.1109/HICSS.2003.1174891},
  issn = { },
  keywords = { ANSI C cross-referencer; ANSI C markup language; ANSI C programs;
	CASE tools; XML tags; attributes; computer-aided software engineering;
	domain-specific language; symbol tables; syntax trees; syntax types;
	ANSI standards; C language; computational linguistics; computer aided
	software engineering; formal specification; program compilers; program
	slicing; software tools; specification languages;}
}

@INPROCEEDINGS{552828,
  author = {Keane, J. and Ellman, T.},
  title = {Knowledge-based re-engineering of legacy programs for robustness
	in automated design},
  booktitle = {Knowledge-Based Software Engineering Conference, 1996., Proceedings
	of the 11th},
  year = {1996},
  pages = {104 -113},
  month = {sep},
  abstract = {Systems for automated design optimization of complex real-world objects
	can, in principle, be constructed by combining domain-independent
	numerical routines with existing domain-specific analysis and simulation
	programs. Unfortunately, such ldquo;legacy rdquo; analysis codes
	are frequently unsuitable for use in automated design. They may crash
	for large classes of input, be numerically unstable or locally non-smooth,
	or be highly sensitive to control parameters. To be useful, analysis
	programs must be modified to reduce or eliminate only the undesired
	behaviors, without altering the desired computation. To do this by
	direct modification of the programs is labor-intensive, and necessitates
	costly revalidation. We have implemented a high-level language and
	runtime environment that allow failure-handling strategies to be
	incorporated into existing Fortran and C analysis programs while
	preserving their computational integrity. Our approach relies on
	globally managing the execution of these programs at the level of
	discretely callable functions so that the computation is only affected
	when problems are detected. Problem handling procedures are constructed
	from a knowledge base of generic problem management strategies. We
	show that our approach is effective in improving analysis program
	robustness and design optimization performance in the domain of conceptual
	design of jet engine nozzles},
  doi = {10.1109/KBSE.1996.552828},
  keywords = {C programs;CAD;Fortran;automated design optimization;complex real-world
	objects;computational integrity;cost;domain-independent numerical
	routines;domain-specific analysis;failure-handling strategies;high-level
	language;jet engine nozzles;knowledge base;knowledge-based re-engineering;labor-intensive;legacy
	programs;numerical stability;problem handling;problem management
	strategies;runtime environment;simulation programs;aerospace computing;aerospace
	engines;high level languages;intelligent design assistants;knowledge
	based systems;mechanical engineering computing;numerical stability;software
	fault tolerance;systems re-engineering;}
}

@INPROCEEDINGS{5697973,
  author = {Kearney, K.T. and Torelli, F. and Kotsokalis, C.},
  title = {SLA CHARPx2605;: An abstract syntax for Service Level Agreements},
  booktitle = {Grid Computing (GRID), 2010 11th IEEE/ACM International Conference
	on},
  year = {2010},
  pages = {217 -224},
  month = {oct.},
  abstract = {This paper describes SLA*, a domain-independent syntax for machine-readable
	Service Level Agreements (SLAs) and SLA templates. Historically,
	SLA* was developed as a generalisation and refinement of the web-service
	specific XML standards: WS-Agreement, WSLA, and WSDL. Instead of
	web-services, however, SLA* deals with services in general, and instead
	of XML, it is language independent. SLA* provides a specification
	of SLA(T) content at a fine-grained level of detail, which is both
	richly expressive and inherently extensible: supporting controlled
	customisation to arbitrary domain-specific requirements. The model
	was developed as part of the FP7 ICT Integrated Project SLA@SOI,
	and has been applied to a range of industrial use-cases, including;
	ERP hosting, Enterprise IT, live-media streaming and health-care
	provision. At the time of writing, the abstract syntax has been realised
	in concrete form as a Java API, XML-Schema, and BNF Grammar.},
  doi = {10.1109/GRID.2010.5697973},
  keywords = {BNF grammar;ERP hosting;Java API;WS agreement;WSDL;WSLA;Web service
	specific XML standards;abstract syntax;enterprise IT;health care
	provision;live media streaming;service level agreements;Web services;XML;business
	data processing;}
}

@INPROCEEDINGS{4736789,
  author = {Keep, A. and Chauhan, A.},
  title = {Concrete Partial Evaluation in Ruby},
  booktitle = {eScience, 2008. eScience '08. IEEE Fourth International Conference
	on},
  year = {2008},
  pages = {346 -347},
  month = {dec.},
  abstract = {Modern scientific research is a collaborative process, with researchers
	from many disciplines and institutions working toward a common goal.
	Dynamic languages, like Ruby, provide a platform for quickly developing
	simulation and analysis tools, freeing researchers to focus on research
	instead of spending time developing infrastructure. Ruby is a particularly
	good fit, allowing incorporation of existing C libraries, simplifying
	Domain Specific Language creation, and providing both REST and SOAP
	web-based API libraries. Ruby also provides RPC-style distributed
	programming. Concrete partial evaluation of Ruby begins to address
	Ruby's biggest flaw, performance. The scientific community has already
	begun to recognize the potential of Ruby. An MPI extension to the
	language allows quick prototyping of MPI programs. More recently
	libraries supporting MapReduce have appeared. Web frameworks, such
	as the popular Ruby on Rails framework, provide tools for producing
	and consuming REST APIs.},
  doi = {10.1109/eScience.2008.141},
  keywords = {C libraries;MPI extension;REST Web-based API libraries;RPC-style distributed
	programming;Ruby;SOAP Web-based API libraries;analysis tools;collaborative
	process;concrete partial evaluation;domain specific language creation;dynamic
	languages;scientific research;simulation tools;Internet;application
	program interfaces;distributed programming;high level languages;remote
	procedure calls;software libraries;}
}

@ARTICLE{5599423,
  author = {Kelil, A. and Nordell-Markovits, A. and Zaralahy, P.O.Y. and Shengrui
	Wang},
  title = {CLASS: a general approach to classifying categorical sequences},
  journal = {Electrical and Computer Engineering, Canadian Journal of},
  year = {2009},
  volume = {34},
  pages = {158 -166},
  number = {4},
  month = {fall },
  abstract = {The rapid burgeoning of available data in the form of categorical
	sequences, such as biological sequences, natural language texts,
	network and retail transactions, makes the classification of categorical
	sequences increasingly important. The main challenge is to identify
	significant features hidden behind the chronological and structural
	dependencies characterizing their intrinsic properties. Almost all
	existing algorithms designed to perform this task are based on the
	matching of patterns in chronological order, but categorical sequences
	often have similar features in non-chronological order. In addition,
	these algorithms have serious difficulties in outperforming domain-specific
	algorithms. In this paper we propose CLASS, a general approach for
	the classification of categorical sequences. By using an effective
	matching scheme called SPM for Significant Patterns Matching, CLASS
	is able to capture the intrinsic properties of categorical sequences.
	Furthermore, the use of Latent Semantic Analysis allows capturing
	semantic relations using global information extracted from large
	number of sequences, rather than comparing merely pairs of sequences.
	Moreover, CLASS employs a classifier called SNN for Significant Nearest
	Neighbours, inspired from the K Nearest Neighbours approach with
	a dynamic estimation of K, which allows the reduction of both false
	positives and false negatives in the classification. The extensive
	tests performed on a range of datasets from different fields show
	that CLASS is oftentimes competitive with domain-specific approaches.},
  doi = {10.1109/CJECE.2009.5599423},
  issn = {0840-8688},
  keywords = {CLASS;K nearest neighbour approach;SNN;SPM;categorical sequences;latent
	semantic analysis;matching scheme;significant nearest neighbour;significant
	patterns matching;pattern matching;statistical analysis;}
}

@INPROCEEDINGS{4425062,
  author = {Keller, R.E. and Poli, R.},
  title = {Linear genetic programming of parsimonious metaheuristics},
  booktitle = {Evolutionary Computation, 2007. CEC 2007. IEEE Congress on},
  year = {2007},
  pages = {4508 -4515},
  month = {sept.},
  abstract = {We use a form of grammar-based linear Genetic Programming (GP) as
	a hyperheuristic, i.e., a search heuristic on the space of heuristics.
	This technique is guided by domain- specific languages that one designs
	taking inspiration from elementary components of specialised heuristics
	and metaheuristics for a domain. We demonstrate this approach for
	traveling- salesperson problems for which we test different languages,
	including one containing a looping construct. Experimentation with
	benchmark instances from the TSPLIB shows that the GP hyperheuristic
	routinely and rapidly produces parsimonious metaheuristics that find
	tours whose lengths are highly competitive with the best real-valued
	lengths from literature.},
  doi = {10.1109/CEC.2007.4425062},
  keywords = {grammar-based linear genetic programming search heuristic;parsimonious
	metaheuristics;traveling-salesperson problem;genetic algorithms;grammars;heuristic
	programming;linear programming;travelling salesman problems;}
}

@INPROCEEDINGS{1509480,
  author = {Kelly, S.},
  title = {Implementing Domain-Specific Modeling Languages and Generators},
  booktitle = {Visual Languages and Human-Centric Computing, 2005 IEEE Symposium
	on},
  year = {2005},
  pages = { 10},
  month = {sept.},
  doi = {10.1109/VLHCC.2005.47}
}

@ARTICLE{5076455,
  author = {Kelly, S. and Pohjonen, R.},
  title = {Worst Practices for Domain-Specific Modeling},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {22 -29},
  number = {4},
  month = {july-aug. },
  abstract = {Interest in creating domain-specific modeling (DSM) languages is surging,
	but little guidance is available on how to do it right. Along with
	heeding best practices, learning what not to do-including how to
	handle common pitfalls and recognize troublesome areas-can help first-time
	developers. The authors have identified several worst practices based
	on an analysis of 76 DSM cases spanning 15 years, four continents,
	several tools, around 100 language creators, and projects with from
	three to more than 300 modelers. They present these worst practices
	in the order that language developers would encounter them over the
	life of a project.},
  doi = {10.1109/MS.2009.109},
  issn = {0740-7459},
  keywords = {DSM language;domain-specific modeling languages;language developers;simulation
	languages;}
}

@INPROCEEDINGS{845999,
  author = {Kennedy, K.},
  title = {Telescoping languages: a compiler strategy for implementation of
	high-level domain-specific programming systems},
  booktitle = {Parallel and Distributed Processing Symposium, 2000. IPDPS 2000.
	Proceedings. 14th International},
  year = {2000},
  pages = {297 -304},
  abstract = {As both machines and programs have become more complex, the programming
	process has become correspondingly more labor-intensive. This has
	created a software gap between the need for new software and the
	aggregate capacity of the current workforce to produce it. This problem
	has been compounded by the slow growth of programming productivity
	over the past two decades. One way to bridge this gap is to make
	it possible end users to develop programs in high-level domain-specific
	programming systems. The principal impediment to the success of these
	systems in the past has be the poor performance of the resulting
	applications. To address this problem, we have developed a new compiler
	technology that supports script-based telescoping languages, which
	can be built from base languages and domain-specific libraries. By
	exhaustively compiling the libraries in advance, we can ensure that
	the performance and portability of the applications produced by such
	systems are high, while the compile times for scripts are acceptable
	to the end user These qualities are essential if script-based systems
	are to be practical for development of production applications},
  doi = {10.1109/IPDPS.2000.845999},
  keywords = {compiler strategy;compiler technology;domain-specific programming
	systems;programming productivity;script-based telescoping languages;high
	level languages;program compilers;}
}

@ARTICLE{5282493,
  author = {Kern, W and Silberbauer, C and Wolff, C},
  title = {The Dimension Architecture: A New Approach to Resource Access},
  journal = {Software, IEEE},
  year = {2009},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {A common and recurring issue in developing software systems is the
	creating of resource access solutions. This seemingly trivial requirement
	becomes increasingly complex if pluggable and generic access is asked
	for. Usually plugin concepts, Factory Pattern based approaches or
	concepts like the Java IO Streaming Architecture, are applied to
	solve this kind of problems. In this article, we suggest a new generic
	though practical approach to resource access. It is based on the
	separation of the resource access aspects, e. g. address, content
	format and location type, to allow their flexible, configurable combination.
	The approach will be illustrated by sample code showing parts of
	its reference implementation and application.},
  doi = {10.1109/MS.2009.165},
  issn = {0740-7459}
}

@INPROCEEDINGS{1600207,
  author = { Keutzer, K.},
  title = {Panel: What Is the Next Big Productivity Boost for Designers?},
  booktitle = {Design Automation, 1993. 30th Conference on},
  year = {1993},
  pages = { 141},
  month = {june},
  abstract = { Keeping up with the significant capacity increases in VLSI processing
	has required continuous design productivity improvements from VLSI
	designers. The transition from transistor-level entry to gate-level
	schematic entry enabled an order-of-magnitude improvement in designer
	productivity. Similarly the transition from schematic entry to HDL
	synthesis has again enabled another order-or-magnitude improvement
	in designer productivity. Hardware designers have come to take such
	improvements for granted but such improvements are nearly unparalleled
	in design science. Nevertheless, processing continues to improve
	and fully exploiting the capability offered by mega-cell gate arrays
	will almost certainly require another leap in design productivity.
	So where will the next productivity boost come from? The panel will
	try to survey each of the potential answers to this question. For
	example, high-level synthesis has long been supposed to be the successor
	to logic synthesis and a great deal of research has been done in
	this area. However, some designers are still skeptical of the ability
	of high-level synthesis to deliver either higher productivity or
	the required circuit quality. Circuit design using pre-designed design
	libraries of regularly used components certainly appears to have
	the potential to boost productivity, but some designers doubt that
	pre-designed modules are flexible enough to be of much use. The failure
	of silicon compilers to succeed as a general design paradigm is used
	to support this negative assessment. Another alternative is the use
	of domain-specific entry mechanisms such as statecharts or dataflow
	diagrams. These description methods provide abstractions that are
	higher level than conventional HDL's and are well-tuned to their
	particular applications, such as control systems or signal processors.
	Still, some designers point to the fact that most hardware designs
	cross more than one domain and therefore these tools are too domain-specific
	to make an impact on the general problem of hardware design. A truly
	radical alternative is that the real productivity boost in hardware
	design will not come from improvements in hardware design methods
	at all, but rather from moving more and more hardware functionality
	into software running on processors. Thus the- source of the required
	productivity boost in hardware design is not at all clear. In this
	panel we will debate the worth of each of the proposed productivity
	improvers and attempt to decide for ourselves: "Where's the next
	productivity boost?"},
  doi = {10.1109/DAC.1993.203934},
  issn = {0738-100X}
}

@INPROCEEDINGS{4626451,
  author = {Khaled, A. and Ma, Y. and Miller, J.},
  title = {A Service Oriented Architecture for CAX concurrent collaboration},
  booktitle = {Automation Science and Engineering, 2008. CASE 2008. IEEE International
	Conference on},
  year = {2008},
  pages = {650 -655},
  month = {aug.},
  abstract = {The competitive and open market nature demands different vendors to
	collaborate during the product life cycle and to reduce the productpsilas
	time to market. In this paper, we propose an infrastructure to enable
	the concurrent collaboration of heterogeneous CAX tools at the feature
	level using a service oriented architecture (SOA) approach. A feature
	markup language (FML) is proposed as the modeling language for feature
	representation and exchange which can be independent to operating
	system and programming language. How to employ the concept of software
	factory to leverage FML as a domain specific language (DSL) is discussed
	for the process of feature development and distribution. Moreover,
	the underlying architecture is described to enable CAX information
	sharing in real-time preserving the semantics and consistency of
	CAX models.},
  doi = {10.1109/COASE.2008.4626451},
  keywords = {CAX data model consistency;CAX information sharing;concurrent engineering;domain
	specific language;feature markup language;heterogeneous CAX concurrent
	collaboration tool;product life cycle management;service oriented
	architecture;software factory;time-to-market reduction;CAD/CAM;Web
	services;concurrent engineering;data models;groupware;product life
	cycle management;time to market;}
}

@INPROCEEDINGS{5461774,
  author = {Khamis, R. and Shatnawi, S.},
  title = {Toward enhanced Natural Language Processing to databases: Building
	a specific domain Ontology derived from database conceptual model},
  booktitle = {Informatics and Systems (INFOS), 2010 The 7th International Conference
	on},
  year = {2010},
  pages = {1 -8},
  month = {march},
  abstract = {Natural Language Interface to database NLIDB applications achieve
	great success when dealing with simple user requests, however most
	of NLIDB applications fail dramatically when users issue indirect
	or sophisticated requests. One modern approach to enhance NLIDB is
	using Ontology. Ontologies are very helpful when used with Natural
	Language Processing applications for supporting extraction of relevant
	elements from databases. This paper proposes a framework and a semi
	automatic procedure for building domain specific Ontology by using
	data conceptual model and general purpose Ontology such as WordNet.
	The aim is to help NLIDB understanding and simplifying indirect users
	data requests.},
  keywords = {WordNet;database conceptual model;general purpose ontology;natural
	language interface to databases;natural language processing;specific
	domain ontology;database management systems;natural language interfaces;natural
	language processing;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{1269064,
  author = {Khawam, S. and Baloch, S. and Pai, A. and Ahmed, I. and Aydin, N.
	and Arslan, T. and Westall, F.},
  title = {Efficient implementations of mobile video computations on domain-specific
	reconfigurable arrays},
  booktitle = {Design, Automation and Test in Europe Conference and Exhibition,
	2004. Proceedings},
  year = {2004},
  volume = {2},
  pages = {1 -6},
  month = {feb.},
  abstract = {Mobile video processing as defined in standards like MPEG-4 and H.263
	contains a number of timeconsuming computations that cannot be efficiently
	executed on current hardware architectures. The authors recently
	introduced a reconfigurable SoC platform that permits a low-power,
	high-throughput and flexible implementation of the motion estimation
	and DCT algorithms. The computations are done using domainspecific
	reconfigurable arrays that have demonstrated up to 75% reduction
	in power consumption when compared to generic FPGA architecture,
	which makes them suitable for portable devices. This paper presents
	and compares different configurations of the arrays to efficiently
	implementing DCT and motion estimation algorithms. A number of algorithms
	are mapped into the various reconfigurable fabrics demonstrating
	the flexibility of the new reconfigurable SoC architecture and its
	ability to support a number of implementations having different performance
	characteristics.},
  doi = {10.1109/DATE.2004.1269064},
  issn = {1530-1591},
  keywords = {SAT;SoC testing;SystemC;SystemVerilog;TPG;analogue system performance
	modelling;analogue test;architectural-level power management;architecture
	exploration;circuit-level performance modelling;communication-centric
	optimisations;diagnosis constrained testing;digital systems simulation;energy
	efficiency;formal verification;functional information;hardware-software
	system design;high security smartcards;high-frequency test;high-level
	synthesis;interconnect technology scaling;low power systems;low-power
	design;low-power logic;memory hierarchies;memory usage;mixed-signal
	circuits;nanometer technologies;on-line testing;parasitic-aware analogue
	design;power aware design;reconfigurable computing;reliability;scheduling;source-level
	optimisations;structural information;system level analysis;system
	level design;system level modelling;C language;analogue-digital conversion;automatic
	test pattern generation;digital simulation;fault diagnosis;formal
	verification;hardware description languages;high level synthesis;integrated
	circuit reliability;integrated circuit testing;low-power electronics;memory
	architecture;microprocessor chips;mixed analogue-digital integrated
	circuits;nanotechnology;pipeline processing;smart cards;system-on-chip;}
}

@ARTICLE{4608719,
  author = {Khoury, R. and Karray, F. and Kamel, M.S.},
  title = {Domain Representation Using Possibility Theory: An Exploratory Study},
  journal = {Fuzzy Systems, IEEE Transactions on},
  year = {2008},
  volume = {16},
  pages = {1531 -1541 },
  number = {6},
  month = {dec. },
  abstract = {This study explores a new domain representation method for natural
	language processing based on an application of possibility theory.
	In our method, domain-specific information is extracted from natural
	language documents using a mathematical process based on Rieger's
	notion of semantic distances, and represented in the form of possibility
	distributions. We implement the distributions in the context of a
	possibilistic domain classifier, which is trained using the SchoolNet
	corpus.},
  doi = {10.1109/TFUZZ.2008.2005011},
  issn = {1063-6706},
  keywords = {SchoolNet corpus;domain representation;domain-specific information;information
	extraction;mathematical process;natural language documents;natural
	language processing;possibilistic domain classifier;possibility distributions;possibility
	theory;semantic distances;information retrieval;natural language
	processing;pattern classification;possibility theory;statistical
	distributions;text analysis;uncertainty handling;}
}

@INPROCEEDINGS{4268236,
  author = {Kiczales, G.},
  title = {Making the Code Look Like the Design - Aspects and Other Recent Work},
  booktitle = {Program Comprehension, 2007. ICPC '07. 15th IEEE International Conference
	on},
  year = {2007},
  pages = {14},
  month = {june},
  abstract = {Summary form only given. The idea that programs should clearly reflect
	the design decisions they embody has a long history. Higher-level
	languages, syntactic macros, domain-specific languages, and intentional
	programming are different approaches to this common goal. Recent
	work from several areas, including aspect-oriented programming, has
	significantly advanced our ability to make code expressive. At the
	same time, it forces us to reconsider a number of basic assumptions,
	including what is a program, what is a module, what is a language,
	and what is an editor.},
  doi = {10.1109/ICPC.2007.25},
  issn = {1063-6897},
  keywords = {aspect-oriented programming;domain-specific language;expressive program
	code design;higher-level language;intentional programming;syntactic
	macros;object-oriented programming;}
}

@INPROCEEDINGS{493448,
  author = {Kieburtz, R.B. and McKinney, L. and Bell, J.M. and Hook, J. and Kotov,
	A. and Lewis, J. and Oliva, D.P. and Sheard, T. and Smith, I. and
	Walton, L.},
  title = {A software engineering experiment in software component generation
	},
  booktitle = {Software Engineering, 1996., Proceedings of the 18th International
	Conference on},
  year = {1996},
  pages = {542 -552},
  month = {mar},
  abstract = {The paper presents results of a software engineering experiment in
	which a new technology for constructing program generators from domain-specific
	specification languages has been compared with a reuse technology
	that employs sets of reusable Ada program templates. Both technologies
	were applied to a common problem domain, constructing message translation
	and validation modules for military command, control, communications
	and information systems (C3I). The experiment employed four subjects
	to conduct trials of use of the two technologies on a common set
	of test examples. The experiment was conducted with personnel supplied
	and supervised by an independent contractor. Test cases consisted
	of message specifications taken from Air Force C3I systems. The main
	results are that greater productivity was achieved and fewer error
	were introduced when subjects used the program generator than when
	they used Ada templates to implement software modules from sets of
	specifications. The differences in the average performance of the
	subjects are statistically significant at confidence levels exceeding
	99 percent},
  doi = {10.1109/ICSE.1996.493448},
  keywords = {Air Force systems;average performance;confidence levels;domain-specific
	specification languages;independent contractor;message specifications;message
	translation modules;message validation modules;military command control
	communications and information systems ;personnel;productivity;program
	generator construction;reusable Ada program templates;reuse technology;software
	component generation;software engineering experiment;software modules;automatic
	programming;command and control systems;formal specification;human
	resource management;military computing;program interpreters;program
	verification;software reusability;specification languages;}
}

@ARTICLE{5156191,
  author = {Kilic, O. and Dogac, A.},
  title = {Achieving Clinical Statement Interoperability Using R-MIM and Archetype-Based
	Semantic Transformations},
  journal = {Information Technology in Biomedicine, IEEE Transactions on},
  year = {2009},
  volume = {13},
  pages = {467 -477},
  number = {4},
  month = {july },
  abstract = {Effective use of electronic healthcare records (EHRs) has the potential
	to positively influence both the quality and the cost of health care.
	Consequently, sharing patient's EHRs is becoming a global priority
	in the healthcare information technology domain. This paper addresses
	the interoperability of EHR structure and content. It describes how
	two different EHR standards derived from the same reference information
	model (RIM) can be mapped to each other by using archetypes, refined
	message information model (R-MIM) derivations, and semantic tools.
	It is also demonstrated that well-defined R-MIM derivation rules
	help tracing the class properties back to their origins when the
	R-MIMs of two EHR standards are derived from the same RIM. Using
	well-defined rules also enable finding equivalences in the properties
	of the source and target EHRs. Yet an R-MIM still defines the concepts
	at the generic level. Archetypes (or templates), on the other hand,
	constrain an R-MIM to domain-specific concepts, and hence, provide
	finer granularity semantics. Therefore, while mapping clinical statements
	between EHRs, we also make use of the archetype semantics. Derivation
	statements are inferred from the Web Ontology Language definitions
	of the RIM, the R-MIMs, and the archetypes. Finally, we show how
	to transform Health Level Seven clinical statement instances to EHRcom
	clinical statement instances and vice versa by using the generated
	mapping definitions.},
  doi = {10.1109/TITB.2008.904647},
  issn = {1089-7771},
  keywords = {archetype semantics;archetype-based semantic transformations;clinical
	statement interoperability;electronic healthcare records;generic
	level;granularity semantics;health level seven clinical statement;healthcare
	information technology domain;reference information model;refined
	message information model derivations;semantic tools;health care;medical
	information systems;programming language semantics;Algorithms;Medical
	Record Linkage;Medical Records Systems, Computerized;Models, Theoretical;Semantics;Systems
	Integration;Vocabulary, Controlled;}
}

@INPROCEEDINGS{4273231,
  author = {Chul Hwee Kim and Hosking, J. and Grundy, J.},
  title = {Generating Web Services for Statistical Survey Packages from Domain-specific
	Visual Languages},
  booktitle = {Incorporating COTS Software into Software Systems: Tools and Techniques,
	2007. IWICSS '07. Second International Workshop on},
  year = {2007},
  pages = {5},
  month = {may},
  abstract = {High-quality, large-scale statistical survey design, data analysis
	and information visualisation can be very complex. Current statistical
	COTS software tools provide powerful statistical processing and analysis
	support but very limited mechanisms for designing, implementing and
	reusing such support. We have been developing a high-level, domain-specific
	visual language, the Survey Design Language (SDL), and associated
	support tool, SDLTool, in conjunction with statisticians. In this
	paper we describe SDLTool's support for turning high-level survey
	design models into widely accessible web services to share complex
	statistical survey technique implementations run by COTS statistical
	packages.},
  doi = {10.1109/IWICSS.2007.6},
  keywords = {COTS software tool;Web service;data analysis;domain-specific visual
	language;information visualisation;statistical survey package;survey
	design language;Web services;mathematics computing;software packages;statistical
	analysis;visual languages;}
}

@INPROCEEDINGS{1181503,
  author = {Dae-Kyoo Kim and France, R. and Ghosh, S. and Eunjee Song},
  title = {Using Role-Based Modeling Language (RBML) to characterize model families},
  booktitle = {Engineering of Complex Computer Systems, 2002. Proceedings. Eighth
	IEEE International Conference on},
  year = {2002},
  pages = { 107 - 116},
  abstract = { Cost-effective development of large, integrated computer-based systems
	can be realized through systematic reuse of development experiences
	throughout the development process. We describe a technique for representing
	reusable modeling experiences. The technique allows developers to
	express domain-specific design patterns as a sub-language of the
	modeling language, the UML. Use of the sub-language to build application-specific
	UML models results in the reuse of the embedded design experiences.
	We use a notation called the (meta)Role-Based Modeling Language (RBML)
	to define UML sub-languages. A (meta-)Role Model is a specialization
	of the UML (Unified Modeling Language) meta-model, that is, it determines
	a sub-language of the UML. We show how RBML can be used to define
	domain-specific design patterns.},
  doi = {10.1109/ICECCS.2002.1181503},
  issn = {1050-4729 },
  keywords = { RBML; Role-Based Modeling Language; UML; Unified Modeling Language;
	cost-effective system development; domain-specific design patterns;
	meta Role-Based Modeling Language; model families; notation; object-oriented
	design models; reusable modeling experience; software reuse; formal
	specification; object-oriented programming; software reusability;
	specification languages;}
}

@INPROCEEDINGS{1563158,
  author = {Kim, D.-K. and Whittle, J.},
  title = {Generating UML models from domain patterns},
  booktitle = {Software Engineering Research, Management and Applications, 2005.
	Third ACIS International Conference on},
  year = {2005},
  pages = { 166 - 173},
  month = {aug.},
  abstract = { The development of a family of applications in a domain can be greatly
	eased if patterns in the domain are systematically reused. Systematic
	use of such patterns can be achieved by tools that support the specification
	of patterns and their instantiation in a specific application context.
	In this paper, we present a prototype tool called RBML-Pattern Instantiator
	(RBML-PI) that generates application-specific UML class diagrams
	and sequence diagrams from a pattern specification described in the
	Role-Based Metamodeling Language (RBML), a pattern specification
	language defining a domain-specific sub-language of the UML. We give
	an overview of the RBML using the Visitor design pattern, and demonstrate
	the tool using an RBML specification for the CheckIn-CheckOut (CICO)
	domain pattern that specifies services to check in and check out
	items. We use the CICO pattern specification to generate an application-specific
	UML model of a library system using RBML-PI.},
  doi = {10.1109/SERA.2005.44},
  keywords = { CICO domain pattern specification; CheckIn-CheckOut domain pattern;
	RBML pattern specification language; RBML-PI prototype tool; RBML-Pattern
	Instantiator; Role-Based Metamodeling Language; UML class diagrams;
	Visitor design pattern; application-specific UML model; domain-specific
	sublanguage; pattern reuse; sequence diagrams; Unified Modeling Language;
	diagrams; formal specification; object-oriented programming; software
	prototyping; software reusability; software tools;}
}

@INPROCEEDINGS{993993,
  author = {Kim, H.M.},
  title = {XML-hoo!: a prototype application for intelligent query of XML documents
	using domain-specific ontologies},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = { 1289 - 1298},
  month = {jan.},
  abstract = { Use of XML holds great promise for standardizing data models for
	realizing benefits such as lowered development costs and tune for
	integrating inter-organizational business processes and inter-organizational
	knowledge management. Further benefits can be realized by formally
	defining common semantics in ontologies using the standardized models.
	Automation of business processes that require sharing knowledge represented
	in XML-based ontologies can then be supported. In this paper a proof-of-concept
	application for using ontologies to support deduction of knowledge
	implicit in existing XML documents is presented. This system, called
	XML-hoo!, employs a customized portal user interface to answer queries
	about Shakespearian plays. Queries are answered by applying inference
	rules about these plays represented as axioms that comprise a Shakespearian
	ontology, composed of terminology corresponding to existing XML DTD's.
	These rules are applied to plays represented in XML that are in the
	public domain. Hence, answers to queries such as, "Who is Romeo's
	father?" can be automatically deduced even though facts required
	for such answers are hot explicitly structured in XML documents.
	This application demonstrates use of re-usable and sharable ontology
	representations to further leverage the expected proliferation of
	XML documents.},
  doi = {10.1109/HICSS.2002.993993},
  keywords = { Shakespearian ontology; Shakespearian plays; XML documents; XML-hoo!;
	business processes; common semantics; customized portal user interface;
	domain-specific ontologies; inference rules; intelligent query; inter-organizational
	business processes; intra-organizational knowledge management; knowledge
	deduction; hypermedia markup languages; knowledge acquisition; query
	processing; semantic networks;}
}

@ARTICLE{469825,
  author = {Jun-Tae Kim and Moldovan, D.I.},
  title = {Acquisition of linguistic patterns for knowledge-based information
	extraction},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  year = {1995},
  volume = {7},
  pages = {713 -724},
  number = {5},
  month = {oct},
  abstract = {The paper presents an automatic acquisition of linguistic patterns
	that can be used for knowledge based information extraction from
	texts. In knowledge based information extraction, linguistic patterns
	play a central role in the recognition and classification of input
	texts. Although the knowledge based approach has been proved effective
	for information extraction on limited domains, there are difficulties
	in construction of a large number of domain specific linguistic patterns.
	Manual creation of patterns is time consuming and error prone, even
	for a small application domain. To solve the scalability and the
	portability problem, an automatic acquisition of patterns must be
	provided. We present the PALKA (Parallel Automatic Linguistic Knowledge
	Acquisition) system that acquires linguistic patterns from a set
	of domain specific training texts and their desired outputs. A specialized
	representation of patterns called FP structures has been defined.
	Patterns are constructed in the form of FP structures from training
	texts, and the acquired patterns are tuned further through the generalization
	of semantic constraints. Inductive learning mechanism is applied
	in the generalization step. The PALKA system has been used to generate
	patterns for our information extraction system developed for the
	fourth Message Understanding Conference (MUC-4)},
  doi = {10.1109/69.469825},
  issn = {1041-4347},
  keywords = {FP structures;PALKA;Parallel Automatic Linguistic Knowledge Acquisition;automatic
	acquisition;domain specific linguistic patterns;domain specific training
	text;input text;knowledge based information extraction;knowledge
	based natural language processing;knowledge-based information extraction;linguistic
	pattern acquisition;semantic constraints;knowledge acquisition;knowledge
	based systems;learning by example;linguistics;natural languages;pattern
	recognition;word processing;}
}

@INPROCEEDINGS{5687670,
  author = {Mi-Young Kim and Goebel, R.},
  title = {Detection and normalization of medical terms using domain-specific
	term frequency and adaptive ranking},
  booktitle = {Information Technology and Applications in Biomedicine (ITAB), 2010
	10th IEEE International Conference on},
  year = {2010},
  pages = {1 -5},
  month = {nov.},
  abstract = {As the volume of clinic notes written in natural language is rapidly
	increasing, physicians need a tool to automatically extract information
	about diseases/treatments. The main problem in extracting medical
	information is that physicians use variant words to describe the
	same disease/treatment. In order to help physicians interpret and
	share disease/treatment information in clinic notes, we need to reliably
	and effectively detect and normalize the medical terms. In this study,
	we perform detection/normalization of medical terms using a UMLS
	meta-thesaurus combined with a document retrieval technique. We regard
	a medical sentence as a query, and a UMLS ontology entry as a document,
	and try to apply a language modeling-based information retrieval
	method as currently used in the document retrieval field. Because
	the term frequency in the UMLS dictionary is uniform, we employ a
	domain-specific term frequency instead of traditional term frequency.
	To retrieve only the relevant terms in 900,000 UMLS entries, we also
	propose an adaptive ranking method which dynamically determines the
	relevant documents for each query without using static cut-off threshold.
	The experimental results outperform the previous methods in detecting
	and normalizing medical terms in Medline clinical trials, and our
	approach can be used in normalizing the real diagnosis list in the
	patient charts of physicians.},
  doi = {10.1109/ITAB.2010.5687670},
  keywords = {Medline clinical trials;UMLS metathesaurus;adaptive ranking;clinic
	notes;diagnosis list;disease treatment;document retrieval;domain-specific
	term frequency;information retrieval;language modeling;medical information
	extraction;medical terms detection;medical terms normalization;ontology;query;static
	cut-off threshold;Unified Modeling Language;diseases;medical computing;medical
	information systems;ontologies (artificial intelligence);query processing;thesauri;}
}

@INPROCEEDINGS{1409933,
  author = {Kimmell, G. and Komp, E. and Alexander, P.},
  title = {Building compilers by combining algebras},
  booktitle = {Engineering of Computer-Based Systems, 2005. ECBS '05. 12th IEEE
	International Conference and Workshops on the},
  year = {2005},
  pages = { 331 - 338},
  month = {april},
  abstract = { Embedded systems present a wide variety of challenges for developers
	of language tools. Verification of correctness, flexibility for adding
	new language features, and retargeting new architectures all present
	significant problems when developing a compiler for embedded systems.
	In this paper we present a domain-specific language based on modular
	monadic semantics which addresses many of these challenges.},
  doi = {10.1109/ECBS.2005.23},
  keywords = { domain-specific language; embedded systems; modular monadic semantics;
	program compiler; embedded systems; process algebra; program compilers;
	program verification; programming language semantics; specification
	languages;}
}

@ARTICLE{798326,
  author = {Klarlund, N. and Schwartzbach, M.I.},
  title = {A domain-specific language for regular sets of strings and trees
	},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {378 -386},
  number = {3},
  month = {may/jun},
  abstract = {We propose a novel high level programming notation, called FIDO, that
	we have designed to concisely express regular sets of strings or
	trees. In particular, it can be viewed as a domain-specific language
	for the expression of finite state automata on large alphabets (of
	sometimes astronomical size). FIDO is based on a combination of mathematical
	logic and programming language concepts. This combination shares
	no similarities with usual logic programming languages. FIDO compiles
	into finite state string or tree automata, so there is no concept
	of run-time. It has already been applied to a variety of problems
	of considerable complexity and practical interest. We motivate the
	need for a language like FIDO, and discuss our design and its implementation.
	Also, we briefly discuss design criteria for domain-specific languages
	that we have learned from the work with FIDO. We show how recursive
	data types, unification, implicit coercions, and subtyping can be
	merged with a variation of predicate logic, called the Monadic Second-order
	Logic (M2L) on trees. FIDO is translated first into pure M2L via
	suitable encodings, and finally into finite state automata through
	the MONA tool },
  doi = {10.1109/32.798326},
  issn = {0098-5589},
  keywords = {FIDO;MONA tool;Monadic Second-order Logic;design criteria;domain-specific
	language;finite state automata;finite state string;high level programming
	notation;implicit coercions;large alphabets;logic programming languages;mathematical
	logic;predicate logic;programming language concepts;pure M2L;recursive
	data types;regular sets;subtyping;tree automata;trees;unification;data
	structures;finite state machines;formal logic;high level languages;set
	theory;string matching;trees (mathematics);type theory;}
}

@INPROCEEDINGS{5754247,
  author = {Kleins, A. and Merkuryev, Y. and Teilans, A. and Krasts, O.},
  title = {A Metamodel Based Approach for UML Notated Domain Specific Modelling
	Language},
  booktitle = {Computer Modelling and Simulation (UKSim), 2011 UkSim 13th International
	Conference on},
  year = {2011},
  pages = {270 -275},
  month = {30 2011-april 1},
  abstract = {This paper focuses on a metamodel based approach to UML systems modelling
	and simulation. The approach allows creating a system model by operating
	with artefacts from the problem domain. As a novelty for UML modelling,
	especially for simulation purposes, the presented meta-model is enriched
	by a set of stochastic attributes of modelled activities. Modelling
	process is ensured by developing UML based Domain Specific Language
	(DSL) that is suitable for the metamodel, where UML diagrams are
	complemented with attributes necessary for model simulation. A modelling
	tool prototype was developed with Microsoft Visual Studio using Microsoft
	Visualization and Modelling SDK. Elaborated models are stored in
	a model base which conforms to the described metamodel. Relevant
	DEVS simulation software will be developed for ability to run those
	models and analyse gathered results. The given approach facilitates
	increases of the productivity in development of domain specific modelling
	and simulation tools up to 10 times.},
  doi = {10.1109/UKSIM.2011.58},
  keywords = {DEVS simulation software;Microsoft Visual Studio;Microsoft Visualization;UML;domain
	specific language;metamodel;modelling process;modelling tool prototype;stochastic
	attributes;unified modelling language;Unified Modeling Language;data
	visualisation;diagrams;}
}

@INPROCEEDINGS{5279910,
  author = {Klint, P. and van der Storm, T. and Vinju, J.},
  title = {RASCAL: A Domain Specific Language for Source Code Analysis and Manipulation},
  booktitle = {Source Code Analysis and Manipulation, 2009. SCAM '09. Ninth IEEE
	International Working Conference on},
  year = {2009},
  pages = {168 -177},
  month = {sept.},
  abstract = {Many automated software engineering tools require tight integration
	of techniques for source code analysis and manipulation. State-of-the-art
	tools exist for both, but the domains have remained notoriously separate
	because different computational paradigms fit each domain best. This
	impedance mismatch hampers the development of new solutions because
	the desired functionality and scalability can only be achieved by
	repeated and ad hoc integration of different techniques. RASCAL is
	a domain-specific language that takes away most of this boilerplate
	by integrating source code analysis and manipulation at the conceptual,
	syntactic, semantic and technical level. We give an overview of the
	language and assess its merits by implementing a complex refactoring.},
  doi = {10.1109/SCAM.2009.28},
  keywords = {RASCAL;ad hoc integration;automated software engineering tool;complex
	software refactoring;conceptual-syntactic-semantic-technical level;domain
	specific language;impedance mismatch;source code analysis;source
	code manipulation;object-oriented languages;program diagnostics;software
	maintenance;}
}

@INPROCEEDINGS{4976369,
  author = {Kloos, J. and Eschbach, R.},
  title = {Generating System Models for a Highly Configurable Train Control
	System Using a Domain-Specific Language: A Case Study},
  booktitle = {Software Testing, Verification and Validation Workshops, 2009. ICSTW
	'09. International Conference on},
  year = {2009},
  pages = {39 -47},
  month = {april},
  abstract = {In this work, we present a results from case study on testing a highly
	configurable, safety-critical system from the railway domain using
	model-based risk-oriented testing. In the construction of the system
	and test models, we face the following problems: (i) A domain expert
	will usually not be knowledgeable in the construction of system models,
	but has very detailed knowledge which configurations of the system
	will be especially critical (e.g., prone to head-on collisions).Thus,
	a method for the construction of system and test models from domain-specific
	descriptions is necessary. (ii)The system model shall be validatable
	against the systempsilas requirements. (iii) The verification of
	the system model against safety requirements should be possible.
	We will demonstrate an approach based on DSLs, compositional construction
	of Mealy machines and a proof technique as a solution to these three
	problems.},
  doi = {10.1109/ICSTW.2009.32},
  keywords = {domain-specific language;railway domain;risk-oriented testing;safety-critical
	system;train control system;control engineering computing;railway
	engineering;risk analysis;safety-critical software;}
}

@INPROCEEDINGS{6048389,
  author = {Klotzbucher, M. and Smits, R. and Bruyninckx, H. and De Schutter,
	J.},
  title = {Reusable hybrid force-velocity controlled motion specifications with
	executable domain specific languages},
  booktitle = {Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International
	Conference on},
  year = {2011},
  pages = {4684 -4689},
  month = {sept.},
  abstract = {Most of today's robotic task descriptions are designed for a single
	software and hardware platform and thus can not be reused without
	modifications. This work follows the meta-model approach of Model
	Driven Engineering (MDE) to introduce the concepts of Domain Specific
	Languages (DSL) and of Model Transformations to the domain of hybrid
	force-velocity controlled robot tasks, as expressed in (i) the Task
	Frame formalism (TFF), and (ii) a Statechart model representing the
	discrete coordination between TFF tasks. The result is a representation
	in MDE's M0, M1, M2 and M3 form, with increasingly robot and software
	independent representations, that do remain instantaneously executable,
	except obviously for the M3 metametamodel. The Platform Specific
	Model information can be added in three steps: (i) the type of the
	hybrid force-velocity controlled task, (ii) the hardware properties
	of the robot, tool and sensor, and (iii) the software properties
	of the applied execution framework. We demonstrate the presented
	approach by means of an alignment task executed on a Willow Garage
	PR2 and a KUKA Light Weight Robot (LWR) arm.},
  doi = {10.1109/IROS.2011.6048389},
  issn = {2153-0858},
  keywords = {KUKA light weight robot arm;M3 metametamodel;Willow Garage PR2;alignment
	task;discrete coordination;executable domain specific languages;hardware
	platform;hardware property;hybrid force-velocity controlled robot
	task;hybrid force-velocity controlled task;manipulator;meta-model
	approach;model driven engineering;model transformation;platform specific
	model information;reusable hybrid force-velocity controlled motion
	specification;robotic task description;sensor;software platform;software
	property;statechart model;task frame formalism;control engineering
	computing;discrete systems;force control;manipulators;software engineering;velocity
	control;}
}

@ARTICLE{5035598,
  author = {Kniesel, G. and Winter, V. and Siy, H. and Zand, M.},
  title = {Making aspect-orientation accessible through syntax-based language
	composition},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {219 -237},
  number = {3},
  month = {june },
  abstract = {A generic syntax-based approach is presented by which a fixed set
	of aspect-oriented features belonging to an aspect language family
	L A can be applied to a domain-specific language (DSL). The approach
	centres on the construction of a grammar in which a predefined and
	fixed set of abstract join points and join point environments are
	linked with their concrete counterparts within the DSL. This connection
	enables the behaviour of static weaving to be expressed in a generic
	manner. The resulting framework is one in which aspect orientation
	is accessible to non-experts across a wide spectrum of abstractions.},
  doi = {10.1049/iet-sen.2007.0125},
  issn = {1751-8806},
  keywords = {aspect language;domain-specific language;grammar;static weaving;syntax-based
	language composition;grammars;object-oriented languages;}
}

@INPROCEEDINGS{5745322,
  author = {Kobayashi, Shinsuke and Mita, Kentaro and Takeuchi, Yoshinori and
	Imai, Masaharu},
  title = {Design space exploration for DSP applications using the ASIP development
	system PEAS-III},
  booktitle = {Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International
	Conference on},
  year = {2002},
  volume = {3},
  pages = {III-3168 -III-3171},
  month = {may},
  abstract = {This paper describes rapid design space exploration for DSP applications
	using the PEAS-III system, which is a configurable processor development
	environment for application domain specific embedded systems. First,
	a compiler generation method, which is one of the key technologies
	in the PEAS-III system, is proposed. The target compiler is generated
	from the same information as used for the synthesizable HDL generation
	of the target processor. Using the PEAS-III system, not only the
	processor HDL description but also its target compiler are generated.
	Therefore, execution time which is computed from execution cycles
	of applications and generated processor's frequency can be rapidly
	evaluated. Experimental results showed that the trade-offs between
	area and performance of processors for DCT and FIR filter applications
	were analyzed in 4.1 hours and the optimal processor was selected
	under the design constraint by using generated compilers and processors.},
  doi = {10.1109/ICASSP.2002.5745322},
  issn = {1520-6149}
}

@INPROCEEDINGS{5630213,
  author = {Koegel, M. and Herrmannsdoerfer, M. and Yang Li and Helming, J. and
	David, J.},
  title = {Comparing State- and Operation-Based Change Tracking on Models},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2010 14th
	IEEE International},
  year = {2010},
  pages = {163 -172},
  month = {oct.},
  abstract = {In recent years, models are increasingly used throughout the entire
	lifecycle in software development projects. In effect, the need for
	collaborating on these models emerged, requiring change tracking
	and versioning. However, many researchers have shown that existing
	methods and tools for Version Control (VC) do not work well on graph-like
	models, such as UML, SysML or domain-specific modeling languages.
	To alleviate this, alternative techniques and methods have been proposed
	which can be classified into state-based and operation-based approaches.
	Existing research shows advantages of operation-based over state-based
	approaches in selected use cases, such as conflict detection or merging.
	However, there are only few results available on the advantages of
	operation-based approaches in the most common use case of a VC system:
	review and understand change. In this paper, we present and discuss
	both approaches and their use cases. Moreover, we present the results
	of an empirical study to compare a state-based with an operation-based
	approach for the use case of reviewing and understanding change.
	For this study, we have mined an operation-based model repository
	and interviewed users to assess their understanding of randomly selected
	changes. Our results indicate that users better understand complex
	changes in the operation-based representation.},
  doi = {10.1109/EDOC.2010.15},
  issn = {1541-7719},
  keywords = {VC system;operation-based change tracking;operation-based model repository;software
	development;state-based change tracking;version control;configuration
	management;software engineering;}
}

@INPROCEEDINGS{1639593,
  author = {Kogekar, A. and Kaul, D. and Gokhale, A. and Vandal, P. and Praphamontripong,
	U. and Gokhale, S. and Jing Zhang and Yuehua Lin and Gray, J.},
  title = {Model-driven generative techniques for scalable performability analysis
	of distributed systems},
  booktitle = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006.
	20th International},
  year = {2006},
  pages = { 8 pp.},
  month = {april},
  abstract = { The ever increasing societal demand for the timely availability of
	newer and feature-rich but highly dependable network-centric applications
	imposes the need for these applications to be constructed by the
	composition, assembly and deployment of off-the-shelf infrastructure
	and domain-specific services building blocks. Service oriented architecture
	(SOA) is an emerging paradigm to build applications in this manner
	by defining a choreography of loosely coupled building blocks. However,
	current research in SOA does not yet address the per for mobility
	(i.e., performance and dependability) challenges of these modern
	applications. Our research is developing novel mechanisms to address
	these challenges. We initially focus on the composition and configuration
	of the infrastructure hosting the individual services. We illustrate
	the use of domain-specific modeling languages and model weavers to
	model infrastructure composition using middleware building blocks,
	and to enhance these models with the desired performability attributes.
	We also demonstrate the use of generative tools that synthesize metadata
	from these models for performability validation using analytical,
	simulation and empirical benchmarking tools.},
  doi = {10.1109/IPDPS.2006.1639593},
  keywords = { analytical benchmarking tool; distributed system; domain-specific
	modeling language; empirical benchmarking tool; generative programming;
	generative tool; metadata; middleware building block; model driven
	development; model-driven generative technique; performability validation;
	scalable performability analysis; service oriented architecture;
	simulation tool; meta data; middleware; simulation languages;}
}

@INPROCEEDINGS{5352766,
  author = {Kollar, J. and Vaclavik, P. and Wassermann, L.},
  title = {Data-driven executable language model},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {667 -674},
  month = {oct.},
  abstract = {Executable language model driven by data streams is proposed. At the
	same time, this model is language architecture developed from context-free
	grammar enriched with communication channels. Four types of formal
	communication channels and one type of informal communication channel
	were identified for functional languages, to provide systematic background
	for human-machine communication. Formal channel positions are determined
	by a grammar, not by a programmer. Data streams are approximately
	as concise as computer machine code but they are semantically equivalent
	to high-level programs. Using simple example of functional language
	we present the principle of functional language architecture construction
	and its driving by data stream. In particular, we show how the program
	is initially recorded, and how it can be repeatedly generated, either
	in original or in a modified version. Data streams radically decrease
	structural complexity of current programs, preserving their semantics,
	since they are not executed at low-level computer architecture but
	at high-level language architecture.},
  doi = {10.1109/IMCSIT.2009.5352766},
  keywords = {context-free grammar;data stream;data-driven executable language model;formal
	communication channel;functional language architecture construction;high-level
	language architecture;human-machine communication;informal communication
	channel;semantics;context-free grammars;functional languages;programming
	language semantics;}
}

@INPROCEEDINGS{941991,
  author = {Kolodnytsky, M. and Kovalchuk, A.},
  title = {Interactive software tool for data visualisation},
  booktitle = {Intelligent Data Acquisition and Advanced Computing Systems: Technology
	and Applications, International Workshop on, 2001.},
  year = {2001},
  pages = {107 -110},
  abstract = {After a brief enumeration of the software for data visualisation,
	we give a set of demands for this sort of software. We discuss the
	user's interactive and adaptive interface of the scientific data
	visualisation software system entitled ldquo;Graph Server rdquo;,
	which was designed and developed by the authors. Examples of the
	interface's control panel and samples of screenshots of 2D/3D hodograph
	curves, 2D/3D histograms, 3D surfaces and 2D areas are presented},
  doi = {10.1109/IDAACS.2001.941991},
  keywords = {2D areas;3D surfaces;Graph Server;control panel;histograms;hodograph
	curves;interactive adaptive user interface;interactive software tool;scientific
	data visualisation software system;screenshots;uniform visual user
	interface;data visualisation;graphical user interfaces;graphs;interactive
	systems;software packages;}
}

@INPROCEEDINGS{4539563,
  author = {Kolovos, D.S. and Paige, R.F. and Polack, F.A.C.},
  title = {Detecting and Repairing Inconsistencies across Heterogeneous Models},
  booktitle = {Software Testing, Verification, and Validation, 2008 1st International
	Conference on},
  year = {2008},
  pages = {356 -364},
  month = {april},
  abstract = {With the advent of domain specific languages for model engineering,
	detecting inconsistencies between models is becoming increasingly
	challenging. Nowadays, it is not uncommon for models participating
	in the same development process to be captured using different modelling
	languages and even different modelling technologies. We present a
	classification of the types of relationships that can arise between
	models participating in a software development process and outline
	the types of inconsistencies each relationship can suffer from. From
	this classification we identify a set of requirements for a generic
	inconsistency detection and reconciliation mechanism and use a case
	study to demonstrate how those requirements are implemented in the
	Epsilon validation language (EVL), a task-specific language developed
	in the context of the Epsilon GMT component.},
  doi = {10.1109/ICST.2008.23},
  keywords = {Domain Specific Language;Epsilon Validation Language;model engineering;modelling
	language;software development process;system requirement;task-specific
	language;formal specification;specification languages;}
}

@INPROCEEDINGS{5069891,
  author = {Kolovos, D.S. and Rose, L.M. and Paige, R.F. and Polack, F.A.C.},
  title = {Raising the level of abstraction in the development of GMF-based
	graphical model editors},
  booktitle = {Modeling in Software Engineering, 2009. MISE '09. ICSE Workshop on},
  year = {2009},
  pages = {13 -19},
  month = {may},
  abstract = {The Eclipse graphical modeling framework (GMF) provides substantial
	infrastructure and tooling for developing diagram-based editors for
	modelling languages atop the Eclipse platform. It is widely accepted
	that implementing a visual editor using the built-in GMF facilities
	is a particularly complex and error-prone task and requires a steep
	learning curve. We present an approach that raises the level of abstraction
	at which a visual editor is specified. The approach uses annotations
	at the metamodel level. Annotations are used for producing the required
	low-level intermediate GMF models necessary for generating an editor
	via model-to-model transformations.},
  doi = {10.1109/MISE.2009.5069891},
  keywords = {Eclipse platform;error-prone task;graphical model editor;graphical
	modeling framework;model driven engineering;model-to-model transformation;modelling
	language;software tool;steep learning curve;visual editor;object-oriented
	programming;programming environments;software tools;specification
	languages;}
}

@INPROCEEDINGS{4019135,
  author = {Jun Kong and Guang-Lei Song and Kang Zhang and Mao Lin Huang},
  title = {A Collaborative Framework for Designers and Developers of Software-Intensive
	Systems},
  booktitle = {Computer Supported Cooperative Work in Design, 2006. CSCWD '06. 10th
	International Conference on},
  year = {2006},
  pages = {1 -6},
  month = {may},
  abstract = {This paper presents a framework supporting collaborative efforts between
	the designer and developer of software-intensive systems. The framework
	realizes a two-layered meta-tool concept: a powerful specification
	language in the form of a grammar for the designer at the upper layer;
	and a generative mechanism for generating domain-specific design
	languages at the lower layer. The paper introduces the spatial graph
	grammar formalism as the specification language and the generation
	mechanism. We also describe successful and potential application
	domains of the two-layered framework},
  doi = {10.1109/CSCWD.2006.253099},
  keywords = {collaborative framework;generating domain-specific design languages;generative
	mechanism;grammar;software-intensive systems;spatial graph grammar
	formalism;specification language;two-layered meta-tool concept;graph
	grammars;groupware;software development management;specification
	languages;team working;}
}

@ARTICLE{4167863,
  author = {Kordon, Fabrice},
  title = {Guest Editor's Introduction: Rapid System Prototyping},
  journal = {Distributed Systems Online, IEEE},
  year = {2007},
  volume = {8},
  pages = {7},
  number = {4},
  month = {april },
  abstract = {The March, April, and May 2007 issues of IEEE Distributed Systems
	Online feature revised versions of the best papers presented at the
	17th International IEEE Workshop on Rapid System Prototyping (RSP
	06). These articles were selected by reviewers from a large selection
	of excellent submissions. In this month's issue, we present "System
	Prototype and Verification Using Metamodel-Based Transformations"
	by Luis Pedro, Levi Lucio, and Didier Buchs. The article discusses
	how mapping domain-specific languages' core concepts into the Concurrent
	Object-Oriented Petri Nets formal specification language can provide
	users with the semantics necessary for developing prototypes for
	these DSLs.},
  doi = {10.1109/MDSO.2007.21},
  issn = {1541-4922}
}

@INPROCEEDINGS{779104,
  author = {Korson, T.},
  title = {Using components, patterns and frameworks to realize architecture},
  booktitle = {Technology of Object-Oriented Languages and Systems, 1999. Proceedings
	of},
  year = {1999},
  pages = {415},
  month = {june},
  abstract = {Summary form only given, as follows. Understanding and using components,
	patterns and frameworks in the design and development of an application
	system architecture is critical to the successful development of
	enterprise solutions. You will learn how components, patterns and
	frameworks can and should be utilized to establish an industrial
	strength architecture that supports the total needs of the business.
	Reuse comes in many forms at numerous levels of abstraction. This
	tutorial will consider the aspects necessary to a successful corporate
	reuse program. We will examine how to plan and attain effective reuse
	by combining class libraries, patterns, frameworks, domain specific
	pattern languages and the corporate infrastructure necessary for
	enabling large scale reuse. Reuse is often touted as one of the primary
	benefits of the object-oriented approach to software development.
	Yet reuse is not an automatic by-product of employing object technology.
	There are as many cultural and organizational barriers to reuse as
	there are technical ones.},
  doi = {10.1109/TOOLS.1999.779104}
}

@INPROCEEDINGS{5352767,
  author = {Kosar, T. and Mernik, M. and Crepinsek, M. and Henriques, P.R. and
	da Cruz, D. and Pereira, M.J.V. and Oliveira, N.},
  title = {Influence of domain-specific notation to program understanding},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {675 -682},
  month = {oct.},
  abstract = {Application libraries are the most commonly used implementation approach
	to solve problems in general-purpose languages. Their competitors
	are domain-specific languages, which can provide notation close to
	the problem domain. We carried out an empirical study on comparing
	domain-specific languages and application libraries regarding program
	understanding. In this paper, one case study is presented. Over 3000
	lines of code were studied and more than 86 pages long questionnaires
	were answered by end-users, answering questions on learning, perceiving
	and evolving programs written in domain-specific language as well
	as general-purpose language using application library. In this paper,
	we present comparison results on end-users' correctness and consumed
	time. For domain-specific language and application library same problem
	domain has been used-a well-known open source graph description language,
	DOT.},
  doi = {10.1109/IMCSIT.2009.5352767},
  keywords = {application libraries;domain specific languages;domain specific notation;general
	purpose languages;learning;open source graph description language;program
	understanding;program diagnostics;programming languages;specification
	languages;}
}

@INPROCEEDINGS{4283866,
  author = {Kosar, T. and Mernik, M. and Martinez Lopez, P.E.},
  title = {Experiences on DSL Tools for Visual Studio},
  booktitle = {Information Technology Interfaces, 2007. ITI 2007. 29th International
	Conference on},
  year = {2007},
  pages = {753 -758},
  month = {june},
  abstract = {Within their application do mains, domain-specific languages offer
	substantial gains in expressiveness, productivity, and ease of use,
	compared with general-purpose programming languages. Despite the
	many advantages of domain-specific languages, their use has been
	unduly limited, by a lack of support in developmental environments.
	Recently, microsoft introduced some support by constructing domain-specific
	languages with a plug-in 'DSL tools for visual studio'. This paper
	gives language designers tips on developing a domain-specific language
	using this tool and describes the experiences of an end-user of constructing
	a language. Another contribution of this paper is a comparison of
	tools with the traditional approach by the implementation of a domain-specific
	language, done on the same representative language.},
  doi = {10.1109/ITI.2007.4283866},
  issn = {1330-1012},
  keywords = {DSL tool;domain-specific language;visual programming;visual studio;visual
	languages;visual programming;}
}

@INPROCEEDINGS{1372441,
  author = {Kosar, T. and Rebernak, D. and Mernik, M. and Zumer, V.},
  title = {Visual language framework for LISA},
  booktitle = {Information Technology Interfaces, 2004. 26th International Conference
	on},
  year = {2004},
  pages = {395 -400 Vol.1},
  month = {june},
  abstract = {Compiler construction is a difficult task. Therefore more suitable
	solutions than the existent ones are needed. In this paper a framework
	(domain-specific visual language) for visual language design is presented.
	The framework automatically generates the compiler/interpreter by
	using visual notation. The main advantage of this approach is the
	use of a more appropriate visual notation and abstractions in comparison
	to classical textual notation of formal language specifications for
	compiler generators. Special attention is paid to better flexibility
	and reusability of the predefined semantic constructs},
  doi = {10.1109/ITI.2004.241974},
  keywords = {LISA;attribute grammars;compiler generator;domain-specific visual
	language;formal language;formal specification;interpreter;semantic
	construction;attribute grammars;compiler generators;formal languages;formal
	specification;program interpreters;programming language semantics;specification
	languages;visual languages;visual programming;}
}

@INPROCEEDINGS{6063156,
  author = {Koshima, Amanuel and Englebert, Vincent and Thiran, Philippe},
  title = {Distributed Collaborative Model Editing Framework for Domain Specific
	Modeling Tools},
  booktitle = {Global Software Engineering (ICGSE), 2011 6th IEEE International
	Conference on},
  year = {2011},
  pages = {113 -118},
  month = {aug.},
  abstract = {Domain Specific Modeling (DSM) tools have matured and became powerful
	over the past few years and are now used more frequently to model
	complex systems. Consequently, the demand for model management and
	collaboration among DSM tools becomes more important. In collaborative
	modeling, domain specific models are mostly edited and elaborated
	concurrently by different semi-autonomous users. Hence, there is
	a need for reconciliating these parallely evolved models so as to
	seamlessly work together. CSCW community proposes tools or techniques
	to ensure collaboration among general purpose modeling languages,
	but they do not give functionalities to support reconciliation and
	merging for asynchronous modification. In addition, management of
	communications among members of collaborative group could also help
	to facilitate collaboration in the group. In this paper, we propose
	a communication framework to manage exchanges of concurrently edited
	DSM models among users. Besides, we present a reconciliation framework
	to merge concurrently evolved DSM models.},
  doi = {10.1109/ICGSE.2011.18}
}

@INPROCEEDINGS{1281729,
  author = {Kothari, S.C. and Bishop, L. and Sauceda, J. and Daugherty, G.},
  title = {Knowledge-centric and language independent framework for safety analysis
	tools},
  booktitle = {High Assurance Systems Engineering, 2004. Proceedings. Eighth IEEE
	International Symposium on},
  year = {2004},
  pages = { 45 - 55},
  month = {march},
  abstract = { This paper presents a knowledge-centric and language independent
	framework and its application to develop safety analysis tools for
	avionics systems. A knowledge-centric approach is important to address
	domain-specific needs, with respect to the types of problems the
	tools detect and the strategies used to analyze and adapt the code.
	The knowledge is captured by formally specified patterns used to
	detect a variety of problems, ranging from simple syntactic issues
	to difficult semantic problems requiring global analysis. Patterns
	can also be used to describe transformations of the software, used
	to rectify problems detected through software inspection, and to
	support interactive inspection and adaptation when full automation
	is impractical. This paper describes the Knowledge Centric Software
	(KCS) framework. It focuses on two key aspects: an eXtensible Common
	Intermediate Language (XCIL) for language independent analysis, and
	an eXtensible Pattern Specification Language (XPSL) for representing
	domain-specific knowledge.},
  doi = {10.1109/HASE.2004.1281729},
  issn = {1530-2059 },
  keywords = { KCS; XCIL; XPSL; avionics; domain-specific knowledge; eXtensible
	Common Intermediate Language; eXtensible Pattern Specification Language;
	knowledge-centric framework; knowledge-centric software; language
	independent framework; safety analysis tools; software inspection;
	software transformation; avionics; knowledge based systems; safety-critical
	software; software architecture; specification languages;}
}

@ARTICLE{4782468,
  author = {Kourie, D.G. and Fick, D. and Watson, B.W.},
  title = {Virtual machine framework for constructing domain-specific languages},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {1 -13},
  number = {1},
  month = {february },
  abstract = {An object-oriented framework is proposed for constructing a virtual
	machine (VM) to be used in the context of incrementally and iteratively
	developing a domain-specific language (DSL). The framework is written
	in C#. It includes abstract instruction and environment classes.
	By extending these, a concrete layer of classes is obtained whose
	instances define the semantics of a set of instructions, as well
	as one or more execution environment instances that can be manipulated
	by the instructions. The framework provides a generic mechanism for
	reading a set of instructions, executing them sequentially by default
	but branching if necessary, storing or retrieving internal variables,
	and accessing and manipulating the environment as per the instructions.
	In general, each instruction can execute an arbitrary C# method as
	specified by the developer. The syntactic form of instructions is
	limited to five possibilities. Using the framework, a range of VMs
	can be generated, each tailored to support a developer-designed target-level
	DSL. Since each such language is built in terms of these five instruction
	formats, these target-level languages share a common syntactic structure.
	The result is a platform to support an incremental iterative language
	design and implementation approach that involves the following three
	phases: determine a set of target-level instructions with semantics
	appropriate to the specific domain; determine source-level language
	instructions whose syntax appeals to the domain specialist and provide
	a simple compiler to map the source to target instructions. The first
	two phases are relatively disjoint and importantly separate syntax
	concerns from semantics concerns. The final phase is quite straightforward.
	Comparative performance results support the use of the framework
	as an alternative to using an interpreter or hardcoded VM for DSL
	development.},
  doi = {10.1049/iet-sen:20060068},
  issn = {1751-8806},
  keywords = {C# method;domain-specific languages;incremental iterative language
	design;object-oriented framework;target-level languages;virtual machine;high
	level languages;object-oriented programming;}
}

@INPROCEEDINGS{5261002,
  author = {Kowalski, M. and Wilkosz, K.},
  title = {A Domain Specific Language in Dependability Analysis},
  booktitle = {Dependability of Computer Systems, 2009. DepCos-RELCOMEX '09. Fourth
	International Conference on},
  year = {2009},
  pages = {324 -331},
  month = {30 2009-july 2},
  abstract = {Domain specific languages gain increasing popularity as they substantially
	leverage software development by bridging the gap between technical
	and business area. After a domain framework is produced, experts
	gain an effective vehicle for assessing quality and performance of
	a system in the business-specific context. We consider the domain
	to be dependability of multi-agent system (MAS), for which a key
	requirement is an efficient verification of a topology model of a
	power system. As a result, we come up with a reliability evaluation
	solution offering a significant rise in the level of abstraction
	towards MAS utilized for purposes of a power system topology verification.
	By means of the mentioned solution safety engineers are enabled to
	perform analysis while the design is still incomplete. A new DSL
	is developed in XText in order to specify a structure of the system
	together with dependability extensions, which are further translated
	into dynamic fault trees using model to model transformations. The
	Eclipse Ecore becomes a common denominator, in which both metamodelspsila
	abstract syntax trees are defined. Finally, an expert is offered
	with two ways of defining a model: through abstract and textual concrete
	syntax, both of which are checked for consistency using object constraint
	language.},
  doi = {10.1109/DepCoS-RELCOMEX.2009.14},
  keywords = {DSL;Eclipse Ecore;XText;business-specific context;dependability analysis;domain
	specific language;multiagent system;object constraint language;power
	system topology verification;software development;digital subscriber
	lines;formal verification;multi-agent systems;object-oriented languages;specification
	languages;}
}

@INPROCEEDINGS{6037606,
  author = {Koznov, D.},
  title = {Process Model of DSM Solution Development and Evolution for Small
	and Medium-Sized Software Companies},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops (EDOCW),
	2011 15th IEEE International},
  year = {2011},
  pages = {85 -92},
  month = {29 2011-sept. 2},
  abstract = {Domain-Specific Modeling (DSM) approach is developing in connection
	with problems of UML practical usage. This approach is intended for
	faster development of new visual languages, graphics editors, and
	tools of code generation, oriented for various problem domains. Commonly,
	DSM approach is applied within the bounds of a single software company
	for development of product lines, large products, etc. Development
	and support of the DSM solution turn out to be company's inner project,
	and the company itself appears to be the customer. Such situation
	discloses a mass of problems. Moreover, the company, commonly does
	not specialize in development and adoption of visual modeling solutions.
	Additionally, for small and medium-size companies, there are other
	problems: they are capable to assign only a relatively small budget
	for a DSM project, companies lack experienced developers for DSM
	project participation, have troubles with supply of stable maintenance
	process of a DSM solution, etc. So, an effective process has to be
	established in such company, if it is desired to design, implement,
	and use a DSM solution. In this paper advantages and problems of
	DSM projects at small and medium-sized companies are considered,
	a new process model based on MSF for DSM solution development and
	evolution is presented. The model includes flexible requirement management,
	a pilot project, provides iterative development, and further maintenance
	and support of the DSM solution.},
  doi = {10.1109/EDOCW.2011.58},
  keywords = {DSM project participation;DSM solution development;DSM solution evolution;UML
	practical usage;code generation;domain specific modeling approach;flexible
	requirement management;graphics editors;small and medium sized software
	companies;stable maintenance process;visual languages;Unified Modeling
	Language;program compilers;small-to-medium enterprises;software development
	management;software maintenance;systems analysis;}
}

@INPROCEEDINGS{5479515,
  author = {Klner, C. and Dummer, G. and Rentschler, A. and Mller-Glaser,
	K.D.},
  title = {Designing a Graphical Domain-Specific Modelling Language Targeting
	a Filter-Based Data Analysis Framework},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	Workshops (ISORCW), 2010 13th IEEE International Symposium on},
  year = {2010},
  pages = {152 -157},
  month = {may},
  abstract = {We demonstrate the application of a Model-Driven Software Development
	(MDSD) methodology using the example of an analysis framework designed
	for a data logging device in the field of vehicle testing. This mobile
	device is capable of recording the data traffic of automotive-specific
	bus systems like Controller Area Network (CAN), Local Interconnect
	Network (LIN), FlexRay and Media Orientied Systems Transport (MOST)
	in real-time. In order to accelerate the subsequent analysis of the
	tremendous amount of data, it is advisable to pre-filter the recorded
	log data on device, during the test-drive. To enable the test engineer
	of creating data analyses we built a component-based library on top
	of the languages System{C}/C++. Problematic with this approach is
	that still substantial programming knowledge is required for implementing
	filter algorithms, which is usually not the domain of a vehicle test
	engineer. In a next step we developed a graphical modelling language
	on top of our library and a graphical editor. The editor is able
	of verifying a model as well as of generating source code which eliminates
	the need of manually implementing a filter algorithm. In our contribution
	we show the design of the graphical language and the editor using
	the Eclipse platform and the Graphical Modelling Framework (GMF).
	We describe the automatic extraction of meta-information, such as
	available components, their interfaces and categorization annotations
	by parsing the library's C++ implementation with the help of Xtext.
	The editor will use that information to build a dedicated tool palette
	providing components that the designer can instantiate and interconnect
	using drag-and-drop.},
  doi = {10.1109/ISORCW.2010.33},
  keywords = {C++ language parsing;Eclipse platform;FlexRay;System C language;automotive-specific
	bus systems;categorization annotations;component-based library;controller
	area network;data logging device;filter-based data analysis framework;graphical
	domain-specific modelling language design;graphical modelling framework;local
	interconnect network;media oriented system transport;meta-information
	automatic extraction;model-driven software development methodology;source
	code;vehicle test engineer;vehicle testing;C++ language;automobile
	industry;automobiles;data analysis;field buses;mechanical engineering
	computing;simulation languages;software engineering;visual languages;}
}

@INPROCEEDINGS{5678062,
  author = {Kramer, D. and Clark, T. and Oussena, S.},
  title = {MobDSL: A Domain Specific Language for multiple mobile platform deployment},
  booktitle = {Networked Embedded Systems for Enterprise Applications (NESEA), 2010
	IEEE International Conference on},
  year = {2010},
  pages = {1 -7},
  month = {nov.},
  abstract = {There is increasing interest in establishing a presence in the mobile
	application market, with platforms including Apple iPhone, Google
	Android and Microsoft Windows Mobile. Because of the differences
	in platform languages, frameworks, and device hardware, development
	of an application for more than one platform can be a difficult task.
	In this paper we address this problem by the creation of a mobile
	Domain Specific Language (DSL). Domain analysis was carried out using
	two case studies, inferring basic requirements of the language. The
	paper further introduces the language calculus definition and provides
	discussion how it fits the domain analysis, and any issues found
	in our approach.},
  doi = {10.1109/NESEA.2010.5678062},
  keywords = {Apple iPhone;Google Android;Microsoft Windows Mobile;MobDSL;domain
	specific language;language calculus;mobile computing;multiple mobile
	platform deployment;mobile computing;mobile handsets;specification
	languages;}
}

@INPROCEEDINGS{5457760,
  author = {Krenn, U. and Thonhauser, M. and Kreiner, C.},
  title = {ECQL: A Query and Action Language for Model-Based Applications},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2010 17th IEEE International
	Conference and Workshops on},
  year = {2010},
  pages = {286 -290},
  month = {march},
  abstract = {Modern distributed computer systems with mobile and embedded devices
	as first class citizens are formed from heterogeneous platforms.
	To support this heterogeneity along with adaptation of the system
	an approach for interpretation of domain specific models at runtime
	has been proposed with the concept of Model-Based Software Components
	(MBSC), separating the domain specific functionality from the current
	technical platform. This is achieved by the usage of different sets
	of high-level models. These sets are interpreted by a portable, plugin-extensible
	runtime environment, utilizing several instances of model-based containers
	(MCC) for models and their corresponding data. In this paper the
	design of a domain specific language is presented, enabling the specification
	of accessing and manipulating data entities provided by various MCCs
	used in the runtime architecture of a MBSC. For demonstration purposes
	the application of the various language elements is presented using
	a case study of an exemplary distributed pervasive system running
	in the business domain of logistics.},
  doi = {10.1109/ECBS.2010.40},
  keywords = {ECQL;action language;distributed computer systems;distributed pervasive
	system;domain specific language;model-based containers;model-based
	software components;plugin-extensible runtime environment;query language;object-oriented
	programming;query languages;ubiquitous computing;}
}

@INPROCEEDINGS{5291018,
  author = {Krueger, L.},
  title = {Individual Access to IT Resources Using User Context},
  booktitle = {Advances in Human-oriented and Personalized Mechanisms, Technologies,
	and Services, 2009. CENTRIC '09. Second International Conference
	on},
  year = {2009},
  pages = {57 -60},
  month = {sept.},
  abstract = {Concepts of individualization represent an enhancement of existing
	role concepts by containing subjective information demands. Role
	concepts are related to personalization and grant access to IT resources.
	By means of context- and situation-based approaches a concept is
	introduced that provides an individual - as opposed to personalized
	- access to IT resources in heterogeneous system landscapes. The
	first part of the concept is the definition of user context and an
	appropriate UML model. The model includes the user context of a person
	combined with three authorization concepts of SAP R/3, AIX and Solaris.
	By means of these authorization concepts the user context model is
	validated.},
  doi = {10.1109/CENTRIC.2009.15},
  keywords = {AIX;DSL;IT resource access control;SAP R/3;Solaris;UML model;authorization
	concept;context awareness;context-based approach;domain-specific
	language;heterogeneous system landscape;human factors;individualization
	concept;interaction partner user;personalized access;role concept;situation-based
	approach;subjective information demand;user context model;Unified
	Modeling Language;authorisation;human computer interaction;human
	factors;ubiquitous computing;user modelling;}
}

@INPROCEEDINGS{4626891,
  author = {Krut, R. and Cohen, S.},
  title = {Service-Oriented Architectures and Software Product Lines - Putting
	Both Together},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {383},
  month = {sept.},
  abstract = {Service-oriented architecture (SOA) and software product line (SPL)
	approaches to software development share a common goal. They both
	encourage an organization to reuse existing assets and capabilities,
	rather than repeatedly redevelop them for new systems. Their distinct
	goals may be stated as: 1) SOA: enable assembly, orchestration, and
	maintenance of enterprise solutions to quickly react to changing
	business requirements. 2) SPL: systematically capture and exploit
	commonality among a set of related systems, while managing variations
	for specific customers or market segments.},
  doi = {10.1109/SPLC.2008.71},
  keywords = {Web service;asset reusability;business requirement;enterprise solution
	maintenance;service-oriented architecture;software development;software
	product line;Web services;product development;software architecture;software
	maintenance;software reusability;}
}

@INPROCEEDINGS{4463728,
  author = {Kubczak, C. and Margaria, T. and Fritsch, A. and Steffen, B.},
  title = {Biological LC/MS Preprocessing and Analysis with jABC, jETI and xcms},
  booktitle = {Leveraging Applications of Formal Methods, Verification and Validation,
	2006. ISoLA 2006. Second International Symposium on},
  year = {2006},
  pages = {303 -308},
  month = {nov.},
  abstract = {LC/MS is a successful analysis technique for the statistical analysis
	used in several branches of biology. It requires an intense screening
	and combination of the raw data, which is usually done with programs
	and libraries invoked by scripts in the domain-specific statistics
	language S or R. We show here how to model and implement this complex
	workflow in a service-oriented fashion, using the jABC service definition
	environment and jETI for remote service integration and execution.},
  doi = {10.1109/ISoLA.2006.48},
  keywords = {biological LC/MS preprocessing;domain-specific statistics language;jABC
	service definition environment;jETI;remote service integration;service-oriented
	fashion;statistical analysis;xcms;Web services;biology computing;software
	engineering;statistical analysis;workflow management software;}
}

@INPROCEEDINGS{4717886,
  author = {Kucuk, D. and Yazici, A.},
  title = {Identification of coreferential chains in video texts for semantic
	annotation of news videos},
  booktitle = {Computer and Information Sciences, 2008. ISCIS '08. 23rd International
	Symposium on},
  year = {2008},
  pages = {1 -6},
  month = {oct.},
  abstract = {People can benefit from todaypsilas video archives of huge sizes only
	through appropriate and effective ways of querying the video data.
	In order to query the video data through high-level semantic entities
	such as objects, events, and relations, these entities should be
	properly extracted and the corresponding video shots should be annotated
	accordingly. Video texts, which comprise the caption texts on the
	frames as well as transcription texts obtained through automatic
	speech recognition techniques, are valuable sources of information
	for semantic modeling of the videos. In this paper, we present an
	approach for the extraction of semantic objects from videos by utilizing
	lexical resources along with the identification of coreference chains
	in the corresponding video texts. Coreference is a phenomenon in
	natural language texts where a number of entities in the text refer
	to the same real world entity. Therefore, while the domain-specific
	lexical resources aid in the determination of salient entities in
	the video text, the identification of coreference chains prevents
	the superfluous extraction of the same underlying entities due to
	their different surface forms in the video texts. The proposed approach
	is significant for its being the first attempt to address the importance
	of coreference phenomenon in video texts for precise entity extraction
	during the semantic modeling of news videos with a hands-on application.
	The approach has been evaluated on Turkish political news texts from
	the METU Turkish corpus and a number of evaluation problems faced
	such as sparseness of annotated evaluation data for Turkish are also
	pointed out together with further research directions to pursue.},
  doi = {10.1109/ISCIS.2008.4717886},
  keywords = {automatic speech recognition techniques;domain-specific lexical resources;natural
	language texts;news videos;semantic annotation;transcription texts;video
	text coreferential chains;feature extraction;query processing;speech
	recognition;text analysis;video signal processing;}
}

@INPROCEEDINGS{5992009,
  author = {Kuhlmann, M. and Sohr, K. and Gogolla, M.},
  title = {Comprehensive Two-Level Analysis of Static and Dynamic RBAC Constraints
	with UML and OCL},
  booktitle = {Secure Software Integration and Reliability Improvement (SSIRI),
	2011 Fifth International Conference on},
  year = {2011},
  pages = {108 -117},
  month = {june},
  abstract = {Organizations with stringent security requirements like banks or hospitals
	frequently adopt role-based access control (RBAC) principles to simplify
	their internal permission management. Authorization constraints represent
	a fundamental advanced RBAC concept enabling precise restrictions
	on access rights. Thereby, the complexity of the resulting security
	policies increases so that tool support for comfortable creation
	and adequate validation is required. We propose a new approach to
	developing and analyzing RBAC policies using UML for modeling RBAC
	core concepts and OCL to realize authorization constraints. Dynamic
	(i. e., time-dependent) constraints, their visual representation
	in UML and their analysis are of special interest. The approach results
	in a domain-specific language for RBAC which is highly configurable
	and extendable with respect to new RBAC concepts and classes of authorization
	constraints and allows the developer to validate RBAC policies in
	an effective way. The approach is supported by a UML and OCL validation
	tool.},
  doi = {10.1109/SSIRI.2011.18},
  keywords = {OCL validation tool;RBAC concept;UML validation tool;access rights;authorization
	constraint;domain specific language;internal permission management;object
	constraint language;role based access control principle;security
	policies;two-level analysis;unified modeling language;Unified Modeling
	Language;authorisation;program verification;}
}

@INPROCEEDINGS{1322616,
  author = {Kulkarni, C. and Brebner, G. and Schelle, G.},
  title = {Mapping a domain specific language to a platform FPGA},
  booktitle = {Design Automation Conference, 2004. Proceedings. 41st},
  year = {2004},
  pages = {924 -927},
  month = {july},
  abstract = {Not available},
  issn = {0738-100X}
}

@INPROCEEDINGS{4273314,
  author = {Kulkarni, D. and Tripathi, A.},
  title = {Generative Programming Approach for Building Pervasive Computing
	Applications},
  booktitle = {Software Engineering for Pervasive Computing Applications, Systems,
	and Environments, 2007. SEPCASE '07. First International Workshop
	on},
  year = {2007},
  pages = {3},
  month = {may},
  abstract = {In this paper we present a generative programming approach for building
	context-aware applications. In this approach, a context-aware application
	is programmed using high-level specification constructs provided
	in our programming framework. The runtime environment of the application
	is generated from this specification by a middleware. We demonstrate
	the utility of this approach by presenting an example case-study.},
  doi = {10.1109/SEPCASE.2007.8},
  keywords = {context-aware applications;generative programming approach;high-level
	specification;middleware;pervasive computing applications;runtime
	environment;distributed programming;middleware;ubiquitous computing;}
}

@INPROCEEDINGS{4027019,
  author = {Bharat Kumar and Daniel Lieuwen and Ming Xiong and Yuan Wei},
  title = {Publish/Subscribe Over Virtual XML Data},
  booktitle = {Services Computing Workshops, 2006. SCW '06. IEEE},
  year = {2006},
  pages = {87},
  month = {sept. },
  abstract = {XML has become the accepted data exchange language for a wide variety
	of application domains. In particular, a large number of telecom
	standards now involve passing a payload of XML data over a variety
	of protocols. For example, the scenarios mentioned (location, presence,
	device status etc.) above all involve passing that information via
	some standardized XML payload. In this position paper, we focus on
	using an XML subscription language over virtual data, which is categorized
	along the following dimensions based on an examination of various
	telecom scenarios: 1) subscription expiry, 2) subscription conditions,
	3) notification semantics, 4) temporal characteristics, and 5) domain-specific
	functions},
  doi = {10.1109/SCW.2006.34},
  keywords = {XML subscription language;data exchange;domain-specific functions;notification
	semantics;publish/subscribe;subscription conditions;subscription
	expiry;telecom;temporal characteristics;virtual XML data;XML;electronic
	data interchange;message passing;middleware;}
}

@INPROCEEDINGS{5972500,
  author = {Kumar, P.S.},
  title = {Plenary talk: Semantic Web: Issues and challenges},
  booktitle = {Recent Trends in Information Technology (ICRTIT), 2011 International
	Conference on},
  year = {2011},
  pages = {9},
  month = {june},
  abstract = {The current day World Wide Web (WWW) is a web of documents interlinked
	by navigable hyperlinks. It has no doubt changed the way information
	is disseminated or exchanged and gave rise to a host of powerful
	applications which are hard to live without in our times. However,
	while the current day web is excellent for humans to browse and understand
	and assimilate information, it is not easy for software agents to
	operate with. The Extensible Markup Language (XML) framework has
	helped to an extent by enabling the process of defining new markup
	languages with domain-specific tags that can be used to annotate
	information. However, XML does not allow description of the domain
	by making precise statements about it. A set of new frameworks such
	as OWL-DL, OWL have been standardized by the W3C for the purpose
	of capturing knowledge of a domain in a machine processable manner.
	It is hoped that these frameworks would enable us to move towards
	the Semantic Web where more of the semantics of the domains we deal
	with are captured in a machine processable manner leading us to smarter
	use of information. The talk would introduce the elements of the
	Semantic Web technology to the audience.},
  doi = {10.1109/ICRTIT.2011.5972500},
  keywords = {Extensible Markup Language;OWL;OWL-DL;W3C;World Wide Web;domain specific
	tags;machine processable manner;navigable hyperlinks;semantic Web;software
	agents;XML;semantic Web;software agents;}
}

@INPROCEEDINGS{1316679,
  author = {Kusy, B. and Ledeczi, A. and Maroti, M. and Volgyesi, P.},
  title = {Domain independent generative modeling},
  booktitle = {Engineering of Computer-Based Systems, 2004. Proceedings. 11th IEEE
	International Conference and Workshop on the},
  year = {2004},
  pages = { 29 - 34},
  month = {may},
  abstract = { Model integrated computing employs domain-specific modeling languages
	for the design of computer based systems and automatically generates
	their implementation. These system models are declarative in nature.
	However, for complex systems with regular structure, as well as for
	adaptive systems, a more algorithmic approach is better suited. Generative
	modeling employs architectural parameters and generator scripts to
	specify model structure. This paper describes an approach that enables
	the addition of generative modeling capabilities to any domain-specific
	modeling language using metamodel composition. The approach is illustrated
	through an image processing application},
  doi = {10.1109/ECBS.2004.1316679},
  issn = { },
  keywords = { computer based system; domain-specific modeling language; generic
	modeling environment; image processing application; metamodel composition;
	model integrated computing; Unified Modeling Language; formal specification;
	program compilers; programming environments; software prototyping;}
}

@INPROCEEDINGS{6049034,
  author = {Young-Woo Kwon and Tilevich, E.},
  title = {A declarative approach to hardening services against QoS vulnerabilities},
  booktitle = {Maintenance and Evolution of Service-Oriented and Cloud-Based Systems
	(MESOCA), 2011 International Workshop on the},
  year = {2011},
  pages = {1 -10},
  month = {sept.},
  abstract = {The Quality of Service (QoS) in a distributed service-oriented application
	can be negatively affected by a variety of factors. Network volatility,
	hostile exploits, poor service management, all can prevent a service-oriented
	application from delivering its functionality to the user. This paper
	puts forward a novel approach to improving the reliability, security,
	and availability of service-oriented applications. To counter service
	vulnerabilities, a special service detects vulnerabilities as they
	emerge at runtime, and then hardens the applications by dynamically
	deploying special components. The novelty of our approach lies in
	using a declarative framework to express both vulnerabilities and
	hardening strategies in a domain-specific language, independent of
	the service infrastructure in place. Thus, our approach will make
	it possible to harden service-oriented applications in a disciplined
	and systematic fashion.},
  doi = {10.1109/MESOCA.2011.6049034},
  keywords = {QoS vulnerability;declarative framework;distributed service-oriented
	application;domain-specific language;hardening strategy;network volatility;poor
	service management;quality of service;service-oriented application
	availability;service-oriented application reliability;service-oriented
	application security;vulnerability detection;quality of service;service-oriented
	architecture;software fault tolerance;specification languages;}
}

@INPROCEEDINGS{4281070,
  author = {Laforcade, P.},
  title = {Graphical representation of abstract learning scenarios: the UML4LD
	experimentation},
  booktitle = {Advanced Learning Technologies, 2007. ICALT 2007. Seventh IEEE International
	Conference on},
  year = {2007},
  pages = {477 -479},
  month = {july},
  abstract = {This communication concerns the presentation of an experimental research
	work about the graphical representation of abstract learning scenarios.
	We also on purpose propose some reflexion elements about the stakes
	of models transformation in order to generate domain-specific representation
	of abstract scenarios. Our experiment concerns the automatic generation
	of a UML activity diagram from an IMS-LD learning scenario.},
  doi = {10.1109/ICALT.2007.152},
  keywords = {UML activity diagram;abstract learning scenarios;domain-specific representation;educational
	modeling languages;graphical representation;Unified Modeling Language;computer
	graphics;educational computing;}
}

@INPROCEEDINGS{1225950,
  author = {Laforcade, P. and Barbier, F. and Sallaberry, C. and Nodenot, T.},
  title = {Profiling cooperative problem-based learning situations},
  booktitle = {Cognitive Informatics, 2003. Proceedings. The Second IEEE International
	Conference on},
  year = {2003},
  pages = { 32 - 38},
  month = {aug.},
  abstract = { Computer-based learning aims in essence to imitate cognitive behaviors,
	assist learners and teachers in pedagogical exchanges and monitor
	their overall cooperation based on learning paradigms. In this respect,
	this paper is concerned with problem-based learning (PBL). Designing
	PBL platforms comes up against the lack of reusable domain-specific
	or educational software assets. To avoid such a problem, metamodeling
	(via a UML profile) is considered in this paper as a reliable way
	for instantiating PBL situation models. A semantics and a notation
	are in particular associated with the proposed metamodel in order
	to provide rigor but also flexibility in pedagogical scenario elaboration.
	An example in particular illustrates the applicability of the offered
	educational modeling language (EML).},
  doi = {10.1109/COGINF.2003.1225950},
  issn = { },
  keywords = { EML; PBL platforms; PBL situation model; UML profile; behavior imitation;
	cognitive behaviors; computer-based learning; cooperation; cooperative
	problem-based learning; educational modeling language; educational
	software assets; learning paradigms; metamodeling; pedagogical exchanges;
	pedagogical scenario elaboration; problem-based Learning; semantics;
	computer aided instruction; educational computing; formal specification;
	groupware; learning (artificial intelligence); specification languages;}
}

@INPROCEEDINGS{5194363,
  author = {Laforcade, P. and Zendagui, B.},
  title = {A Domain-Specific Modeling Approach for Supporting the Development
	of Visual Instructional Design Languages and Tools},
  booktitle = {Advanced Learning Technologies, 2009. ICALT 2009. Ninth IEEE International
	Conference on},
  year = {2009},
  pages = {744 -746},
  month = {july},
  abstract = {In this paper we present, discuss and illustrate our domain-specific
	modeling orientation for helping communities of instructional designers
	to specify visual instructional design languages and to develop dedicated
	user-friendly graphical editors.},
  doi = {10.1109/ICALT.2009.128},
  keywords = {domain-specific modeling approach;instructional designer;user-friendly
	graphical editor;visual instructional design language;simulation
	languages;visual languages;}
}

@INPROCEEDINGS{4561841,
  author = {Laforcade, P. and Zendagui, B. and Barre, V.},
  title = {Supporting the Specification of Educational Modeling Languages and
	Learning Scenarios with a Domain-Specific-Modeling Approach},
  booktitle = {Advanced Learning Technologies, 2008. ICALT '08. Eighth IEEE International
	Conference on},
  year = {2008},
  pages = {819 -821},
  month = {july},
  abstract = {Over recent years, model-driven-engineering has attracted growing
	interest both as research domain and as an industrial process that
	can be applied to various educational domains. This paper aims to
	propose such an application for learning-scenario-centered instructional
	design processes, based on a 3-domain categorization for learning
	scenarios. We also discuss and explain why we think domain-specific
	modeling techniques are the future new trend in order to support
	the emergence of communities of practices for scenario-based instructional
	design.},
  doi = {10.1109/ICALT.2008.240},
  keywords = {3-domain categorization;domain-specific-modeling approach;educational
	modeling languages;learning scenarios;learning-scenario-centered
	instructional design;model-driven-engineering;computer aided instruction;formal
	specification;}
}

@INPROCEEDINGS{5488464,
  author = {Lahmadi, A. and Festor, O.},
  title = {VeTo: An exploit prevention language from known vulnerabilities in
	SIP services},
  booktitle = {Network Operations and Management Symposium (NOMS), 2010 IEEE},
  year = {2010},
  pages = {216 -223},
  month = {april},
  abstract = {We present VeTo a language to specify protection rules for VoIP systems,
	supported by the SecSip prevention framework. VeTo offers a unique
	way to specify both vulnerabilities and countermeasures to protect
	SIP services against known vulnerabilities. We illustrate the applicability
	of the language through the specification of several known attacks
	and assess its efficiency through a target testbed.},
  doi = {10.1109/NOMS.2010.5488464},
  issn = {1542-1201},
  keywords = {SIP services;SecSip prevention framework;VeTo;VoIP systems;exploit
	prevention language;session initiation protocol;Internet telephony;signalling
	protocols;telecommunication security;}
}

@INPROCEEDINGS{5358865,
  author = {Laird, P. and Dondio, P. and Barrett, S.},
  title = {Dynamic Domain Specific Languages for Trust Models},
  booktitle = {Future Computing, Service Computation, Cognitive, Adaptive, Content,
	Patterns, 2009. COMPUTATIONWORLD '09. Computation World:},
  year = {2009},
  pages = {713 -718},
  month = {nov.},
  abstract = {We propose the development of a framework for the dynamic interpretation
	of trust models, defined via a domain specific language. A trust
	model usually defines abstractions, the interpretation of which change
	in conjunction with changes in the domain or changes in the context
	in which the program executes. In a scenario where trust model assumptions
	encoded in the DSL change, programmers must still work with the existing
	DSL, and therefore take more effort to describe their program or
	sometimes fail to specify their intent. In such changing circumstances
	a trust model risks becoming less effective and fit for purpose.
	We seek to develop an approach in which a trust model adapts to a
	changing environment by making the underlying DSL less restrictive,
	maintaining flexibility and adaptability to cope with changing or
	novel contexts without reducing the expressiveness of the abstractions
	used.},
  doi = {10.1109/ComputationWorld.2009.82},
  keywords = {context sensitivity;domain specific language;dynamic adaptation;trust
	model;security of data;specification languages;}
}

@INPROCEEDINGS{926779,
  author = {Landauer, C.},
  title = {Wrappings as design patterns},
  booktitle = {System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International
	Conference on},
  year = {2000},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { Design Patterns provide a "higher-level" view of interactions among
	collections of objects. They are a response to a common problem in
	large, object-oriented systems: there is no explicit representation
	of the patterns of activity of collections of objects. Wrappings
	are a Knowledge-Based integration infrastructure for constructing
	complex software systems, that we have developed over several years.
	We use results from our wrapping research to show how to implement
	Design Patterns in a form that retains the essential pattern information
	in the code. This application of wrappings greatly reduces the difficulty
	of the specific implementation process often described in the Design
	Pattern literature. The approach does not so much reduce the modeling
	complexity of implementing patterns as it better organizes the implementation,
	so that the pattern shape is not lost. In addition, we note that
	the Design Pattern catalogs are a first step towards domain models
	in many application domains, and even provide a basis for domain-specific
	modeling and programming languages. They are therefore a first step
	towards building a new level of programming languages, at a level
	of abstraction and power above the traditional compiled versions
	of object oriented languages.},
  doi = {10.1109/HICSS.2000.926779},
  issn = { },
  keywords = { abstraction; design patterns; domain models; domain-specific modeling;
	knowledge-based integration infrastructure; modeling complexity;
	object oriented languages; object-oriented systems; programming languages;
	wrappings; object-oriented programming; software reusability;}
}

@ARTICLE{1605177,
  author = {Lange, C.F.J. and Chaudron, M.R.V. and Muskens, J.},
  title = {In practice: UML software architecture and design description},
  journal = {Software, IEEE},
  year = {2006},
  volume = {23},
  pages = { 40 - 46},
  number = {2},
  month = {march-april},
  abstract = { The Unified Modeling Language has attracted many organizations and
	practitioners. UML is now the de facto modeling language for software
	development. Several features account for its popularity: it's a
	standardized notation, rich in expressivity; UML 2.0 provides 13
	diagram types that enable modeling several different views and abstraction
	levels. Furthermore, UML supports domain-specific extensions using
	stereotypes and tagged values. Finally, several case tools integrate
	UML modeling with other tasks such as generating code and reverse-engineering
	models from code. Our study focused on UML use and model quality
	in actual projects rather than on its adequacy as a notation or language.},
  doi = {10.1109/MS.2006.50},
  issn = {0740-7459},
  keywords = { UML; Unified Modeling Language; domain-specific modelling; software
	architecture; software design description; software development;
	Unified Modeling Language; formal specification; software architecture;
	software metrics;}
}

@INPROCEEDINGS{1227656,
  author = {Lank, E.H.},
  title = {A retargetable framework for interactive diagram recognition},
  booktitle = {Document Analysis and Recognition, 2003. Proceedings. Seventh International
	Conference on},
  year = {2003},
  pages = { 185 - 189 vol.1},
  month = {aug.},
  abstract = { The design of new diagram recognition systems remains a challenging
	problem. Ideally, recognition systems should accept real-world input,
	perform robustly, fail gracefully, and be implemented in a timely
	manner. In reality, the intricacy involved in implementing recognition
	systems for diagram notations makes this a challenging open problem.
	One solution to these challenges is the design of middleware to speed
	the development of robust applications. Middleware takes the form
	of a framework or toolkit for the creation of applications. This
	paper describes a retargetable framework which can be used to speed
	the development of robust interactive sketch recognition systems.
	The system includes a drawing surface to capture interactively created
	drawings, a set of generic segmentation routines, a character recognizer,
	and a common interface for integrating domain-specific components.
	The framework has been used to construct systems for the recognition
	of UML, math, and molecular diagrams. Work is on-going on the design
	of additional generic recognizers of logical structure and spatial
	layout of diagrams.},
  doi = {10.1109/ICDAR.2003.1227656},
  issn = { },
  keywords = { UML recognition; character recognizer; domain-specific component
	integration; drawing surface; generic segmentation routine; interactive
	diagram recognition; math recognition; middleware design; molecular
	diagram recognition; retargetable framework; character recognition;
	diagrams; document image processing; image recognition; image segmentation;
	middleware;}
}

@INPROCEEDINGS{953813,
  author = {Lank, E. and Thorley, J. and Chen, S. and Blostein, D.},
  title = {On-line recognition of UML diagrams},
  booktitle = {Document Analysis and Recognition, 2001. Proceedings. Sixth International
	Conference on},
  year = {2001},
  pages = {356 -360},
  abstract = {Unified Modeling Language (UML) diagrams are widely used by software
	engineers to describe the structure of software systems. Early in
	the software design cycle, software engineers informally sketch initial
	UML diagrams on paper or whiteboards. The information provided by
	these UML diagrams needs to be made available to computer assisted
	software engineering (CASE) tools. In order to smooth this transition
	from paper to electronic form, we have developed an online recognition
	system for UML diagrams. The system accepts input from an electronic
	whiteboard, a data tablet or a mouse. Efforts have been made to separate
	the domain-independent and domain-specific parts of the recognition
	system. The kernel of the system is retargetable, providing a general
	front end for online recognition of any glyph-based diagram notation.
	The kernel is extended with UML-specific routines for segmentation,
	recognition of glyphs, and recognition of glyph relationships},
  doi = {10.1109/ICDAR.2001.953813},
  keywords = {CASE;UML-specific routines;Unified Modeling Language diagrams;computer
	assisted software engineering tools;data tablet;electronic form;electronic
	whiteboard;glyph relationship recognition;glyph-based diagram notation;initial
	UML diagrams;mouse;online UML diagram recognition;online recognition
	system;retargetable kernel;software design cycle;software engineers;system
	kernel;computer aided software engineering;computer graphics;image
	recognition;specification languages;}
}

@INPROCEEDINGS{711017,
  author = {Larsen, A. and Holmes, P.D.},
  title = {An architecture for unified dialogue in distributed object systems
	},
  booktitle = {Technology of Object-Oriented Languages, 1998. TOOLS 26. Proceedings},
  year = {1998},
  pages = {244 -258},
  month = {aug},
  abstract = {In traditional information systems, the user interface is controlled
	by one single application. In distributed systems, several distributed
	components may want to influence the appearance and logic of the
	user interface. This paper describes a unified dialogue architecture
	which enables several distributed components to control the logic
	and contents of the user dialogue while keeping the dialogue consistent.
	This architecture is a practical example of using dialogue agents,
	CORBA and Java. Details are described in connection with a large
	domain-specific distributed system. Discussion is also provided as
	to other manners in which this architecture may be implemented, followed
	by a discussion concerning other problem areas in which the unified
	dialogue architecture can be effectively applied},
  doi = {10.1109/TOOLS.1998.711017},
  keywords = {CORBA;Java;dialogue agents;distributed object systems;distributed
	systems;domain-specific distributed system;information systems;unified
	dialogue architecture;user dialogue;user interface;information systems;object-oriented
	languages;object-oriented programming;systems analysis;}
}

@INPROCEEDINGS{5521756,
  author = {Lmmel, R. and Pek, E.},
  title = {Vivisection of a Non-Executable, Domain-Specific Language - Understanding
	(the Usage of) the P3P Language},
  booktitle = {Program Comprehension (ICPC), 2010 IEEE 18th International Conference
	on},
  year = {2010},
  pages = {104 -113},
  month = {30 2010-july 2},
  abstract = {P3P is the policy language with which websites declare the intended
	use of data that is collected about users of the site. We have systematically
	collected P3P-based privacy policies from websites listed in the
	Google directory, and analysed the resulting corpus with regard to
	different levels of validity, size or complexity metrics, different
	cloning levels, coverage of language constructs, and the use of the
	language's extension mechanism. In this manner, we have found interesting
	characteristics of P3P in the wild. For instance, cloning is exceptionally
	common in this domain, and encountered language extensions exceed
	the base language in terms of grammar complexity. Overall, this effort
	helps understanding the de-facto usage of the non-executable, domain-specific
	language P3P. Some elements of our methodology may be useful for
	other software languages as well.},
  doi = {10.1109/ICPC.2010.45},
  issn = {1063-6897},
  keywords = {P3P language;P3P-based privacy policies;domain-specific language;grammar
	complexity;privacy preferences project language;software languages;data
	privacy;programming languages;}
}

@ARTICLE{730849,
  author = {Le Metayer, D. and Nicolas, V.-A. and Ridoux, O.},
  title = {Exploring the software development trilogy},
  journal = {Software, IEEE},
  year = {1998},
  volume = {15},
  pages = {75 -81},
  number = {6},
  month = {nov/dec},
  abstract = {Software development is concerned with more than just generation of
	code; the program must have the desired properties, and these must
	be demonstrated via suitable tests and correctness arguments. One
	way of viewing these aspects is to group them into programs, properties,
	and data. When represented as vertices in a triangle, the edges represent
	processes to produce one element from another. The research reported
	in this article aims at practical methods for automatic test generation
	by restricting the use of both predicate logic and programming constructs.
	It promises to eventually result in practical domain-specific programming
	languages, with a significant boost in both quality and productivity},
  doi = {10.1109/52.730849},
  issn = {0740-7459},
  keywords = {automatic test generation;domain-specific;predicate logic;programming
	constructs;software development;automatic test pattern generation;program
	testing;software engineering;}
}

@INPROCEEDINGS{4222583,
  author = {Leclercq, M. and Ozcan, A.E. and Quema, V. and Stefani, J.-B.},
  title = {Supporting Heterogeneous Architecture Descriptions in an Extensible
	Toolset},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {209 -219},
  month = {may},
  abstract = {Many architecture description languages (ADLs) have been proposed
	to model, analyze, configure, and deploy complex software systems.
	To face this diversity, extensible ADLs (or ADL interchange formats)
	have been proposed. These ADLs provide linguistic support for integrating
	various architectural aspects within the same description. Nevertheless,
	they do not support extensibility at the tool level, i.e. they do
	not provide an extensible toolset for processing ADL descriptions.
	In this paper, we present an extensible toolset for easing the development
	of architecture-based software systems. This toolset is not bound
	to a specific ADL, but rather uses a grammar description mechanism
	to accept various input languages, e.g. ADLs, interface definition
	languages (IDLs), domain specific languages (DSLs). Moreover, it
	can easily be extended to implement many different features, such
	as behavioral analysis, code generation, deployment, etc. Its extensibility
	is obtained by designing its core functionalities using fine-grained
	components that implement flexible design patterns. Experiments are
	presented to illustrate both the functionalities implemented by the
	toolset and the way it can be extended.},
  doi = {10.1109/ICSE.2007.82},
  issn = {0270-5257},
  keywords = {ADL interchange formats;architecture description languages;architecture-based
	software systems;behavioral analysis;code generation;complex software
	systems;domain specific languages;extensible toolset;grammar description;heterogeneous
	architecture descriptions;interface definition languages;software
	architecture;software tools;}
}

@ARTICLE{963443,
  author = {Ledeczi, A. and Bakay, A. and Maroti, M. and Volgyesi, P. and Nordstrom,
	G. and Sprinkle, J. and Karsai, G.},
  title = {Composing domain-specific design environments},
  journal = {Computer},
  year = {2001},
  volume = {34},
  pages = {44 -51},
  number = {11},
  month = {nov},
  abstract = {Domain-specific integrated development environments can help capture
	specifications in the form of domain models. These tools support
	the design process by automating analysis and simulating essential
	system behavior. In addition, they can automatically generate, configure,
	and integrate target application components. The high cost of developing
	domain-specific, integrated modeling, analysis, and application-generation
	environments prevents their penetration into narrower engineering
	fields that have limited user bases. Model-integrated computing (MIC),
	an approach to model-based engineering that helps compose domain-specific
	design environments rapidly and cost effectively, is particularly
	relevant for specialized computer-based systems domains-perhaps even
	single projects. The authors describe how MIC provides a way to compose
	such environments cost effectively and rapidly by using a metalevel
	architecture to specify the domain-specific modeling language and
	integrity constraints. They also discuss the toolset that implements
	MIC and describe a practical application in which using the technology
	in a tool environment for the process industry led to significant
	reductions in development and maintenance costs},
  doi = {10.1109/2.963443},
  issn = {0018-9162},
  keywords = {domain models;domain-specific modeling language;integrated modeling;integrity
	constraints;maintenance costs;metalevel architecture;model-integrated
	computing;specifications;system behavior;formal specification;logic
	CAD;}
}

@INPROCEEDINGS{4420439,
  author = {Byeong-Joon Lee and Dong-Ju Lee and Gyun Woo},
  title = {Functional Reactive Program Translator for Controlling Robot Systems},
  booktitle = {Convergence Information Technology, 2007. International Conference
	on},
  year = {2007},
  pages = {1322 -1325},
  month = {nov.},
  abstract = {FRP (functional reactive programming) is a domain- specific sub-language
	embedded in Haskell. FRP is developed based on the arrow types and
	suitable for programming reactive systems such as robot systems.
	An essential higher-order type, namely signal, provides higher levels
	of abstractions of reactive systems. However, FRP can not be directly
	applicable to most robot systems since it is implemented on top of
	a heavy language Haskell. To overcome this limitation, RT-FRP (real-time
	functional reactive programming) was proposed, but any practical
	implementation of RT-FRP has not been proposed currently. In this
	paper, RT-FRP is implemented as a form of C translator. The proposed
	translator converts RT-FRP programs into C programs. To measure the
	effectiveness of the RT-FRP translator, some robot control programs
	are developed, translated, and loaded on top of LEGO MindStorm. According
	to the experimental result, the reactive logic can be coded more
	concisely using RT-FRP than C although the size of the binary code
	is somewhat increased.},
  doi = {10.1109/ICCIT.2007.340},
  keywords = {C programs;C translator;Haskell;controlling robot systems;domain-
	specific sub-language;functional reactive program translator;C language;control
	engineering computing;program interpreters;robot programming;}
}

@INPROCEEDINGS{4430420,
  author = {Lee, C.K. and Chiu, D.K.W.},
  title = {Ontology Based Personalized Service in E-markeplaces: A Case Study
	in a Used Car Matching System},
  booktitle = {Innovations in Information Technology, 2007. IIT '07. 4th International
	Conference on},
  year = {2007},
  pages = {631 -635},
  month = {nov.},
  abstract = {In order to provide effective personalized recommendations, the understanding
	of products as well as the users ' preferences is crucial and this
	requires domain- specific knowledge. Recently, ontologies have been
	developed in various business domains with the recent maturing of
	the Semantic Web technologies, which can be applied for this purpose.
	In this paper, we present our enhanced meta-model to address this
	problem. To demonstrate the applicability and effectiveness, this
	paper present an application of the Semantic Web based technologies
	to a used car matching system (UCMS) in an e-marketplace as a case
	study. We present the ontology for modeling used cars with RDF (resource
	description framework) and RDF Schema, together with some sample
	coding and information model to demonstrate the implementation.},
  doi = {10.1109/IIT.2007.4430420},
  keywords = {domain- specific knowledge;e-markeplaces;ontology based personalized
	service;resource description framework;semantic Web based technologies;semantic
	Web technologies;used car matching system;electronic commerce;ontologies
	(artificial intelligence);semantic Web;}
}

@INPROCEEDINGS{1459848,
  author = {Lee, E. and Neuendorffer, S.},
  title = {Classes and subclasses in actor-oriented design},
  booktitle = {Formal Methods and Models for Co-Design, 2004. MEMOCODE '04. Proceedings.
	Second ACM and IEEE International Conference on},
  year = {2004},
  pages = { 161 - 168},
  month = {june},
  abstract = { Actor-oriented languages provide a component composition methodology
	that emphasizes concurrency. The interfaces to actors are parameters
	and ports (vs. members and methods in object-oriented languages).
	Actors interact with one another through their ports via a messaging
	schema that can follow any of several concurrent semantics (vs. procedure
	calls, with prevail in OO languages). Domain-specific actor-oriented
	languages and frameworks are common (e.g. Simulink, LabVIEW, and
	many others). However, they lack many of the modularity and abstraction
	mechanisms that programmers have become accustomed to in 00 languages,
	such as classes, inheritance, interfaces, and polymorphism. This
	extended abstract shows the form that such mechanisms might take
	in AO languages. A prototype of these mechanisms realized in Ptolemy
	II is described.},
  doi = {10.1109/MEMCOD.2004.1459848},
  issn = { },
  keywords = { Ptolemy II; actor-oriented design; domain-specific actor-oriented
	languages; object-oriented languages; object-oriented languages;
	object-oriented programming;}
}

@ARTICLE{5963629,
  author = {HyoukJoong Lee and Brown, K.J. and Sujeeth, A.K. and Chafi, H. and
	Olukotun, K. and Rompf, T. and Odersky, M.},
  title = {Implementing Domain-Specific Languages for Heterogeneous Parallel
	Computing},
  journal = {Micro, IEEE},
  year = {2011},
  volume = {31},
  pages = {42 -53},
  number = {5},
  month = {sept.-oct. },
  abstract = {Domain-specific languages offer a solution to the performance and
	the productivity issues in heterogeneous computing systems. The Delite
	compiler framework simplifies the process of building embedded parallel
	DSLs. DSL developers can implement domain-specific operations by
	extending the DSL framework, which provides static optimizations
	and code generation for heterogeneous hardware. The Delite runtime
	automatically schedules and executes DSL operations on heterogeneous
	hardware.},
  doi = {10.1109/MM.2011.68},
  issn = {0272-1732},
  keywords = {Delite compiler framework;Delite runtime;code generation;domain specific
	languages;embedded parallel DSL;heterogeneous computing systems;heterogeneous
	parallel computing;productivity issues;static optimizations;embedded
	systems;optimisation;parallel processing;program compilers;scheduling;specification
	languages;}
}

@INPROCEEDINGS{284354,
  author = {Lee, H.G. and Lee, R.M. and Yu, G.},
  title = {Constraint logic programming and mixed integer programming},
  booktitle = {System Sciences, 1993, Proceeding of the Twenty-Sixth Hawaii International
	Conference on},
  year = {1993},
  volume = {iii},
  pages = {543 -552 vol.3},
  month = {jan},
  abstract = {Constraint logic programming (CLP), which combines the complementary
	strengths of the artificial intelligence (AI) and OR approaches,
	is introduced as a new tool for formalizing constraint satisfaction
	problems that include both qualitative and quantitative constraints.
	CLP(R), one CLP language, is used to contrast the CLP approach with
	mixed integer programming (MIP). Three relative advantages of CLP
	over MIP are analyzed: representational efficiency for domain-specific
	knowledge; partial solutions; and ease of model revision. A case
	example of constraint satisfaction problems is implemented by MIP
	and CLP(R) for comparison of the two approaches. The results exhibit
	the representational economics of CLP with computational efficiency
	comparable to that of MIP},
  doi = {10.1109/HICSS.1993.284354},
  keywords = {OR;artificial intelligence;computational efficiency;constraint satisfaction
	problems;domain-specific knowledge;mixed integer programming;model
	revision;operations research;qualitative constraints;quantitative
	constraints;constraint handling;integer programming;logic programming;}
}

@ARTICLE{4052310,
  author = {Lee, J.-S. and Chae, H.S.},
  title = {Domain-specific language approach to modelling UI architecture of
	mobile telephony systems},
  journal = {Software, IEE Proceedings -},
  year = {2006},
  volume = {153},
  pages = {231 -240},
  number = {6},
  month = {dec. },
  abstract = {Although there has been a considerable increase in the use of embedded
	software including mobile telephony applications, the development
	of embedded software has not proved so manageable as compared with
	conventional software. From the experience of working with mobile
	telephony systems for over three years, it is the author's belief
	that the huge amount of variance in application logics, not the diversity
	of hardware platforms, is the major obstacle to the development of
	embedded software. A domain specific language (DSL) for modelling
	the user interface (UI) architecture of embedded software, especially
	focusing on telephony applications is proposed. With the proposed
	DSL, developers can describe the UI architecture of applications
	by the fundamental domain concepts at a higher level of abstraction.
	The proposed DSL is based on the concept of scene. A scene is proposed
	as a unit of UI in the UI architecture and UI-related behaviours
	are associated with scenes. The result of a pilot project conducted
	in a major company dedicated to developing mobile telephony applications
	is also described},
  doi = {10.1049/ip-sen:20060022},
  issn = {1462-5970},
  keywords = {domain-specific language;embedded software;mobile telephony systems;user
	interface architecture modelling;embedded systems;mobile computing;telephony;user
	interfaces;}
}

@INPROCEEDINGS{1393854,
  author = {Jin-Shyan Lee and Meng-Chu Zhou and Pau-Lo Hsu},
  title = {A multi-paradigm modeling approach for hybrid dynamic systems},
  booktitle = {Computer Aided Control Systems Design, 2004 IEEE International Symposium
	on},
  year = {2004},
  pages = {77 -82},
  month = {sept.},
  abstract = {In the past years, modeling and simulation of hybrid dynamic systems
	(HDS) have attracted much attention. However, since simultaneously
	dealing with the discrete and continuous variables is very difficult,
	most of the models result in a unified, but more complicated and
	unnatural format. Moreover, design engineers cannot be allowed to
	use their preferred domain models. Based on the multi-paradigm modeling
	(MPaM) concept, this paper proposed a Petri net (PN) framework with
	associated state equations to model the HDS. In the presented approach,
	modeling schemes of the hybrid systems are separated, but combined
	in a hierarchical way through specified interfaces. Designers can
	still work in their familiar domain-specific modeling paradigms and
	the heterogeneity is hidden when composing large systems. An application
	to a rapid thermal process (RTP) in semiconductor manufacturing is
	provided to demonstrate the practicability of the developed approach},
  doi = {10.1109/CACSD.2004.1393854},
  keywords = {Petri nets;design engineers;domain specific modeling;hybrid dynamic
	system modeling;hybrid dynamic system simulation;multiparadigm modeling
	method;rapid thermal process;semiconductor manufacture;state equations;Petri
	nets;continuous time systems;discrete event systems;rapid thermal
	processing;semiconductor process modelling;}
}

@INPROCEEDINGS{545643,
  author = {Mike Tien-Chien Lee and Yu-Chin Hsu and Ben Chen and Fujita, M.},
  title = {Domain-specific high-level modeling and synthesis for ATM switch
	design using VHDL},
  booktitle = {Design Automation Conference Proceedings 1996, 33rd},
  year = {1996},
  pages = {585 -590},
  month = {jun,},
  abstract = {This paper presents our experience on domain-specific high-level modeling
	and synthesis for Fujitsu ATM switch design. We propose a high-level
	design methodology using VHDL, where ATM switch architectural features
	are considered during behavior modeling, and a high-level synthesis
	compiler, MEBS, is prototyped to synthesize the behavior model down
	to a gate-level implementation. Since the specific ATM switch architecture
	is incorporated into both modeling and syntheses phases, a high-quality
	design is efficiently derived. The synthesis results show that given
	the design constraints, the proposed high-level design methodology
	can produce a gate-level implementation by MEBS with about 15% area
	reduction in shorter design cycle when compared with manual design},
  doi = {10.1109/DAC.1996.545643},
  keywords = {ATM switch design;MEBS;VHDL;architectural features;behavior modeling;domain-specific
	high-level modeling;gate-level implementation;high-level design methodology;high-level
	synthesis;high-level synthesis compiler;high-quality design;asynchronous
	transfer mode;hardware description languages;high level synthesis;logic
	CAD;}
}

@INPROCEEDINGS{4351349,
  author = {Leff, A. and Rayfield, J.T.},
  title = {Relational Blocks: A Visual Dataflow Language for Relational Web-Applications},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {205 -208},
  month = {sept.},
  abstract = {Many Web-applications can be characterized as "relational". In this
	paper we show that developers can greatly benefit from relational
	blocks, a visual, domain-specific, dataflow language that is designed
	to facilitate the construction of such web-applications. We define
	the relational blocks visual language, discuss our WebRB implementation,
	and show how WebRB is used to construct non-trivial web-pages.},
  doi = {10.1109/VLHCC.2007.11},
  keywords = {nontrivial Web-pages;relational Web-applications;relational blocks;visual
	dataflow language;Internet;relational databases;visual languages;}
}

@INPROCEEDINGS{4730487,
  author = {Leijon, V. and Wallin, S. and Ehnmark, J.},
  title = {SALmon A Service Modeling Language and Monitoring Engine},
  booktitle = {Service-Oriented System Engineering, 2008. SOSE '08. IEEE International
	Symposium on},
  year = {2008},
  pages = {202 -207},
  month = {dec.},
  abstract = {To be able to monitor complex services and examine their properties
	we need a modeling language that can express them in an efficient
	manner. As telecom operators deploy and sell increasingly complex
	services the need to monitor these services increases. We propose
	a novel domain specific language called SALmon, which allows for
	efficient representation of service models, together with a computational
	engine for evaluation of service models. This working prototype allows
	us to perform experiments with full scale service models, and proves
	to be a good trade-off between simplicity and expressive power.},
  doi = {10.1109/SOSE.2008.29},
  keywords = {SALmon domain-specific language;object-oriented functional language;service
	modeling language;service monitoring engine;telecom operator;functional
	languages;object-oriented languages;specification languages;telecommunication
	computing;telecommunication services;}
}

@INPROCEEDINGS{5623639,
  author = {Lengyel, L. and Levendovszky, T. and Charaf, H.},
  title = {Managing constraints of validation in model transformations},
  booktitle = {Software, Telecommunications and Computer Networks (SoftCOM), 2010
	International Conference on},
  year = {2010},
  pages = {230 -234},
  month = {sept.},
  abstract = {Model-Driven Development (MDD) facilitates the synthesis of application
	programs from models using customized, domain-specific model processors.
	MDD appears in many, different areas including telecommunication
	services, communication protocols, enterprise networking and multimedia
	systems. Model compilers can be realized by graph rewriting-based
	model transformations, where constraints ensuring the validation
	can be assigned to model transformation rules. The approach supports
	validated model processing. This paper introduces new algorithms
	applied to manage validation constraints in model transformation
	rules. The presented algorithms facilitate the better understanding
	of the transformations, their easier constraint-based configuration,
	and make both the constraints and the rewriting rules reusable.},
  keywords = {communication protocols;constraint-based configuration;domain-specific
	model processors;enterprise networking;graph rewriting-based model
	transformations;model compilers;model-driven development;multimedia
	systems;rewriting rules;telecommunication services;constraint handling;multimedia
	computing;object-oriented programming;program verification;rewriting
	systems;software architecture;telecommunication services;transport
	protocols;}
}

@ARTICLE{5386631,
  author = {Leroux, D. and Nally, M. and Hussey, K.},
  title = {Rational Software Architect: A tool for domain-specific modeling},
  journal = {IBM Systems Journal},
  year = {2006},
  volume = {45},
  pages = {555 -568},
  number = {3},
  month = { },
  abstract = {Rational Software Architect (RSA), the latest generation Rational
	#x00AE; modeling tool, is based on Eclipse #x2122; Modeling Framework
	(EMF) technology. RSA offers all the important features of the previous
	generation of Rational modeling tools, while supporting a much wider
	range of model formats. RSA diagrams can be used in editing and displaying
	models derived from any EMF-based metamodel. The combination of RSA
	and EMF provides a powerful capability for integrating domain-specific
	languages (DSLs) with UML #x00AE; in a single toolset. This paper
	describes how RSA and EMF provide these capabilities and explores
	some of the ways that IBM is currently exploiting them.},
  doi = {10.1147/sj.453.0555},
  issn = {0018-8670}
}

@INPROCEEDINGS{4368067,
  author = {Fang-Yie Leu and Chih-Chieh Ko},
  title = {An Automated Term Definition Extraction using the Web Corpus in Chinese
	Language},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2007. NLP-KE
	2007. International Conference on},
  year = {2007},
  pages = {435 -440},
  month = {30 2007-sept. 1},
  abstract = {This paper proposes a system, named DefExplorer, which extracts term
	definitions from the Web, determines the type of question terms,
	and selects answers from noisy Web pages automatically. DefExplorer
	filters out invalid data with a semantic approach. We deployed two
	types of candidate sets, common and domain specific, to group similar
	candidates and determine candidates' importance for selecting final
	answers. Experimental results show that DefExplorer can effectively
	extract term definitions from the Web, especially for the definitions
	of out-of-vocabulary terms.},
  doi = {10.1109/NLPKE.2007.4368067},
  keywords = {Chinese language;DefExplorer;Web corpus;automated term definition
	extraction;noisy Web pages;semantic approach;information retrieval;natural
	language processing;semantic Web;vocabulary;}
}

@INPROCEEDINGS{730596,
  author = {Leupers, R.},
  title = {HDL-based modeling of embedded processor behavior for retargetable
	compilation},
  booktitle = {System Synthesis, 1998. Proceedings. 11th International Symposium
	on},
  year = {1998},
  pages = {51 -54},
  month = {dec},
  abstract = {The concept of retargetability enables compiler technology to keep
	pace with the increasing variety of domain-specific embedded processors.
	In order to achieve user retargetability, powerful processor modeling
	formalisms are required. Most of the recent modeling formalisms concentrate
	on horizontal, VLIW-like instruction formats. However, for encoded
	instruction formats with restricted instruction-level parallelism
	(ILP), a large number of ILP constraints might need to be specified,
	resulting in less concise processor models. This paper presents an
	HDL-based approach to processor modeling for retargetable compilation,
	in which ILP may be implicitly constrained. As a consequence, the
	formalism allows for concise models also for encoded instruction
	formats. The practical applicability of the modeling formalism is
	demonstrated by means of a case study for a complex DSP },
  doi = {10.1109/ISSS.1998.730596},
  keywords = {ILP;compiler technology;embedded processor behavior;instruction-level
	parallelism;retargetability;retargetable compilation;embedded systems;hardware
	description languages;program compilers;}
}

@ARTICLE{55230,
  author = {Lewis, T. and Funkhouser, G. and Lou, K.H. and Ito, A. and Bahlke,
	R.},
  title = {Code generators},
  journal = {Software, IEEE},
  year = {1990},
  volume = {7},
  pages = {67 -70},
  number = {3},
  month = {may},
  abstract = {Code generators, which take a programmer's inputs in the form of some
	abstraction, design, or direct interaction with the system and write
	out a source program that implements the details of the application,
	are reviewed. After a brief overview, four separate presentations
	cover examples of such systems from four domains. The Tags tools
	uses abstraction to specify real-time control systems. In this tool,
	the details of synchronization code are hidden from the designer,
	but some functionality must still be provided in detail. Microstep
	is a similar kind of tool for data-processing applications. One can
	view both Tags and Microstep as high-level specification languages.
	PSG generates programming environments, i.e. one gives it a nonprocedural
	specification of a language's syntax and it generates an integrated
	programming environment for that language. While one must specify
	the environment in detailed form, the actual code generation is hidden.
	Escort is a similar system specialized for telecommunications applications.
	It lets one build, execute, and test specifications for telecommunication
	systems. Both PSG and Escort illustrate the power of domain-specific
	tools},
  doi = {10.1109/52.55230},
  issn = {0740-7459},
  keywords = {Escort;Microstep;PSG;Tags tools;abstraction;code generators;design;direct
	interaction;domain-specific tools;integrated programming environment;nonprocedural
	specification;programming environments;real-time control systems;synchronization
	code;application generators;}
}

@INPROCEEDINGS{4724700,
  author = {Fang Li and Di Li and Jiafu Wan and Xin Huang},
  title = {Towards a Component-Based Model Integration Approach for Embedded
	Computer Control System},
  booktitle = {Computational Intelligence and Security, 2008. CIS '08. International
	Conference on},
  year = {2008},
  volume = {1},
  pages = {495 -499},
  month = {dec.},
  abstract = {A component-based model integration approach for the embedded computer
	control system (ECS) development is proposed in this paper. The three-layer
	architecture for modeling, verification as well as implementation
	is described. Model strategies such as multi-aspect amp; multi-view
	description method, DSML (domain specific modeling language) amp;
	FML(formal modeling language) description method as well as hierarchical
	component based modeling method are put forward. The focus of our
	approach is on creating an integrated embedded computer control system
	development environment for design, verification as well as implementation.},
  doi = {10.1109/CIS.2008.218},
  keywords = {DSML;FML;component-based model integration approach;domain specific
	modeling language;embedded computer control system;formal modeling
	language;formal verification;multiaspect description method;multiview
	description method;software reusability;three-layer architecture;control
	engineering computing;embedded systems;formal verification;object-oriented
	programming;software reusability;specification languages;}
}

@INPROCEEDINGS{5645051,
  author = {Fei Li and Sehic, S. and Dustdar, S.},
  title = {COPAL: An adaptive approach to context provisioning},
  booktitle = {Wireless and Mobile Computing, Networking and Communications (WiMob),
	2010 IEEE 6th International Conference on},
  year = {2010},
  pages = {286 -293},
  month = {oct.},
  abstract = {Context-aware services need to acquire context information from heterogeneous
	context sources. The diversity of service requirements posts challenges
	on context provisioning systems as well as their programming models.
	This paper proposes COPAL (COntext Provisioning for ALl) - an adaptive
	approach to context provisioning. COPAL is at first a runtime middleware,
	which provides loose-coupling between context and its processing.
	The component architecture of COPAL ensures that new context processing
	functions can be added dynamically. A set of context processing patterns
	are proposed to customize context attributes and compose context
	provisioning schemes. The COPAL components and models are reflected
	in a Domain Specific Language (DSL), which can further reduce the
	development efforts of context provisioning using automatic code
	generation. A motivating scenario is used throughout the paper to
	illustrate COPAL approach.},
  doi = {10.1109/WIMOB.2010.5645051},
  keywords = {COPAL;adaptive approach;automatic code generation;context aware services;context
	information;context provisioning;diversity;domain specific language;heterogeneous
	context sources;loose coupling;programming models;runtime middleware;diversity
	reception;middleware;mobile computing;programming theory;}
}

@INPROCEEDINGS{1531083,
  author = {Li, K. and Dewar, R.G. and Pooley, R.J.},
  title = {Computer-assisted and customer-oriented requirements elicitation},
  booktitle = {Requirements Engineering, 2005. Proceedings. 13th IEEE International
	Conference on},
  year = {2005},
  pages = { 479 - 480},
  month = {aug.-2 sept.},
  abstract = { In this short paper, we represent how user interaction could be integrated
	into an NLP-based system to support stakeholders' participation in
	terms of improving requirements elicitation. In contrast to commercial
	CASE tools that support user interface, the proposed iterative and
	incremental process allows the refinement of preliminary requirements
	extracted from domain-specific data, according to users' responses.
	Not only the quality of requirements could be improved, but also
	requirements engineers could gain the benefits in ambiguity detection,
	requirements interpretation and specification and modeling.},
  doi = {10.1109/RE.2005.16},
  keywords = { CASE tools; NLP-based system; computer-assisted requirements elicitation;
	customer-oriented requirements elicitation; natural language processing;
	requirements engineering; requirements interpretation; requirements
	modeling; requirements specification; user interaction; user interface;
	computer aided software engineering; formal specification; natural
	languages; user interfaces;}
}

@INPROCEEDINGS{1443194,
  author = {Peng Li and Zdancewic, S.},
  title = {Practical information flow control in Web-based information systems},
  booktitle = {Computer Security Foundations, 2005. CSFW-18 2005. 18th IEEE Workshop},
  year = {2005},
  pages = { 2 - 15},
  month = {june},
  abstract = { This paper presents a practical application of language-based information-flow
	control, namely, a domain-specific Web scripting language designed
	for interfacing with databases. The primary goal is to provide strong
	enforcement of confidentiality and integrity policies: confidential
	data can be released only in permitted ways and trustworthy data
	must result from expected computations or conform to expected patterns.
	Such security policies are specified in the database layer and statically
	enforced for the rest of the system in an end-to-end fashion. In
	contrast with existing Web-scripting languages, which provide only
	ad hoc mechanisms for information security, the scripting language
	described here uses principles based on the well-studied techniques
	in information-flow type systems. However, because Web scripts often
	need to downgrade confidential data and manipulated untrusted user
	input, they require practical and convenient ways of downgrading
	secure data. To achieve this goal, the language allows safe downgrading
	according to downgrading policies specified by the programmer. This
	novel, pattern-based approach provides a practical instance of recent
	work on delimited release and relaxed noninterference and extends
	that work by accounting for integrity policies.},
  doi = {10.1109/CSFW.2005.23},
  issn = {1063-6900},
  keywords = { Web-based information systems; data confidentiality; data integrity;
	domain-specific Web scripting language; information security; information-flow
	type systems; language-based information flow control; pattern-based
	approach; security policies; Internet; data encapsulation; data integrity;
	data privacy; security of data;}
}

@INPROCEEDINGS{4906772,
  author = {Shoushan Li and Chengqing Zong},
  title = {Multi-domain adaptation for sentiment classification: Using multiple
	classifier combining methods},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2008. NLP-KE
	'08. International Conference on},
  year = {2008},
  pages = {1 -8},
  month = {oct.},
  abstract = {Sentiment classification is very domain-specific and good domain adaptation
	methods, when the training and testing data are drawn from different
	domains, are sorely needed. In this paper, we address a new approach
	to domain adaptation for sentiment classification in which classifiers
	are adapted for a specific domain with training data from multiple
	source domains. We call this new approach dasiamulti-domain adaptationpsila
	and present a multiple classifier system (MCS) framework to describe
	and understand it. Under this framework, we propose a new combining
	method, called Multi-label Consensus Training (MCT), to combine the
	base classifiers for selecting dasiaautomatically-labeledpsila samples
	from unlabeled data in the target domain. The experimental results
	for sentiment classification show that multi-domain adaptation using
	this method improves adaptation performance.},
  doi = {10.1109/NLPKE.2008.4906772},
  keywords = {automatically-labeled samples;multidomain adaptation;multilabel consensus
	training;multiple classifier combining methods;multiple classifier
	system;multiple source domains;sentiment classification;unlabeled
	data;data handling;pattern classification;}
}

@INPROCEEDINGS{5359219,
  author = {Ta Li and Weiqun Xu and Jielin Pan and Yonghong Yan},
  title = {Improving Automatic Speech Recognizer of Voice Search Using System
	Combination},
  booktitle = {Fuzzy Systems and Knowledge Discovery, 2009. FSKD '09. Sixth International
	Conference on},
  year = {2009},
  volume = {4},
  pages = {477 -480},
  month = {aug.},
  abstract = {Voice search is the technology that enables users to access information
	using spoken queries. Automatic speech recognizer (ASR) is one of
	the key modules for voice search systems. However, the high error
	rate of the state-of-the-art large vocabulary continuous speech recognition
	(LVCSR) is the bottleneck for most voice search systems. In this
	paper, we first build a baseline system using language model (LM)
	with domain-specific information. To improve our system, we propose
	a forward-backward LVCSR system combination method to decrease the
	search errors in speech recognition. Experiment results show that
	our proposed method improves the performance of speech recognition
	by 5.7% relative character error rate (CER) reduction.},
  doi = {10.1109/FSKD.2009.497},
  keywords = {automatic speech recognizer;character error rate;domain-specific information;language
	model;large vocabulary continuous speech recognition;spoken queries;system
	combination;voice search systems;speech recognition;}
}

@INPROCEEDINGS{4734002,
  author = {Xinghua Li and Xindong Wu and Xuegang Hu and Fei Xie and Zhaozhong
	Jiang},
  title = {Keyword Extraction Based on Lexical Chains and Word Co-occurrence
	for Chinese News Web Pages},
  booktitle = {Data Mining Workshops, 2008. ICDMW '08. IEEE International Conference
	on},
  year = {2008},
  pages = {744 -751},
  month = {dec.},
  abstract = {This paper presents a new keyword extraction algorithm for Chinese
	news Web pages using lexical chains and word co-occurrence combined
	with frequency features, cohesion features, and corelation features.
	A lexical chain is an external performance consistency by semantically
	related words of a text, and is the representation of the semantic
	content of a portion of the text. Word co-occurrence distribution
	is an important statistical model widely used in natural language
	processing that reflects the correlation of the words. Lexical chains
	and word co-occurrence are combined in this paper to extract keywords
	for Chinese news Web pages in our proposed algorithm KELCC. This
	algorithm is not domain-specific and can be applied to a single Web
	page without corpus. Experiments on randomly selected Web pages have
	been performed to demonstrate the quality of the keywords extracted
	by our proposed algorithm.},
  doi = {10.1109/ICDMW.2008.122},
  keywords = {Chinese news Web page;cohesion features;corelation features;frequency
	features;keyword extraction;lexical chain;natural language processing;statistical
	model;word cooccurrence;word correlation;natural language processing;statistical
	analysis;text analysis;}
}

@INPROCEEDINGS{5382351,
  author = {Yali Li and Changchun Bao and Yonghong Yan},
  title = {A novel similarity measure for semantic class induction in human-computer
	spoken dialogues},
  booktitle = {Information, Computing and Telecommunication, 2009. YC-ICT '09. IEEE
	Youth Conference on},
  year = {2009},
  pages = {351 -354},
  month = {sept.},
  abstract = {In this paper, we introduced a new semantic induction metric which
	can induce some semantic classes from a set of domain-specific unannotated
	data. We emphasized on the co-occurrence probability instead of just
	distances of word probability distribution. Compared to the traditional
	approach on right or left context to calculate the similarity, we
	used both left and right information simultaneously in the metric.
	Before processing, we removed fillers based on their unigram and
	bigram context distribution. We can find that co-occurrence metric
	is simple, effective and have lower misclassified portion. We test
	the metric on our Chinese voice-search data, and get F1 for 84.3.},
  doi = {10.1109/YCICT.2009.5382351},
  keywords = {Chinese voice-search data;bigram context distribution;co-occurrence
	probability;domain-specific unannotated data;human-computer spoken
	dialogues;semantic class induction;similarity measure;unigram context
	distribution;word probability distribution;human computer interaction;interactive
	systems;natural language interfaces;natural language processing;probability;speech-based
	user interfaces;}
}

@INPROCEEDINGS{6032534,
  author = {Yang Li and Narayan, N. and Helming, J. and Koegel, M.},
  title = {A domain specific requirements model for scientific computing: NIER
	track},
  booktitle = {Software Engineering (ICSE), 2011 33rd International Conference on},
  year = {2011},
  pages = {848 -851},
  month = {may},
  abstract = {Requirements engineering is a core activity in software engineering.
	However, formal requirements engineering methodologies and documented
	requirements are often missing in scientific computing projects.
	We claim that there is a need for methodologies, which capture requirements
	for scientific computing projects, because traditional requirements
	engineering methodologies are difficult to apply in this domain.
	We propose a novel domain specific requirements model to meet this
	need. We conducted an exploratory experiment to evaluate the usage
	of this model in scientific computing projects. The results indicate
	that the proposed model facilitates the communication across the
	domain boundary, which is between the scientific computing domain
	and the software engineering domain. It supports requirements elicitation
	for the projects efficiently.},
  doi = {10.1145/1985793.1985922},
  issn = {0270-5257},
  keywords = {NIER track;domain specific requirements model;formal requirements
	engineering;scientific computing projects;software engineering;formal
	specification;natural sciences computing;}
}

@INPROCEEDINGS{1344719,
  author = {Zheyin Li and Barbeau, M.},
  title = {Performance of generative programming based protocol implementation},
  booktitle = {Communication Networks and Services Research, 2004. Proceedings.
	Second Annual Conference on},
  year = {2004},
  pages = { 113 - 120},
  month = {may},
  abstract = { The protocol implementation framework for Linux (PIX) is a protocol
	development tool using generative programming. It aims at capturing
	the similarities in behavior among different layers of protocols
	and grouping solutions to cross-cutting concerns of communication
	systems. It achieves a high degree of configurability by providing
	several combinations which could be chosen to generate the desired
	protocols. The paper investigates how the performance of the generative
	programming based protocol implementation compares with traditional
	protocol implementation techniques. A benchmark is developed to give
	a thorough performance analysis of PIX to contrast it with other
	protocol development frameworks. The benchmark compares the performance
	of bulk data transfer. The file transfer protocol (FTP) is used for
	comparison purposes. Latency, throughput and resource usage measurements
	are provided in order to compare the performance of PIX and generative
	programming with NcFTP, which uses structured programming, and x-Kernel,
	which uses structured and object-based programming.},
  doi = {10.1109/DNSR.2004.1344719},
  issn = { },
  keywords = { FTP; Linux; bulk data transfer; file transfer protocol; generative
	programming based protocol implementation; latency; object-based
	programming; protocol development tool; protocol layers; resource
	usage; structured programming; throughput; x-Kernel; computer communications
	software; object-oriented programming; operating systems (computers);
	software tools; structured programming; transport protocols;}
}

@INPROCEEDINGS{6041610,
  author = {Zhaopeng Li and Yang Zhang and Yiyun Chen},
  title = {A Method to Generate Verification Condition Generator},
  booktitle = {Theoretical Aspects of Software Engineering (TASE), 2011 Fifth International
	Symposium on},
  year = {2011},
  pages = {239 -242},
  month = {aug.},
  abstract = {We propose a method to generate certain verification condition generators
	(VCGens, for short) automatically to be used in certifying compilers
	or other verification tools in this paper, to alleviate the burden
	of developing various kinds of VCGens in the domain-specific program
	verification tools. We introduce a new methodology for describing
	the rules in the verification condition calculation. We have implemented
	a prototype of VCGEN2(VCGenGen) using C++. This tool provides a series
	of interfaces named action functions to the users. Users can describe
	the calculation rules by combining these action functions. And our
	tool also embeds a parser generator, so users need to feed in the
	grammar of the languages along with the calculation rules. If there
	is no error, VCGEN2 outputs the corresponding VCGen with respect
	to the user-defined languages and rules. We have used our prototype
	to generate a number of VCGens successfully as demonstration.},
  doi = {10.1109/TASE.2011.25},
  keywords = {C++ language;VCGEN2;VCGenGen;VCGens;compilers;verification condition
	generator;verification tools;C++ language;formal verification;program
	compilers;}
}

@INPROCEEDINGS{5709179,
  author = {Haode Liao and Jun Jiang and Yuxin Zhang},
  title = {A Study of Automatic Code Generation},
  booktitle = {Computational and Information Sciences (ICCIS), 2010 International
	Conference on},
  year = {2010},
  pages = {689 -691},
  month = {dec.},
  abstract = {Automatic code generation is the study of generative programming in
	the sense that the source code is generated automatically. In this
	paper, an approach based on component techniques that can produce
	in a systematic way correct, compatible and efficient database structures
	and manipulation function modules from abstract models is proposed.
	In contrast to some conventional software engineering methods, this
	approach has certain merits of improving software quality and shortening
	the software development cycle.},
  doi = {10.1109/ICCIS.2010.171},
  keywords = {automatic code generation;database structures;generative programming;software
	development cycle;software engineering methods;software quality;program
	compilers;software quality;}
}

@INPROCEEDINGS{5305976,
  author = {Liegl, P. and Mayrhofer, D.},
  title = {A Domain Specific Language for UN/CEFACT's Core Components},
  booktitle = {Services - II, 2009. SERVICES-2 '09. World Conference on},
  year = {2009},
  pages = {123 -131},
  month = {sept.},
  abstract = {In order to overcome the heterogeneities of different business document
	standards the United Nations Center for Trade Facilitation and Electronic
	Business (UN/CEFACT) has released the Core Components Technical Specification
	(CCTS). Core components are reusable building blocks for assembling
	business documents in an implementation neutral manner. However,
	core components are standardized without considering a specific implementation
	format and thus no tool integration is possible. Currently a syntax
	specific solution for core components, based on the Unified Modeling
	Language (UML), is provided with the UML Profile for Core Components
	(UPCC). In this paper we circumvent the complex UML meta model and
	provide a dedicated core component modeling environment based on
	a Domain Specific Language (DSL). Thereby, core component models
	are assembled on a conceptual level. In a next step the conceptual
	document model is used for the generation of domain specific artifacts.
	Our DSL based solution provides in situ validation of conceptual
	core component models and the flexible generation of deployment artifacts
	such as XML Schema definitions, used for the definition of interfaces
	in a service oriented environment.},
  doi = {10.1109/SERVICES-2.2009.24},
  keywords = {DSL;UML;UN/CEFACT core components;United Nations Center for Trade
	Facilitation and Electronic Business;XML schema definitions;business
	document standardization;core component modeling;core components
	technical specification;domain specific language;service oriented
	environment;syntax specific solution;unified modeling language;Unified
	Modeling Language;business data processing;document handling;}
}

@INPROCEEDINGS{4781724,
  author = {Liehr, A.W. and Buchenrieder, K.J. and Nageldinger, U.},
  title = {Visual feedback for design-space exploration with UML MARTE},
  booktitle = {Innovations in Information Technology, 2008. IIT 2008. International
	Conference on},
  year = {2008},
  pages = {44 -48},
  month = {dec.},
  abstract = {UML and specialized profiles, such as MARTE, constitute promising
	specification and modeling tools in the field of system development.
	Language-based specification and resource modeling shortens the design
	cycle, and allows the generation of performance simulation models
	for design-space exploration. Thereby, current solutions in the field
	of UML based system-development will not allow the feedback of such
	performance estimation processes to be included into the UML system
	model and its visual representation. Enabling such feedback yields
	a consistent EDA design flow, that provides a transparent abstraction
	layer for performance simulation results, independent from the utilized
	performance simulation methods. With this work, we introduce a novel
	UML stereotype, providing such a visualization of results from performance
	estimation methods. Additionally, we demonstrate a method to implement
	the generation of UML diagrams using this stereotype. The proposed
	approach fosters the comparison of design alternatives, in the context
	of performance constrains, at an early stage in the development process.
	Thereby, no domain specific knowledge concerning the underlying performance
	evaluation approach is required by users of this method.},
  doi = {10.1109/INNOVATIONS.2008.4781724},
  keywords = {UML MARTE;design-space exploration;language-based specification;modeling
	tools;performance evaluation;resource modeling;specification tools;visual
	feedback;Unified Modeling Language;formal specification;software
	performance evaluation;}
}

@INPROCEEDINGS{1613370,
  author = {Liew, P. and Kontogiannis, K. and Tong, T.},
  title = {A framework for business model driven development},
  booktitle = {Software Technology and Engineering Practice, 2004. STEP 2004. The
	12th International Workshop on},
  year = {2004},
  pages = {8 pp. -56},
  month = {sept.},
  abstract = {Typically, large companies in an effort to increase efficiency specify
	business processes using workflow languages, while software designers
	specify the systems that implement these processes with the use of
	languages like UML. This separation of domain expertise allows for
	software engineers from each individual area to work more efficiently
	using domain specific languages and tools. However, models in these
	two domains evolve independently and inconsistencies may occur when
	two models become unsynchronized due to constant revision or evolution
	of processes and design artifacts. In this paper, we present a set
	of transformations to automatically generate a specific set of UML
	artifacts from the business process specifications. In particular,
	we examine and investigate a preliminary framework for the necessary
	annotations that need be applied to a business process model so that
	the generation of UML use cases, activity diagrams, collaboration
	diagrams and deployment diagrams could be feasible. The objective
	of this work is to be able to generate rich platform independent
	UML models that can be used for automating the generation of design
	artifacts and source code by using a model driven architecture approach.
	By doing so, we aim to decrease software design time, reduce maintenance
	costs and better support system evolution},
  doi = {10.1109/STEP.2004.5},
  keywords = {UML artifacts;UML use cases;activity diagrams;business model driven
	development;business process specifications;collaboration diagrams;deployment
	diagrams;design artifact generation;domain specific languages;software
	design time;source code generation;support system evolution;workflow
	languages;Unified Modeling Language;business data processing;workflow
	management software;}
}

@INPROCEEDINGS{1035756,
  author = {Joo-Hwee Lim and Jin, J.S.},
  title = {Image indexing and retrieval using visual keyword histograms},
  booktitle = {Multimedia and Expo, 2002. ICME '02. Proceedings. 2002 IEEE International
	Conference on},
  year = {2002},
  volume = {1},
  pages = { 213 - 216 vol.1},
  abstract = { We propose a novel image representation called visual keyword histogram
	(VKH) for content-based indexing and retrieval. Visual keywords are
	domain-relevant visual prototypes (e.g. faces, foliage, buildings
	etc) with both perceptual appearance and textual semantics. Collectively,
	VKHs axe computed over spatial tessellation to represent the distribution
	of visual keywords in various parts of an image. To construct a vocabulary
	of visual keywords, an incremental neural network is deployed to
	learn visual keywords from examples. This allows us to build domain-specific
	visual vocabularies rapidly and incrementally. Last but not least,
	we propose a new visual query language called Query by Spatial Icons
	(QBSI) that allows a user to specify a query in terms of "what" and
	"where". A visual query term constrains whether a visual keyword
	should be present and a query formals chains these terms into a disjunctive
	normal form via logical operators. We show our approach on real and
	complex home photos with very promising results.},
  doi = {10.1109/ICME.2002.1035756},
  issn = { },
  keywords = { content-based image indexing; domain-relevant visual prototypes;
	image indexing; image representation; incremental neural network;
	n visual keywords; perceptual appearance; query by Spatial Icons;
	textual semantics; visual keyword histograms; visual query language;
	visual vocabularies; content-based retrieval; image representation;
	image retrieval;}
}

@INPROCEEDINGS{4376679,
  author = {Kunhui Lin},
  title = {Using Statistical Machine Translation Model to Improve Domain-Specific
	Metasearch Engines},
  booktitle = {Control and Automation, 2007. ICCA 2007. IEEE International Conference
	on},
  year = {2007},
  pages = {1837 -1839},
  month = {30 2007-june 1},
  abstract = {In order to improve the recall of the domain-specific information
	retrieval, an efficient query expansion mechanism is proposed for
	the metasearch engines. This mechanism uses the statistical machine
	translation model to compute the relevance between general query
	words and domain-relevant query words and dispatches the expanded
	queries to component search engines. The key ingredient of translation
	model is the expectation maximization (EM) algorithm. The experimental
	results show that the proposed expansion mechanism is a desirable
	and efficient method to improve the domain-relevance of the pages
	returned by a metasearch engine.},
  doi = {10.1109/ICCA.2007.4376679},
  keywords = {EM algorithm;Internet;domain-relevant query words;domain-specific
	information retrieval;domain-specific metasearch engines;expectation
	maximization algorithm;query expansion mechanism;statistical machine
	translation model;computational linguistics;expectation-maximisation
	algorithm;language translation;query formulation;search engines;}
}

@INPROCEEDINGS{4281467,
  author = {Szu-Yin Lin and Bo-Yuan Chen and Hsien-Tzung Wu and Von-Wun Soo and
	Ku, C.C.},
  title = {Dynamic Change of a Multi-Agent Workflow for Patent Invention Using
	a Utility Function},
  booktitle = {Computer Supported Cooperative Work in Design, 2007. CSCWD 2007.
	11th International Conference on},
  year = {2007},
  pages = {389 -394},
  month = {april},
  abstract = {Patent invention includes various types of knowledge processing tasks
	such as patent document analysis, patent search, patent classification,
	patent valuation, to ensure the usefulness and novelty of the new
	patent invention. In the past, patent invention for an industry relied
	solely on tedious interactions among domain-specific human experts.
	In this paper, we propose a cooperative multi-agent and Web service
	platform to facilitate the invention of a new patent. The platform
	integrates web service technique as a standard information exchange
	mechanism for agents to communicate with each other in FIPA Agent
	Communication Language (ACL). We design a utility function in terms
	of cost, time and value of information service for an agent to decide
	whether a service should be selected in the workflow of the patent
	invention. The agents are implemented in JADE and we illustrate with
	two explicit examples in the domain of inventing a new mechanical
	design patent and show how the cooperative multi-agent and web service
	platform supports the patent invention process.},
  doi = {10.1109/CSCWD.2007.4281467},
  keywords = {FIPA agent communication language;Web service;cooperative multiagent
	system;information exchange;information service;knowledge processing
	task;patent classification;patent document analysis;patent invention;patent
	search;patent valuation;utility function;workflow management;Web
	services;business data processing;classification;document handling;information
	retrieval;multi-agent systems;patents;utility programs;workflow management
	software;}
}

@INPROCEEDINGS{5425910,
  author = {Xiangtao Lin and Bo Cheng and Junliang Chen},
  title = {A Situation-Aware Approach for Dealing with Uncertain Context-Aware
	Paradigm},
  booktitle = {Global Telecommunications Conference, 2009. GLOBECOM 2009. IEEE},
  year = {2009},
  pages = {1 -6},
  month = {30 2009-dec. 4},
  abstract = {Context-aware paradigm is intended to make decisions proactively for
	users to adapt to contexts changes in a pervasive environment so
	as to improve users' work efficiency. However, this commitment deduces
	because of the intrinsic uncertainty i.e. incompleteness, inaccuracy
	and inconsistency of context-aware paradigm. We bring forward a situation-aware
	approach, supported by Bayesian Networks, ontology and Domain Specific
	Language techniques, for dealing with uncertain context-aware paradigm.
	Situation, a description of logically combined contexts, is high-level
	abstract contexts; hence, it can shield the trivialness and inconstancy
	of low-level contexts. BN mapping contexts to situations is good
	at dealing with incomplete, inaccurate and erroneous low-level contexts;
	ontology is referred to eliminate inconsistencies among situations
	and contexts. Besides, DSL can offer flexibilities for its easy readability
	and good reusability. Also, interruption ratio, precision and efficiency
	are evaluated to validate the effectiveness of our approach w.r.t.
	a situation-aware multimedia conference application.},
  doi = {10.1109/GLOCOM.2009.5425910},
  issn = {1930-529X},
  keywords = {Bayesian networks;domain specific language techniques;high level abstract
	contexts;intrinsic uncertainty;multimedia conference application;ontology;pervasive
	environment;reusability;situation aware approach;uncertain context
	aware paradigm;belief networks;multimedia computing;ontologies (artificial
	intelligence);ubiquitous computing;uncertainty handling;}
}

@INPROCEEDINGS{5392879,
  author = {Haifeng Ling and Xianzhong Zhou and Yujun Zheng},
  title = {Design and Implementation of DSL via Category Theoretic Computations},
  booktitle = {Frontier of Computer Science and Technology, 2009. FCST '09. Fourth
	International Conference on},
  year = {2009},
  pages = {460 -466},
  month = {dec.},
  abstract = {Domain specific languages provide appropriate built-in abstractions
	and notations in a particular problem domain, and have been suggested
	as means for developing highly adaptable software systems. The paper
	presents a theory-based framework to support domain-specific design
	and implementation. Focusing concern on reasoning about interactive
	relationships among software models and objects at different levels
	of abstraction and granularity, our framework provides a unified
	categorial environment for intra-model composition and inter-model
	refinement of specifications via category theoretic computations,
	and therefore enables a high-level of reusability and dynamic adaptability.},
  doi = {10.1109/FCST.2009.102},
  keywords = {DSL;abstraction;adaptable software systems;category theoretic computations;domain
	specific languages;domain-specific design;dynamic adaptability;granularity;inter-model
	refinement;intra-model composition;software models;software reusability;category
	theory;object-oriented methods;software engineering;specification
	languages;}
}

@INPROCEEDINGS{4810019,
  author = {Lisboa, E.B. and Silva, L. and Lima, T. and Chaves, I. and Barros,
	E.},
  title = {An approach to concurrent development of device drivers and device
	controller},
  booktitle = {Advanced Communication Technology, 2009. ICACT 2009. 11th International
	Conference on},
  year = {2009},
  volume = {01},
  pages = {571 -575},
  month = {feb.},
  abstract = {Embedded Systems must communicate with different peripheral devices
	to provide easy interactivity and mobility. The communication structure
	combines hardware and software solutions. The design of a communication
	structure demands great effort, long time, tends to cause many errors
	and has relevant impact in system performance. To minimize these
	questions, this paper presents an approach to concurrent development
	of device controller simulation models and its respective device
	drivers. The approach is based on a domain specific language, named
	DevC, that allows to specify several aspects of both: device controller
	and driver. From this specification, the hardware simulation model
	and the device driver are synthesized. The device controller and
	the driver are validated using a hardware virtual platform to reduce
	development time and, then, are validated in real hardware.},
  issn = {1738-9445},
  keywords = {DevC language;concurrent device driver development;device controller;domain
	specific language;embedded systems;hardware virtual platform;operating
	system;C language;device drivers;embedded systems;}
}

@INPROCEEDINGS{4211880,
  author = {Lissel, R. and Gerlach, J.},
  title = {Introducing New Verification Methods into a Company's Design Flow:
	An Industrial User's Point of View},
  booktitle = {Design, Automation Test in Europe Conference Exhibition, 2007. DATE
	'07},
  year = {2007},
  pages = {1 -6},
  month = {april},
  abstract = {Today the task of design verification has become one of the key bottlenecks
	in hardware and system design. To address this topic, several verification
	languages, methods and tools, which address several issues of the
	verification process, were developed by multiple EDA vendors over
	the last years. This paper takes an industrial user's point of view
	and explores the difficulties introducing new verification methods
	into a company's "naturally grown" and well established design flow
	- taking into account application domain specific requirements, constraints
	given by the existing design environment and economical aspects.
	The presented approach extends the capabilities of an existing verification
	strategy by powerful new features while keeping in mind integration,
	reuse and applicability aspects. Based on an industrial design example
	the effectiveness and potential of the developed approach is shown},
  doi = {10.1109/DATE.2007.364675},
  keywords = {EDA vendors;design verification;hardware design;system design;verification
	languages;verification methods;verification process;verification
	tools;hardware description languages;hardware-software codesign;logic
	design;}
}

@INPROCEEDINGS{5968029,
  author = {Bailin Liu and Mingye Duan and Gang Zhao},
  title = {An Object Frame Knowledge Representation Approach for Fault Diagnosis
	Expert System},
  booktitle = {Future Computer Sciences and Application (ICFCSA), 2011 International
	Conference on},
  year = {2011},
  pages = {74 -77},
  month = {june},
  abstract = {Knowledge quality and usability is core to a fault diagnosis system.
	Frame knowledge technique has been used to represent knowledge in
	many information and expert systems. To simplify the complexity of
	knowledge representation in fault diagnosis expert system, a novel
	object-frame knowledge representation approach based on hierarchical
	model was proposed. In this approach, domain specific knowledge is
	expressed by combinations of object-frames. An Object-frame is composed
	by relevant state-object, test-object and rule-object or repair-object.
	Production rules are used to connect relevant objects' states. General
	features of object-frame and inference algorithm are introduced.
	Object-frame based knowledge items are stored in SQL database, inference
	engine performs the inference operation of knowledge using forward
	chaining strategy, implements reasoning, finds the cause of faults
	and gives repair suggestion driven by test data. Inference interpretation
	completes the task of explanation, which improved the clarity of
	reasoning. The advantage of this method is that we do not need knowledge
	representation language support. Experimental results show that the
	method proposed is effective, which improved the fault diagnosis
	and maintenance for a meteorological vehicle system.},
  doi = {10.1109/ICFCSA.2011.24},
  keywords = {SQL database;domain specific knowledge;fault diagnosis expert system;forward
	chaining strategy;frame knowledge technique;inference algorithm;inference
	engine;inference interpretation;knowledge quality;knowledge usability;meteorological
	vehicle system;object frame knowledge representation approach;object-frame;production
	rules;repair-object;rule-object;state-object;test-object;diagnostic
	expert systems;fault diagnosis;frame based representation;inference
	mechanisms;}
}

@INPROCEEDINGS{4351333,
  author = {Na Liu and Hosking, J. and Grundy, J.},
  title = {MaramaTatau: Extending a Domain Specific Visual Language Meta Tool
	with a Declarative Constraint Mechanism},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {95 -103},
  month = {sept.},
  abstract = {It is increasingly common to use metatools to specify and generate
	domain specific visual language tools. A common problem for such
	metatools is specification of model level behaviours, such as constraints
	and dependencies. These often need to be specified using conventional
	code in the form of event handlers or the like. We report our experience
	in integrating a declarative constraint/dependency specification
	mechanism into a domain specific visual language metatool, focussing
	on the tradeoffs we have made in the notational design and environmental
	support used. The expressive power of the mechanism developed is
	illustrated by a substantial case study where we have redeveloped
	a complex visual tool for architectural modelling, eliminating conventional
	event handlers.},
  doi = {10.1109/VLHCC.2007.10},
  keywords = {MaramaTatau;architectural modelling;declarative constraint mechanism;declarative
	constraint specification;declarative dependency specification;domain
	specific visual language metatool;event handlers;model level behaviour
	specification;formal specification;visual languages;visual programming;}
}

@INPROCEEDINGS{5676270,
  author = {Qichao Liu and Bryant, B.R. and Mernik, M.},
  title = {Metamodel Recovery from Multi-tiered Domains Using Extended MARS},
  booktitle = {Computer Software and Applications Conference (COMPSAC), 2010 IEEE
	34th Annual},
  year = {2010},
  pages = {279 -288},
  month = {july},
  abstract = {With the rapid development of model-driven engineering (MDE), domain-specific
	modeling has become a widely used software development technique.
	In MDE, metamodels represent a schema definition of the syntax and
	static semantics to which an instance model conforms (i.e., a model
	conforms to its metamodel in a similar manner to how a program conforms
	to a grammar). However, in order to address new feature requests
	of the domain and language, the metamodel often undergoes frequent
	evolution that may result in the inability of users to load and view
	previous model instances. MARS is a metamodel recovery system to
	address the problems of metamodel evolution. This paper presents
	our extensions to MARS to infer models for multi-tiered domains.
	A new XSLT translator has been developed to generate a domain-specific
	language (DSL) called MRL (model representation language) for the
	XML representation of domain instances. The metamodel inference engine
	has been revised to translate the MRL back into a metamodel.},
  doi = {10.1109/COMPSAC.2010.35},
  issn = {0730-3157},
  keywords = {MRL;XML representation;XSLT translator;domain specific modeling;extended
	MARS;metamodel recovery system;model driven engineering;model representation
	language;multitiered domains;schema definition;software development
	technique;static semantics;grammars;hypermedia markup languages;program
	interpreters;software engineering;}
}

@INPROCEEDINGS{5198533,
  author = {Qichao Liu and Javed, F. and Mernik, M. and Bryant, B.R. and Gray,
	J. and Sprague, A. and Hrncic, D.},
  title = {MARS: Metamodel Recovery from Multi-tiered Models Using Grammar Inference},
  booktitle = {Theoretical Aspects of Software Engineering, 2009. TASE 2009. Third
	IEEE International Symposium on},
  year = {2009},
  pages = {325 -326},
  month = {july},
  abstract = {In model-driven engineering, metamodels may get lost over time resulting
	in the inability to load and view existing model instances. MARS
	is a system that recovers metamodels from model instances using grammar
	inference. This paper discusses advances in MARS that improve accuracy
	and scalability.},
  doi = {10.1109/TASE.2009.29},
  keywords = {MARS;grammar inference;metamodel recovery;model-driven engineering;multitiered
	model;software development technique;XML;grammars;inference mechanisms;software
	reliability;}
}

@INPROCEEDINGS{1409918,
  author = {Liu, S.-H. and Bryant, B.R. and Gray, J.G. and Raje, R.R. and Olson,
	A.M. and Auguston, M.},
  title = {QoS-UniFrame: a Petri net-based modeling approach to assure QoS requirements
	of distributed real-time and embedded systems},
  booktitle = {Engineering of Computer-Based Systems, 2005. ECBS '05. 12th IEEE
	International Conference and Workshops on the},
  year = {2005},
  pages = { 202 - 209},
  month = {april},
  abstract = { Assuring quality of service (QoS) requirements is critical when assembling
	a distributed real-time and embedded (DRE) system from a repository
	of existing components. This paper presents a two-level approach
	for assuring satisfaction of QoS requirements in the context of a
	reduced design space for DRE systems. A dynamic and parallel approach
	is introduced to prune off the infeasible design spaces at the first
	level. Evolutionary algorithms cooperating with a domain-specific
	scripting language then discard less probable design spaces using
	statistics. These techniques fulfill the collective objectives of
	pruning and assuring the design space at system assembly time.},
  doi = {10.1109/ECBS.2005.57},
  keywords = { Petri net; QoS requirements; distributed real-time system; domain-specific
	scripting language; embedded system; evolutionary algorithm; formal
	specification; quality of service; specification languages; Petri
	nets; distributed processing; embedded systems; evolutionary computation;
	formal specification; quality of service; specification languages;}
}

@INPROCEEDINGS{5575476,
  author = {Shih-Hsi Liu and Cardenas, A. and Xang Xiong and Mernik, M. and Bryant,
	B.R. and Gray, J.},
  title = {A SOA Approach for Domain-Specific Language Implementation},
  booktitle = {Services (SERVICES-1), 2010 6th World Congress on},
  year = {2010},
  pages = {535 -542},
  month = {july},
  abstract = {Although there have been many benefits of Domain-Specific Languages
	(DSLs) reported from both academia and industry, implementation of
	DSLs continue to face challenges with respect to frequent evolution
	of both syntax and semantics. Techniques for implementing DSLs also
	lack interoperable capabilities among base languages and limited
	tool support. Such challenges result in increasing DSL development
	cost and constrain DSL adoption opportunities. This paper introduces
	a Service-Oriented Architecture (SOA) approach to address such problems.
	The approach utilizes WSDL to perform lexical and syntax analysis.
	Web services are used to define the semantics of a DSL, and WS-BPEL
	is then used to specify a DSL program. We present two case studies
	representing different DSL categories to show the feasibility of
	SOA-based DSL implementation. The case studies demonstrate the potential
	for easing the burden of DSL evolution and offering interoperability
	and tool support. Improved modularization and removal of tokenization/parsing
	are two additional advantages. Discussion and comparison among interpreter-based,
	model-driven and SOA-based DSL implementations are provided, which
	may raise more research interests in this area.},
  doi = {10.1109/SERVICES.2010.119},
  keywords = {DSL adoption opportunities;DSL development cost;SOA approach;WS-BPEL;WSDL;Web
	services;domain-specific language implementation;interoperability;interoperable
	capabilities;lexical analysis;parsing;semantics;service-oriented
	architecture;syntax analysis;tokenization;tool support;Web services;computational
	linguistics;grammars;open systems;programming language semantics;software
	architecture;software tools;}
}

@INPROCEEDINGS{4413834,
  author = {Tao Liu and Xiao-Long Wang and Bing-Quan Liu and Yuan-Chao Liu and
	Ming-Hui Li},
  title = {Extracting domain-specific terms from unlabeled web documents by
	bootstrapping and term classifiers},
  booktitle = {Systems, Man and Cybernetics, 2007. ISIC. IEEE International Conference
	on},
  year = {2007},
  pages = {3875 -3880},
  month = {oct.},
  abstract = {Domain-specific term extraction contributes to all domain-oriented
	natural language processing tasks. Given a small set of domain-specific
	terms as seed terms, new terms from unlabeled corpora can be extracted
	by bootstrapping a term classifier to discover the association between
	seed terms and new terms. Traditional term representation method
	for domain-specific term extraction represents a term in a feature
	space of documents, which depicts association of terms which share
	common documents. This representation can't depict the inner-document
	information of terms and requires extracted terms to occur in multiple
	documents. A new term representation method in global contextual
	space is proposed for domain-specific term extraction in this paper.
	This representation mechanism depicts the association of terms which
	share common global contexts. The information of terms within certain
	document and among corpora is depicted by global contexts. Experiments
	on Chinese web corpus show that the proposed domain-specific term
	extraction method with global contextual representation outperforms
	traditional method with representation mechanism in documents space.
	The improvement for low frequency terms is much higher for the proposed
	method.},
  doi = {10.1109/ICSMC.2007.4413834},
  keywords = {bootstrapping;contextual representation method;domain-oriented natural
	language processing task;domain-specific term classifier extraction;unlabeled
	Web documents;Internet;information retrieval;natural language processing;pattern
	classification;text analysis;}
}

@INPROCEEDINGS{4769417,
  author = {Wenshuo Liu and Wenxin Li},
  title = {To Determine the Weight in a Weighted Sum Method for Domain-Specific
	Keyword Extraction},
  booktitle = {Computer Engineering and Technology, 2009. ICCET '09. International
	Conference on},
  year = {2009},
  volume = {1},
  pages = {11 -15},
  month = {jan.},
  abstract = {Keyword extraction has been a very traditional topic in Natural Language
	Processing. However, most methods have been too complicated and slow
	to be applied in real applications, for example in web-based system.
	This paper proposes an approach which will complete some preparing
	works focusing on exploring the linguistic characteristics of a specific
	domain. This part can be completed once and for all and thus reduce
	the burden in the real extraction process. It is a weighted sum method
	and the preparing work focus on finding out the weight. Once we have
	the weight, the extraction can be completed by addition, multiplication
	and sort, which are quite simple for modern computer. Experimental
	results show the effectiveness of the proposed approach.},
  doi = {10.1109/ICCET.2009.136},
  keywords = {domain-specific keyword extraction;linguistic characteristics;modern
	computer;natural language processing;weighted sum method;feature
	extraction;linguistics;natural language processing;text analysis;}
}

@ARTICLE{6065017,
  author = {Lloyd, David and Dykes, Jason},
  title = {Human-Centered Approaches in Geovisualization Design: Investigating
	Multiple Methods Through a Long-Term Case Study},
  journal = {Visualization and Computer Graphics, IEEE Transactions on},
  year = {2011},
  volume = {17},
  pages = {2498 -2507},
  number = {12},
  month = {dec. },
  abstract = {Working with three domain specialists we investigate human-centered
	approaches to geovisualization following an ISO13407 taxonomy covering
	context of use, requirements and early stages of design. Our case
	study, undertaken over three years, draws attention to repeating
	trends: that generic approaches fail to elicit adequate requirements
	for geovis application design; that the use of real data is key to
	understanding needs and possibilities; that trust and knowledge must
	be built and developed with collaborators. These processes take time
	but modified human-centred approaches can be effective. A scenario
	developed through contextual inquiry but supplemented with domain
	data and graphics is useful to geovis designers. Wireframe, paper
	and digital prototypes enable successful communication between specialist
	and geovis domains when incorporating real and interesting data,
	prompting exploratory behaviour and eliciting previously unconsidered
	requirements. Paper prototypes are particularly successful at eliciting
	suggestions, especially for novel visualization. Enabling specialists
	to explore their data freely with a digital prototype is as effective
	as using a structured task protocol and is easier to administer.
	Autoethnography has potential for framing the design process. We
	conclude that a common understanding of context of use, domain data
	and visualization possibilities are essential to successful geovis
	design and develop as this progresses. HC approaches can make a significant
	contribution here. However, modified approaches, applied with flexibility,
	are most promising. We advise early, collaborative engagement with
	data amp;#8211; through simple, transient visual artefacts supported
	by data sketches and existing designs amp;#8211; before moving to
	successively more sophisticated data wireframes and data prototypes.},
  doi = {10.1109/TVCG.2011.209},
  issn = {1077-2626}
}

@ARTICLE{4543988,
  author = {Lohmann, W. and Riedewald, G. and Wachsmuth, G.},
  title = {Aspect-oriented prolog in a language processing context},
  journal = {Software, IET},
  year = {2008},
  volume = {2},
  pages = {241 -259},
  number = {3},
  month = {june },
  abstract = {Language processors can be derived from logic grammars. That several
	concerns in the processor such as parsing, several kinds of analysis
	or transformations, can be specified as aspects of the logic grammar
	is demonstred. For that purpose, the authors bring the concepts of
	aspect-oriented programming to Prolog in a systematic way, based
	on established Prolog technology. The authors illustrate that typical
	Prolog programming techniques can be described as generic aspects
	and provided in a library to support reusable concerns. A domain-specific
	language (DSL) is developed to improve readability of aspect-oriented
	specifications.},
  doi = {10.1049/iet-sen:20070064},
  issn = {1751-8806},
  keywords = {PROLOG programming techniques;aspect-oriented PROLOG;domain-specific
	language;language processing context;logic grammar;program compiler;PROLOG;grammars;object-oriented
	programming;program compilers;}
}

@INPROCEEDINGS{5349839,
  author = {Loiret, F. and Plsek, A. and Merle, P. and Seinturier, L. and Malohlava,
	M.},
  title = {Constructing Domain-Specific Component Frameworks through Architecture
	Refinement},
  booktitle = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th
	Euromicro Conference on},
  year = {2009},
  pages = {375 -382},
  month = {aug.},
  abstract = {A plethora of domain-specific component frameworks (DSCF) emerges.
	Although the current trend emphasizes generative programming methods
	as cornerstones of software development, they are commonly applied
	in a costly, ad-hoc fashion. However, we believe that DSCFs share
	the same subset of concepts and patterns. In this paper we propose
	two contributions to DSCF development. First, we propose domain components
	- a high-level abstraction to capture semantics of domain concepts
	provided by containers, and we identify patterns facilitating their
	implementation. Second, we develop a generic framework that automatically
	generates implementation of domain components semantics, thus addressing
	domain-specific services with one unified approach. To evaluate benefits
	of our approach we have conducted several case studies that span
	different domain-specific challenges.},
  doi = {10.1109/SEAA.2009.24},
  issn = {1089-6503},
  keywords = {architecture refinement;domain components semantics;domain-specific
	component frameworks;generative programming methods;high-level abstraction;software
	development;object-oriented programming;software architecture;}
}

@INPROCEEDINGS{6021604,
  author = {Lolong, S. and Kistijantoro, A.I.},
  title = {Domain Specific Language (DSL) development for desktop-based database
	application generator},
  booktitle = {Electrical Engineering and Informatics (ICEEI), 2011 International
	Conference on},
  year = {2011},
  pages = {1 -6},
  month = {july},
  abstract = {Application Generator (AG) can help save time of software development.
	Some AG has its own DSL (Domain Specific Language) to direct the
	generated application outcome. In this research, we developed a DSL
	using the syntax notation text with simple structure that can assist
	AG in the process of generating source code for desktop-based database
	application using Java. DSL development is applying the methodology
	of DSL development from Czarnecki. The development for AG involves
	three areas of knowledge, i.e. domain engineering, DSL, and AG (compiler).
	Domain engineering is required to understand the target application
	domain to be generated, i.e. MySQL database and Java programming
	language with Swing UI. The DSL is developed in five phases, i.e.
	decision-making, analysis, design, implementation, and development.
	The AG itself is developed in Java platform. The DSL and AG developed
	in this research has shown that the DSL can help programmer to develop
	Java desktop-based database applications by utilizing DSL to map
	MySQL database into Java Swing UI, and employ AG to generate applications
	directly from DSL source. It is expected that further development
	of this research is to support more flexible application development.},
  doi = {10.1109/ICEEI.2011.6021604},
  issn = {2155-6822},
  keywords = {Java programming language;MySQL database;Swing UI;desktop based database
	application generator;domain engineering;domain specific language
	development;source code generation;syntax notation text;Java;SQL;database
	management systems;program compilers;specification languages;user
	interfaces;}
}

@INPROCEEDINGS{715803,
  author = {Looye, A. and Hekstra, G. and Deprettere, E.},
  title = {Multiport memory and floating point Cordic pipeline in Jacobium processing
	elements},
  booktitle = {Signal Processing Systems, 1998. SIPS 98. 1998 IEEE Workshop on},
  year = {1998},
  pages = {406 -416},
  month = {oct},
  abstract = {The Jacobium is a dataflow processor intended for high-speed execution
	of a set of algorithms that are akin to the so-called Jacobi method
	for reducing a symmetric matrix to diagonal form using Givens rotations.
	The design of this processor has been undertaken as one of two cases
	in a recently proposed method for the quantitative analysis of domain-specific
	dataflow architectures. The method presupposes that the exploration
	of the processor's design space starts out of a given architecture
	template whose free parameters are to be determined in such a way
	that the ultimate specification is in some sense optimal for a set
	of applications that are given from the outset. Two templates have
	been considered in the Jacobium case: one for the complete (multiple
	processor element) processor and one for a typical processor element
	(PE). A parametrized VHDL version of the latter has been designed
	as well. The architecture of such a typical PE is presented here.
	It is equipped with a deep floating point Cordic pipeline, on-chip
	multiport memory to buffer operands and results, and several high-speed
	communication buses for communication between processing elements
	and the host. This parametrized architecture serves two purposes:
	it can provide realistic estimates for the PE parameters at the level
	of the complete processor; and it can be used to validate the exploration
	results},
  doi = {10.1109/SIPS.1998.715803},
  issn = {1520-6130},
  keywords = {Givens rotations;Jacobian symmetric matrix;Jacobium processing elements;VHDL;buffering;dataflow
	processor;floating-point Cordic pipeline;high-speed communication
	buses;multiple processor element;on-chip multiport memory;parameter
	estimates;Jacobian matrices;data flow computing;digital signal processing
	chips;floating point arithmetic;hardware description languages;parameter
	estimation;pipeline processing;}
}

@ARTICLE{4267594,
  author = {Louridas, P.},
  title = {Declarative GUI Programming in Microsoft Windows},
  journal = {Software, IEEE},
  year = {2007},
  volume = {24},
  pages = {16 -19},
  number = {4},
  month = {july-aug. },
  abstract = {"Software technology" will provide concise, hands-on information on
	hot software technologies early in their life cycle. Its objective
	is to promote sharing of technology experiences from a user perspective.
	We'll look at promising software components, technologies, and development
	tools and ask questions about their maturity, applications, readiness,
	and business cases. Early adopter experience will be highlighted,
	including how to deal with conflicting product development standards.
	Our vision is to interpret the software technology hype cycle and
	provide down-to-earth, useful information for practitioners. The
	inaugural column is on GUI programming for Microsoft Windows. Although
	this topic isn't new-we've struggled with it for decades-both Windows
	and its supporting GUI technologies are changing.},
  doi = {10.1109/MS.2007.105},
  issn = {0740-7459},
  keywords = {GUI programming;Microsoft Windows;software technology;user perspective;graphical
	user interfaces;software engineering;}
}

@INPROCEEDINGS{989793,
  author = {Lowry, M. and Pressburger, T. and Rosu, G.},
  title = {Certifying domain-specific policies},
  booktitle = {Automated Software Engineering, 2001. (ASE 2001). Proceedings. 16th
	Annual International Conference on},
  year = {2001},
  pages = { 81 - 90},
  month = {nov.},
  abstract = { Proof-checking code for compliance to safety policies potentially
	enables a product-oriented approach to certain aspects of software
	certification. To date, previous research has focused on generic,
	low-level programming-language properties such as memory type safety.
	In this paper we consider proof-checking higher-level domain-specific
	properties for compliance to safety policies. The paper first describes
	a framework related to abstract interpretation in which compliance
	to a class of certification policies can be efficiently calculated.
	Membership equational logic is shown to provide a rich logic for
	carrying out such calculations, including partiality, for certification.
	The architecture for a domain-specific certifier is described, followed
	by an implemented case study. The case study considers consistency
	of abstract variable attributes in code that performs geometric calculations
	in Aerospace systems.},
  doi = {10.1109/ASE.2001.989793},
  issn = {1527-1366},
  keywords = { abstract interpretation; abstract variable attributes; domain-specific
	certifier; domain-specific policies certification; geometric calculations;
	higher-level domain-specific properties; membership equational logic;
	memory type safety; product-oriented approach; proof checking code;
	software certification; certification; program verification; software
	standards;}
}

@ARTICLE{1413175,
  author = {Ruqian Lu},
  title = {From hardware to software to knowware: IT's third liberation?},
  journal = {Intelligent Systems, IEEE},
  year = {2005},
  volume = {20},
  pages = { 82 - 85},
  number = {2},
  month = {march-april},
  abstract = { Knowware is a natural development in IT after hardware and software.
	It is a knowledge module that is independent, commercialized, suitable
	for computer manipulation, and directly usable by a class of software.
	It is directly related to intelligence: if software is the condensation
	and crystallization of knowledge, then knowware is the condensation
	and crystallization of intelligence. Pseudo natural languages (PNLs)
	such as ZHIWEN make it possible to develop knowware in a fast and
	massive fashion. They also offer a way to separate software developers
	from knowware developers, so that software engineers need only develop
	software tools, leaving the task of knowware development to the domain-specific
	experts.},
  doi = {10.1109/MIS.2005.27},
  issn = {1541-1672},
  keywords = { intelligence crystallization; knowware; pseudo natural languages;
	software developers; knowledge based systems; knowledge engineering;
	natural languages;}
}

@INPROCEEDINGS{1290739,
  author = {Lu, S. and Halang, W.A. and Gumzej, R.},
  title = {Towards a comprehensive environment for the engineering of embedded
	control systems based on UML},
  booktitle = {Industrial Technology, 2003 IEEE International Conference on},
  year = {2003},
  volume = {2},
  pages = { 693 - 698 Vol.2},
  month = {dec.},
  abstract = { With the ever increasing complexity of embedded control systems,
	design and implementation have to fulfill demanding requirements
	with respect to functionality, timing, reliability, cost, safety
	and security. To meet these requirements, an integrated development
	environment based on an extended UML is designed. It aims at providing
	a comprehensive set of methods and tools for all development phases
	of complex embedded control systems. For the environment to be fully
	integrated, the gaps between the applicability of UML models for
	proper modeling as well as for validation, verification, simulation
	and code generation need to be filled. To enable domain-specific
	specification, in form of profiles UML is enhanced by comprehensive
	sets of constructs addressing the aspects real time, distribution
	and safety characteristic for embedded control systems. Corresponding
	models should automatically be translated to several other model
	notations. Here, a UML profile for simulation models is defined,
	which is process-oriented and based on extensions of UML oriented
	at PEARL for distributed systems and real-time extensions of UML
	statecharts which enable to simulate real-time constraints. This
	UML profile is expected to automatically translate UML models into
	simulation models described as XML documents, utilising existing
	model analysis tools for quantitative system analysis without the
	need for individual complex and expensive formal modeling.},
  doi = {10.1109/ICIT.2003.1290739},
  issn = { },
  keywords = { UML; XML documents; code generation; distributed systems; embedded
	control systems; extensible markup language; integrated development
	environment; model analysis tools; quantitative system analysis;
	real time constraints; real time extensions; reliability; unified
	modeling language; control system analysis computing; digital simulation;
	embedded systems; large-scale systems; specification languages;}
}

@INPROCEEDINGS{5631928,
  author = {Lucredio, D. and Fortes, R.P.M. and Almeida, E.S. and Meira, S.L.},
  title = {Designing Domain Architectures for Model-Driven Engineering},
  booktitle = {Software Components, Architectures and Reuse (SBCARS), 2010 Fourth
	Brazilian Symposium on},
  year = {2010},
  pages = {100 -109},
  month = {sept.},
  abstract = {Model-Driven Engineering (MDE) can leverage domain engineering by
	offering support to complex variability and automatic implementation.
	However, little attention is given to the process of designing a
	domain architecture that is well suited to MDE techniques such as
	domain-specific languages and software transformations. A domain-specific
	software architecture is normally developed based on a few selected
	and important requirements, called architectural drivers. This paper
	presents three types of architectural drivers that can be used to
	build a software architecture that can take full advantage of the
	benefits of MDE. It also presents some patterns that can be used
	to help in the architectural design. An evaluation is also presented,
	showing that, when used together in a model-driven domain engineering
	project, these drivers and patterns can lead to some benefits in
	terms of reusability and complexity, but that in some cases there
	are drawbacks that need to be considered in a trade-off analysis.},
  doi = {10.1109/SBCARS.2010.20},
  keywords = {architectural drivers;domain specific language;domain specific software
	architecture;model driven engineering;software complexity;software
	reusability;software transformation;software architecture;software
	metrics;software reusability;}
}

@INPROCEEDINGS{5641224,
  author = {Lukman, T. and Godena, G. and Gray, J. and Strmcnik, S.},
  title = {Model-driven engineering of industrial process control applications},
  booktitle = {Emerging Technologies and Factory Automation (ETFA), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {1 -8},
  month = {sept.},
  abstract = {Software is an important part of industrial process control systems.
	However, the state-of-the-practice for developing industrial process
	control software still has several key challenges that need to be
	addressed (e.g., migration to platforms of different vendors, lack
	of automation). This paper introduces a model-driven engineering
	approach to the development of industrial process control software,
	which is based on the ProcGraph domain-specific modeling language.
	The paper discusses and offers solutions to several of the development
	challenges that have not been addressed by existing techniques in
	the process controls domain. The contributions of the paper are a
	model-driven engineering approach for the industrial process control
	domain and a supporting tool infrastructure. The approach is demonstrated
	by a case study focused on the development of a control system for
	a TiO2 (titanium dioxide) pigment production subprocess.},
  doi = {10.1109/ETFA.2010.5641224},
  issn = {1946-0740},
  keywords = {ProcGraph domain-specific modeling language;TiO2;industrial process
	control;model-driven engineering;software enginwweing;titanium dioxide
	pigment production subprocess;metallurgy;pigments;process control;production
	engineering computing;software engineering;titanium compounds;}
}

@INPROCEEDINGS{5352764,
  author = {Lukovic, I. and Popovic, A. and Mostic, J. and Ristic, S.},
  title = {A tool for modeling form type check constraints},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {683 -690},
  month = {oct.},
  abstract = {IIS*Case is a software tool that provides information system modeling
	and generating executable application prototypes. At the level of
	platform independent model specifications, IIS*Case provides conceptual
	modeling of database schemas that include specifications of various
	database constraints, such as domain, not null key and unique constraints,
	as well as various kinds of inclusion dependencies. In the paper,
	we present new concepts and a tool embedded into IIS*Case, that are
	aimed to support specification of check constraints. We present a
	domain specific language for specifying check constraints and a tool
	that enables visually oriented design and parsing check constraints.},
  doi = {10.1109/IMCSIT.2009.5352764},
  keywords = {IIS*Case;check constraints parsing;check constraints specification;database
	schemas;domain specific language;form type check constraints modeling;information
	system modeling;integrated information systems;software tool;constraint
	handling;formal specification;information systems;program compilers;software
	tools;specification languages;}
}

@ARTICLE{4626945,
  author = {Lushbough, C. and Bergman, M.K. and Lawrence, C.J. and Jennewein,
	D. and Brendel, V.},
  title = {BioExtract Server CHARPx02014;An Integrated Workflow-Enabling System
	to Access and Analyze Heterogeneous, Distributed Biomolecular Data},
  journal = {Computational Biology and Bioinformatics, IEEE/ACM Transactions on},
  year = {2010},
  volume = {7},
  pages = {12 -24},
  number = {1},
  month = {jan.-march },
  abstract = {Many in silico investigations in bioinformatics require access to
	multiple, distributed data sources and analytic tools. The requisite
	data sources may include large public data repositories, community
	databases, and project databases for use in domain-specific research.
	Different data sources frequently utilize distinct query languages
	and return results in unique formats, and therefore researchers must
	either rely upon a small number of primary data sources or become
	familiar with multiple query languages and formats. Similarly, the
	associated analytic tools often require specific input formats and
	produce unique outputs which make it difficult to utilize the output
	from one tool as input to another. The BioExtract Server (http://bioextract.org)
	is a Web-based data integration application designed to consolidate,
	analyze, and serve data from heterogeneous biomolecular databases
	in the form of a mash-up. The basic operations of the BioExtract
	Server allow researchers, via their Web browsers, to specify data
	sources, flexibly query data sources, apply analytic tools, download
	result sets, and store query results for later reuse. As a researcher
	works with the system, their ??steps?? are saved in the background.
	At any time, these steps can be preserved long-term as a workflow
	simply by providing a workflow name and description.},
  doi = {10.1109/TCBB.2008.98},
  issn = {1545-5963},
  keywords = {BioExtract Server;Web browsers;Web-based data integration application;bioinformatics;community
	databases;data consolidattion;data sources;heterogeneous distributed
	biomolecular data analysis;integrated workflow-enabling system;mash-up;project
	databases;public data repositories;query languages;bioinformatics;distributed
	databases;molecular biophysics;online front-ends;query languages;query
	processing;Biopolymers;Computational Biology;Data Mining;Database
	Management Systems;Databases, Factual;Information Dissemination;Internet;Software;Workflow;}
}

@INPROCEEDINGS{1045880,
  author = {Lyons, S. and Smith, D.},
  title = {Domain-specific information extraction structures},
  booktitle = {Database and Expert Systems Applications, 2002. Proceedings. 13th
	International Workshop on},
  year = {2002},
  pages = { 80 - 84},
  month = {sept.},
  abstract = { Information describing an event can frequently be found on several
	Web pages, each of which is often poorly structured and incomplete;
	the set of pages is typically repetitive, often contradictory and
	verbose. In order to deliver high quality information to a variety
	of devices in different contexts and roles, we need to provide information
	that is more succinct. Our approach to this problem is to use domain-specific
	templates to extract information selectively from the original pages
	into XML documents, which act as a canonical structure. The extracted
	information can then be transformed into a form suitable for the
	intended application. A further series of output transformations
	are then applied to format the information appropriately, e.g. as
	speech. simple text or published on the Web. We illustrate our approach
	with an application in reporting soccer matches.},
  doi = {10.1109/DEXA.2002.1045880},
  issn = {1529-4188 },
  keywords = { Web pages; XML documents; canonical structure; domain-specific templates;
	high quality information delivery; information extraction structures;
	information format; output transformations; soccer match reporting;
	Web sites; data models; document delivery; hypermedia markup languages;
	information retrieval; meta data;}
}

@INPROCEEDINGS{5952285,
  author = {Yue Ma and Huafeng Yu and Gautier, T. and Talpin, J.-P. and Besnard,
	L. and Le Guernic, P.},
  title = {System synthesis from AADL using Polychrony},
  booktitle = {Electronic System Level Synthesis Conference (ESLsyn), 2011},
  year = {2011},
  pages = {1 -6},
  month = {june},
  abstract = {The increasing system complexity and time to market constraints are
	great challenges in current electronic system design. Raising the
	level of abstraction in the design and performing fast yet efficient
	high-level analysis, validation and synthesis has been widely advocated
	and considered as a promising solution. Motivated by the same approach,
	our work on system-level synthesis is presented in this paper: use
	the high-level modeling, domain-specific, language AADL for system-level
	co-design; use the formal framework Polychrony, based on the synchronous
	language Signal, for analysis, validation and synthesis. According
	to SIGNAL's polychronous model of computation, we propose a model
	for AADL, which takes both software, hardware and allocation into
	account. This model enables an early phase timing analysis and synthesis
	via tools associated with Polychrony.},
  doi = {10.1109/ESLsyn.2011.5952285},
  keywords = {AADL language;SIGNAL synchronous language;abstraction level;electronic
	system design;formal framework;high-level analysis;high-level modeling;phase
	timing analysis;system complexity;system-level codesign;system-level
	synthesis;time to market constraint;hardware description languages;high
	level synthesis;}
}

@INPROCEEDINGS{4497406,
  author = {Madden, Samuel and Gehrke, Johannes},
  title = {Declarative, Domain-Specific Languages - Elegant Simplicity or a
	Hammer in Search of a Nail?},
  booktitle = {Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference
	on},
  year = {2008},
  pages = {7},
  month = {april},
  abstract = {Not available},
  doi = {10.1109/ICDE.2008.4497406}
}

@INPROCEEDINGS{5934817,
  author = {Mader, R. and Griessnig, G. and Leitner, A. and Kreiner, C. and Bourrouilh,
	Q. and Armengaud, E. and Steger, C. and Weiss, R.},
  title = {A Computer-Aided Approach to Preliminary Hazard Analysis for Automotive
	Embedded Systems},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {169 -178},
  month = {april},
  abstract = {Powertrain electrification of automobiles leads to a higher number
	of sensors, actuators and control functions, which in turn increases
	the complexity of automotive embedded systems. The safety-criticality
	of the system requires the application of Preliminary Hazard Analysis
	early in the development process. This is a necessary first step
	for the development of an automotive embedded system that is acceptably
	safe. Goal of this activity is the identification and classification
	of hazards and the definition of top level safety requirements that
	are the basis for designing a safety-critical embedded system that
	is able to control or mitigate the identified hazards. A computeraided
	framework to support Preliminary Hazard Analysis for automotive embedded
	systems is presented in this work. The contribution consists of (1)
	an enhancement for Preliminary Hazard Analysis to the domain-specific
	language EAST-ADL, as well as (2) the identification of properties
	that indicate the correct application of Preliminary Hazard Analysis
	using the language. These properties and an analysis model reflecting
	the results of the Preliminary Hazard Analysis are used for the automated
	detection of an erroneously applied Preliminary Hazard Analysis (property
	checker) and the automated suggestion and application of corrective
	measures (model corrector). The applicability of the approach is
	evaluated by the case study of hybrid electric vehicle development.},
  doi = {10.1109/ECBS.2011.43},
  keywords = {EAST-ADL;automobiles;automotive embedded systems;computer-aided approach;domain-specific
	language;hazard analysis;hybrid electric vehicle development;powertrain
	electrification;safety requirements;safety-critical embedded system;automotive
	engineering;electric vehicles;embedded systems;hazards;power transmission
	(mechanical);safety-critical software;}
}

@ARTICLE{5175491,
  author = {Madl, G. and Pasricha, S. and Dutt, N. and Abdelwahed, S.},
  title = {Cross-Abstraction Functional Verification and Performance Analysis
	of Chip Multiprocessor Designs},
  journal = {Industrial Informatics, IEEE Transactions on},
  year = {2009},
  volume = {5},
  pages = {241 -256},
  number = {3},
  month = {aug. },
  abstract = {This paper introduces the cross-abstraction real-time analysis (Carta)
	framework for the model-based functional verification and performance
	estimation of chip multiprocessors (CMPs) utilizing bus matrix (crossbar
	switch) interconnection networks. We argue that the inherent complexity
	in CMP designs requires the synergistic use of various models of
	computation to efficiently manage the tradeoffs between accuracy
	and complexity. Our approach builds on domain-specific modeling languages
	(DSMLs) driving an open-source tool-chain that provides a cross-abstraction
	bridge between the finite-state machine (FSM), discrete-event (DE),
	and timed automata (TA) models of computation, and utilizes multiple
	model checkers to analyze formal properties at the cycle-accurate
	and transaction-level abstractions. The cross-abstraction analysis
	exploits accuracy for functional verification, and achieves significant
	speedups for performance estimation with marginal accuracy loss.
	We demonstrate results on an industrial strength networking CMP design
	utilizing a bus matrix interconnection network. To the best of our
	knowledge, the Carta framework is the first model-based tool-chain
	that utilizes multiple abstractions and model checkers for the comprehensive
	and formal functional verification, performance estimation, and real-time
	verification of bus matrix-based CMP designs.},
  doi = {10.1109/TII.2009.2026896},
  issn = {1551-3203},
  keywords = {CMP designs;Carta framework;bus matrix interconnection networks;chip
	multiprocessor designs;cross-abstraction functional verification;cross-abstraction
	real-time analysis;crossbar switch;discrete-event model;domain-specific
	modeling languages;finite-state machine;formal functional verification;industrial
	strength networking CMP design;model checkers;model-based functional
	verification;open-source tool-chain;performance analysis;performance
	estimation;real-time verification;timed automata model;transaction-level
	abstractions;formal verification;logic design;microprocessor chips;multiprocessor
	interconnection networks;real-time systems;specification languages;transaction
	processing;}
}

@INPROCEEDINGS{4626839,
  author = {Magro, B. and Garbajosa, J. and Perez, J.},
  title = {A Software Product Line Definition for Validation Environments},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {45 -54},
  month = {sept.},
  abstract = {Functional requirements must be tested to check if the system executes
	as the end user expects. Validation environments must be able to
	test multiple kinds of applications that belong to different domains
	and technologies. Since this wide validation spectrum is very difficult
	to cope with, validation environments are usually specialized in
	domains, programming languages, technologies, etc. However, it is
	possible to identify that the validation processes for different
	systems share a set of commonalities and variability points. This
	is a perfect framework to apply the software product line approach
	to develop domain specific validation environments for testing specific
	products. In this paper we present our experience of applying software
	product lines to support the variability of validation environments.
	We illustrate our product-line experience of developing two domain-specific
	validation environments for two different case studies: digital TV
	and slots machines.},
  doi = {10.1109/SPLC.2008.35},
  keywords = {functional requirements;software product line definition;specific
	products testing;validation environments;product development;program
	testing;program verification;software reusability;}
}

@INPROCEEDINGS{809412,
  author = {Mahemoff, M.J. and Johnston, L.J.},
  title = {Handling multiple domain objects with Model-View-Controller},
  booktitle = {Technology of Object-Oriented Languages and Systems, 1999. TOOLS
	32. Proceedings},
  year = {1999},
  pages = {28 -39},
  abstract = {The Model-View-Controller (MVC) architecture style separates software
	into models representing core functionality, views which display
	the models to the user, and controllers which let the user change
	the models. Although more sophisticated architectures have since
	been developed, MVC is interesting to explore because its simplicity
	makes it more acceptable to practitioners and it is beginning to
	become well-known in industry. However, MVC is rarely studied with
	regard to systems containing more than one domain model. Several
	issues are either ambiguous or missing in the literature. The distinction
	between views and controllers, the way model states are updated in
	a multiple-model architecture, and the creation of reusable domain-specific
	components. A program was developed to investigate these issues,
	and this paper documents the corresponding design decisions. MVC
	proved helpful in creating a multiple-model system with reusable
	components, although some weaknesses remain},
  doi = {10.1109/TOOLS.1999.809412},
  keywords = {MVC;Model-View-Controller;design decisions;multiple domain objects;reusable
	components;user-interface;object-oriented programming;user interface
	management systems;}
}

@ARTICLE{5204094,
  author = {Malavolta, I. and Muccini, H. and Pelliccione, P. and Tamburri, D.A.},
  title = {Providing Architectural Languages and Tools Interoperability through
	Model Transformation Technologies},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2010},
  volume = {36},
  pages = {119 -140},
  number = {1},
  month = {jan.-feb. },
  abstract = {Many architectural languages have been proposed in the last 15 years,
	each one with the chief aim of becoming the ideal language for specifying
	software architectures. What is evident nowadays, instead, is that
	architectural languages are defined by stakeholder concerns. Capturing
	all such concerns within a single, narrowly focused notation is impossible.
	At the same time, it is also impractical to define and use a "universal"
	notation, such as UML. As a result, many domain-specific notations
	for architectural modeling have been proposed, each one focusing
	on a specific application domain, analysis type, or modeling environment.
	As a drawback, a proliferation of languages exists, each one with
	its own specific notation, tools, and domain specificity. No effective
	interoperability is possible to date. Therefore, if a software architect
	has to model a concern not supported by his own language/tool, he
	has to manually transform (and, eventually, keep aligned) the available
	architectural specification into the required language/tool. This
	paper presents DUALLy, an automated framework that allows architectural
	languages and tools interoperability. Given a number of architectural
	languages and tools, they can all interoperate thanks to automated
	model transformation techniques. DUALLy is implemented as an Eclipse
	plugin. Putting it in practice, we apply the DUALLy approach to the
	Darwin/FSP ADL and to a UML2.0 profile for software architectures.
	By making use of an industrial complex system, we transform a UML
	software architecture specification in Darwin/FSP, make some verifications
	by using LTSA, and reflect changes required by the verifications
	back to the UML specification.},
  doi = {10.1109/TSE.2009.51},
  issn = {0098-5589},
  keywords = {DUALLy approach;Darwin-FSP ADL;Eclipse plugin;UML software architecture
	specification;UML2.0 profile;architectural languages;automated model
	transformation techniques;industrial complex system;tools interoperability;Unified
	Modeling Language;open systems;software architecture;}
}

@INPROCEEDINGS{5572419,
  author = {Malek, J. and Laroussi, M. and Derycke, A. and Ben Ghezala, H.},
  title = {Model-Driven Development of Context-aware Adaptive Learning Systems},
  booktitle = {Advanced Learning Technologies (ICALT), 2010 IEEE 10th International
	Conference on},
  year = {2010},
  pages = {432 -434},
  month = {july},
  abstract = {This paper presents the results of our innovative approach for the
	realization of a model driven development framework for modeling
	context-aware adaptive learning activities within Context-aware and
	adaptive learning environments. Its core element consists of a domain
	specific visual modeling language called CAAML (Context-aware Adaptive
	Activities Modeling Language). After, we present the developed authoring
	tool based on CAAML language and that aims to support pedagogical
	designers to model context-aware adaptive learning activities and
	transform them into executable models represented in IMS-LD.},
  doi = {10.1109/ICALT.2010.125},
  keywords = {CAAML language;context aware adaptive activity modeling;context aware
	adaptive learning system;domain specific visual modeling language;model
	driven development;adaptive systems;computer aided instruction;simulation
	languages;software engineering;ubiquitous computing;visual languages;}
}

@INPROCEEDINGS{919085,
  author = {Maletic, J.I. and Marcus, A.},
  title = {Supporting program comprehension using semantic and structural information},
  booktitle = {Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International
	Conference on},
  year = {2001},
  pages = { 103 - 112},
  month = {may},
  abstract = { Focuses on investigating the combined use of semantic and structural
	information of programs to support the comprehension tasks involved
	in the maintenance and reengineering of software systems. "Semantic
	information" refers to the domain-specific issues (both the problem
	and the development domains) of a software system. The other dimension,
	structural information, refers to issues such as the actual syntactic
	structure of the program, along with the control and data flow that
	it represents. An advanced information retrieval method, latent semantic
	indexing, is used to define a semantic similarity measure between
	software components. Components within a software system are then
	clustered together using this similarity measure. Simple structural
	information (i.e. the file organization) of the software system is
	then used to assess the semantic cohesion of the clusters and files
	with respect to each other. The measures are formally defined for
	general application. A set of experiments is presented which demonstrates
	how these measures can assist in the understanding of a nontrivial
	software system, namely a version of NCSA Mosaic.},
  doi = {10.1109/ICSE.2001.919085},
  issn = {0270-5257 },
  keywords = { NCSA Mosaic; control flow; data flow; domain-specific issues; file
	organization; information retrieval method; latent semantic indexing;
	problem domain; program comprehension; semantic cohesion; semantic
	information; semantic similarity measure; software component clustering;
	software development domain; software systems maintenance; software
	systems reengineering; structural information; syntactic program
	structure; data flow analysis; indexing; information retrieval; online
	front-ends; program control structures; reverse engineering; software
	maintenance; software metrics; systems re-engineering;}
}

@INPROCEEDINGS{5775166,
  author = {Mallet, Frederic},
  title = {UMES2: Time modelling with MARTE},
  booktitle = {Specification Design Languages, 2010. IC 2010. Forum on},
  year = {2010},
  pages = {1},
  month = {sept.},
  abstract = {The UML profile for MARTE has been adopted recently by the OMG. Its
	time model proposes several extensions to the UML Simple Time Model
	and comes with a companion language, called CCSL (Clock Constraint
	Specification Language), dedicated to the specification of causal
	and chronological constraints. CCSL elaborates on the work around
	synchronous and polychronous languages and advocates for the use
	of multiform logical time to a broad family of constraints common
	in reactive, real-time and embedded systems. This tutorial introduces
	the MARTE Time Model and CCSL and shows how CCSL can be used to build
	libraries of constraints dedicated to specific analysis domains.
	CCSL models are then used as an explicit timed causality model for
	executing purely syntactic UML or domain-specific models. As an example,
	we build CCSL libraries for East-ADL, AADL, and SDF.},
  doi = {10.1049/ic.2010.0171}
}

@INPROCEEDINGS{5232028,
  author = {Mallet, F. and Andre, C.},
  title = {On the Semantics of UML/MARTE Clock Constraints},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing,
	2009. ISORC '09. IEEE International Symposium on},
  year = {2009},
  pages = {305 -312},
  month = {march},
  abstract = {The UML goal of being a general-purpose modeling language discards
	the possibility to adopt too precise and strict a semantics. Users
	are to refine or define the semantics in their domain specific profiles.
	In the UML profile for MARTE, we devised a broadly expressive time
	model to provide a generic timed interpretation for UML models. Our
	clock constraint specification language supports the specification
	of systems with multiple clock domains. Starting with a priori independent
	clocks, we progressively constrain them to get a family of possible
	executions. Our language supports both synchronous and asynchronous
	constraints, just like the synchronous language Signal, but also
	allows explicit non determinism. In this paper, we give a formal
	semantics to a core subset of MARTE clock constraint language and
	we give an equivalent interpretation of this kernel in two other
	very different formal languages, Signal and time Petri nets.},
  doi = {10.1109/ISORC.2009.27},
  issn = {1555-0885},
  keywords = {MARTE clock constraints;Signal synchronous language;UML;clock constraint
	specification language;domain specific profiles;general-purpose modeling
	language;time Petri nets;time model;Petri nets;Unified Modeling Language;}
}

@INPROCEEDINGS{5305986,
  author = {Mangala, N. and Singh, M. and Maan, A. and Chintalapati, J. and Chattopadhyay,
	S.},
  title = {Seamless Grid Service Generator for Applications on a Service Oriented
	Grid},
  booktitle = {Services - II, 2009. SERVICES-2 '09. World Conference on},
  year = {2009},
  pages = {49 -54},
  month = {sept.},
  abstract = {Providing services on grid is becoming important these days, as there
	are several inter disciplinary collaborative projects which need
	an amalgam of services from diverse domains. However domain specific
	scientific programmers are still using traditional programming languages
	like Fortran and C to develop their applications and are quite unfamiliar
	with technologies such as Java and Web Services. The automatic grid
	service generator is a tool that helps a programmer to build grid
	services with ease. This paper brings out the complexities faced
	by programmers to write grid services on a heterogeneous grid and
	the design and implementation of the automatic grid service generator
	tool.},
  doi = {10.1109/SERVICES-2.2009.22},
  keywords = {automatic grid service generator;heterogeneous grid;seamless grid
	service generator;service oriented grid;grid computing;software architecture;}
}

@INPROCEEDINGS{5708492,
  author = {Manjunatha, A. and Ranabahu, A. and Sheth, A. and Thirunarayan, K.},
  title = {Power of Clouds in Your Pocket: An Efficient Approach for Cloud Mobile
	Hybrid Application Development},
  booktitle = {Cloud Computing Technology and Science (CloudCom), 2010 IEEE Second
	International Conference on},
  year = {2010},
  pages = {496 -503},
  month = {30 2010-dec. 3},
  abstract = {The advancements in computing have resulted in a boom of cheap, ubiquitous,
	connected mobile devices as well as seemingly unlimited, utility
	style, pay as you go computing resources, commonly referred to as
	Cloud computing. However, taking full advantage of this mobile and
	cloud computing landscape, especially for the data intensive domains
	has been hampered by the many heterogeneities that exist in the mobile
	space as well as the Cloud space. Our research focuses on exploiting
	the capabilities of the mobile and cloud landscape by defining a
	new class of applications called cloud mobile hybrid (CMH) applications
	and a Domain Specific Language (DSL) based methodology to develop
	these applications. We define Cloud-mobile hybrid as a collective
	application that has a Cloud based back-end and a mobile device front-end.
	Using a single DSL script, our toolkit is capable of generating a
	variety of CMH applications. These applications are composed of multiple
	combinations of native Cloud and mobile applications. Our approach
	not only reduces the learning curve but also shields developers from
	the complexities of the target platforms. We provide a detailed description
	of our language and present the results obtained using our prototype
	generator implementation. We also present a list of extensions that
	will enhance the various aspects of this platform.},
  doi = {10.1109/CloudCom.2010.78},
  keywords = {cloud based back-end;cloud computing;cloud mobile hybrid application
	development;cloud space;domain specific language based methodology;mobile
	device front-end;mobile space;ubiquitous connected mobile devices;cloud
	computing;mobile computing;}
}

@INPROCEEDINGS{989583,
  author = {Maragoudakis, M. and Kermanidis, K. and Fakotakis, N. and Kokkinakis,
	G.},
  title = {Learning automatic acquisition of subcategorization frames using
	Bayesian inference and support vector machines},
  booktitle = {Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference
	on},
  year = {2001},
  pages = {623 -625},
  abstract = {Learning Bayesian belief networks (BBN) from corpora and support vector
	machines (SVM) have been applied to the automatic acquisition of
	verb subcategorization frames for Modern Greek. We are incorporating
	minimal linguistic resources, i.e. basic morphological tagging and
	phrase chunking, to demonstrate that verb subcategorization, which
	is of great significance for developing robust natural language human
	computer interaction systems, could be achieved using large corpora,
	without having any general-purpose, syntactic parser at all. In addition,
	apart from BBN and SVM, which have not previously used for this task,
	we have experimented with three well-known machine learning methods
	(feedforward backpropagation neural networks, learning vector quantization
	and decision tables), which are also being applied to the task of
	verb subcategorization frame defection for the first time. We argue
	that both BBN and SVM are well suited for learning to identify verb
	subcategorization frames. Empirical results will support this claim.
	Performance has been methodically evaluated using two different corpora
	types, one balanced and one domain-specific in order to determine
	the unbiased behaviour of the trained models. Limited training data
	are proved to endow with satisfactory results. We have been able
	to achieve precision exceeding 80% on the identification of subcategorization
	frames which were not known beforehand},
  doi = {10.1109/ICDM.2001.989583},
  keywords = {Bayesian belief network learning;Bayesian inference;Modern Greek;automatic
	verb subcategorization frame acquisition;corpora;decision tables;feedforward
	backpropagation neural networks;learning vector quantization;machine
	learning methods;minimal linguistic resources;morphological tagging;natural
	language human computer interaction systems;performance evaluation;phrase
	chunking;support vector machines;backpropagation;belief networks;computational
	linguistics;feedforward neural nets;inference mechanisms;learning
	automata;vector quantisation;}
}

@INPROCEEDINGS{938539,
  author = {Marathe, A.P.},
  title = {Tracing lineage of array data},
  booktitle = {Scientific and Statistical Database Management, 2001. SSDBM 2001.
	Proceedings. Thirteenth International Conference on},
  year = {2001},
  pages = {69 -78},
  abstract = {Arrays are a common and important class of data. They can model digital
	images, digital video, scientific and experimentation data, matrices,
	finite element grids, and many other types of data. Although array
	manipulations are diverse and domain-specific, they often exhibit
	structural regularities. The paper presents an algorithm called SUN-pushdown
	to compute data lineage in such array computations. The array manipulations
	are expressed in the Array Manipulation Language (AML) that was introduced
	previously (A.P. Marathe and K. Salem, 1997). SUB-pushdown has several
	useful features. First, the lineage computation is expressed as an
	AML query. Second, it is not necessary to evaluate the AML lineage
	query to compute the array data lineage. Third, SUB-pushdown never
	gives false-negative answers. SUB-pushdown has been implemented as
	part of the ArrayDB prototype array database system that we built
	(A.P. Marathe, 2001)},
  doi = {10.1109/SSDM.2001.938539},
  keywords = {AML lineage query;AML query;Array Manipulation Language;ArrayDB prototype
	array database system;SUN-pushdown;array computations;array data
	lineage tracing;array manipulations;digital images;digital video;experimentation
	data;finite element grids;lineage computation;matrices;structural
	regularities;array signal processing;data analysis;data models;database
	languages;query processing;}
}

@INPROCEEDINGS{5070830,
  author = {Marcondes, F.S. and Fernandes, D.D. and Motini, D.A. and Tasinaffo,
	P.M. and Vega, I.S. and Dias, L.},
  title = {Systematic and Formal Approach to get a Domain Specific Language},
  booktitle = {Information Technology: New Generations, 2009. ITNG '09. Sixth International
	Conference on},
  year = {2009},
  pages = {1447 -1450},
  month = {april},
  abstract = {This paper presents a systematic approach applied over state machine
	(since it is a wide know model and easy to be used to formal specification)
	to improve the domain analysis procedure, besides been out of scope
	of this paper, this approach can also helps to improve the enterprise's
	business process as well. The motivation which leads to the this
	paper is how to got a domain specific language (DSL) that is completely
	correspondent to a domain analysis sharing both the same business
	rules. This is a very important property to be achieved since a DSL
	must be used to help the codding procedure in a specific domain,
	so, it must be a direct relation over them and this relation is explored
	in this paper. It has a briefly discussion over the need for formalization
	procedures concluding that too much formalization can be a problem
	and lack of it can also be, so, formal transformations can be performed
	at mark point (as baselines or any other mark that can be defined)
	bringing important contributions to the rigor of the model improving
	it.},
  doi = {10.1109/ITNG.2009.265},
  keywords = {domain analysis;domain specific language;formal approach;formal specification;formal
	transformations;state machine;systematic approach;finite state machines;formal
	specification;}
}

@INPROCEEDINGS{1225364,
  author = {Martincic-Ipsic, S. and Ipsic, I.},
  title = {VEPRAD: a Croatian speech database of weather forecasts},
  booktitle = {Information Technology Interfaces, 2003. ITI 2003. Proceedings of
	the 25th International Conference on},
  year = {2003},
  pages = { 321 - 326},
  month = {june},
  abstract = { We present some results of the project in Croatian speech data collection
	and speech recognition of Croatian weather forecasts. We describe
	the procedures we have performed in order to obtain a domain specific
	speech database from broadcast news of national programmes. The speech
	signal acquisition, transcription and segmentation process is described.
	We present the database structure and give some database statistics.
	Preliminary results of Croatian speech recognition experiments using
	context-independent and context-dependent acoustic models are presented.},
  doi = {10.1109/ITI.2003.1225364},
  issn = {1330-1012 },
  keywords = { Croatian speech recognition; Croatian weather forecast; context-dependent
	acoustic model; context-independent model; speech annotation; speech
	database design; speech segmentation process; speech signal acquisition;
	speech transcription; data acquisition; geophysics computing; multimedia
	databases; speech recognition; weather forecasting;}
}

@INPROCEEDINGS{1248168,
  author = {Martincic-Ipsic, S. and Zibert, J. and Ipsic, I. and Mihelic, F.},
  title = {A bilingual spoken dialog system for Slovenian and Croatian weather
	forecast},
  booktitle = {EUROCON 2003. Computer as a Tool. The IEEE Region 8},
  year = {2003},
  volume = {2},
  pages = { 140 - 143 vol.2},
  month = {sept.},
  abstract = { In the paper we present a design strategy, current activities and
	some results of a joint project in development of a spoken dialog
	system for Slovenian and Croatian weather forecasts. We give a brief
	description of system design, procedures we have performed in order
	to obtain domain specific speech databases and monolingual and bilingual
	speech recognition experiments. Recognition results for Croatian
	and Slovenian speech are presented, as well as bilingual speech recognition
	results when using common acoustic models. We propose two different
	approaches to the language identification problem and show recognition
	results for the two acoustically similar languages like Slovenian
	and Croatian. We describe the software tools we have used for speech
	database design as well as tools for acoustic and language modelling.},
  doi = {10.1109/EURCON.2003.1248168},
  issn = { },
  keywords = { Croatian; Slovenian; acoustic modelling; acoustic models; acoustically
	similar languages; bilingual speech recognition; bilingual spoken
	dialog system; design strategy; domain specific speech databases;
	language identification problem; language modelling; monolingual
	speech recognition; software tools; speech analysis tools; speech
	database design; system design; weather forecast; natural languages;
	speech processing; speech recognition; weather forecasting;}
}

@INPROCEEDINGS{1611580,
  author = {Martinez-Ortiz, I. and Moreno-Ger, P. and Sierra, J.L. and Fernandez-Manjon,
	B.},
  title = {Production and Maintenance of Content-Intensive Videogames: A Document-Oriented
	Approach},
  booktitle = {Information Technology: New Generations, 2006. ITNG 2006. Third International
	Conference on},
  year = {2006},
  pages = {118 -123},
  month = {april},
  abstract = { lt;e-Game gt; is a tool for the rapid development of adventure videogames
	with an educational purpose. It provides a markup language (the lt;e-Game
	gt; language) for structuring documents containing storyboards, and
	a processor (the lt;e-Game gt; engine) for executing games from these
	marked documents. This paper describes how lt;e-Game gt; facilitates
	new development models for the production and maintenance of content-intensive
	applications with domain-specific markup languages by applying our
	ADDS approach (approach for document-oriented development of software)},
  doi = {10.1109/ITNG.2006.110},
  keywords = {content-intensive videogames maintenance;content-intensive videogames
	production;document-oriented software development;domain-specific
	markup languages;e-game;educational videogames;computer games;educational
	aids;hypermedia markup languages;software engineering;}
}

@INPROCEEDINGS{4561722,
  author = {Martinez-Ortiz, I. and Sierra, J.L. and Fernandez-Manjon, B.},
  title = {Enhancing Reusability of IMS LD Units of Learning: The e-LD Approach},
  booktitle = {Advanced Learning Technologies, 2008. ICALT '08. Eighth IEEE International
	Conference on},
  year = {2008},
  pages = {402 -404},
  month = {july},
  abstract = {In this paper we describe the e-LD approach for the design and repurposing
	of units of learning (UoLs). This approach is centered in domain-specific
	educational modeling languages (EMLs) built with the close collaboration
	between instructors and developers. The products of this collaboration
	are: (i) the definition of a suitable authoring EML, and (ii) the
	construction of software tools to allow the importation and authoring
	of UoLs. The domain-specific authoring EMLs and tools simplify the
	production and repurposing of UoLs.},
  doi = {10.1109/ICALT.2008.175},
  keywords = {domain-specific authoring;domain-specific educational modeling languages;repurposing;software
	tools;units of learning;authoring systems;computer aided instruction;software
	tools;teaching;}
}

@ARTICLE{5659639,
  author = {Martinho, R. and Varajo, J. and Domingos, D.},
  title = {Using the semantic web to define a language for modelling controlled
	flexibility in software processes},
  journal = {Software, IET},
  year = {2010},
  volume = {4},
  pages = {396 -406},
  number = {6},
  month = {december },
  abstract = {Software processes and corresponding models are dynamic entities that
	must evolve to cope with changes occurred in the enacting process,
	the software development organisation, the market and the methodologies
	used to produce software. However, in the everyday practice, software
	team members do not want total flexibility. They rather prefer to
	learn about and follow previously defined controlled flexibility,
	that is, advices on which, where, how and by whom process models
	and related instances can change/adapt. Process engineers can express
	these advices within a process model with a domain-specific language
	(DSL), which complements the core process modelling language with
	additional controlled flexibility information. Then, software team
	members can browse and learn on this information in process models
	and instances, and be guided when performing changes. In this study,
	the authors propose the use of the semantic web and associated ontology-based
	technologies to develop and evolve their controlled flexibility DSL
	for software processes. They use an ontology-based format to define
	the controlled flexibility-related concepts, descriptions and axioms
	that specify the formal semantics of their DSL. In addition, the
	authors provide concrete mappings between these ontology concepts
	and a unified modelling language class-based DSL metamodel and describe
	how it supports changes made in the ontology.},
  doi = {10.1049/iet-sen.2010.0045},
  issn = {1751-8806},
  keywords = {associated ontology-based technologies;controlled flexibility modelling;core
	process modelling language;domain-specific language;formal semantics;process
	model;semantic Web;software development organisation;software processes;software
	team members;unified modelling language class-based DSL metamodel;ontologies
	(artificial intelligence);semantic Web;simulation languages;software
	engineering;}
}

@ARTICLE{5076459,
  author = {Mathe, J.L. and Ledeczi, A. and Nadas, A. and Sztipanovits, J. and
	Martin, J.B. and Weavind, L.M. and Miller, A. and Miller, P. and
	Maron, D.J.},
  title = {A Model-Integrated, Guideline-Driven, Clinical Decision-Support System},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {54 -61},
  number = {4},
  month = {july-aug. },
  abstract = {Using evidence-based guidelines to standardize the care of patients
	with complex medical problems is a difficult challenge. In acute
	care settings, such as intensive care units, the inherent problems
	of stabilizing and improving vital patient parameters is complicated
	by the division of responsibilities among different healthcare team
	members. Computerized support for implementing such guidelines has
	tremendous potential. The use of model-integrated techniques for
	specifying and implementing guidelines as coordinated asynchronous
	processes is a promising new methodology for providing advanced clinical
	decision support. Combined with visual dashboards, which show the
	status of the implemented guidelines, a new approach to computer-supported
	care is possible. The Vanderbilt Medical Center is applying these
	techniques to the management of sepsis.},
  doi = {10.1109/MS.2009.84},
  issn = {0740-7459},
  keywords = {asynchronous process;computer supported care;evidence-based guideline;guideline
	driven clinical decision support system;healthcare team member;model
	integrated system;patient care;visual dashboard;decision support
	systems;health care;medical information systems;patient care;}
}

@INPROCEEDINGS{5767022,
  author = {Mathew, G. and Obradovic, Z.},
  title = {Vocabularies in collaboration channels},
  booktitle = {Collaborative Computing: Networking, Applications and Worksharing
	(CollaborateCom), 2010 6th International Conference on},
  year = {2010},
  pages = {1 -5},
  month = {oct.},
  abstract = {Collaborators use vocabulary germane to the domain in context. Collaboration
	applications and collaborating systems use vocabularies at different
	(lower) layers that are specific to the state in which they execute.
	Distributed, yet collaborative domain-specific applications have
	demonstrated success when lower layer vocabularies are well defined.
	These standard vocabularies enable platform neutral, programming
	language neutral and client neutral mechanisms to realize successful
	handshake between collaboration applications. The concept can be
	extended to an application neutral, protocol neutral, platform neutral,
	programming language neutral and client neutral vocabulary model
	that will facilitate harmonious handshake of collaboration channels.
	This paper addresses the need for standardizing vocabulary at the
	collaboration channel level and presents a model for realizing vocabulary-awareness
	in a generalized neutral format. A pilot study done to implement
	the model using a sample vocabulary is presented.},
  keywords = {client neutral mechanisms;collaboration channels;collaborative domain
	specific applications;handshake;platform neutral mechanisms;programming
	language neutral mechanisms;vocabulary germane;computational linguistics;groupware;vocabulary;}
}

@INPROCEEDINGS{6014007,
  author = {Matsui, A.A.M. and Aida, H.},
  title = {Advanced client-service compatibility assessment via analysis of
	references to service-side FSMs},
  booktitle = {Communication Software and Networks (ICCSN), 2011 IEEE 3rd International
	Conference on},
  year = {2011},
  pages = {69 -77},
  month = {may},
  abstract = {In a service oriented environment, service contracts play an important
	role to provide interoperation between clients and services. As contracts
	are the de facto insulation layer between clients and services, we
	argue that contracts should not only present specifications of method
	formats, but also pre and post-conditions that could provide more
	sophisticated client-service interactions. We have proposed that
	service contracts should contain specifications of service-side finite
	state machines (FSM). The immediate benefits of pre and post-conditions
	in distributed services are less defensive source codes in both sides,
	and avoidance to execute remote service with invalid parameters,
	which translates into rationalization of resources. But we argue
	that FSMs can also be used to provide client-service synchronization
	and advanced compatibility assessment, if the client source code
	is specially prepared to support these features. In other words,
	if the client source code contains constructs that are specific to
	deal with distributed services that follow this format. In this paper,
	we provide a framework for such extensions and present details about
	our implementation.},
  doi = {10.1109/ICCSN.2011.6014007},
  keywords = {client source code;client-service compatibility assessment;client-service
	synchronization;de facto insulation layer;distributed services;finite
	state machines;service contracts;service oriented architecture;Web
	services;client-server systems;finite state machines;service-oriented
	architecture;source coding;}
}

@INPROCEEDINGS{674160,
  author = {Matthews, J. and Cook, B. and Launchbury, J.},
  title = {Microprocessor specification in Hawk},
  booktitle = {Computer Languages, 1998. Proceedings. 1998 International Conference
	on},
  year = {1998},
  pages = {90 -101},
  month = {may},
  abstract = {Modern microprocessors require an immense investment of time and effort
	to create and verify, from the high level architectural design downwards.
	We are exploring ways to increase the productivity of design engineers
	by creating a domain specific language for specifying and simulating
	processor architectures. We believe that the structuring principles
	used in modern functional programming languages, such as static typing,
	parametric polymorphism, first class functions, and lazy evaluation
	provide a good formalism for such a domain specific language, and
	have made initial progress by creating a library on top of the functional
	language Haskell. We have specified the integer subset of an out
	of order, superscalar DLX microprocessor, with register renaming,
	a reorder buffer, a global reservation station, multiple execution
	units, and speculative branch execution. Two key abstractions of
	this library are the signal abstract data type (ADT), which models
	the simulation history of a wire, and the transaction ADT, which
	models the state of an entire instruction as it travels through the
	microprocessor},
  doi = {10.1109/ICCL.1998.674160},
  issn = {1074-8970},
  keywords = {Hawk language;design engineers;domain specific language;first class
	functions;functional language Haskell;functional programming languages;global
	reservation station;high level architectural design;integer subset;lazy
	evaluation;microprocessor specification;multiple execution units;out
	of order superscalar DLX microprocessor;parametric polymorphism;processor
	architecture simulation;register renaming;reorder buffer;signal abstract
	data type;simulation history;software library;speculative branch
	execution;static typing,;structuring principles;transaction ADT;abstract
	data types;formal specification;functional languages;functional programming;hardware
	description languages;microprocessor chips;software libraries;}
}

@INPROCEEDINGS{994488,
  author = {Mauw, S. and Wiersma, W.T. and Willemse, T.A.C.},
  title = {Language-driven system design},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = { 3637 - 3646},
  month = {jan.},
  abstract = { Studies have shown significant benefits of the use of domain-specific
	languages. However, designing a DSL still seems to be an art, rather
	than a craft, following a clear methodology. In this paper we discuss
	a first step towards a methodology for designing such languages.
	The presented approach, which is referred to as the language-driven
	approach, is rooted informal techniques and independent of accepted
	software engineering process models. We illustrate the approach with
	a small and instructive case study.},
  doi = {10.1109/HICSS.2002.994488},
  keywords = { domain specific languages; informal techniques; language driven system
	design; high level languages; software engineering;}
}

@ARTICLE{4620092,
  author = {Maximilien, E.M. and Ranabahu, A. and Gomadam, K.},
  title = {An Online Platform for Web APIs and Service Mashups},
  journal = {Internet Computing, IEEE},
  year = {2008},
  volume = {12},
  pages = {32 -43},
  number = {5},
  month = {sept.-oct. },
  abstract = {On the newly programmable Web, mashups are flourishing. Designers
	create mashups by combining components of existing Web sites and
	applications. Although rapid mashup proliferation offers many opportunities,
	a lack of standarization and compatibility offers considerable challenges.
	IBM Sharable Code is an online service platform for developing and
	sharing situational Web 2.0 applications and mashups. The platform
	is based on an innovative domain-specific language that streamlines
	and standardizes the development and deployment of applications consuming
	and exposing Web APIs. Parts of the DSL and the resulting applications
	and mashups can be shared and reused by members of the IBM Sharable
	Code community. In this article, the authors offer an overview of
	the platform's architecture and the DSL language at its core.},
  doi = {10.1109/MIC.2008.92},
  issn = {1089-7801},
  keywords = {IBM Sharable Code;Web 2.0 application;Web API;Web application;Web
	sites;online platform;online service platform;programmable Web;service
	mashups;Web services;Web sites;application program interfaces;}
}

@INPROCEEDINGS{4578572,
  author = {Mayer, P. and Schroeder, A. and Koch, N.},
  title = {A Model-Driven Approach to Service Orchestration},
  booktitle = {Services Computing, 2008. SCC '08. IEEE International Conference
	on},
  year = {2008},
  volume = {2},
  pages = {533 -536},
  month = {july},
  abstract = {Software systems based on Service-Oriented Architectures (SOAs) promise
	high flexibility, improved maintainability, and simple re-use of
	functionality. A variety of languages and standards have emerged
	for working with SOA artifacts; however, service computing still
	lacks an effective and intuitive model-driven approach starting from
	models written in an established modeling language like UML and,
	in the end, generating comprehensive executable code. In this paper,
	we present a conservative extension to the UML2 for modeling service
	orchestrations at a high level of abstraction, and a fully automatic
	approach for transforming these orchestrations down to the well known
	Web Service standard BPEL.},
  doi = {10.1109/SCC.2008.91},
  keywords = {BPEL;UML;model-driven approach;modeling language;service computing;service
	orchestration;service-oriented architectures;software systems;Unified
	Modeling Language;formal specification;software architecture;}
}

@INPROCEEDINGS{4634771,
  author = {Mayer, P. and Schroeder, A. and Koch, N.},
  title = {MDD4SOA: Model-Driven Service Orchestration},
  booktitle = {Enterprise Distributed Object Computing Conference, 2008. EDOC '08.
	12th International IEEE},
  year = {2008},
  pages = {203 -212},
  month = {sept.},
  abstract = {Service-Oriented Architectures (SOAs) have become an important cornerstone
	of the development of enterprise-scale software applications. Although
	a range of domain-specific languages and standards are available
	for dealing with such architectures, model-driven approaches starting
	from models written in an established modelling language like UML
	and including the ability for model transformation (in particular,
	for code generation) are still in their infancy. In this paper, we
	show (1) how our UML-based domain-specific language for working with
	SOA artefacts, UML4SOA, can be used for modelling service orchestrations,
	and (2) how to exploit so-designed models in the MDD4SOA approach
	to generate code in multiple languages, among them BPEL and WSDL,
	Java, and the formal language Jolie. We use a case study for illustrating
	this approach. Our main contributions are an easy-to-use, conservative
	extension to the UML2 for modelling service orchestrations on a high
	level of abstraction, and a fully automated, model-driven approach
	for transforming these orchestrations down to code.},
  doi = {10.1109/EDOC.2008.55},
  issn = {1541-7719},
  keywords = {Java;UML-based domain-specific language;domain-specific languages;enterprise-scale
	software applications;formal language;model-driven service orchestration;modelling
	service orchestrations;service-oriented architectures;Java;Unified
	Modeling Language;Web services;formal languages;software architecture;}
}

@INPROCEEDINGS{4639094,
  author = {Mazanek, S. and Maier, S. and Minas, M.},
  title = {Auto-completion for diagram editors based on graph grammars},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {242 -245},
  month = {sept.},
  abstract = {Graphs are known to be well-suited as an intermediate data structure
	in diagram editors. The syntax of a particular visual language can
	be defined by means of a graph grammar. In recent work we have proposed
	approaches to graph completion: given a possibly ldquoincompleterdquo
	graph, this graph is modified in such a way that the resulting graph
	is a member of the grammarpsilas language. In this paper we describe
	how graph completion can be used to realize diagram completion, an
	important requirement for the realization of content assist in diagram
	editors. With our approach, the advantages of free-hand and structured
	editing can be effectively combined: drawing of diagrams with maximal
	freedom and powerful guidance whenever needed.},
  doi = {10.1109/VLHCC.2008.4639094},
  issn = {1943-6092},
  keywords = {auto-completion;data structure;diagram completion;diagram editors;graph
	completion;graph grammars;incomplete graph;visual language syntax;graph
	grammars;visual languages;}
}

@INPROCEEDINGS{5090837,
  author = {Mazzini, S. and Puri, S. and Vardanega, T.},
  title = {An MDE methodology for the development of high-integrity real-time
	systems},
  booktitle = {Design, Automation Test in Europe Conference Exhibition, 2009. DATE
	'09.},
  year = {2009},
  pages = {1154 -1159},
  month = {april},
  abstract = {This paper reports on experience gained and lessons learned from an
	intensive investigation of model-driven engineering methodology and
	technology for application to high-integrity systems. Favourable
	experimental context was provided for by ASSERT, a 40-month project
	partly funded by the EC as part of the 6th Framework Program. The
	goodness of fit of the MDE paradigm for the industrial domain of
	interest was critically assessed on a small number of candidate solutions.
	One of the main axes of investigation concerned HRT-UML/RCM, an advanced
	method and integrated tool for the model-driven development of embedded
	real-time software systems. HRT-UML/RCM vastly leveraged on version
	2 of the OMG UML standard and combined it with the development of
	a domain-specific metamodel in the quest to attain correctness-by-construction
	from the ground up. The prototype tool developed in the project supported:
	(1) the separation of functional (sequential) design from the specification
	of real-time and concurrency requirements and properties to be preserved
	at run time; and (2) the exploitation of a fully generative approach
	to the development, equipped with support for model-based feasibility
	analysis and round-trip engineering.},
  issn = {1530-1591},
  keywords = {embedded real-time software systems;functional design;high-integrity
	systems;model-based feasibility analysis;model-driven engineering
	methodology;round-trip engineering;sequential design;computer architecture;electronic
	engineering computing;embedded systems;real-time systems;sequential
	circuits;}
}

@INPROCEEDINGS{730572,
  author = {McDermid, J. and Galloway, A. and Burton, S. and Clark, J. and Toyn,
	I. and Tracey, N. and Valentine, S.},
  title = {Towards industrially applicable formal methods: three small steps,
	and one giant leap},
  booktitle = {Formal Engineering Methods, 1998. Proceedings. Second International
	Conference on},
  year = {1998},
  pages = {76 -88},
  month = {dec},
  abstract = {We discuss issues in the development of formal methods for use in
	aerospace applications, reflecting our experience in working with
	both Rolls-Royce and British Aerospace. We discuss some of the key
	factors which we believe govern the application of discrete mathematics
	to aerospace applications, drawing comparisons with applied engineering
	mathematics in other domains. We give an overview of three projects
	(the three ldquo;small steps rdquo;): the development of a domain-specific
	language for aircraft engine control system specification; the development
	of a formal semantics and tool support for state transition systems
	to facilitate analysis of specifications produced by systems engineers;
	the use of formalism in support of test automation. We then discuss
	the ldquo;gap rdquo; we see between the needs of industry and the
	current focus of the formal methods research community by pointing
	out important facets of industrial applicable formal methods which
	are not receiving adequate attention. We refer to this as a ldquo;giant
	leap rdquo; due to the need for a cultural shift in the research
	community and the need for a coherent approach to the identified
	research issues rather than piecemeal studies of the issues. Our
	conclusions are to be optimistic for the future use of formal methods
	in industry albeit with concern that their potential will not be
	realised unless there is a shift in emphasis within the research
	community?},
  doi = {10.1109/ICFEM.1998.730572},
  keywords = {British Aerospace;Rolls-Royce;aerospace applications;aircraft engine
	control system;discrete mathematics;domain-specific language;formal
	semantics;formal specification;industrially applicable formal methods;state
	transition systems;test automation;aerospace computing;aerospace
	industry;formal specification;program testing;specification languages;}
}

@INPROCEEDINGS{4228373,
  author = {McFarlin, D. and Chauhan, A.},
  title = {Library Function Selection in Compiling Octave},
  booktitle = {Parallel and Distributed Processing Symposium, 2007. IPDPS 2007.
	IEEE International},
  year = {2007},
  pages = {1 -8},
  month = {march},
  abstract = {One way to address the continuing performance problem of high-level
	domain-specific languages, such as Octave or Matlab, is to compile
	them to a relatively lower level language for which good compilers
	are available. As a first step in this direction, specializing the
	high-level operations in the source, based on operand types, leads
	to significant gains. However, simple translation of the high-level
	operations to the underlying libraries can often miss important opportunities
	to improve performance. This paper presents a global algorithm to
	select functions from a target library, utilizing the semantics of
	the operations as well as the platform-specific performance characteristics
	of the library. Making use of the library properties, the simple
	and easy-to-implement selection algorithm, is able to achieve as
	much as three times performance improvement for certain linear algebra
	kernels, over a straight mapping of operations, which are compiled
	to the vendor-tuned BLAS.},
  doi = {10.1109/IPDPS.2007.370645},
  keywords = {Matlab;Octave;high-level domain-specific language;library function
	selection;linear algebra kernel;optimizing compiler;vendor-tuned
	BLAS;authoring languages;linear algebra;mathematics computing;optimising
	compilers;}
}

@INPROCEEDINGS{989831,
  author = {McLaren, L. and Wicks, T.},
  title = {Developing generative frameworks using XML},
  booktitle = {Automated Software Engineering, 2001. (ASE 2001). Proceedings. 16th
	Annual International Conference on},
  year = {2001},
  pages = { 368 - 372},
  month = {nov.},
  abstract = { Generative programming methods provide some significant advantages
	for the repeated deployment of product line architectures. The paper
	considers XML as a tool for building and describing applications
	that use generative programming methods. It describes techniques
	for the creation of a generative framework, presents a case study
	and discusses the results of practical application of these methods
	in a real world, enterprise scale, product line architecture. The
	paper presents the advantages of using an XML descriptor that can
	be easily transformed to generate both static and dynamically configurable
	software components for direct deployment in an application framework.
	Two implementation approaches are considered: an indirect approach
	using XSL for the transformations; and a direct approach where the
	XML descriptor is parsed and dealt with programmatically. The relative
	advantages of these two approaches are discussed. The paper provides
	practical examples and presents lessons learned from the application
	of the techniques.},
  doi = {10.1109/ASE.2001.989831},
  issn = {1527-1366},
  keywords = { XML descriptor; XSL; application framework; case study; direct deployment;
	dynamically configurable software components; generative framework
	development; generative programming methods; implementation approaches;
	indirect approach; practical application; product line architectures;
	real world enterprise scale product line architecture; application
	generators; formal specification; hypermedia markup languages; program
	compilers;}
}

@INPROCEEDINGS{6005378,
  author = {Meier, P. and Kounev, S. and Koziolek, H.},
  title = {Automated Transformation of Component-Based Software Architecture
	Models to Queueing Petri Nets},
  booktitle = {Modeling, Analysis Simulation of Computer and Telecommunication Systems
	(MASCOTS), 2011 IEEE 19th International Symposium on},
  year = {2011},
  pages = {339 -348},
  month = {july},
  abstract = {Performance predictions early in the software development process
	can help to detect problems before resources have been spent on implementation.
	The Palladio Component Model (PCM) is an example of a mature domain-specific
	modeling language for component-based systems enabling performance
	predictions at design time. PCM provides several alternative model
	solution methods based on analytical and simulation techniques. However,
	existing solution methods suffer from scalability issues and provide
	limited flexibility in trading-off between results accuracy and analysis
	overhead. Queueing Petri Nets (QPNs) are a general-purpose modeling
	formalism, at a lower level of abstraction, for which efficient and
	mature simulation-based solution techniques are available. This paper
	contributes a formal mapping from PCM to QPN models, implemented
	by means of an automated model-to-model transformation as part of
	a new PCM solution method based on simulation of QPNs. The limitations
	of the mapping and the accuracy and overhead of the new solution
	method compared to existing methods are evaluated in detail in the
	context of five case studies of different size and complexity. The
	new solution method proved to provide good accuracy with solution
	overhead up to 20 times lower compared to PCM's reference solver.},
  doi = {10.1109/MASCOTS.2011.23},
  issn = {1526-7539},
  keywords = {Palladio component model;automated model-to-model transformation;component-based
	software architecture model;domain-specific modeling language;formal
	mapping;general-purpose modeling formalism;performance prediction;queueing
	Petri nets;scalability issue;software development process;Petri nets;object-oriented
	programming;queueing theory;software architecture;software performance
	evaluation;specification languages;}
}

@ARTICLE{5432136,
  author = {Melia, S and Gomez, J and Perez, S and Diaz, O},
  title = {Facing Architectural and Technological Variability of Rich Internet
	Applications},
  journal = {Internet Computing, IEEE},
  year = {2010},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {The advent of Rich Internet Applications has involved an authentic
	technological revolution providing Web users with advanced requirements
	similar to desktop applications. At the same time, RIAs have multiplied
	the possible architectural and technological alternatives complicating
	development and increasing risks. The real challenge is to select
	the right alternatives among the existing RIA variability, thus creating
	an optimal solution able to satisfy most user requirements. To face
	this challenge, for the RIA development process, we propose an extended
	OOH4RIA approach to introduce architectural and technological aspects
	at the design phase, to propagate these decisions to the rest of
	concerns and to provide a closer match between the modeled system
	and the final implementation.},
  doi = {10.1109/MIC.2010.53},
  issn = {1089-7801}
}

@INPROCEEDINGS{6042677,
  author = {Menezes, A.L. and Cirilo, C.E. and de Moraes, J.L.C. and de Souza,
	W.L. and do Prado, A.F.},
  title = {Using archetypes and Domain Specific Languages on development of
	ubiquitous applications to pervasive healthcare},
  booktitle = {Computer-Based Medical Systems (CBMS), 2010 IEEE 23rd International
	Symposium on},
  year = {2010},
  pages = {395 -400},
  month = {oct.},
  abstract = {Pervasive healthcare focuses on the use of new technologies, tools,
	and services, in order to help patients to play a more active role
	in the treatment of their diseases. Since pervasive healthcare environments
	demand a huge amount of information exchange, the use of technologies
	like Health Level Seven (HL7) and archetypes has been proposed to
	provide interoperability between applications for these environments.
	However, the complexity of such technologies difficults their full
	adoption as well as the migration from centralized healthcare environments
	into pervasive ones. Aiming at collaborating to bridge this gap,
	this paper proposes an approach to integrate archetypes into HL7
	v3 messages for the development of pervasive healthcare applications.
	The approach suggests the use of Domain Specific Languages (DSLs),
	which simplify the HL7 messages modeling and allow to automate most
	of the messages schema codification.},
  doi = {10.1109/CBMS.2010.6042677},
  issn = {1063-7125},
  keywords = {HL7 v3 messages;archetypes;disease treatment;domain specific languages;health
	level seven;information exchange;interoperability;messages schema
	codification;pervasive healthcare environments;ubiquitous applications;diseases;health
	care;medical computing;open systems;patient treatment;specification
	languages;ubiquitous computing;}
}

@ARTICLE{979980,
  author = {Meng, H.M. and Kai-Chung Siu},
  title = {Semiautomatic acquisition of semantic structures for understanding
	domain-specific natural language queries},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  year = {2002},
  volume = {14},
  pages = {172 -181},
  number = {1},
  month = {jan/feb},
  abstract = {This paper describes a methodology for semiautomatic grammar induction
	from unannotated corpora of information-seeking queries in a restricted
	domain. The grammar contains both semantic and syntactic structures,
	which are conducive to (spoken) natural language understanding. Our
	work aims to ameliorate the reliance of grammar development on expert
	handcrafting or on the availability of annotated corpora. To strive
	for reasonable coverage on real data, as well as portability across
	domains and languages, we adopt a statistical approach. Agglomerative
	clustering using the symmetrized divergence criterion groups words
	"spatially". These words have similar left and right contexts and
	tend to form semantic classes. Agglomerative clustering using mutual
	information groups words "temporally". These words tend to co-occur
	sequentially to form phrases or multiword entities. Our approach
	is amenable to the optional injection of prior knowledge to catalyze
	grammar induction. The resultant grammar is interpretable by humans
	and is amenable to hand-editing for refinement. Hence, our approach
	is semiautomatic in nature. Experiments were conducted using the
	ATIS (Air Travel Information Service) corpus and the semiautomatically-induced
	grammar GSA is compared to an entirely handcrafted grammar GH. GH
	took two months to develop and gave concept error rates of 7 percent
	and 11.3 percent, respectively, in language understanding of two
	test corpora. GSA took only three days to produce and gave concept
	errors of 14 percent and 12.2 percent on the corresponding test corpora.
	These results provide a desirable trade-off between language understanding
	performance and grammar development effort},
  doi = {10.1109/69.979980},
  issn = {1041-4347},
  keywords = {agglomerative clustering;concepts extraction;grammar induction;information-seeking
	queries;knowledge acquisition;natural language query;natural language
	understanding;semiautomatic grammar induction;grammars;knowledge
	acquisition;natural language interfaces;query processing;}
}

@INPROCEEDINGS{5637845,
  author = {Menotti, R. and Cardoso, J.M.P. and Fernandes, M.M. and Marques,
	E.},
  title = {On using LALP to map an audio encoder/decoder on FPGAs},
  booktitle = {Industrial Electronics (ISIE), 2010 IEEE International Symposium
	on},
  year = {2010},
  pages = {3063 -3068},
  month = {july},
  abstract = {This paper presents the use of LALP to implement typical industrial
	application kernels, ADPCM Encoder and Decoder, in FPGAs. LALP is
	a domain specific language and its compilation framework aims to
	the direct mapping of algorithms originally described in a high-level
	language onto FPGAs. In particular, LALP focuses on loop pipelining,
	a key technique for the design of hardware accelerators. While the
	language syntax resembles C, it contains certain constructs that
	allow programmer interventions to enforce or relax data dependences
	as needed, and so optimize the performance of the generated hardware.
	We present experimental results showing significant performance gains
	using this approach, while still keeping the language syntax and
	semantics close to popular high level software languages, a desirable
	feature when considering time to market constraints. We believe the
	performance gains observed for the ADPCM implementation can be extended
	to other industrial applications relying on algorithms spending most
	of their execution time on loop structures, such signal and image
	processing.},
  doi = {10.1109/ISIE.2010.5637845},
  keywords = {ADPCM Decoder;ADPCM Encoder;FPGA;LALP;audio encoder/decoder;compilation
	framework;data dependences;direct algorithm mapping;domain specific
	language;hardware accelerators;high-level language;industrial application
	kernels;language semantics;language syntax;loop pipelining;programmer
	interventions;audio coding;codecs;field programmable gate arrays;pipeline
	processing;program compilers;program control structures;telecommunication
	computing;}
}

@INPROCEEDINGS{5552780,
  author = {Menzel, M. and Warschofsky, R. and Meinel, C.},
  title = {A Pattern-Driven Generation of Security Policies for Service-Oriented
	Architectures},
  booktitle = {Web Services (ICWS), 2010 IEEE International Conference on},
  year = {2010},
  pages = {243 -250},
  month = {july},
  abstract = {Service-oriented Architectures support the provision, discovery, and
	usage of services in different application contexts. The Web Service
	specifications provide a technical foundation to implement this paradigm.
	Moreover, mechanisms are provided to face the new security challenges
	raised by SOA. To enable the seamless usage of services, security
	requirements can be expressed as security policies (e.g. WS-Policy
	and WS-SecurityPolicy) that enable the negotiation of these requirements
	between clients and services. However, the codification of security
	policies is a difficult and error-prone task due to the complexity
	of the Web Service specifications. In this paper, we introduce our
	model-driven approach that facilitates the transformation of architecture
	models annotated with simple security intentions to security policies.
	This transformation is driven by security configuration patterns
	that provide expert knowledge on Web Service security. Therefore,
	we will introduce a formalised pattern structure and a domain-specific
	language to specify these patterns.},
  doi = {10.1109/ICWS.2010.25},
  keywords = {Web Service security;Web service specification;domain-specific language;formalised
	pattern structure;model-driven approach;pattern-driven generation;security
	policy codification;service-oriented architecture;Web services;formal
	specification;security of data;software architecture;}
}

@INPROCEEDINGS{5298426,
  author = {Merilinna, J. and Raty, T.},
  title = {Bridging the Gap between the Quality Requirements and Implementation},
  booktitle = {Software Engineering Advances, 2009. ICSEA '09. Fourth International
	Conference on},
  year = {2009},
  pages = {3 -8},
  month = {sept.},
  abstract = {There is an increasing need for providing software products with different
	quality attributes. Especially in the context of product families
	each customer group may demand different quality attributes from
	a product while functional requirements remain the same. Although
	there are many languages for expressing quality requirements, still
	there is a gap between the requirements specification and their implementation.
	In this paper, the means for expressing quality attributes and affecting
	the qualities in software systems are scrutinized and illustrated
	in a laboratory case of stream-oriented computing system. We take
	a domain-specific modelling approach in order to express qualities
	in software models explicitly. As a result, there is a comprehensive
	link between the quality attributes in a software system and the
	quality requirements. In addition, modifying a software system according
	to the quality requirements is facilitated.},
  doi = {10.1109/ICSEA.2009.9},
  keywords = {customer group;domain-specific modelling approach;formal language;functional
	requirement;requirement specification;software product;software quality
	attribute;formal specification;product development;software quality;specification
	languages;}
}

@INPROCEEDINGS{5348441,
  author = {Mernik, M. and Hrncic, D. and Bryant, B.R. and Sprague, A.P. and
	Gray, J. and Qichao Liu and Javed, F.},
  title = {Grammar inference algorithms and applications in software engineering},
  booktitle = {Information, Communication and Automation Technologies, 2009. ICAT
	2009. XXII International Symposium on},
  year = {2009},
  pages = {1 -7},
  month = {oct.},
  abstract = {Many problems exist whose solutions take the form of patterns that
	may be expressed using grammars (e.g., speech recognition, text processing,
	genetic sequencing). Construction of these grammars is usually carried
	out by computer scientists working with domain experts. In the case
	when there is a lack of domain experts, grammar inference can be
	applied. In this paper, two grammar inference algorithms are briefly
	described and their application to software engineering is presented.},
  doi = {10.1109/ICAT.2009.5348441},
  keywords = {genetic sequencing;grammar inference algorithms;software engineering;speech
	recognition;text processing;grammars;software engineering;speech
	recognition;text analysis;word processing;}
}

@INPROCEEDINGS{927266,
  author = {Mernik, M. and Zumer, V.},
  title = {Domain-specific languages for software engineering},
  booktitle = {System Sciences, 2001. Proceedings of the 34th Annual Hawaii International
	Conference on},
  year = {2001},
  pages = { 4002},
  month = {jan.},
  abstract = {Not available},
  doi = {10.1109/HICSS.2001.927266},
  issn = { }
}

@INPROCEEDINGS{4555848,
  author = {Milder, P.A. and Franchetti, F. and Hoe, J.C. and Puschel, M.},
  title = {Formal datapath representation and manipulation for implementing
	DSP transforms},
  booktitle = {Design Automation Conference, 2008. DAC 2008. 45th ACM/IEEE},
  year = {2008},
  pages = {385 -390},
  month = {june},
  abstract = {We present a domain-specific approach to representing datapaths for
	hardware implementations of linear signal transform algorithms. We
	extend the tensor structure for describing linear transform algorithms,
	adding the ability to explicitly characterize two important dimensions
	of datapath architecture. This representation allows both algorithm
	and datapath to be specified within a single formula and gives the
	designer the ability to easily consider a wide space of possible
	datapaths at a high level of abstraction. We have constructed a formula
	manipulation system based on this representation and have written
	a compiler that can translate a formula into a hardware implementation.
	This enables an automatic "push button" compilation flow that produces
	a register transfer level hardware description from high-level datapath
	directives and an algorithm (written as a formula). In our experimental
	results, we demonstrate that this approach yields efficient designs
	over a large tradeoff space.},
  issn = {0738-100X},
  keywords = {DSP transforms;automatic push button compilation flow;compiler;datapath
	architecture;discrete Fourier transform;formal datapath representation;formula
	manipulation system;hardware implementations;high-level datapath
	directives;high-level synthesis;linear signal transform algorithms;register
	transfer level hardware description;tensor structure;digital signal
	processing chips;discrete Fourier transforms;formal specification;hardware
	description languages;high level synthesis;tensors;}
}

@ARTICLE{995438,
  author = {Milicev, D.},
  title = {Automatic model transformations using extended UML object diagrams
	in modeling environments},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2002},
  volume = {28},
  pages = {413 -431},
  number = {4},
  month = {apr},
  abstract = {One of the most important features of modeling tools is generation
	of output. The output may be documentation, source code, net list,
	or any other presentation of the system being constructed. The process
	of output generation may be considered as automatic creation of a
	target model from a model in the source modeling domain. This translation
	does not need to be accomplished in a single step. Instead, a tool
	may generate multiple intermediate models as other views to the system.
	These models may be used either as better descriptions of the system,
	or as a descent down the abstraction levels of the user-defined model,
	gradually leading to the desired implementation. If the modeling
	domains have their metamodels defined in terms of object-oriented
	concepts, the models consist of instances of the abstractions from
	the metamodels and links between them. A new technique for specifying
	the mapping between different modeling domains is proposed in the
	paper. It uses UML object diagrams that show the instances and links
	of the target model that should be created during automatic translations.
	The diagrams are extended with the proposed concepts of conditional,
	repetitive, parameterized, and polymorphic model creation, implemented
	by the standard UML extensibility mechanisms. Several examples from
	different engineering domains are provided, illustrating the applicability
	and benefits of the approach. The first experimental results show
	that the specifications may lead to better reuse and shorter production
	time when developing customized output generators},
  doi = {10.1109/TSE.2002.995438},
  issn = {0098-5589},
  keywords = {UML object diagrams;Unified Modeling Language;automatic model transformations;documentation;domain-specific
	modeling;experimental results;extensibility mechanisms;metamodels;model-based
	output generation;modeling tools;net list;object-oriented concepts;polymorphic
	model;software reuse;source code;user-defined model;diagrams;formal
	specification;object-oriented programming;software reusability;software
	tools;specification languages;}
}

@INPROCEEDINGS{5254116,
  author = {Miller, A. and Kumar, B. and Singhal, A.},
  title = {Photon: A Domain-Specific Language for Testing Converged Applications},
  booktitle = {Computer Software and Applications Conference, 2009. COMPSAC '09.
	33rd Annual IEEE International},
  year = {2009},
  volume = {2},
  pages = {269 -274},
  month = {july},
  abstract = {Automated testing of converged applications can be complex, as it
	is rare for a single testing tool to provide a single solution for
	all access points which a given application supports. As such, testing
	teams often create customized testing frameworks, which integrate
	several different testing tools, and a myriad of programming languages
	and scripting tools. When an applicationpsilas unique set of access
	points changes, or a new testing tool comes to market which offers
	a competitive advantage over existing test tools, the cost of updating
	these customized frameworks can be difficult to justify. This paper
	provides a solution to this problem by introducing ldquoPhotonese,rdquo
	a domain-specific language which testers can use to compose automation
	scripts which are independent of the test tool used for automation.
	In this way, the tester creates reusable testing assets in a framework
	which is reusable across multiple projects.},
  doi = {10.1109/COMPSAC.2009.143},
  issn = {0730-3157},
  keywords = {Photonese;automated testing;automation script;customized testing framework;domain-specific
	language;programming language;reusable testing asset;scripting tool;testing
	converged application;authoring languages;program testing;}
}

@INPROCEEDINGS{5679113,
  author = {Miller, J.A. and Jun Han and Hybinette, M.},
  title = {Using Domain Specific Language for modeling and simulation: ScalaTion
	as a case study},
  booktitle = {Winter Simulation Conference (WSC), Proceedings of the 2010},
  year = {2010},
  pages = {741 -752},
  month = {dec.},
  abstract = {Progress in programming paradigms and languages has over time influenced
	the way that simulation programs are written. Modern object-oriented,
	functional programming languages are expressive enough to define
	embedded Domain Specific Languages (DSLs). The Scala programming
	language is used to implement ScalaTion that supports several popular
	simulation modeling paradigms. As a case study, ScalaTion is used
	to consider how language features of object-oriented, functional
	programming languages and Scala in particular can be used to write
	simulation programs that are clear, concise and intuitive to simulation
	modelers. The dichotomy between #x201C;model specification #x201D;
	and #x201C;simulation program #x201D; is also considered both historically
	and in light of the potential narrowing of the gap afforded by embedded
	DSLs.},
  doi = {10.1109/WSC.2010.5679113},
  issn = {0891-7736},
  keywords = {DSL;ScalaTion simulation;domain specific language;object-oriented
	programming;programming languages;programming paradigms;scala programming
	language;simulation program;programming languages;}
}

@INPROCEEDINGS{252917,
  author = {Miller, L. and Johnson, L. and Ning, J.Q. and Quilici, A. and Devanbu,
	P.},
  title = {Program understanding-does it offer hope for aging software?},
  booktitle = {Knowledge-Based Software Engineering Conference, 1992., Proceedings
	of the Seventh},
  year = {1992},
  pages = {238 -242},
  month = {sep},
  abstract = {Two questions are examined: what does it mean to understand a program?
	What solutions to the aging software crisis does program understanding
	offer? One view is that the aging software problem is a form of support
	for program maintenance. Another view is that the problem of applying
	program understanding techniques to aging software is one of both
	extending the life of existing relics, and mining them for valuable
	components. Yet another view is that domain specific cliches are
	necessary in program understanding and that program understanding
	support for software is how programmers can come to understand a
	program through cooperative interaction with a knowledge base. The
	work focuses on the flexibility provided by natural language interaction},
  doi = {10.1109/KBSE.1992.252917},
  keywords = { aging software crisis; aging software problem; cooperative interaction;
	domain specific cliches; existing relics; knowledge base; natural
	language interaction; program maintenance; program understanding
	support; human factors; knowledge based systems; natural language
	interfaces; programming; software maintenance; software reusability;}
}

@INPROCEEDINGS{4061409,
  author = {Milne, D. and Medelyan, O. and Witten, I.H.},
  title = {Mining Domain-Specific Thesauri from Wikipedia: A Case Study},
  booktitle = {Web Intelligence, 2006. WI 2006. IEEE/WIC/ACM International Conference
	on},
  year = {2006},
  pages = {442 -448},
  month = {dec.},
  abstract = {Domain-specific thesauri are high-cost, high-maintenance, high-value
	knowledge structures. We show how the classic thesaurus structure
	of terms and links can be mined automatically from Wikipedia. In
	a comparison with a professional thesaurus for agriculture we find
	that Wikipedia contains a substantial proportion of its concepts
	and semantic relations; furthermore it has impressive coverage of
	contemporary documents in the domain. Thesauri derived using our
	techniques capitalize on existing public efforts and tend to reflect
	contemporary language usage better than their costly, painstakingly-constructed
	manual counterparts},
  doi = {10.1109/WI.2006.119},
  keywords = {Wikipedia;agriculture;domain-specific thesauri mining;knowledge structures;Web
	sites;agriculture;data mining;document handling;thesauri;}
}

@INPROCEEDINGS{5273836,
  author = {Mizera-Pietraszko, J.},
  title = {Interactive document retrieval from multilingual digital repositories},
  booktitle = {Applications of Digital Information and Web Technologies, 2009. ICADIWT
	'09. Second International Conference on the},
  year = {2009},
  pages = {423 -428},
  month = {aug.},
  abstract = {On the contrary to multilingual information retrieval systems, digital
	libraries usually do not provide any support for query translation.
	This paper presents a study aimed at exploring the role of the translation
	component in a digital library system designed for multilingual access
	to resources. Thus, we propose a user oriented approach to one word
	query formulation aimed at browsing English and French document collections
	of digital libraries. In particular, the focus is on comparison of
	bilingual to monolingual harvesting of digital collections. We explore
	some multilingual repositories based on OPAC (online public catalogue)
	systems by utilization of metadata based search. The evaluation of
	the digital library systems selected relies on usability criteria.
	In addition, the search results are grouped into categories according
	to the language pair phenomena. A diversity of the library contents
	oriented towards domain specific network communities is also regarded
	as a relevance factor at the query formulation stage.},
  doi = {10.1109/ICADIWT.2009.5273836},
  keywords = {English document collection browsing;French document collection browsing;OPAC
	system;bilingual harvesting;digital library system;domain specific
	network community;interactive document retrieval;metadata based search;monolingual
	harvesting;multilingual access;multilingual digital repository;multilingual
	information retrieval system;multilingual resource;online public
	catalogue;query formulation stage;query translation;translation component;usability
	criteria;user oriented approach;digital libraries;document handling;interactive
	systems;language translation;meta data;public information systems;query
	formulation;}
}

@ARTICLE{5196681,
  author = {Moha, N. and Gueheneuc, Y.-G. and Duchien, L. and Le Meur, A.-F.},
  title = {DECOR: A Method for the Specification and Detection of Code and Design
	Smells},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2010},
  volume = {36},
  pages = {20 -36},
  number = {1},
  month = {jan.-feb. },
  abstract = {Code and design smells are poor solutions to recurring implementation
	and design problems. They may hinder the evolution of a system by
	making it hard for software engineers to carry out changes. We propose
	three contributions to the research field related to code and design
	smells: (1) DECOR, a method that embodies and defines all the steps
	necessary for the specification and detection of code and design
	smells, (2) DETEX, a detection technique that instantiates this method,
	and (3) an empirical validation in terms of precision and recall
	of DETEX. The originality of DETEX stems from the ability for software
	engineers to specify smells at a high level of abstraction using
	a consistent vocabulary and domain-specific language for automatically
	generating detection algorithms. Using DETEX, we specify four well-known
	design smells: the antipatterns Blob, Functional Decomposition, Spaghetti
	Code, and Swiss Army Knife, and their 15 underlying code smells,
	and we automatically generate their detection algorithms. We apply
	and validate the detection algorithms in terms of precision and recall
	on XERCES v2.7.0, and discuss the precision of these algorithms on
	11 open-source systems.},
  doi = {10.1109/TSE.2009.50},
  issn = {0098-5589},
  keywords = {DECOR;DETEX;Spaghetti code;Swiss army knife;antipatterns Blob;code
	detection;code specification;design smells;domain-specific language;empirical
	validation;functional decomposition;open-source systems;formal specification;program
	verification;software quality;}
}

@INPROCEEDINGS{4682081,
  author = {Mohan, S. and Eunmi Choi and Dugki Min},
  title = {Domain Specific Modeling of Business Processes and Entity Mapping
	Using Generic Modeling Environment (GME)},
  booktitle = {Convergence and Hybrid Information Technology, 2008. ICCIT '08. Third
	International Conference on},
  year = {2008},
  volume = {1},
  pages = {533 -538},
  month = {nov.},
  abstract = {Designing business process is very complex as it involves activities,
	resources, products and tools. Adopting the concept of domain specific
	modeling helps creating and using business processes, packaged as
	services and enable reusability, loose coupling, higher abstraction,
	agility, and interoperability. Domain specific modeling can be used
	to design the business processes as it increases the business centric
	value as well the productivity and quality. In this paper, GME tool
	is used for creating DSM of business process and also discussed how
	business processes are captured and defined at domain level. Also
	the design of business process and entity meta-model's and its interpretation
	to generate the Input model is shown in detail. GReAT Tool is used
	for applying configuration and transformation rules on the imported
	models. Finally full code generation is shown.},
  doi = {10.1109/ICCIT.2008.231},
  keywords = {business centric value;business process design;domain specific modeling;entity
	mapping;full code generation;generic modeling environment;interoperability;business
	data processing;open systems;}
}

@INPROCEEDINGS{4351362,
  author = {Mohd Ali, N.},
  title = {A Generic Visual Critic Authoring Tool},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {260 -261},
  month = {sept.},
  abstract = {Critic tools have been used for many domains, including design sketches,
	education, general engineering, and software design. The focus of
	this research is to develop a generic visual critic authoring framework
	embedded within an end user oriented domain specific visual language
	meta tool. This will allow tool critic support to be rapidly developed
	in parallel with the tools themselves.},
  doi = {10.1109/VLHCC.2007.30},
  keywords = {design sketches;generic visual critic authoring tool;oriented domain
	specific visual language meta tool;software design;software engineering;software
	tools;visual languages;}
}

@INPROCEEDINGS{1631137,
  author = {Moise, D.L. and Wong, K. and Hoover, H.J. and Hou, D.},
  title = {Reverse Engineering Scripting Language Extensions},
  booktitle = {Program Comprehension, 2006. ICPC 2006. 14th IEEE International Conference
	on},
  year = {2006},
  pages = {295 -306},
  month = {0-0 },
  abstract = {Software systems are often written in more than one programming language.
	During development, programmers need to understand not only the dependencies
	among code in a particular language, but dependencies that span languages.
	In this paper, we focus on the problem of scripting languages (such
	as Perl) and their extension mechanisms to calling functions with
	a C interface. Our general approach involves building a fact extractor
	for each scripting language, by hooking into the language interpreter
	itself. The produced facts conform to a common schema, and an analyzer
	is extended to recognize the cross-language dependencies. We present
	how these statically discovered dependencies can be represented,
	visualized, and explored in the Eclipse environment},
  doi = {10.1109/ICPC.2006.42},
  keywords = {Eclipse environment;Perl script language;cross-language dependency;programming
	language;reverse engineering;software system;C language;object-oriented
	programming;reverse engineering;software reusability;}
}

@ARTICLE{1377184,
  author = {Mok, A.K. and Konana, P. and Guangtian Liu and Chan-Gun Lee and Honguk
	Woo},
  title = {Specifying timing constraints and composite events: an application
	in the design of electronic brokerages},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2004},
  volume = {30},
  pages = { 841 - 858},
  number = {12},
  month = {dec.},
  abstract = { Increasingly, business applications need to capture consumers' complex
	preferences interactively and monitor those preferences by translating
	them into event-condition-action (ECA) rules and syntactically correct
	processing specification. An expressive event model to specify primitive
	and composite events that may involve timing constraints among events
	is critical to such applications. Relying on the work done in active
	databases and real-time systems, this research proposes a new composite
	event model based on real-time logic (RTL). The proposed event model
	does not require fixed event consumption policies and allows the
	users to represent the exact correlation of event instances in defining
	composite events. It also supports a wide-range of domain-specific
	temporal events and constraints, such as future events, time-constrained
	events, and relative events. This event model is validated within
	an electronic brokerage architecture that unbundles the required
	functionalities into three separable components - business rule manager,
	ECA rule manager, and event monitor - with well-defined interfaces.
	A proof-of-concept prototype was implemented in the Java programming
	language to demonstrate the expressiveness of the event model and
	the feasibility of the architecture. The performance of the composite
	event monitor was evaluated by varying the number of rules, event
	arrival rates, and type of composite events.},
  doi = {10.1109/TSE.2004.105},
  issn = {0098-5589},
  keywords = { ECA rule manager; Java programming language; active databases; business
	application; business rule manager; correct processing specification;
	domain-specific temporal event; electronic brokerages architecture;
	event arrival rates; event monitor; event-condition-action; real-time
	logic; real-time systems; time-constrained event; timing constraints;
	Java; active databases; electronic commerce; electronic trading;
	formal specification; real-time systems; temporal logic;}
}

@INPROCEEDINGS{144087,
  author = {Montanari, A. and Ratto, E. and Corsetti, E. and Morzenti, A.},
  title = {Embedding time granularity in logical specifications of real-time
	systems},
  booktitle = {Real Time Systems, 1991. Proceedings., Euromicro '91 Workshop on},
  year = {1991},
  pages = {88 -97},
  month = {jun},
  abstract = {The paper extends the TRIO logical specification formalism with the
	notion of time granularity. Such an extension provides the specifier
	with the ability of dealing with different time granularities within
	a single specification. It allows one to maintain the description
	of the dynamics of processes that evolve according to different time
	constants as separate as possible. It also makes it possible to model
	the dynamics of a given process with respect to different time scales.
	The paper first introduces time granularity in a completely general
	way, that is, it defines the weakest semantics of time granularity.
	Then a number of possible specializations of such a semantics taking
	into account both common-sense and domain-specific knowledge are
	identified. They result in a taxonomic classification of predicates
	that makes the formalism more expressive and easier to use},
  doi = {10.1109/EMWRT.1991.144087},
  keywords = {TRIO logical specification formalism;common-sense;domain-specific
	knowledge;real-time systems;specification language;taxonomic classification
	of predicates;time constants;time granularity;weakest semantics;real-time
	systems;software tools;specification languages;}
}

@INPROCEEDINGS{5773390,
  author = {Montecchi, L. and Lollini, P. and Bondavalli, A.},
  title = {Towards a MDE Transformation Workflow for Dependability Analysis},
  booktitle = {Engineering of Complex Computer Systems (ICECCS), 2011 16th IEEE
	International Conference on},
  year = {2011},
  pages = {157 -166},
  month = {april},
  abstract = {In the last ten years, Model Driven Engineering (MDE) approaches have
	been extensively used for the analysis of extra-functional properties
	of complex systems, like safety, dependability, security, predictability,
	quality of service. To this purpose, engineering languages (like
	UML and AADL) have been extended with additional features to model
	the required non-functional attributes, and transformations have
	been used to automatically generate the analysis models to be solved
	by appropriate analysis tools. In most of the available works, however,
	the transformations are not inte grated into a more general development
	process, aimed to support both domain-specific design analysis and
	verification of extra-functional properties. In this paper we explore
	this research direction presenting a transformation work flow for
	dependability analysis that is part of an industrial-quality infrastructure
	for the specification, analysis and verification of extra-functional
	properties, currently under development within the ARTEMIS-JU CHESS
	project. Specifically, the paper provides the following major contributions:
	i) definition of the required transformation steps to automatically
	assess the system dependability properties starting from the CHESS
	Modeling Language, ii) definition of a new Intermediate Dependability
	Model (IDM) acting as a bridge between the CHESS Modeling Language
	and the low-level analysis models, iii) definition of transformations
	from the CHESS Modeling Language to IDM models.},
  doi = {10.1109/ICECCS.2011.23},
  keywords = {AADL;ARTEMIS-JU CHESS project;CHESS modeling language;IDM models;MDE
	transformation workflow;UML;dependability analysis;industrial quality
	infrastructure;model driven engineering;Unified Modeling Language;formal
	verification;software engineering;}
}

@INPROCEEDINGS{5501573,
  author = {Montini, D.A. and Fernandes, D.D. and Marcondes, F.S. and Tasinaffo,
	P.M. and Vega, I.S. and Dias, L.},
  title = {Formal Approach Use to Choose a Software Manufactoring Cell's SDLC},
  booktitle = {Information Technology: New Generations (ITNG), 2010 Seventh International
	Conference on},
  year = {2010},
  pages = {1304 -1305},
  month = {april},
  abstract = {This paper shows how to use state machines and systematic approaches
	to software modeling to help modeler to improve, verify and validate
	a Domain Analysis and also refine and improve enterprise business
	processes. The main objective of this approach is how to systematic
	got a DSL from a Domain Analysis which can be used code system respecting,
	all business rules without complex definitions or documents. Many
	problems of Computer Software Systems (CSS) are derived from a lack
	of its behavior specification in order to solve that problem, but
	even with a well defined system behavior, many business rules are
	not properly treated since formalization becomes on the design phase.
	This paper shows an approach on how to systematically refine domain
	analysis to consider all business rules. It considers a state machine
	which represent all aspects of the domain choice. The state machines
	use is based on user friendliness and formality.},
  doi = {10.1109/ITNG.2010.81},
  keywords = {DSL;business rules;computer software systems;design phase;domain analysis;domain
	specific language;enterprise business processes;formal methods;software
	manufacturing cell SDLC;software modeling;state machines;system behavior;finite
	state machines;formal verification;specification languages;systems
	software;}
}

@INPROCEEDINGS{5295275,
  author = {Moody, D.},
  title = {Theory development in visual language research: Beyond the cognitive
	dimensions of notations},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {151 -154},
  month = {sept.},
  abstract = {The cognitive dimensions of notations (CDs) framework has become the
	predominant theoretical paradigm for analysing visual languages (VLs).
	This paper evaluates the CDs framework using established conceptual
	frameworks for analysing scientific theories. It concludes that while
	it is a paradigm, it is not scientific according to standards normally
	applied in scientific research. It also represents the earliest evolutionary
	form of theory, which is appropriate when no prior theory exists.
	This paper asks whether such a theory is an appropriate paradigm
	for VL research after a quarter of a century of research in this
	field. The CDs framework has performed a valuable role in advancing
	the analysis of VLs beyond the level of intuition, but should not
	be seen as the end point for theory development in this field. This
	paper proposes a more powerful, domain-specific theory (the physics
	of notations) as an alternative paradigm for VL research.},
  doi = {10.1109/VLHCC.2009.5295275},
  issn = {1943-6092},
  keywords = {cognitive dimension of notation;evolutionary progression;visual language;evolutionary
	computation;visual languages;}
}

@ARTICLE{5388943,
  author = {Moreno, J. H. and Zyuban, V. and Shvadron, U. and Neeser, F. D. and
	Derby, J. H. and Ware, M. S. and Kailas, K. and Zaks, A. and Geva,
	A. and Ben-David, S. and Asaad, S. W. and Fox, T. W. and Littrell,
	D. and Biberstein, M. and Naishlos, D. and Hunter, H.},
  title = {An innovative low-power high-performance programmable signal processor
	for digital communications},
  journal = {IBM Journal of Research and Development},
  year = {2003},
  volume = {47},
  pages = {299 -326},
  number = {2.3},
  month = {march },
  abstract = {We describe an innovative, low-power, high-performance, programmable
	signal processor (DSP) for digital communications. The architecture
	of this processor is characterized by its explicit design for low-power
	implementations, its innovative ability to jointly exploit instruction-level
	parallelism and data-level parallelism to achieve high performance,
	its suitability as a target for an optimizing high-level language
	compiler, and its explicit replacement of hardware resources by compile-time
	practices. We describe the methodology used in the development of
	the processor, highlighting the techniques deployed to enable application/architecture/compiler/implementation
	co-development, and the optimization approach and metric used for
	power-performance evaluation and tradeoff analysis. We summarize
	the salient features of the architecture, provide a brief description
	of the hardware organization, and discuss the compiler techniques
	used to exercise these features. We also summarize the simulation
	environment and associated software development tools. Coding examples
	from two representative kernels in the digital communications domain
	are also provided. The resulting methodology, architecture, and compiler
	represent an advance of the state of the art in the area of low-power,
	domain-specific microprocessors.},
  doi = {10.1147/rd.472.0299},
  issn = {0018-8646}
}

@ARTICLE{4259293,
  author = {Moreno, N. and Fraternali, P. and Vallecillo, A.},
  title = {WebML modelling in UML},
  journal = {Software, IET},
  year = {2007},
  volume = {1},
  pages = {67 -80},
  number = {3},
  month = {june },
  abstract = {In recent years, we have witnessed how the Web Engineering community
	has started using the standard unified modelling language (UML) notation,
	techniques and supporting tools for modelling Web systems, which
	has led to the adaptation to UML of several existing modelling languages,
	notations and development processes. This interest for being MOF
	and UML-compliant arises from the increasing need to interoperate
	with other notations and tools, and to exchange data and models,
	thus facilitating reuse. WebML, like any other domain-specific language,
	allows one to express in a precise and natural way the concepts and
	mechanisms of its domain of reference. However, it cannot fully interoperate
	with other notations, nor be integrated with other model-based tools.
	As a solution to these requirements, a UML 2.0 profile for WebML
	which allows WebML models to be used in conjunction with other notations
	and modelling tools has been described. The paper also evaluates
	UML 2.0 as a platform for Web modelling and identifies some key requirements
	for making this version of the standard more usable.},
  issn = {1751-8806},
  keywords = {UML;Web Engineering community;Web systems;WebML modelling;domain-specific
	language;unified modelling language;Internet;Unified Modeling Language;data
	models;}
}

@INPROCEEDINGS{6084038,
  author = {Moshirpour, Mohammad and Alhajj, Reda and Moussavi, Mahmood and Far,
	Behrouz H.},
  title = {Detecting emergent behavior in distributed systems using an ontology
	based methodology},
  booktitle = {Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference
	on},
  year = {2011},
  pages = {2407 -2412},
  month = {oct.},
  abstract = {Lack of central control makes the design of distributed software systems
	a challenging task because of possible unwanted behavior at runtime,
	commonly known as emergent behavior. Developing methodologies to
	detect emergent behavior prior to the implementation stage of the
	system can lead to huge savings in time and cost. However manual
	review of requirements and design documents for real-life systems
	is inefficient and error prone; thus automation of analysis methodologies
	is considered greatly beneficial. This paper proposes the utilization
	of an ontology-based approach to analyze system requirements expressed
	by a set of message sequence charts (MSC). This methodology involves
	building a domain-specific ontology of the system, and examines the
	requirements based on this ontology. The advantages of this approach
	in comparison with other methodologies are its consistency and increased
	level of automation. The effectiveness of this approach is explained
	using a case study of an IntelliDrive system.},
  doi = {10.1109/ICSMC.2011.6084038},
  issn = {1062-922X}
}

@INPROCEEDINGS{5305975,
  author = {Motal, T. and Zapletal, M. and Werthner, H.},
  title = {The Business Choreography Language (BCL) - A Domain-Specific Language
	for Global Choreographies},
  booktitle = {Services - II, 2009. SERVICES-2 '09. World Conference on},
  year = {2009},
  pages = {150 -159},
  month = {sept.},
  abstract = {UN/CEFACT's modeling methodology (UMM) is a modeling approach for
	describing the choreography of B2B processes. UMM is developed by
	the United Nations Center for Trade Facilitation and Electronic Business
	(UN/CEFACT) and currently defined as a UML profile. Thereby, it constrains
	the UML for the specific needs of B2B. As we learned, using UML as
	the underlying notation for UMM results in several shortcomings.
	Furthermore, some workarounds are required to fit the concepts of
	UMM to the UML meta model. Thus, in this paper we examine an alternative
	notation for UMM following the concepts of a domain-specific language
	(DSL). The contribution of this paper is twofold: (i) we identify
	general concepts for modeling global choreographies by taking UMM
	as a starting point, (ii) We introduce the business choreography
	language (BCL), a domain-specific language designed to efficiently
	support the prior identified concepts. The concepts of the BCL are
	exemplified by an implementation using the Microsoft DSL Tools for
	Visual Studio. In fact, the BCL is an approach tailored to support
	the specific needs of global B2B choreographies.},
  doi = {10.1109/SERVICES-2.2009.25},
  keywords = {B2B processes;Microsoft DSL Tools;UN/CEFACT modeling methodology;United
	Nations Center for Trade Facilitation and Electronic Business;Visual
	Studio;business choreography language;domain-specific language;global
	choreographies;unified modeling language;Unified Modeling Language;business
	process re-engineering;humanities;}
}

@INPROCEEDINGS{369617,
  author = {Moulding, M.R. and Newton, A.R.},
  title = {A formal knowledge-based data-fusion language for safety-critical
	applications},
  booktitle = {Knowledge-Based Systems for Safety Critical Applications, IEE Colloquium
	on},
  year = {1994},
  pages = {6/1 -618},
  abstract = {This paper outlines the features of a novel applications-specific
	language which has been developed to describe naval data fusion solutions
	using the DRA's blackboard approach. This DFL is paradigm-specific
	in that it defines a rule-based blackboard system, and domain-specific
	in that the blackboard has been configured to support the fusion
	of radar and ESM data in a naval context. A primary design concept
	of the language is that solutions expressed in it have a unique and
	precise mapping to a formal specification comprising inter-linked
	VDM and CSP models. In so doing, we aim to provide a language which
	aids the natural expression of a data-fusion solution, whilst at
	the same time providing a formal underpinning which makes that solution
	amenable to formal analysis. Thus we attempt to create a bridge between
	the informal world of the domain expert and the formal world of the
	safety-critical system developer. Having provided a formal model
	of a DFL program, we can exploit this to provide animation and rapid
	prototyping, as well as formal bespoke system development},
  keywords = { CSP models; VDM; applications-specific language; bespoke system development;
	blackboard approach; formal knowledge-based data-fusion language;
	formal specification; naval data fusion solutions; rapid prototyping;
	rule-based blackboard system; safety-critical applications; blackboard
	architecture; formal specification; military computing; naval engineering
	computing; safety; sensor fusion;}
}

@INPROCEEDINGS{5767619,
  author = {Moustafa, W.E. and Namata, G. and Deshpande, A. and Getoor, L.},
  title = {Declarative analysis of noisy information networks},
  booktitle = {Data Engineering Workshops (ICDEW), 2011 IEEE 27th International
	Conference on},
  year = {2011},
  pages = {106 -111},
  month = {april},
  abstract = {There is a growing interest in methods for analyzing data describing
	networks of all types, including information, biological, physical,
	and social networks. Typically the data describing these networks
	is observational, and thus noisy and incomplete; it is often at the
	wrong level of fidelity and abstraction for meaningful data analysis.
	This has resulted in a growing body of work on extracting, cleaning,
	and annotating network data. Unfortunately, much of this work is
	ad hoc and domain-specific. In this paper, we present the architecture
	of a data management system that enables efficient, declarative analysis
	of large-scale information networks. We identify a set of primitives
	to support the extraction and inference of a network from observational
	data, and describe a framework that enables a network analyst to
	easily implement and combine new extraction and analysis techniques,
	and efficiently apply them to large observation networks. The key
	insight behind our approach is to decouple, to the extent possible,
	(a) the operations that require traversing the graph structure (typically
	the computationally expensive step), from (b) the operations that
	do the modification and update of the extracted network. We present
	an analysis language based on Datalog, and show how to use it to
	cleanly achieve such decoupling. We briefly describe our prototype
	system that supports these abstractions. We include a preliminary
	performance evaluation of the system and show that our approach scales
	well and can efficiently handle a wide spectrum of data cleaning
	operations on network data.},
  doi = {10.1109/ICDEW.2011.5767619},
  keywords = {Datalog;data analysis;data cleaning operations;data management system;declarative
	analysis;graph structure;noisy information networks;DATALOG;data
	analysis;information networks;}
}

@INPROCEEDINGS{1656946,
  author = {Mueller-Glaser, K.D.},
  title = {Domain specific model driven design for automotive electronic control
	units},
  booktitle = {Design, Automation and Test in Europe, 2006. DATE '06. Proceedings},
  year = {2006},
  volume = {1},
  pages = {1 pp.},
  month = {march},
  abstract = {Summary form only given. To enhance efficiency and reliability in
	the design of distributed electronic control units with hard real-time
	constraints new methods and computer aided tools are required, especially
	to support early system design phases. Domain specific tools are
	required to support design space exploration in the concept phase
	of electric/electronic systems. Design and verification based on
	heterogeneous models (closed loop control systems, reactive systems
	and UML based software intensive systems) and using a CASE-tool integration
	platform will allow for a seemless design flow},
  doi = {10.1109/DATE.2006.243924},
  keywords = {CASE-tool integration platform;UML;automotive electronic;closed loop
	control systems;computer aided tools;design flow;distributed electronic
	control units;domain specific model driven design;heterogeneous models;reactive
	systems;reliability;verification;Unified Modeling Language;automotive
	electronics;circuit CAD;computer aided software engineering;formal
	verification;transportation;}
}

@INPROCEEDINGS{1261563,
  author = {Mukkamala, P.S. and Smith, J.S. and Valenzuela, J.F.},
  title = {Designing reusable simulation modules for electronics manufacturing
	systems},
  booktitle = {Simulation Conference, 2003. Proceedings of the 2003 Winter},
  year = {2003},
  volume = {2},
  pages = { 1281 - 1289 vol.2},
  month = {dec.},
  abstract = { Developing simulation models for related problems in the same domain
	is generally a repetitive process. Such simulation models are similar
	in many aspects and have only minor differences. Modeling efforts
	can be reduced to a great extent through the development of domain
	specific modules or templates that encapsulate the domain-specific
	logic and hide many of the modeling details. We describe the development
	of such a domain-specific template for electronics assembly. In particular,
	the template focuses on the automated assembly of printed circuit
	boards. The template encompasses the complexity of the target domain
	and simplifies the model-building process. While this focuses on
	a language-neutral description of the template, specific experience
	with Arena is described.},
  doi = {10.1109/WSC.2003.1261563},
  keywords = { Arena; automated assembly; domain-specific template; electronics
	assembly; electronics manufacturing systems; language-neutral description;
	model-building process; printed circuit boards; reusable simulation
	module design; assembling; digital simulation; electronic engineering
	computing; manufacturing systems; printed circuit manufacture; printed
	circuits; production engineering computing; software reusability;}
}

@INPROCEEDINGS{4783710,
  author = {Munnelly, J. and Clarke, S.},
  title = {A Domain-Specific Language for Ubiquitous Healthcare},
  booktitle = {Pervasive Computing and Applications, 2008. ICPCA 2008. Third International
	Conference on},
  year = {2008},
  volume = {2},
  pages = {757 -762},
  month = {oct.},
  abstract = {The development of ubiquitous healthcare applications has proved to
	be significantly more complex than traditional healthcare applications.
	In software engineering research, there are two approaches of interest
	to us for handling the kind of complexity that emerges. The first
	is the use of domain-specific languages, which abstracts the low-level
	domain knowledge required when using general-purpose programming
	languages into more expressive domain-specific constructs. The second
	is advanced modularity techniques, such as aspect-oriented programming,
	that provide for modularisation of concerns that complicate code
	by cutting across a broad code base and tangling with other concerns.
	In this paper, we identify a set of ubiquitous healthcare concerns
	that complicate their software development. We use advanced modularity
	techniques to provide good separation of these concerns and encapsulate
	their behaviour within a new domain-specific language, ALPH that
	provides the application developer with a high level of abstraction.
	The result is a means to develop ubiquitous healthcare applications
	more easily and in a more timely fashion, while improving software
	quality by increasing modularity in the code.},
  doi = {10.1109/ICPCA.2008.4783710},
  keywords = {aspect-oriented programming;domain-specific language;general-purpose
	programming languages;low-level domain knowledge;software development;ubiquitous
	healthcare;health care;object-oriented programming;specification
	languages;ubiquitous computing;}
}

@INPROCEEDINGS{6032268,
  author = {Muscar, A. and Badica, C.},
  title = {A Functional Approach to Agent Development: Research Agenda},
  booktitle = {Computer Software and Applications Conference Workshops (COMPSACW),
	2011 IEEE 35th Annual},
  year = {2011},
  pages = {380 -385},
  month = {july},
  abstract = {A vast array of solutions for developing multiagent systems has been
	proposed over the past decades. While every solution has its strong
	points, a trade-off has to be made between expressivity and generality.
	It is our opinion that by embedding an agent development language
	in a general purpose functional language we can obtain a solution
	that can address this gap. In this paper we present our approach
	on developing a complete solution for implementing multi-agent systems.
	We motivate our claims about embedded domain specific languages by
	means of an example. We also expand on the future directions of our
	research regarding workflow-oriented coordination and organizational
	aspects in multi-agent systems.},
  doi = {10.1109/COMPSACW.2011.70},
  keywords = {agent development language;embedded domain specific languages;expressivity;functional
	approach;general purpose functional language;generality;multiagent
	systems;organizational aspects;workflow-oriented coordination;functional
	languages;multi-agent systems;}
}

@INPROCEEDINGS{5328559,
  author = {Mussbacher, G. and Whittle, J. and Amyot, D.},
  title = {Semantic-Based Interaction Detection in Aspect-Oriented Scenarios},
  booktitle = {Requirements Engineering Conference, 2009. RE '09. 17th IEEE International},
  year = {2009},
  pages = {203 -212},
  month = {31 2009-sept. 4},
  abstract = {Interactions between dependent or conflicting aspects are a well-known
	problem with aspect-oriented development (and related paradigms).
	These interactions are potentially dangerous and can lead to unexpected
	or incorrect results when aspects are composed. To date, most aspect
	interaction detection methods have been based either on purely syntactic
	comparisons or have relied on heavyweight formal methods. We present
	a new approach that is based instead on lightweight semantic annotations
	of aspects. Each aspect is annotated with domain-specific markers
	and a separate influence model describes how semantic markers from
	different domains influence each other. Automated analysis can then
	be used both to highlight semantic aspect conflicts and to trade-off
	aspects. We apply this technique to early aspects, namely, aspect
	scenarios, because it is desirable to detect aspect interactions
	as early in the software lifecycle as possible. We evaluate the technique
	using an industrial case study and show that the technique detects
	interactions that cannot be discovered using syntactic techniques.},
  doi = {10.1109/RE.2009.13},
  issn = {1090-705X},
  keywords = {aspect-oriented scenarios;automated analysis;domain-specific markers;lightweight
	semantic annotations;semantic-based interaction detection;software
	lifecycle;software engineering;}
}

@INPROCEEDINGS{5749873,
  author = {Mhlbach, S. and Koch, A.},
  title = {A novel network platform for secure and efficient malware collection
	based on reconfigurable hardware logic},
  booktitle = {Internet Security (WorldCIS), 2011 World Congress on},
  year = {2011},
  pages = {9 -14},
  month = {feb.},
  abstract = {With the growing diversity of malware, researchers must be able to
	quickly collect many representative samples for study. This can be
	done, e.g., by using honeypots. As an alternative to software-based
	honeypots, we propose a singlechip honeypot appliance that is entirely
	hardware-based and thus significantly more resilient against compromising
	attacks. Additionally, it can easily keep up with network speeds
	of 10+ Gb/s and emulate thousands of vulnerable hosts. As base technology,
	we employ reconfigurable hardware devices whose functionality is
	not fixed by the manufacturing process. We present improvements to
	the platform, aiming to simplify management and updates. To this
	end, we introduce the domain-specific language VEDL, which can be
	used to describe the honeypot behavior in a highlevel manner by security
	experts not proficient in hardware design.},
  keywords = {VEDL;domain specific language;malware collection security;reconfigurable
	hardware logic;security experts;single-chip honeypot appliance;software
	based honeypot;vulnerability emulation description language;hardware
	description languages;invasive software;reconfigurable architectures;}
}

@INPROCEEDINGS{4381625,
  author = {Myrvang, P.H. and Kulo, T.S.},
  title = {Gaining Flexibility by Security Protocol Transfer},
  booktitle = {Computers and Communications, 2007. ISCC 2007. 12th IEEE Symposium
	on},
  year = {2007},
  pages = {859 -864},
  month = {july},
  abstract = {Even though PDAs in general - but smartcards in particular - are preferred
	over general-purpose computers to keep secrets, because they have
	meager resources, including them in security protocols is difficult.
	The PDAs - and again smartcards in particular - end up as a mere
	key-store and their processing power is not used. We describe a mechanism
	that allows such anemic computers to fully participate in protocols,
	even if the protocol in question by far exceeds their capabilities.
	This is done by means of machinery for transferring, at runtime,
	the protocol proper to a more powerful machine. We describe the mechanisms
	that make this possible (mainly a domain specific programming language
	named Obol and its implementation) and we discuss the credentials
	and certificates needed for the solution to maintain correctness.},
  doi = {10.1109/ISCC.2007.4381625},
  issn = {1530-1346},
  keywords = {Obol;PDA;anemic computers;authentication;domain specific programming
	language;security protocol transfer;smartcards;notebook computers;protocols;security
	of data;}
}

@INPROCEEDINGS{5340932,
  author = {Nakajima, H. and Sagisaka, Y.},
  title = {F0 analysis for Japanese conversational speech synthesis},
  booktitle = {Natural Language Processing, 2009. SNLP '09. Eighth International
	Symposium on},
  year = {2009},
  pages = {137 -142},
  month = {oct.},
  abstract = {This paper proposes a conversational style text-to-speech synthesis
	scheme based on an analysis of fundamental frequency, F0. Through
	the analysis, we confirm that conversational F0 can be represented
	by the superpositional model using three components ranging utterance,
	major phrase, and minor phrase. We compare each component of the
	model between conversational style and reading style to investigate
	the following points: where big F0 discrepancies are found, what
	linguistic factors concern to the discrepancies, and to what extent
	do such discrepancies occur. This paper uses real domain data that
	includes a lot of linguistic context. Analysis confirms that large
	differences occur in global components such as single span whole
	utterances and phrases, and that the differences occur at or around
	domain-specific expressions. The analysis also reveals that local
	components are almost the same in both styles. These analyses show
	that it is necessary to estimate the utterance and phrase components
	from words attributes other than the grammatical clues to realize
	conversational synthesis in the super positional manner.},
  doi = {10.1109/SNLP.2009.5340932},
  keywords = {F0 analysis;Japanese speech synthesis;conversational style text-speech
	synthesis scheme;domain-specific expression;fundamental frequency
	analysis;linguistic factor;speech utterance;superpositional model;linguistics;natural
	languages;speech synthesis;}
}

@INPROCEEDINGS{5599751,
  author = {Nakatsuka, M. and Yasunaga, S. and Kuwabara, K.},
  title = {Extending a multilingual chat application: Towards collaborative
	language resource building},
  booktitle = {Cognitive Informatics (ICCI), 2010 9th IEEE International Conference
	on},
  year = {2010},
  pages = {137 -142},
  month = {july},
  abstract = {We are developing a multilingual chat application to help children
	in Japanese schools whose native language is not Japanese communicate
	with teachers and support staff. Our multilingual chat application
	utilizes machine translation to allow communication in their native
	languages. Since special words or expressions are used in school
	settings, a machine translation service often produces inadequate
	translations. To cope with such problems, Language Grid provides
	a machine translation service that utilizes specialized domain specific
	dictionaries to improve translation quality. However, constructing
	such dictionaries is not easy. In this paper, we present a system
	that helps users expand the dictionaries using morphological analysis
	in the chat application itself. In addition, we present other extensions
	to the system to further support multilingual communication and build
	language resources.},
  doi = {10.1109/COGINF.2010.5599751},
  keywords = {Japanese school;collaborative language resource building;language
	grid;machine translation;machine translation service;morphological
	analysis;multilingual chat application;multilingual communication;computational
	linguistics;computer aided instruction;dictionaries;educational institutions;groupware;language
	translation;natural language processing;user interfaces;}
}

@INPROCEEDINGS{1201223,
  author = {Nentwich, C. and Emmerich, W. and Finkelstein, A.},
  title = {Consistency management with repair actions},
  booktitle = {Software Engineering, 2003. Proceedings. 25th International Conference
	on},
  year = {2003},
  pages = { 455 - 464},
  month = {may},
  abstract = { Comprehensive consistency management requires a strong mechanism
	for repair once inconsistencies have been detected In this paper
	we present a repair framework for inconsistent distributed documents.
	The core piece of the framework is a new method for generating interactive
	repairs from full first order logic formulae that constrain these
	documents. We present a full implementation of the components in
	our repair framework, as well as their application to the UML and
	related heterogeneous documents such as EJB deployment descriptors.
	We describe how our approach can be used as an infrastructure for
	building higher-level, domain specific frameworks and provide an
	overview of related work in the database and software development
	environment community.},
  doi = {10.1109/ICSE.2003.1201223},
  issn = {0270-5257 },
  keywords = { UML; consistency management; distributed document management; first
	order logic formulae; formal verification; programming language semantics;
	software repair; data integrity; formal verification; programming
	language semantics; software maintenance; software tools; specification
	languages;}
}

@INPROCEEDINGS{1264981,
  author = {Neto, C.S.S. and Rodrigues, R.F. and Soares, L.F.G.},
  title = {Architectural description of QoS provisioning for multimedia application
	support},
  booktitle = {Multimedia Modelling Conference, 2004. Proceedings. 10th International},
  year = {2004},
  pages = { 161 - 166},
  month = {jan.},
  abstract = { The increasing number of multimedia applications has motivated the
	construction of platforms with end-to-end quality of service (QoS)
	support. This work proposes the use of Wright architecture description
	language (ADL) in the QoS provisioning domain, as the basis for the
	formal verification of QoS system properties. To smooth this task,
	the LindaQoS domain-specific language was designed as a high-level
	notation for the specification of resource (QoS) orchestration. As
	a result of this approach, we expect that designers can define clear
	and unambiguous instantiated platforms, with support to multimedia
	application implementations, in a reduced development time.},
  doi = {10.1109/MULMM.2004.1264981},
  keywords = { LindaQoS; QoS system properties; Wright ADL; architecture description
	language; development time; domain-specific language; formal verification;
	high-level notation; multimedia application support; quality of service;
	resource orchestration; formal verification; multimedia communication;
	quality of service; resource allocation;}
}

@INPROCEEDINGS{5784752,
  author = {Niazi, M.F. and Seceleanu, T. and Tenhunen, H.},
  title = {An automated control code generation approach for the SegBus platform},
  booktitle = {SOC Conference (SOCC), 2010 IEEE International},
  year = {2010},
  pages = {199 -204},
  month = {sept.},
  abstract = {We present here a model-driven approach for the generation of low-level
	control code for the arbiters, to support application implementation
	and scheduled execution on a multi-core segmented bus platform, SegBus.
	The approach considers Model-Driven Architecture as a key to model
	the application at two different abstraction levels, namely as Packet-Synchronous
	Dataflow and Platform Specific Model, using the SegBus platform's
	Domain Specific Language. Both models are transformed into Extensible
	Markup Language schemes, and then utilized by an emulator program
	to generate the #x201C;application-dependent #x201D; VHDL code, the
	so-called #x201C;snippets #x201D;. The obtained code is inserted
	in a specific section of the platform arbiters. We present an example
	of a simplified stereo MP3 decoder where the methodology is employed
	to generate the control code of arbiters.},
  doi = {10.1109/SOCC.2010.5784752},
  issn = {Pending},
  keywords = {SegBus platform;abstraction level;application-dependent VHDL code;automated
	control code generation;domain specific language;emulator program;extensible
	markup language scheme;low-level control code;model-driven architecture;multicore
	segmented bus platform;packet-synchronous dataflow;platform specific
	model;stereo MP3 decoder;hardware description languages;multiprocessing
	systems;system buses;}
}

@INPROCEEDINGS{1410916,
  author = { Noah, S.A. and Zakaria, L. and Alhadi, A.C. and Tengku Sembok, T.M.
	and Saad, S.},
  title = {Towards Building Semantic Rich Model for Web Documents Using Domain
	Ontology},
  booktitle = {Web Intelligence, 2004. WI 2004. Proceedings. IEEE/WIC/ACM International
	Conference on},
  year = {2004},
  pages = { 769 - 770},
  month = {sept.},
  abstract = { Accessing and extracting semantic meanings from web documents is
	crucial for the realization of Semantic Web. While the web offers
	the flexibility of making information easily available, it is considerably
	hard to find a fruitful way to describe, classify and present this
	information with rich semantic content. Therefore, the semantic information
	content of web documents need to be specified in order to make the
	tangled information more accessible to search engines and other applications.
	In this paper we propose an approach meant to assist in constructing
	semantic document models using natural language analysis technique
	and a domain specific ontology.},
  doi = {10.1109/WI.2004.10019}
}

@INPROCEEDINGS{4815342,
  author = {Nodler, J. and Neukirchen, H. and Grabowski, J.},
  title = {A Flexible Framework for Quality Assurance of Software Artefacts
	with Applications to Java, UML, and TTCN-3 Test Specifications},
  booktitle = {Software Testing Verification and Validation, 2009. ICST '09. International
	Conference on},
  year = {2009},
  pages = {101 -110},
  month = {april},
  abstract = {Manual reviews and inspections of software artefacts are time consuming
	and thus, automated analysis tools have been developed to support
	the quality assurance of software artefacts. Usually, software analysis
	tools are implemented for analysing only one specific language as
	target and for performing only one class of analyses. Furthermore,
	most software analysis tools support only common programming languages,
	but not those domain-specific languages that are used in a test process.
	As a solution, a framework for software analysis is presented that
	is based on a flexible, yet high-level facade layer that mediates
	between analysis rules and the underlying target software artefact;
	the analysis rules are specified using high-level XQuery expressions.
	Hence, further rules can be quickly added and new types of software
	artefacts can be analysed without needing to adapt the existing analysis
	rules. The applicability of this approach is demonstrated by examples
	from using this framework to calculate metrics and detect bad smells
	in Java source code, in UML models, and in test specifications written
	using the testing and test control notations (TTCN-3).},
  doi = {10.1109/ICST.2009.34},
  keywords = {Java source code;TTCN-3 test specifications;UML;XQuery expressions;automated
	analysis tools;bad smell detection;domain-specific languages;metrics
	calculation;quality assurance;software analysis tools;software artefact
	inspections;test control notations;Java;Unified Modeling Language;program
	diagnostics;program testing;software metrics;software quality;software
	tools;}
}

@INPROCEEDINGS{5349862,
  author = {Noguera, C. and Loiret, F.},
  title = {Checking Architectural and Implementation Constraints for Domain-Specific
	Component Frameworks Using Models},
  booktitle = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th
	Euromicro Conference on},
  year = {2009},
  pages = {125 -132},
  month = {aug.},
  abstract = {Software components are used in various application domains, and many
	component models and frameworks have been proposed to fulfill domain-specific
	requirements. The ad-hoc development of these component frameworks
	hampers the reuse of tools and abstractions across different frameworks.
	We believe that in order to promote the reuse of components within
	various domain contexts an homogeneous design approach is needed.
	A key requirement of such an approach is the definition and validation
	of reusable domain-specific constraints. In this paper we propose
	an extension to the Hulotte component framework that allows the definition
	and checking of domain-specific concerns. From the components' architecture
	to their implementations, concerns are defined and checked in an
	homogeneous manner. Our approach is illustrated and evaluated through
	the design of an example component-based application for the multitasking
	and distributed domains.},
  doi = {10.1109/SEAA.2009.18},
  issn = {1089-6503},
  keywords = {Hulotte component framework;ad-hoc development;checking architectural
	constraints;component-based software engineering;domain-specific
	component frameworks;reusable domain-specific constraints;software
	architecture;software components;formal verification;object-oriented
	programming;software architecture;software tools;}
}

@INPROCEEDINGS{4026867,
  author = {Noguera, C. and Pawlak, R.},
  title = {AVal: an Extensible Attribute-Oriented Programming Validator for
	Java},
  booktitle = {Source Code Analysis and Manipulation, 2006. SCAM '06. Sixth IEEE
	International Workshop on},
  year = {2006},
  pages = {175 -183},
  month = {sept. },
  abstract = {Attribute Oriented Programming (@OP ) permits programmers to extend
	the semantics of a base program by annotating it with attributes
	that are related to a set of concerns. Examples of this are applications
	that rely on XDoclet (such as Hibernate) or, with the release of
	Java5s annotations, EJB3. The set of attributes that implements
	a concern defines a Domain Specific Language, and as such, imposes
	syntactic and semantic rules on the way attributes are included in
	the program or even on the program itself. We propose a framework
	for the definition and checking of these rules for @OP that uses
	Java5 annotations. We define an extensible set of meta-annotations
	to allow the validation of @OP programs, as well as the means to
	extend them using a compile-time model of the programs source code.
	We show the usefulness of the approach by presenting two examples
	of its use: an @OP extension for the Fractal component model called
	Fraclet, and the JSR 181 for web services definition.},
  doi = {10.1109/SCAM.2006.5}
}

@INPROCEEDINGS{4618336,
  author = {Hyungjong Noh and Cheongjae Lee and Lee, G.G.},
  title = {Ontology-based inference for information-seeking in natural language
	dialog system},
  booktitle = {Industrial Informatics, 2008. INDIN 2008. 6th IEEE International
	Conference on},
  year = {2008},
  pages = {1469 -1474},
  month = {july},
  abstract = {Many natural language dialog systems have been developed with relational
	database (RDB) as a machine-readable knowledge source. However, RDB
	has some problems for answering the questions which need complex
	domain-specific information. In addition to the typical problems
	of RDB such as dependency and redundancy problems, limitations of
	meaning representation and storing various domain knowledge problems
	also exist. To solve the problems of RDB, we adopted ontology concepts
	as a knowledge representation method. Ontology knowledge has some
	advantages about representing and querying information. In this paper,
	we developed ontology-based approach for information-seeking to improve
	traditional RDB-based natural language dialog systems. This is more
	flexible than RDB-based dialog system for answering to complex questions.
	To implement our system, we designed hand-crafted ontology schemas
	for an electronic program guide (EPG) application and we populated
	ontology instances from Web pages semi-automatically. We believe
	that our preliminary evaluations show the possibility of our new
	system.},
  doi = {10.1109/INDIN.2008.4618336},
  issn = {1935-4576},
  keywords = {Web pages;dependency problems;electronic program guide application;hand-crafted
	ontology schemas;information querying;information seeking;knowledge
	representation;machine-readable knowledge source;meaning representation;natural
	language dialog system;ontology-based inference;question answering;redundancy
	problems;relational database;inference mechanisms;interactive systems;natural
	language interfaces;ontologies (artificial intelligence);query processing;relational
	databases;}
}

@INPROCEEDINGS{755863,
  author = {Nordstrom, G. and Sztipanovits, J. and Karsai, G. and Ledeczi, A.},
  title = {Metamodeling-rapid design and evolution of domain-specific modeling
	environments},
  booktitle = {Engineering of Computer-Based Systems, 1999. Proceedings. ECBS '99.
	IEEE Conference and Workshop on},
  year = {1999},
  pages = {68 -74},
  month = {mar},
  abstract = {Model integrated computing (MIC) is gaining increased attention as
	an effective and efficient method for developing, maintaining, and
	evolving large-scale, domain-specific software applications for computer-based
	systems. MIC is a model-based approach to software development, allowing
	the synthesis of application programs from models created using customized,
	domain-specific model integrated program synthesis (MIPS) environments.
	Until now, these MIPS environments have been handcrafted. Analysis
	has shown that it is possible to ldquo;model the modeling environment
	rdquo; by creating a metamodel that specifies both the syntactic
	and semantic behavior of the desired domain-specific MIPS environment
	(DSME). Such a metamodel could then be used to synthesize the DSME
	itself allowing the entire design environment to safely and efficiently
	evolve in the face of changing domain requirements. This paper discusses
	the use of the Unified Modeling Language and the Object Constraint
	Language to specify, such metamodels, and describes a method for
	incorporating these metamodels into the MultiGraph Architecture,
	a MIPS creation toolset},
  doi = {10.1109/ECBS.1999.755863},
  keywords = {MIPS creation toolset;MultiGraph Architecture;Object Constraint Language;Unified
	Modeling Language;domain-specific software;model integrated computing;model
	integrated program synthesis;software development;computer aided
	software engineering;software engineering;software tools;}
}

@INPROCEEDINGS{584686,
  author = {Nori, A.K. and Kumar, S.},
  title = {Bringing objects to the mainstream},
  booktitle = {Compcon '97. Proceedings, IEEE},
  year = {1997},
  pages = {136 -142},
  month = {feb},
  abstract = {Oracle provides an open type system that is consistent with ANSI SQL3
	and provides interoperability of SQL with C/C++, Java and CORBA data
	models. By providing native support for objects in the database and
	navigational access to database objects from different host languages,
	we are reducing the impedance mismatch between the applications and
	the database. The Oracle server also provides a database extensibility
	framework that allows a tight integration of domain-specific data
	and logic with the database server. This database extensibility is
	achieved in an open, safe and manageable fashion in a network-centric
	computing architecture},
  doi = {10.1109/CMPCON.1997.584686},
  keywords = {ANSI SQL3;C language;C++ language;CORBA data models;Java;Oracle;SQL;database
	extensibility framework;database server;domain-specific data;host
	languages;impedance mismatch;interoperability;logic;native support;navigational
	access;network-centric computing architecture;object-oriented database;open
	type system;C language;SQL;abstract data types;distributed databases;file
	servers;object-oriented databases;object-oriented languages;open
	systems;relational databases;}
}

@INPROCEEDINGS{5464138,
  author = {Noubissi, A.C. and Iguchi-Cartigny, J. and Lanet, J.-L.},
  title = {Incremental Dynamic Update for Java-Based Smart Cards},
  booktitle = {Systems (ICONS), 2010 Fifth International Conference on},
  year = {2010},
  pages = {110 -113},
  month = {april},
  abstract = {One of the most appealing feature for multi-application smart cards
	is their ability to dynamically download or delete applications once
	the card has been issued. Applications can be updated by deleting
	old versions and loading the new ones. Nevertheless, for system components,
	the update is sligthly more complex because the systems never stop.
	Indeed, for smart cards based on Java called JavaCard, the virtual
	machine has a life cycle similar to the card because persistent objects
	are preserved after the communication sessions with the reader have
	expired. We present in this paper, our research in dynamic system
	components updating of JavaCard. Our technique requires a lot of
	off-card and on-card mechanisms. Our approach uses control flow graph
	to determine change between versions, a domain specific language
	to represent the change for minimization of the download overhead
	throughout the communication link with the card.},
  doi = {10.1109/ICONS.2010.27},
  keywords = {Java-based smart cards;JavaCard;incremental dynamic update;multiapplication
	smart cards;off-card mechanism;on-card mechanism;virtual machine;Java;smart
	cards;software engineering;virtual machines;}
}

@INPROCEEDINGS{1647580,
  author = {O'Connor, M. and Shankar, R. and Das, A.},
  title = {An Ontology-Driven Mediator for Querying Time-Oriented Biomedical
	Data},
  booktitle = {Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International
	Symposium on},
  year = {2006},
  pages = {264 -269},
  month = {0-0 },
  abstract = {Most biomedical research databases contain considerable amounts of
	time-oriented data. However, temporal knowledge about the contextual
	meaning of such data is not usually represented in a principled fashion.
	As a result, investigators often develop custom techniques for temporal
	data analysis that are difficult to reuse. We addressed this problem
	by developing a set of knowledge-driven methods and tools for temporally
	representing and querying biomedical data, and have integrated them
	using a mediator approach. A central issue driving our work is a
	need to integrate temporal representations of data in relational
	databases with the domain-specific semantics of temporal patterns
	used in querying. This paper presents a formal temporal knowledge
	model using the semantic Web ontology and rule languages, OWL and
	SWRL, respectively. The model informs the mediator of the temporal
	semantics used for data analysis. We show that our approach provides
	the computational foundation for much-needed software to make sense
	of complex temporal patterns in two biomedical research domains},
  doi = {10.1109/CBMS.2006.41},
  issn = {1063-7125},
  keywords = {OWL;SWRL;biomedical research databases;knowledge-driven methods;ontology-driven
	mediator;querying time-oriented biomedical data;relational databases;semantic
	Web ontology;temporal data analysis;knowledge based systems;medical
	computing;ontologies (artificial intelligence);query processing;relational
	databases;semantic Web;temporal databases;}
}

@INPROCEEDINGS{5402582,
  author = {O'Shea, K. and Bandar, Z. and Crockett, K.},
  title = {A semantic-based conversational agent framework},
  booktitle = {Internet Technology and Secured Transactions, 2009. ICITST 2009.
	International Conference for},
  year = {2009},
  pages = {1 -8},
  month = {nov.},
  abstract = {This paper focuses on the implementation of a novel semantic-based
	Conversational Agent (CA) framework. Traditional CA frameworks interpret
	scripts consisting of structural patterns of sentences. User input
	is matched against such patterns and an associated response is sent
	as output. This technique, which takes into account solely surface
	information, that is, the structural form of a sentence, requires
	the scripter to anticipate the inordinate ways that a user may send
	input. This is a tiresome and time-consuming process. As such, a
	semantic-based CA that interprets scripts consisting of natural language
	sentences will alleviate such burden. Using a pre-determined, domain-specific
	scenario, the CA was evaluated by participants indicating promising
	results.},
  keywords = {natural language sentences;script interpretation;semantic-based conversational
	agent framework;structural sentence patterns;user input;natural language
	processing;software agents;}
}

@INPROCEEDINGS{5069892,
  author = {Oberortner, E. and Zdun, U. and Dustdar, S.},
  title = {Tailoring a model-driven Quality-of-Service DSL for various stakeholders},
  booktitle = {Modeling in Software Engineering, 2009. MISE '09. ICSE Workshop on},
  year = {2009},
  pages = {20 -25},
  month = {may},
  abstract = {Many service-oriented business systems have to comply to various contracts
	and agreements. Multiple technical and non-technical stakeholders
	with different background and knowledge are involved in modeling
	such business concerns. In many cases, these concerns are only encoded
	in the technical models and implementations of the systems, making
	it hard for non-technical stakeholders to get involved in the modeling
	process. In this paper we propose to tackle this problem by providing
	model-driven Domain-specific Languages (DSL) for specifying the contracts
	and agreements, as well as an approach to separate these DSLs into
	sub-languages at different abstraction levels, where each sub-language
	is tailored for the appropriate stakeholders. We exemplify our approach
	by describing a Quality-of-Service (QoS) DSL which can be used to
	describe Service Level Agreements (SLA). This work provides insights
	into how DSLs can be utilized to model and enrich service-oriented
	business systems with concerns defined in contracts and agreements.},
  doi = {10.1109/MISE.2009.5069892},
  keywords = {Web service;contract specification;domain-specific language;model-driven
	quality-of-service DSL;model-driven software development;service
	level agreement;service-oriented business system;Web services;business
	data processing;contracts;object-oriented programming;quality of
	service;specification languages;}
}

@INPROCEEDINGS{5707172,
  author = {Oberortner, E. and Zdun, U. and Dustdar, S. and Cavalcante, A.B.
	and Tluczek, M.},
  title = {Supporting the evolution of model-driven service-oriented systems:
	A case study on QoS-aware process-driven SOAs},
  booktitle = {Service-Oriented Computing and Applications (SOCA), 2010 IEEE International
	Conference on},
  year = {2010},
  pages = {1 -4},
  month = {dec.},
  abstract = {Process-driven service-oriented architectures (SOA) need to cope with
	constant changing requirements of various compliance requirements,
	such as quality of service (QoS) constraints within service level
	agreements (SLA). To the best of our knowledge, only little evidence
	is available if and in how far process-driven SOAs deal with the
	evolution of the requirements. In this work, we evaluate an incremental
	and model-driven development approach on the evolution of the requirements
	and the domain model in the context of an industrial case study.
	The case study focuses on advanced telecom services that need to
	be compliant to QoS constraints. This paper answers questions about
	the applicability of the incremental development approach, the impact
	of requirement changes, possible drawbacks of using a non-incremental
	development approach, and general recommendations based on the findings.
	Our results provide guidelines for dealing with the evolution of
	model-driven service-oriented systems.},
  doi = {10.1109/SOCA.2010.5707172},
  keywords = {QoS constraint;QoS-aware process-driven SOA;compliance requirement;incremental
	development;model-driven development;model-driven service-oriented
	system;quality of service;requirement change;service level agreement;service-oriented
	architecture;telecom service;formal specification;quality of service;service-oriented
	architecture;}
}

@INPROCEEDINGS{5447483,
  author = {de Oliveira Castro, P. and Louise, S. and Barthou, D.},
  title = {A Multidimensional Array Slicing DSL for Stream Programming},
  booktitle = {Complex, Intelligent and Software Intensive Systems (CISIS), 2010
	International Conference on},
  year = {2010},
  pages = {913 -918},
  month = {feb.},
  abstract = {Stream languages offer a simple multi-core programming model and achieve
	good performance. Yet expressing data rearrangement patterns (like
	a matrix block decomposition) in these languages is verbose and error
	prone. In this paper, we propose a high-level programming language
	to elegantly describe n-dimensional data reorganization patterns.
	We show how to compile it to stream languages.},
  doi = {10.1109/CISIS.2010.135},
  keywords = {Domain Specific Language;data rearrangement patterns;error prone;high-level
	programming language;multi-core programming model;multidimensional
	array slicing DSL;n-dimensional data reorganization patterns;stream
	languages;stream programming;high level languages;parallel programming;program
	slicing;}
}

@INPROCEEDINGS{5625665,
  author = {Oliveira, A.R. and Arajo, J. and Amaral, V.},
  title = {The VisualAORE DSL},
  booktitle = {Requirements Engineering Visualization (REV), 2010 Fifth International
	Workshop on},
  year = {2010},
  pages = {11 -19},
  month = {sept.},
  abstract = {Aspect-Oriented Requirements Engineering consists of identifying,
	modularizing, specifying, and composing crosscutting concerns, also
	known as aspects. AORE is a pioneer systematic approach used to discover
	and structure requirements based on viewpoints and aspects. One of
	its limitations to be widely adopted is due to the fact that it lacks
	visual support to improve its usability. This paper describes a new
	Eclipse plug-in entitled VisualAORE. It is based on a DSL (Domain
	Specific Language) representing all the concepts of the AORE domain,
	allowing an implementation of an editor to specify models using the
	defined visual language. The work presented in this paper is a major
	contribution to AORE, since it replaces the traditional AORE's XML
	textual representation by a graphical one. This contribution helps
	software engineers to decrease AORE's model specification time, improving
	its understanding and usability.},
  doi = {10.1109/REV.2010.5625665},
  issn = {2157-0256},
  keywords = {AORE domain;VisualAORE DSL;XML textual representation;aspect oriented
	requirement engineering;domain specific language;eclipse plugin entitled
	VisualAORE;model specification;pioneer systematic approach;software
	engineer;structure requirement;visual language;XML;aspect-oriented
	programming;data structures;formal specification;formal verification;specification
	languages;systems analysis;visual languages;}
}

@INPROCEEDINGS{5352762,
  author = {Oliveira, N. and Henriques, P.R. and da Cruz, D. and Varanda Pereira,
	M.J. and Mernik, M. and Kosar, T. and Crepinsek, M.},
  title = {Applying program comprehension techniques to karel robot programs},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {699 -706},
  month = {oct.},
  abstract = {In the context of program understanding, a challenge research topic1
	is to learn how techniques and tools for the comprehension of General-Purpose
	Languages (GPLs) can be used or adjusted to the understanding of
	Domain-Specific Languages (DSLs). Being DSLs tailored for the description
	of problems within a specific domain, it becomes easier to improve
	these tools with specific visualizations (at a higher abstraction
	level, closer to the problem level) in order to understand the DSLs
	programs. In this paper, comprehension techniques will be applied
	to Karel language. This will allow us to explore the creation of
	problem domain visualizations for this language and to combine both
	problem and program domains in order to reach a full understanding
	of Karel programs.},
  doi = {10.1109/IMCSIT.2009.5352762},
  keywords = {Karel language;Karel robot programs;domain-specific languages;general-purpose
	languages;problem domain visualizations;program comprehension techniques;program
	understanding;program visualisation;reverse engineering;robot programming;}
}

@INPROCEEDINGS{5336426,
  author = {Oliveira, N. and Pereira, M.J.V. and Henriques, P.R. and da Cruz,
	D.},
  title = {Visualization of domain-specific programs' behavior},
  booktitle = {Visualizing Software for Understanding and Analysis, 2009. VISSOFT
	2009. 5th IEEE International Workshop on},
  year = {2009},
  pages = {37 -40},
  month = {sept.},
  abstract = {Program domain concepts are rather complex and low level for a fast
	assimilation. On the other hand, problem domain concepts are closer
	to human's mind, hence they are easier to perceive. Based on Brook's
	theory, a full comprehension of a program is only achieved if both
	domains are connected and visualized in synchronization, resulting
	on an action-effect visualization. Domain-specific languages, as
	languages tailored for a specific class of problems, raise the abstraction
	of the program domain concepts and approximate them to the problem
	domain's. This way, a systematic approach can be used to perform
	the action-effect visualization of a program written in a domain-specific
	language. In this paper, we use a domain-specific language to exemplify
	how the concepts involved in both domains are visualized and how
	it is possible to map each problem domain situation (depicted by
	images) to the program domain operations.},
  doi = {10.1109/VISSOF.2009.5336426},
  keywords = {Brooks theory;action-effect visualization;domain specific languages;domain-specific
	programs behavior;data visualisation;program visualisation;specification
	languages;}
}

@INPROCEEDINGS{674159,
  author = {Onder, S. and Gupta, R.},
  title = {Automatic generation of microarchitecture simulators},
  booktitle = {Computer Languages, 1998. Proceedings. 1998 International Conference
	on},
  year = {1998},
  pages = {80 -89},
  month = {may},
  abstract = {We describe the UPFAST system that automatically generates a cycle
	level simulator, an assembler and a disassembler from a microarchitecture
	specification written in a domain specific language called the Architecture
	Description Language (ADL). Using the UPFAST system, it is easy to
	retarget a simulator for an existing architecture to a modified architecture
	since one has to simply modify the input specification and the new
	simulator is generated automatically. UPFAST also allows porting
	of simulators to different platforms with minimal effort. We have
	been able to develop three simulators ranging from simple pipelined
	processors to complicated out-of-order issue processors over a short
	period of three months. While the specifications of the architectures
	varied from 5000 to 6000 lines of ADL code, the sizes of automatically
	generated software varied from 20000 to 300000 lines of C++ code.
	The automatically generated simulators are less than 2 times slower
	than hand coded simulators for similar architectures},
  doi = {10.1109/ICCL.1998.674159},
  issn = {1074-8970},
  keywords = {ADL code;Architecture Description Language;C++ code;UPFAST system;assembler;automatic
	generation;automatically generated simulators;automatically generated
	software;complicated out-of-order issue processors;cycle level simulator;disassembler;domain
	specific language;input specification;microarchitecture simulators;modified
	architecture;porting;simple pipelined processors;automatic programming;computer
	architecture;formal specification;high level languages;object-oriented
	programming;program assemblers;virtual machines;}
}

@INPROCEEDINGS{5629077,
  author = {Opdahl, A.L.},
  title = {A Platform for Interoperable Domain-Specific Enterprise Modelling
	Based on ISO 15926},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops (EDOCW),
	2010 14th IEEE International},
  year = {2010},
  pages = {301 -310},
  month = {oct.},
  abstract = {The paper shows how to describe the semantics of domain-specific enterprise
	and information system (IS) modelling languages using the Unified
	Enterprise Modelling Language (UEML). The approach is illustrated
	with a fragment of a domain-specific modelling language based on
	the new ISO 15926 standard for data integration, sharing and hand-over
	between computer systems. The aim is to ensure that the semantics
	of new domain-specific languages (DSLs) can be quickly, easily and
	precisely defined in relation both to one another and to existing
	general-purpose languages.},
  doi = {10.1109/EDOCW.2010.29},
  keywords = {ISO 15926 standard;Unified Enterprise Modelling Language;data hand;data
	integration;data sharing;domain-specific modelling language;information
	system modelling languages;interoperable domain-specific enterprise
	modelling;Unified Modeling Language;information systems;open systems;}
}

@INPROCEEDINGS{4639378,
  author = {Ordonez Camacho, D. and Kim Mens},
  title = {APPAREIL: A Tool for Building Automated Program Translators Using
	Annotated Grammars},
  booktitle = {Automated Software Engineering, 2008. ASE 2008. 23rd IEEE/ACM International
	Conference on},
  year = {2008},
  pages = {489 -490},
  month = {sept.},
  abstract = {Operations languages are used to write spacecraft operations procedures.
	The APPAREIL tool automates the process of generating program translators
	between operations languages, from a specification of their language
	grammar annotated with extra information. From these annotated grammars
	the tool automatically produces a partial translator that covers
	most of the translation. This translator needs to be augmented manually
	with specific transformations, to deal with the more complicated
	cases. To get more confidence on the correctness of the translation,
	the tool offers a control-flow equivalence verification module.},
  doi = {10.1109/ASE.2008.85},
  issn = {1527-1366},
  keywords = {APPAREIL tool;annotated grammar;automated program translator;control-flow
	equivalence verification module;operations language;spacecraft application;aerospace
	computing;grammars;program interpreters;specification languages;}
}

@INPROCEEDINGS{1604742,
  author = { Ortiz-Cornejo, A.I. and Cuayahuitl, H. and Perez-Corona, C.},
  title = {WISBuilder: A Framework for Facilitating Development of Web-Based
	Information Systems},
  booktitle = {Electronics, Communications and Computers, 2006. CONIELECOMP 2006.
	16th International Conference on},
  year = {2006},
  pages = { 46},
  month = {feb.},
  abstract = { This paper presents WISBuilder, a framework that investigates an
	approach for facilitating the development of Web-based Information
	Systems. The approach is based on the Model-View-Controller and Generative
	Programming paradigms. Firstly, the WISBuilder framework applies
	the following separation of tasks: structure, business-logic and
	visual style. The system specification is performed with two high-level
	XML-based languages: WSML and WAML. Secondly, WISBuilder proposes
	the creation of reusable code templates for each high-level language.
	Thirdly, WISBuilder uses predefined code templates for expanding
	the system specification in high-level annotations. The goals of
	this framework are twofold: to speed-up development by writing less
	code and development in parallel; and to promote software reusability
	by reusing generic code templates. Experimental results with three
	small-scale Webbased information systems show an important reduction
	of programming effort, using the proposed framework in comparison
	with an equivalent manual coding. These results show initial evidence
	that software development based on both paradigms is a good practice.},
  doi = {10.1109/CONIELECOMP.2006.65}
}

@INPROCEEDINGS{5573178,
  author = {Oubahssi, L. and Laforcade, P. and Cottier, P.},
  title = {Re-engineering of the Apprenticeship Electronic Booklet: Adaptation
	to New Users Requirements},
  booktitle = {Advanced Learning Technologies (ICALT), 2010 IEEE 10th International
	Conference on},
  year = {2010},
  pages = {511 -515},
  month = {july},
  abstract = {In this article we illustrate and discuss a techno-centric aspect
	of re-engineering realized on an existent TEL system: the Apprenticeship
	Electronic Booklet. Although this system has been designed with end-users
	following a participatory process, the first version had also been
	found too rigid in regard to the roles management and to the underlying
	academic structures. In order to improve this TEL system, two approaches
	of re-engineering have been conducted. The first solution focuses
	on an internal modification of the system functionalities and parametrization
	facilities. The second re-engineering work follows a Domain-Specific
	Modeling approach that led us to propose a graphical editor communicating
	with the TEL system. This external component aims to provide end-users
	with a more user-friendly facility to configure booklets. This second
	work is particularly illustrated and discussed in regard to the promising
	underlying re-engineering process.},
  doi = {10.1109/ICALT.2010.146},
  keywords = {TEL system;apprenticeship electronic booklet;domain-specific modeling
	approach;end-user system;graphical editor;model driven engineering;re-engineering;techno-centric
	aspect;technology-enhanced-learning system;user-friendly facility;computer
	aided instruction;electronic publishing;human computer interaction;personal
	computing;software engineering;systems re-engineering;visual languages;}
}

@INPROCEEDINGS{5992302,
  author = {Ouraiba, E.A. and Choquet, C. and Cottier, P.},
  title = {Domain-Specific Modeling Approach to Support Instructional Design
	Rationale},
  booktitle = {Advanced Learning Technologies (ICALT), 2011 11th IEEE International
	Conference on},
  year = {2011},
  pages = {205 -206},
  month = {july},
  abstract = {We investigate in this paper the instructional design rationale of
	open pedagogical scenarios. We use the Domain-Specific Modeling (DSM)
	approach for proposing Domain-Specific Educational Modeling Languages
	(DSEML) and dedicated editors to teachers/designers in order to allow
	them to work at a high-level of abstraction. Thanks to EMF tools
	we have realized a first implementation of our proposal about learning
	sessions of Hop3x. The Hop3x's DSEML is described by a metamodel
	and accordingly a first version of specific editor has been generated.},
  doi = {10.1109/ICALT.2011.66},
  issn = {2161-3761},
  keywords = {DSEML;EMF tools;abstraction;domain-specific modeling;educational modeling
	languages;instructional design rationale;learning sessions;open pedagogical
	scenarios;computer aided instruction;simulation languages;}
}

@INPROCEEDINGS{5571300,
  author = {Ouraiba, E.-A. and Choquet, C. and Cottier, P. and Desprs,
	C. and Jacoboni, P.},
  title = {Engineering of Open Learning Scenarios - The Case of Hop3x Learning
	Scenarios},
  booktitle = {Advanced Learning Technologies (ICALT), 2010 IEEE 10th International
	Conference on},
  year = {2010},
  pages = {264 -268},
  month = {july},
  abstract = {Within an educational context, adaptation could improve the learning's
	quality. Many researches are done in the learning-situations adaptation
	field. The Educational Modeling Languages (EML) and tools provided
	currently to the teacher for a learning design, as preexistent means
	in our sense, remain useless by practitioners. In our work we aim
	to support the practitioner teacher to design and adapt his/her learning
	scenarios which are by nature open, influenced by the context in
	which they are executed. The approach adopted is the combination
	between Model-Driven Engineering (MDE) and Domain-Specific Modeling
	(DSM). In this paper we present a model of open learning scenario.
	So we take the learning scenarios of Hop3x as a case study.},
  doi = {10.1109/ICALT.2010.78},
  keywords = {Hop3x learning scenario;domain-specific modeling;educational context;educational
	modeling languages;model-driven engineering;open learning scenarios;practitioner
	teacher support;computer aided instruction;digital simulation;software
	engineering;}
}

@INPROCEEDINGS{5753609,
  author = {Owens, H. and Hill, J.H.},
  title = {Generating Valid Interface Definition Language from Succinct Models},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing
	(ISORC), 2011 14th IEEE International Symposium on},
  year = {2011},
  pages = {205 -212},
  month = {march},
  abstract = {Source code generation from models (e.g., domain specific models)
	for distributed real-time and embedded (DRE) systems is intended
	to alleviate tedious, error-prone, and time consume tasks associated
	with manually hand-crafting the same code. When generating code from
	models for DRE system programming languages that accidentally support
	circular dependencies, e.g., the Interface Definition Language (IDL)
	and C++, it is necessary to resolve circular dependencies in order
	to generate valid and usable code. Moreover, it is important to do
	some automatically instead of requiring modelers to construct models
	that do not contain any circular dependencies, which is hard. This
	paper provides two contributions to research on source code generation
	from models for DRE systems. First, it presents A-Circle, an algorithm
	that automatically removes circular dependencies when generating
	source code from models for programming languages that inherently
	enable circular dependencies. Secondly, this paper quantitatively
	evaluates A-Circle when generating CORBA IDL files. The results show
	that A Circle algorithm is able to generate IDL files in linear-time.},
  doi = {10.1109/ISORC.2011.33},
  issn = {1555-0885},
  keywords = {A Circle algorithm;C++;CORBA IDL files;DRE system programming languages;automated
	circular dependency resolver;distributed real-time systems;embedded
	systems;source code generation;succinct models;valid interface definition
	language generation;C++ language;embedded systems;program compilers;}
}

@INPROCEEDINGS{5601890,
  author = {zhan, G. and Din, A.C. and Oguztzn, H.},
  title = {Model-Integrated Development of Field Artillery Federation Object
	Model},
  booktitle = {Advances in System Simulation (SIMUL), 2010 Second International
	Conference on},
  year = {2010},
  pages = {109 -114},
  month = {aug.},
  abstract = {This paper presents the automatic transformation of a Field Artillery
	Conceptual Data Model (FADM) into a High Level Architecture (HLA)
	Object Model Template (OMT) Model (HOM). It is part of a series of
	transformations from field artillery mission space to federation
	architecture to executable distributed simulation code. The approach
	followed in the course of this work adheres to the Model-Driven Engineering
	(MDE) philosophy. The model transformation is carried out with the
	Graph Rewriting and Transformation (GReAT) tool and partly hand-coded
	in C++. Given a conceptual data model, a simulation object model
	is produced, tailored according to one of the three object class
	generation schemes, expounded in the paper.},
  doi = {10.1109/SIMUL.2010.27},
  keywords = {C++;field artillery conceptual data model;field artillery federation
	object model;graph rewriting and transformation tool;high level architecture;model
	driven engineering;model integrated development;object class generation
	schemes;object model template;C++ language;graph theory;software
	engineering;}
}

@INPROCEEDINGS{5558166,
  author = {Pace, G.J. and Tabone, C.},
  title = {Meta-functional Languages for Hardware Design and Verification},
  booktitle = {Advances in Circuits, Electronics and Micro-Electronics (CENICS),
	2010 Third International Conference on},
  year = {2010},
  pages = {45 -50},
  month = {july},
  abstract = {General purpose functional languages have been widely used as host
	languages for the embedding of domain specific languages, especially
	for hardware description languages. The embedding approach provides
	various abstraction techniques, enabling the description of generators
	for whole families of circuits, in particular parameterised regular
	circuits. The two-stage language setting that is achieved by means
	of embedding, provides a means to reason about the generated circuits
	as data objects within the host language. Nonetheless, these circuit
	objects lack information about their generators, and about the manner
	in which these where generated. In this paper, we explore the use
	of a meta-programming language to extend the embedding approach thus
	enabling us to access the underlying circuit generators, and not
	just the circuits themselves. We show the applicability of this approach
	by using circuit generator analysis techniques to extract information
	from a hardware compiler to enable verification, through the use
	of model-checking, of compiler invariants. The main contribution
	of this paper is to show how automatic verification of whole families
	of circuits can be used in an embedded language setting to verify
	hardware compiler invariants.},
  doi = {10.1109/CENICS.2010.15},
  keywords = {circuit generator analysis;hardware compiler;hardware description
	languages;hardware design;meta-functional languages;meta-programming
	language;model-checking;verification;functional languages;program
	verification;}
}

@INPROCEEDINGS{1008052,
  author = {Pace, J.A.D. and Campo, M.R.},
  title = {An object-oriented bridge among architectural styles, aspects and
	frameworks},
  booktitle = {Software Engineering, 2002. ICSE 2002. Proceedings of the 24rd International
	Conference on},
  year = {2002},
  pages = {717},
  month = {may},
  abstract = {Summary form only given. Proposes an architecture-driven design approach
	based on the concept of proto-frameworks, aiming to provide an intermediate
	stage in the transition from architectural models to object-oriented
	frameworks or applications. The approach relies on an object-oriented
	materialization of domain-specific architectures derived from domain
	models, i.e. the production of concrete computational representations
	of abstract architectural descriptions using object-oriented terminology.
	A proto-framework materializes, in object-oriented terms, the infrastructure
	required for cooperation and communication of each architectural
	component type. The framework gives abstract hooks to map specific
	domain components into a class hierarchy in a white-box fashion.
	This mapping can produce a specific application, but it can also
	produce new domain-specific frameworks that adopt the underlying
	architectural model. In the proposed approach, we can basically identify
	two stages. First, developers should figure out the problem architecture;
	aspects are initially mapped to architectural constructs, instead
	of being coded using framework language constructs. Second, the approach
	enables a materialization into a proto-framework, and then several
	kinds of frameworks implementations. These frameworks retain the
	properties inherited from the original architecture.},
  keywords = {abstract architectural descriptions;architectural component types;architectural
	models;aspect-oriented programming;class hierarchy;computational
	representations;domain models;domain-specific architectures;inheritance;object-oriented
	bridge;object-oriented frameworks;object-oriented materialization;object-oriented
	terminology;proto-frameworks;software architectural styles;software
	architecture-driven design approach;inheritance;object-oriented methods;object-oriented
	programming;software architecture;}
}

@INPROCEEDINGS{4278794,
  author = {Padmanabhuni, S. and Kunti, K. and Lipika Sahoo and Bharti, S.},
  title = {Coupling RDF/RSS, WSRP and AJAX for Dynamic Reusable Portlets: An
	Approach and a Use Case},
  booktitle = {Services, 2007 IEEE Congress on},
  year = {2007},
  pages = {175 -182},
  month = {july},
  abstract = {XML vocabularies like RSS (really simple syndication) or domain specific
	syntactic syndication frameworks allow for creation of raw data that
	can be understood by a wide range of consuming portals and Web sites.
	Often individual aggregators create content from such feeds however,
	every single aggregator needs to render UI content separately. Avoiding
	this via reuse requires creation of presentation-oriented services
	for dynamic generation of content from same base XML data. Web services
	for remote portlets (WSRP) provides standards based interfaces for
	creation of presentation oriented services using Web services technology.However,
	any WSRP based remote portlet needs to be refreshed every time when
	even a single data element changes in the portlet. Complementarity,
	technologies like Asynchronous JavaScript and XML (AJAX) also allow
	for content aggregation and asynchronous data retrieval. Unlike WSRP,
	in AJAX only the required control needs to be refreshed, whenever
	data changes. In AJAX too, every aggregator is required to include
	AJAX control accessing the same back end service leading to considerable
	replication of effort. These two technologies complement each other
	in terms of ease of sharing of aggregated content and better usability
	of such content. Overall, often there is a requirement to dynamically
	generate UI content from XML data, customize and share aggregated
	content with remote consumers and finally allow for partial updates
	of remote aggregated content for better usability. We take a holistic
	view of the problem statement and propose an end to end architectural
	approach that combines usage of WSRP, AJAX along with XML syndication
	feeds like RSS for creation of standards based, customizable, and
	dynamically generated reusable UI that has better interactivity,
	speed and usability.},
  doi = {10.1109/SERVICES.2007.26},
  keywords = {AJAX;Asynchronous JavaScript;RDF;Web services for remote portlets;Web
	sites;XML vocabularies;asynchronous data retrieval;content aggregation;domain
	specific syntactic syndication frameworks;dynamic reusable portlets;presentation-oriented
	services;really simple syndication;user interface content;Java;Web
	services;XML;authoring languages;portals;software reusability;user
	interfaces;vocabulary;}
}

@INPROCEEDINGS{5090524,
  author = {Paige, R.F. and Kolovos, D.S. and Rose, L.M. and Drivalos, N. and
	Polack, F.A.C.},
  title = {The Design of a Conceptual Framework and Technical Infrastructure
	for Model Management Language Engineering},
  booktitle = {Engineering of Complex Computer Systems, 2009 14th IEEE International
	Conference on},
  year = {2009},
  pages = {162 -171},
  month = {june},
  abstract = {Model management is the discipline of managing artefacts used in Model-Driven
	Engineering (MDE). A model management framework defines and implements
	the operations (such as transformation or code generation) required
	to manipulate MDE artefacts. Modern approaches to model management
	generally implement these operations via domain-specific languages
	(DSLs). This paper presents and compares the principles behind three
	approaches to implementing DSLs for model management and identifies
	some of the key differences between DSL engineering in general and
	for model management. It then shows how theory relates to practice
	by illustrating how DSL design and implementation approaches have
	been used in practice to build working languages from the Epsilon
	model management framework. A set of questions for guiding the development
	of new model management DSLs is summarised, and data on development
	costs for the different approaches is presented.},
  doi = {10.1109/ICECCS.2009.14},
  keywords = {conceptual framework design;domain-specific language;model management
	language engineering;model-driven engineering;technical infrastructure;object-oriented
	programming;specification languages;}
}

@ARTICLE{4302730,
  author = {Pakin, S.},
  title = {The Design and Implementation of a Domain-Specific Language for Network
	Performance Testing},
  journal = {Parallel and Distributed Systems, IEEE Transactions on},
  year = {2007},
  volume = {18},
  pages = {1436 -1449},
  number = {10},
  month = {oct. },
  abstract = {CONCEPTUAL is a toolset designed specifically to help measure the
	performance of high-speed interconnection networks such as those
	used in workstation clusters and parallel computers. It centers around
	a high-level domain-specific language, which makes it easy for a
	programmer to express, measure, and report the performance of complex
	communication patterns. The primary challenge in implementing a compiler
	for such a language is that the generated code must be extremely
	efficient so as not to misattribute overhead costs to the messaging
	library. At the same time, the language itself must not sacrifice
	expressiveness for compiler efficiency, or there would be little
	point in using a high-level language for performance testing. This
	paper describes the CONCEPTUAL language and the CONCEPTUAL compiler's
	novel code-generation framework. The language provides primitives
	for a wide variety of idioms needed for performance testing and emphasizes
	a readable syntax. The core code-generation technique, based on unrolling
	CONCEPTUAL programs into sequences of communication events, is simple
	yet enables the efficient implementation of a variety of high-level
	constructs. The paper further explains how CONCEPTUAL implements
	time-bounded loops - even those that comprise blocking communication
	- in the absence of a time-out mechanism as this is a somewhat unique
	language/implementation feature.},
  doi = {10.1109/TPDS.2007.1065},
  issn = {1045-9219},
  keywords = {CONCEPTUAL language;blocking communication;code generation;domain-specific
	language;high-level language;high-speed interconnection networks;messaging
	library;network performance testing;parallel computers;program compiler;readable
	syntax;time-bounded loops;time-out mechanism;workstation clusters;computational
	linguistics;high level languages;message passing;program compilers;program
	control structures;program testing;software performance evaluation;specification
	languages;}
}

@INPROCEEDINGS{1303014,
  author = {Pakin, S.},
  title = {CONCEPTUAL: a network correctness and performance testing language},
  booktitle = {Parallel and Distributed Processing Symposium, 2004. Proceedings.
	18th International},
  year = {2004},
  pages = { 79},
  month = {april},
  abstract = {Summary form only given. We introduce a new, domain-specific specification
	language called CONCEPTUAL. CONCEPTUAL enables the expression of
	sophisticated communication benchmarks and network validation tests
	in comparatively few lines of code. Besides helping programmers save
	time writing and debugging code, CONCEPTUAL addresses the important-but
	largely unrecognized-problem of benchmark opacity. Benchmark opacity
	refers to the current impracticality of presenting performance measurements
	in a manner that promotes reproducibility and independent evaluation
	of the results. For example, stating that a performance graph was
	produced by a "bandwidth" test says nothing about whether that test
	measures the data rate during a round-trip transmission or the average
	data rate over a number of back-to-back unidirectional messages;
	whether the benchmark preregisters buffers, sends warm-up messages,
	and/or preposts asynchronous receives before starting the clock;
	how many runs were performed and whether these were aggregated by
	taking the mean, median, or maximum; or, even whether a data unit
	such as "MB/s" indicates 106 or 220 bytes per second. Because CONCEPTUAL
	programs are terse, a benchmark's complete source code can be listed
	alongside performance results, making explicit all of the design
	decisions that went into the benchmark program. Because CONCEPTUAL's
	grammar is English-like, CONCEPTUAL programs can easily be understood
	by nonexperts. And because CONCEPTUAL is a high-level language, it
	can target a variety of messaging layers and networks, enabling fair
	and accurate performance comparisons.},
  doi = {10.1109/IPDPS.2004.1303014},
  keywords = { CONCEPTUAL performance testing language; back-to-back unidirectional
	messages; bandwidth test; benchmark preregisters buffer; debugging
	code; design decision; domain-specific specification language; network
	correctness; network validation tests; round-trip transmission; sophisticated
	communication benchmarks; warm-up messages; message passing; performance
	evaluation; program debugging; specification languages;}
}

@INPROCEEDINGS{5671416,
  author = {Pandit, S. and Honavar, V.},
  title = {Ontology-guided Extraction of Complex Nested Relationships},
  booktitle = {Tools with Artificial Intelligence (ICTAI), 2010 22nd IEEE International
	Conference on},
  year = {2010},
  volume = {2},
  pages = {173 -178},
  month = {oct.},
  abstract = {Many applications call for methods to enable automatic extraction
	of structured information from unstructured natural language text.
	Due to inherent challenges of natural language processing, most of
	the existing methods for information extraction from text tend to
	be domain specific. We explore a modular ontology-based approach
	to information extraction that decouples domain-specific knowledge
	from the rules used for information extraction. We describe a framework
	for extraction of a subset of complex nested relationships (e.g.,
	Joe reports that Jim is a reliable employee). The extracted relationships
	are output in the form of sets of RDF (resource description framework)
	triples, which can be queried using query languages for RDF and mined
	for knowledge acquisition.},
  doi = {10.1109/ICTAI.2010.98},
  issn = {1082-3409},
  keywords = {complex nested relationships;domain specific knowledge;information
	extraction;knowledge acquisition;natural language processing;ontology
	guided extraction;query languages;resource description framework;unstructured
	natural language text;knowledge acquisition;natural language processing;ontologies
	(artificial intelligence);query languages;}
}

@INPROCEEDINGS{1046372,
  author = {Pane, J.F. and Myers, B.A. and Miller, L.B.},
  title = {Using HCI techniques to design a more usable programming system},
  booktitle = {Human Centric Computing Languages and Environments, 2002. Proceedings.
	IEEE 2002 Symposia on},
  year = {2002},
  pages = { 198 - 206},
  abstract = { A programming system is the user interface between the programmer
	and the computer. Programming is a notoriously difficult activity,
	and some of this difficulty can be attributed to the user interface
	as opposed to other factors. Historically, the designs of programming
	languages and tools have not emphasized usability. This paper describes
	the process we used to design HANDS, a new programming system for
	children that focuses on usability, where HCI knowledge, principles,
	and methods guided all design decisions. The features of HANDS are
	presented along with their motivations from prior empirical research
	on programmers and new studies conducted by the authors. HANDS is
	an event-based language that features a concrete model for computation,
	provides operators that match the way non-programmers express problem
	solutions, and includes domain-specific features for the creation
	of interactive animations and simulations. In user tests, children
	using HANDS performed significantly better than children using a
	reduced-feature version of the system where more traditional methods
	were required to solve tasks.},
  doi = {10.1109/HCC.2002.1046372},
  issn = { },
  keywords = { HANDS; HCI techniques; event-based language; programming system;
	usable programming system; user interface; computer aided instruction;
	graphical user interfaces; visual languages; visual programming;}
}

@INPROCEEDINGS{5381001,
  author = {Wenbo Pang and Xiaozhong Fan},
  title = {Inducing Gazetteer for Chinese Named Entity Recognition Based on
	Local High-Frequent Strings},
  booktitle = {Future Information Technology and Management Engineering, 2009. FITME
	'09. Second International Conference on},
  year = {2009},
  pages = {357 -360},
  month = {dec.},
  abstract = {Gazetteers, or entity dictionaries, are important for named entity
	recognition (NER). Although the dictionaries extracted automatically
	by the previous methods from a corpus, web or Wikipedia are very
	huge, they also misses some entities, especially the domain-specific
	entities. We present a novel method of automatic entity dictionary
	induction, which is able to construct a dictionary more specific
	to the processing text at a much lower computational cost than the
	previous methods. It extracts the local high-frequent strings in
	a document as candidate entities, and filters the invalid candidates
	with the accessor variety (AV) as our entity criterion. The experiments
	show that the obtained dictionary can effectively improve the performance
	of a high-precision baseline of NER.},
  doi = {10.1109/FITME.2009.95},
  keywords = {Chinese named entity recognition;accessor variety;automatic entity
	dictionary induction;gazetteer;information extraction;local high-frequent
	strings;natural language processing;natural language processing;}
}

@INPROCEEDINGS{5952287,
  author = {Papailiopoulou, V. and Potop-Butucaru, D. and Sorel, Y. and de Simone,
	R. and Besnard, L. and Talpin, J.},
  title = {From design-time concurrency to effective implementation parallelism:
	The multi-clock reactive case},
  booktitle = {Electronic System Level Synthesis Conference (ESLsyn), 2011},
  year = {2011},
  pages = {1 -6},
  month = {june},
  abstract = {We have defined a full design flow starting from high-level domain
	specific languages (Simulink, SCADE, AADL, SysML, MARTE, SystemC)
	and going all the way to the generation of deterministic concurrent
	(multi-threaded) executable code for (distributed) simulation or
	implementation. Based on the theory of weakly endochronous systems,
	our flow allows the automatic detection of potential parallelism
	in the functional specification, which is then used to allow the
	generation of concurrent (multi-thread) code for parallel, possibly
	distributed implementations.},
  doi = {10.1109/ESLsyn.2011.5952287},
  keywords = {AADL;MARTE;SCADE;Simulink;SysML;SystemC;design-time concurrency;deterministic
	concurrent executable code;distributed simulation;full design flow;high-level
	domain specific language;multiclock reactive case;multithreaded executable
	code;parallelism;weakly endochronous system;clocks;concurrency control;hardware
	description languages;high level synthesis;multi-threading;parallel
	processing;}
}

@INPROCEEDINGS{4570781,
  author = {Papakonstantinou, A. and Deming Chen and Wen-Mei Hwu},
  title = {Application Acceleration with the Explicitly Parallel Operations
	System - the EPOS Processor},
  booktitle = {Application Specific Processors, 2008. SASP 2008. Symposium on},
  year = {2008},
  pages = {20 -25},
  month = {june},
  abstract = {Different approaches have been proposed over the years for automatically
	transforming high-level-languages (HLL) descriptions of applications
	into custom hardware implementations. Most of these approaches however
	are confined by basic block level parallelism described within the
	CDFGs (control-data flow graphs). In this work we propose a new high-level
	synthesis flow which can leverage instruction-level parallelism (ILP)
	beyond the boundary of the basic blocks. We extract statistical parallelism
	from the applications through the use of Superblocks and Hyperblocks
	formed by advanced front-end compilation techniques. The output of
	the front-end compilation is then used in our high-level synthesis
	in order to map the application onto a new domain-specific architecture
	named EPOS (explicitly parallel operations system). EPOS is a stylized
	micro-code driven processor equipped with novel architectural features
	that help take advantage of the instruction-level parallelism generated
	in the front-end compilation. A novel forwarding-path optimization
	engine is also employed during the high-level synthesis flow in order
	to minimize the long interconnection wires and the multiplexers in
	the processor. To evaluate the EPOS processor, we compare its performance
	with a previous domain-specific processor NISC on a common set of
	benchmarks. Experimental results show that significant performance
	gain (3.45times on average) is obtained compared to NISC.},
  doi = {10.1109/SASP.2008.4570781},
  keywords = {EPOS processor;application acceleration;explicitly parallel operations
	system;forwarding-path optimization engine;front-end compilation
	techniques;high-level synthesis flow;instruction-level parallelism;microcode
	driven processor;statistical parallelism;firmware;high level synthesis;microprocessor
	chips;statistical analysis;}
}

@INPROCEEDINGS{4736341,
  author = {Paredis, C.J.J. and Johnson, T.},
  title = {Using OMG CHARPx2019;S SYSML to support simulation},
  booktitle = {Simulation Conference, 2008. WSC 2008. Winter},
  year = {2008},
  pages = {2350 -2352},
  month = {dec.},
  abstract = {Currently, system engineering problems are solved using a wide range
	of domain-specific models and corresponding languages. It is unlikely
	that a single unified modeling language will be able to model in
	sufficient detail the large number of system aspects addressed by
	these domain-specific languages. Instead, a model integration framework
	is needed for managing the various modeling languages used to solve
	systems engineering problems. The Systems Modeling Language (OMG
	SysMLTM) can provide an answer to this need for model integration.
	Using SysML, a modeler can abstract a domain-specific language to
	a level that permits its interaction with other system models. In
	addition, graph transformation approach can be use to accomplishing
	automated, bidirectional transformation between SysML and the domain
	specific language. In this paper, a generic approach for defining
	such graph transformations is presented.},
  doi = {10.1109/WSC.2008.4736341},
  keywords = {OMG SYSML;Systems Modeling Language;automated bidirectional transformation;domain-specific
	languages;domain-specific models;graph transformation;model integration
	framework;modeling language;system aspects;system engineering problem;graph
	grammars;specification languages;systems engineering;}
}

@INPROCEEDINGS{4493721,
  author = {Jung Choon Park and Yong Hoon Choi and Tae ho Kim},
  title = {Domain Specific Code Generation For Linux Device Driver},
  booktitle = {Advanced Communication Technology, 2008. ICACT 2008. 10th International
	Conference on},
  year = {2008},
  volume = {1},
  pages = {101 -104},
  month = {feb.},
  abstract = {Device driver is the most important software of operating system to
	interact with hardware devices. As an essential part of operating
	system, device drivers must be reliable and efficient, because wrong
	operation can make a fatal system error and hardware performance
	is depends on the device driver. Therefore, it must be developed
	carefully and considered to be difficult to develop. Previously,
	we presented an automated source code generation framework and tool
	for Linux device drivers. In this paper, we present enhanced code
	generation framework for domain-specific device driver by usage domain
	and show detailed view of implementation.},
  doi = {10.1109/ICACT.2008.4493721},
  issn = {1738-9445},
  keywords = {Linux;code generation;device driver;operating system;Linux;device
	drivers;operating systems (computers);program compilers;}
}

@INPROCEEDINGS{4338345,
  author = {Youngja Park and Ying Li},
  title = {Semantic Analysis for Topical Segmentation of Videos},
  booktitle = {Semantic Computing, 2007. ICSC 2007. International Conference on},
  year = {2007},
  pages = {161 -168},
  month = {sept.},
  abstract = {Topic segmentation of videos enables topic-based categorization, retrieval
	and browsing and also facilitates efficient video authoring. Existing
	video topic segmentation techniques, however, are domain specific
	to news or narrative videos while generic approaches based on video
	shot analysis generate too fine-grained micro-segments. This paper
	addresses this challenge through a multi-modal semantic analysis
	technique for recognizing topical segments. We analyze the content
	of a video by using textual and audio features such as keyword synonym
	sets, sentence boundary information, silence/music breaks and speech
	similarity. Specifically, we propose a new natural language processing
	(NLP) technique for constructing synonym sets from video transcripts.
	A synonym set is a list of domain- specific keywords that are semantically
	related and represent a topic. We align the synonym sets with audio
	cues to identify the topical segments. Our experiments with six instructional
	videos show that the system produced very small number of false positives,
	and the topical segments generated by our system are 5.5 times longer
	on average compared to those generated by a state-of-the-art micro-segmentation
	system. The system has been embedded in an e-Learning project, and
	the user feedback on using the generated topical segments is very
	encouraging. The experiments were conducted with instructional videos,
	but our approach is domain-general and is not restricted to instructional
	videos.},
  doi = {10.1109/ICSC.2007.31},
  keywords = {audio cues;audio features;instructional videos;keyword synonym sets;microsegmentation
	system;multimodal semantic analysis;natural language processing;sentence
	boundary information;silence/music breaks;speech similarity;textual
	features;topic-based categorization;topical video segmentation;video
	authoring;video browsing;video retrieval;video transcript;authoring
	systems;image segmentation;natural language processing;video retrieval;}
}

@INPROCEEDINGS{5298648,
  author = {Parreiras, F.S. and Saathoff, C. and Walter, T. and Franz, T. and
	Staab, S.},
  title = {APIs CHARPx0E0; gogo: Automatic Generation of Ontology APIs},
  booktitle = {Semantic Computing, 2009. ICSC '09. IEEE International Conference
	on},
  year = {2009},
  pages = {342 -348},
  month = {sept.},
  abstract = {When developing application programming interfaces of ontologies that
	include many instances of ontology design patterns, developers of
	semantic web applications usually have to handle complex mappings
	between descriptions of information given by ontologies and object
	oriented representations of the same information. In current approaches,
	annotations on API source code handle these mappings, leading to
	problems with reuse and maintenance. We propose a domain-specific
	language to tackle these mappings in a platform independent way -
	agogo. Agogo provides improvements on software engineering quality
	attributes like usability, reusability, maintainability, and portability.},
  doi = {10.1109/ICSC.2009.90},
  keywords = {API source code;application programming interface;domain-specific
	language;maintainability;object oriented representation;ontologies;ontology
	design pattern;portability;reusability;semantic Web application;software
	engineering quality attributes;application program interfaces;object-oriented
	methods;ontologies (artificial intelligence);semantic Web;}
}

@INPROCEEDINGS{779007,
  author = {Parsons, D. and Rashid, A. and Speck, A. and Telea, A.},
  title = {A ldquo;framework rdquo; for object oriented frameworks design},
  booktitle = {Technology of Object-Oriented Languages and Systems, 1999. Proceedings
	of},
  year = {1999},
  pages = {141 -151},
  month = {jul},
  abstract = {Object-oriented frameworks are established tools for domain-specific
	reuse. Many framework design patterns have been documented, e.g.
	reverse engineering framework architectures from conventionally built
	applications for a given domain. The framework development cycle
	generally evolves from an open framework to a closed application.
	We describe a more flexible component-based approach to framework
	design that stresses a common interface for `plugging-in' new components
	at different lifecycle stages. An analysis of framework-related user
	roles shows that the classical developer/end-user boundary is too
	rigid. We see the framework's development as a continuum within which
	its `actors' can customise its behavior. This both increases the
	system's flexibility and reduces its maintenance requirement. A case
	study of three frameworks for different application domains illustrates
	the presented principles},
  doi = {10.1109/TOOLS.1999.779007},
  keywords = {common interface;component-based approach;domain-specific reuse tools;framework
	design patterns;framework development cycle;framework-related user
	roles;maintenance requirement;object oriented framework design;object-oriented
	methods;object-oriented programming;software reusability;}
}

@INPROCEEDINGS{5196194,
  author = {Paska, M.},
  title = {Generative programming with support for formal verification},
  booktitle = {Industrial Embedded Systems, 2009. SIES '09. IEEE International Symposium
	on},
  year = {2009},
  pages = {58 -61},
  month = {july},
  abstract = {This paper presents a novel approach to software development, mainly
	useful for embedded devices. Embedded software is described in a
	programming language with very high level of abstraction. Efficient
	production code is generated from this description; also code suitable
	for formal verification is generated. The paper investigates efficiency
	of both the verifiable and the production code.},
  doi = {10.1109/SIES.2009.5196194},
  keywords = {embedded devices;embedded software;formal verification;generative
	programming;programming language;software development;embedded systems;formal
	verification;program compilers;programming languages;software engineering;}
}

@INPROCEEDINGS{1331736,
  author = {Patwardhan, A. and Korolev, V. and Kagal, L. and Joshi, A.},
  title = {Enforcing policies in pervasive environments},
  booktitle = {Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS
	2004. The First Annual International Conference on},
  year = {2004},
  pages = { 299 - 308},
  month = {aug.},
  abstract = { This work presents an architecture and a proof of concept implementation
	of a security infrastructure for mobile devices in an infrastructure
	based pervasive environment. The security infrastructure primarily
	consists of two parts, the policy engine and the policy enforcement
	mechanism. Each mobile device within a pervasive environment is equipped
	with its own policy enforcement mechanism and is responsible for
	protecting its resources. A mobile device consults the nearest policy
	server, notifies its current state including its present user, network
	presence, other accessible devices and location information if available.
	Using this information the policy server queries the "Rei" engine
	to dynamically create a policy certificate and issues it to the requesting
	device. The system wide policy is described in a semantic language
	"Rei", a lightweight and extensible language which is able to express
	comprehensive policies using domain specific information. The "Rei"
	policy engine is able to dynamically decide what rights, prohibitions,
	obligations, dispensations an actor has on the domain actions. A
	policy certificate is created and issued to the device. The policy
	certificate contains a set of granted permissions and a validity
	period and scope within which the permissions are valid. The policy
	certificate can be revoked by the policy enforcer based on expiration
	of the validity period or a combination of timeout, loss of contact
	with an assigned network. X.509 based public key infrastructure is
	used to provide identification and authentication.},
  doi = {10.1109/MOBIQ.2004.1331736},
  issn = { },
  keywords = { Rei policy engine; infrastructure based pervasive environment; mobile
	devices; policy certificate; policy enforcement mechanism; public
	key infrastructure; security infrastructure; semantic language; message
	authentication; mobile computing; public key cryptography; ubiquitous
	computing;}
}

@INPROCEEDINGS{760463,
  author = {Paul, J.M. and Thomas, D.E. and Weber, S.J. and Peffers, S.N.},
  title = {Hardware and software as dual languages for computer system modeling},
  booktitle = {VLSI '99. Proceedings IEEE Computer Society Workshop On},
  year = {1999},
  pages = {20 -25},
  abstract = {Complex computer systems can no longer be effectively designed without
	some consideration of the interaction of the hardware and software
	domains. Language-based behavioral specification for both simulation
	and synthesis in the hardware domain has made it possible to consider
	common models of computer system behavior with domain-specific inferences
	on the physical means of implementing the behavior. A dual computation
	modeling analogy is drawn for illustrating physical modeling inferences
	that overlap each domain and those which differ from domain to domain.
	The dual analogy illustrates the importance of preserving semantics
	of hardware and software modeling in separate languages. Peer-based
	co-execution of behavior specified in both modeling domains is possible
	because of reasoning about the interaction of the computation and
	state resources that are implied by modeling in each domain. We are
	uniting two existing hardware and software languages, Verilog and
	C using pthreads for an executable co-specification. We illustrate
	our approach with examples of our cosimulator},
  doi = {10.1109/IWV.1999.760463},
  keywords = {C language;Verilog;codesign simulation environment;complex computer
	systems;computer system modeling;cosimulator;dual computation modeling
	analogy;executable cospecification;hardware modeling;hardware/software
	interaction;peer-based co-execution;semantics;software modeling;C
	language;digital simulation;hardware description languages;hardware-software
	codesign;}
}

@INPROCEEDINGS{5372880,
  author = {Paulik, M. and Waibel, A.},
  title = {Automatic translation from parallel speech: Simultaneous interpretation
	as MT training data},
  booktitle = {Automatic Speech Recognition Understanding, 2009. ASRU 2009. IEEE
	Workshop on},
  year = {2009},
  pages = {496 -501},
  month = {13 2009-dec. 17},
  abstract = {State-of-the art statistical machine translation depends heavily on
	the availability of domain-specific bilingual parallel text. However,
	acquiring large amounts of bilingual parallel text is costly and,
	depending on the language pair, sometimes impossible. We propose
	an alternative to parallel text as machine translation (MT) training
	data; audio recordings of parallel speech (pSp) as it occurs in any
	scenario where interpreters are involved. Although interpretation
	(pSp) differs significantly from translation (parallel text), we
	achieve surprisingly strong translation results with our pSp-trained
	MT and speech translation systems.We argue that the presented approach
	is of special interest for developing speech translation in the context
	of resource-deficient languages where even monolingual resources
	are scarce.},
  doi = {10.1109/ASRU.2009.5372880},
  keywords = {MT training data;audio recording;automatic speech translation;domain-specific
	bilingual parallel text;parallel speech;resource-deficient language;statistical
	machine translation;language translation;linguistics;speech processing;statistical
	analysis;text analysis;}
}

@INPROCEEDINGS{1607369,
  author = {Paunov, S. and Hill, J. and Schmidt, D. and Baker, S.D. and Slaby,
	J.M.},
  title = {Domain-specific modeling languages for configuring and evaluating
	enterprise DRE system quality of service},
  booktitle = {Engineering of Computer Based Systems, 2006. ECBS 2006. 13th Annual
	IEEE International Symposium and Workshop on},
  year = {2006},
  pages = {10 pp. -208},
  month = {march},
  abstract = {The quality of service (QoS) of enterprise distributed real-time and
	embedded (DRE) systems can degrade under certain operating conditions
	and system architectures. This paper provides two contributions to
	research on model-driven development (MDD) tools and methods that
	help identify and rectify these QoS problems in component-based enterprise
	DRE systems. First, we show how MDD tools can be used to simplify
	and automate the evaluation of component-based DRE systems to identify
	QoS problems. Second, we show how MDD tools can be used to specify
	alternative QoS polices for component-based DRE systems and synthesize
	metadata automatically to simplify system (re)configurations that
	rectify QoS problems. We illustrate our MDD tools on a case study
	of multi-layer resource management services for shipboard computing
	systems that automate many aspects of power, navigation, command
	and control, and tactical operations},
  doi = {10.1109/ECBS.2006.39},
  keywords = {component-based enterprise DRE system;distributed real-time embedded
	system;domain specific modeling language;model driven development
	tool;multilayer resource management service;quality of service;shipboard
	computing system;system architecture;system configuration;distributed
	processing;embedded systems;enterprise resource planning;formal specification;object-oriented
	programming;quality of service;simulation languages;}
}

@INPROCEEDINGS{5966594,
  author = {Pavon, J. and Gomez-Sanz, J. and Paredes, A.L.},
  title = {The SiCoSSyS approach to SoS engineering},
  booktitle = {System of Systems Engineering (SoSE), 2011 6th International Conference
	on},
  year = {2011},
  pages = {179 -184},
  month = {june},
  abstract = {This paper presents a methodology and tools for SoS engineering. The
	approach is based on the experience acquired in several developments
	made in the context of the SiCoSSyS project. It considers that the
	nature of SoS requires (1) the participation of a diversity of people
	with different backgrounds and profiles, and (2) to cope with the
	heterogeneity of systems. These goals can be achieved by model-driven
	engineering methods and tools. In our work we have applied as well
	an agent-based modeling approach for the definition of domain-specific
	languages that constitute the main tool for specification, simulation
	and deployment of solutions.},
  doi = {10.1109/SYSOSE.2011.5966594},
  keywords = {SiCoSSyS project;SoS Engineering;agent based modeling;domain specific
	languages;formal specification;model driven engineering;engineering
	computing;formal specification;programming languages;systems engineering;}
}

@ARTICLE{4039271,
  author = {Pawlak, R.},
  title = {Spoon: Compile-time Annotation Processing for Middleware},
  journal = {Distributed Systems Online, IEEE},
  year = {2006},
  volume = {7},
  pages = {1},
  number = {11},
  month = {nov. },
  abstract = {Spoon is a Java-based program analysis and transformation tool for
	compile-time annotation processing. It combines compile-time reflection
	with a pure Java template framework for well-typed and intuitive
	fine-grained metaprogramming, which is applied to the middleware
	context},
  doi = {10.1109/MDSO.2006.67},
  issn = {1541-4922},
  keywords = {Java template framework;Java-based program analysis tool;Java-based
	program transformation tool;Spoon;compile-time annotation processing;compile-time
	reflection;intuitive fine-grained metaprogramming;middleware;well-typed
	metaprogramming;Java;middleware;object-oriented programming;program
	compilers;software tools;}
}

@ARTICLE{4167865,
  author = {Pedro, L. and Lucio, L. and Buchs, D.},
  title = {System Prototype and Verification Using Metamodel-Based Transformations},
  journal = {Distributed Systems Online, IEEE},
  year = {2007},
  volume = {8},
  pages = {1},
  number = {4},
  month = {april },
  abstract = {Mapping domain-specific languages' core concepts into the concurrent
	object-oriented Petri nets formal specification language provides
	users with the semantics necessary for developing prototypes for
	these DSLs. Different knowledge domains demand different types of
	support from software languages. Domain engineers often use domain-specific
	languages to overcome this problem. DSLs are difficult to design,
	implement, and maintain and are often less efficient than hand-coded
	software. To address some of these problems, we propose transforming
	a DSL into the concurrent object-oriented Petri nets formalism. The
	DSL metamodel serves as the transformation's starting point. The
	transformation represents the semantic mapping between the DSL and
	CO-OPN. We aim both to provide a formally defined semantics for the
	DSL and, because we integrate CO-OPN in a framework, to provide the
	functionalities that allow model verification and fast prototype
	generation for the DSL},
  doi = {10.1109/MDSO.2007.22},
  issn = {1541-4922},
  keywords = {concurrent object-oriented Petri nets formal specification language;domain-specific
	language;metamodel-based transformation;semantic mapping;system prototyping;system
	verification;Petri nets;concurrency control;object-oriented languages;program
	verification;programming language semantics;software prototyping;specification
	languages;}
}

@INPROCEEDINGS{1630744,
  author = {Pedro, L. and Lucio, L. and Buchs, D.},
  title = {Principles for System Prototype and Verification Using Metamodel
	Based Transformations},
  booktitle = {Rapid System Prototyping, 2006. Seventeenth IEEE International Workshop
	on},
  year = {2006},
  pages = {10 -17},
  month = {june},
  abstract = {Using domain specific modeling (DSM) allows solutions to be expressed
	in the idiom and at the level of abstraction of the problem domain.
	However, this does not imply that prototypes can be easily and rapidly
	generated. In reality, domain specific languages (DSLs) are difficult
	to design, implement and maintain, and usually there is a potential
	loss of efficiency when compared with hand-coded software. In this
	paper we explain the principles based on which we expect to solve
	some of these problems by means of transformation from a DSL to a
	formalism with a well define semantics named concurrent object oriented
	Petri-nets (CO-OPN). The proposed methodology uses the metamodel
	of the DSL as the principle for the transformation. This transformation
	represents the semantic mapping between the DSL and CO-OPN. The achievement
	is both to provide a formally defined semantics to the DSL and, since
	CO-OPN is integrated in a framework, to provide the functionalities
	that allow model verification and fast prototype generation for the
	DSL},
  doi = {10.1109/RSP.2006.29},
  issn = {1074-6005},
  keywords = {concurrent object oriented Petri-nets;domain specific languages;domain
	specific modeling;metamodel;system prototype;system verification;Petri
	nets;object-oriented programming;program verification;software prototyping;specification
	languages;}
}

@INPROCEEDINGS{1137708,
  author = {Peltier, M.L.},
  title = {MTrans, a DSL for model transformation},
  booktitle = {Enterprise Distributed Object Computing Conference, 2002. EDOC '02.
	Proceedings. Sixth International},
  year = {2002},
  pages = { 190 - 199},
  abstract = { After having undergone a major evolution with the transition between
	the procedural paradigm and oriented-object paradigm, computer science
	seems to know a new evolution in that models become first class entities
	in the process of software development. The Model-Driven Architecture
	proposed by the OMG reflects this new orientation. The core of this
	architecture is based on modeling standards and meta-modeling standards
	(UML as a general purpose graphical modeling language, MOF as the
	basis for meta-modeling and model repositories, XMI for exchanging
	models as XML documents). There recently appeared an increased need
	for transformation between models (adaptation of the data models
	between applications, shared information between models, mapping
	inside the MDA architecture, etc.). Existing solutions to express
	model transformation are not completely satisfactory and the author
	proposes a solution in which a language specially designed for transforming
	models will be defined. In others words the author defines a domain
	specific language (a DSL). He shows that a DSL approach has some
	advantages compared to other techniques using general programming
	languages. Furthermore, this DSL allows specifying executable transformations,
	which are comprehensive and unambiguous.},
  doi = {10.1109/EDOC.2002.1137708},
  issn = { },
  keywords = { DSL; MOF; MTrans; Model-Driven Architecture; OMG; UML; XMI; XML;
	data models; domain specific language; graphical modeling language;
	meta-modeling standards; model transformation; modeling standards;
	oriented-object paradigm; procedural paradigm; software development;
	data models; distributed object management; formal specification;
	hypermedia markup languages; software architecture; specification
	languages;}
}

@INPROCEEDINGS{1241680,
  author = {Pembeci, I. and Hager, G.},
  title = {Functional reactive programming as a hybrid system framework},
  booktitle = {Robotics and Automation, 2003. Proceedings. ICRA '03. IEEE International
	Conference on},
  year = {2003},
  volume = {1},
  pages = { 727 - 734 vol.1},
  month = {sept.},
  abstract = { In previous work we presented functional reactive programming (FRP),
	a general framework for designing hybrid systems and developing domain-specific
	languages for related domains. FRP's synchronous dataflow features,
	like event driven switching, supported by higher-order lazy functional
	abstractions of Haskell allows rapid development of modular and reusable
	specifications. In this paper, we look at more closely to the relation
	of arrowized FRP (AFRP), the FRP implementation, and formal specification
	of hybrid systems. We show how a formally specified hybrid system
	can be expressed in FRP and present a constructive proof showing
	that, for a subset of AFRP programs, there is a corresponding formal
	hybrid system specification.},
  doi = {10.1109/ROBOT.2003.1241680},
  issn = {1050-4729},
  keywords = { arrowized FRP; domain-specific languages; event driven switching;
	functional reactive programming; higher-order lazy functional abstraction;
	hybrid system framework; hybrid system specification; functional
	programming;}
}

@ARTICLE{1377176,
  author = {Penzo, W.},
  title = {Rewriting rules to permeate complex similarity and fuzzy queries
	within a relational database system},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  year = {2005},
  volume = {17},
  pages = { 255 - 270},
  number = {2},
  month = {feb.},
  abstract = { In recent years, the availability of complex data repositories (e.g.,
	multimedia, genomic, semistructured databases) has paved the way
	to new potentials as to data querying. In this scenario, similarity
	and fuzzy techniques have proven to be successful principles for
	effective data retrieval. However, most proposals are domain specific
	and lack of a general and integrated approach to deal with generalized
	complex queries, i.e., queries where multiple conditions are expressed,
	possibly on complex as well as on traditional data. To overcome such
	limitations, much work has been devoted to the development of middleware
	systems to support query processing on multiple repositories. On
	a similar line, We present a formal framework to permeate complex
	similarity and fuzzy queries within a relational database system.
	As an example, we focus on multimedia data, which is represented
	in an integrated view with common database data. We have designed
	an application layer that relies on an algebraic query language,
	extended with MM-tailored operators, and that maps complex similarity
	and fuzzy queries to standard SQL statements that can be processed
	by a relational database system, exploiting standard facilities of
	modern extensible RDBMS. To show the applicability of our proposal,
	we implemented a prototype that provides the user with rich query
	capabilities, ranging from traditional database queries to complex
	queries gathering a mixture of Boolean, similarity, and fuzzy predicates
	on the data.},
  doi = {10.1109/TKDE.2005.33},
  issn = {1041-4347},
  keywords = { MM-tailored operators; SQL statements; algebraic query language;
	complex similarity; data repositories; fuzzy predicates; fuzzy queries;
	middleware systems; multimedia database; query processing; relational
	database system; rewriting rules; Boolean algebra; SQL; data mining;
	data warehouses; fuzzy logic; middleware; multimedia databases; query
	processing; relational databases; rewriting systems;}
}

@INPROCEEDINGS{6070397,
  author = {Perez, Francisca and Valderas, Pedro and Fons, Joan},
  title = {Allowing end-users to participate within model-driven development
	approaches},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2011 IEEE
	Symposium on},
  year = {2011},
  pages = {187 -190},
  month = {sept.},
  abstract = {Model-Driven Development (MDD) uses models to guide the development
	of software systems. However, models are usually conceived to be
	used by software professionals, so it is quite difficult for end-users
	to understand them because they are not familiar with domain-specific
	languages or modeling tools as software professionals are. This makes
	the end-users' involvement within MDD approaches a difficult task.
	In this paper, we present a method that introduces a set of techniques
	and tools that allow end-users to participate from early modeling
	stages in cooperation with software professionals within any MDD
	approach. To validate the application of the method, we have applied
	it in a non-intrusive way within an existing MDD approach for developing
	smart homes.},
  doi = {10.1109/VLHCC.2011.6070397},
  issn = {1943-6092}
}

@INPROCEEDINGS{4123380,
  author = {Perez, G. and Amores, G. and Manchon, P.},
  title = {A MULTIMODAL ARCHITECTURE FOR HOME CONTROL BY DISABLED USERS},
  booktitle = {Spoken Language Technology Workshop, 2006. IEEE},
  year = {2006},
  pages = {134 -137},
  month = {dec.},
  abstract = {This paper describes the architecture of MIMUS, a multimodal, multilingual
	interaction system which allows users to control the devices in their
	houses, interacting by voice and clicks. MIMUS design relies on Wizard
	of Oz experiments and is targeted at disabled users in the in-home
	scenario. MIMUS consists of a set of OAA collaborative agents with
	a clear distinction between knowledge resources, dialogue management
	and domain specific functionalities. The MIMUS system includes contributions
	to the scientific community in multimodal fusion, knowledge management
	and presentation strategies.},
  doi = {10.1109/SLT.2006.326836},
  keywords = {MIMUS architecture design;OAA collaborative agent;dialogue management;disabled
	user home control;domain specific functionality;knowledge management;knowledge
	presentation;multimodal fusion;multimodal multilingual interaction
	system;groupware;handicapped aids;home automation;interactive systems;software
	agents;}
}

@INPROCEEDINGS{4276335,
  author = {Perseil, I. and Pautet, L.},
  title = {Co-Modeling Methodology Designed for RT Architecture Models Integration},
  booktitle = {Engineering Complex Computer Systems, 2007. 12th IEEE International
	Conference on},
  year = {2007},
  pages = {371 -376},
  month = {july},
  abstract = {Architecture models are built in parallel with applicative models,
	all along the development process. Since they equally refer to software
	and hardware components in which they are implemented, these models
	call for more heterogeneous design languages, with a larger granularity
	range too. At the level of design and verification languages, some
	standards have risen up, like UML, AADL, but none of them is driven
	by a standard methodology. To be able to follow the requirements
	traceability, at each step of the life cycle, we need a methodology
	that states for bidirectional links between each granularity level
	and each development level. In this article, we propose an approach
	based on systematic reuse of low-level concepts (borrowed from an
	algorithm language like +CAL and a programming language like Ada)
	into encapsulated algorithm structures which are implemented into
	the highest conceptual levels of development (e.g. from the UML and
	AADL component models). These structures are only activated and enriched
	as they advanced with the life cycle. Therefore, some new concepts
	presents from the beginning of the design, will play an important
	role in the further development steps without being explicitly expressed
	from the beginning.},
  doi = {10.1109/ICECCS.2007.19},
  keywords = {RT architecture;hardware components;heterogeneous design languages;requirements
	traceability;software components;verification languages;formal specification;formal
	verification;software architecture;specification languages;}
}

@INPROCEEDINGS{5352778,
  author = {Petera, M. and Lulfesmann, M. and Bucker, H.M.},
  title = {Partial Jacobian computation in the domain-specific program transformation
	system ADiCape},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {595 -599},
  month = {oct.},
  abstract = {Sensitivities of functions given in the form of computer models are
	crucial in various areas of computational science and engineering.
	We consider computer models written in CapeML, a domain-specific
	XML-based language used in process engineering. Rather than computing
	all nonzero entries of a sparse Jacobian matrix, we are interested
	in obtaining only a subset of these entries. For the solution of
	this problem called partial Jacobian computation, we transform a
	CapeML model of an industrial distillation column using the automatic
	differentiation system ADiCape.},
  doi = {10.1109/IMCSIT.2009.5352778},
  keywords = {ADiCape system;CapeML language;automatic differentiation system;computer
	model;domain specific XML based language;domain specific program
	transformation system;industrial distillation column;partial Jacobian
	computation;process engineering;Jacobian matrices;XML;mathematics
	computing;}
}

@INPROCEEDINGS{1570869,
  author = {Peterson, J.},
  title = {Media centered languages for new computing experiences},
  booktitle = {Diversity in Computing Conference, 2005 Richard Tapia Celebration
	of},
  year = {2005},
  pages = { 23 - 25},
  month = {oct.},
  abstract = { In this paper, we discuss the use of specialized languages to give
	students experience with the basic concepts of computer science without
	encumbering the novice with the details of a fully-featured programming
	language. Using techniques developed in the area of domain-specific
	programming languages, we have created languages which lead directly
	into basic computational concepts such as abstraction, naming, types
	systems, and user interfaces without the associated clutter of a
	language such as Java. Using these languages, we can address traditional
	problem solving and, more importantly address computing as a creative
	discipline. These programs are declarative: they describe media objects
	such as sound or animations.},
  doi = {10.1109/RTCDC.2005.201637},
  keywords = { declarative program; domain-specific programming; functional programming;
	introductory programming languages; media centered languages; media-based
	language; computer science education; multimedia computing; programming
	languages;}
}

@INPROCEEDINGS{772516,
  author = {Peterson, J. and Hager, G.D. and Hudak, P.},
  title = {A language for declarative robotic programming},
  booktitle = {Robotics and Automation, 1999. Proceedings. 1999 IEEE International
	Conference on},
  year = {1999},
  volume = {2},
  pages = {1144 -1151 vol.2},
  abstract = {We have applied methodologies developed for domain-specific embedded
	languages to create a high-level robot control language called Frob,
	for functional robotics. Frob supports a programming style that cleanly
	separates the what from the how of a robotic control program. That
	is, the what is a simple, easily understood definition of the control
	strategy using groups of equations and primitives which combine sets
	of these control system equations into a complex system. The how
	aspect of the program addresses the unpleasant details, such as the
	method used to realize these equations, the connection between the
	control equations and the sensors and effectors in the robot, and
	communication with other elements of the system. Frob is a system
	that supports rapid prototyping of new control strategies, enables
	software reuse through composition, and defines a system in a way
	that can be formally reasoned about and transformed},
  doi = {10.1109/ROBOT.1999.772516},
  keywords = {Frob;declarative robotic programming;domain-specific embedded languages;functional
	robotics;high-level robot control language;robotic control program;high
	level languages;robot programming;software prototyping;software reusability;}
}

@INPROCEEDINGS{1013188,
  author = {Peterson, J. and Hager, G. and Serjentov, A.},
  title = {Composable robot controllers},
  booktitle = {Computational Intelligence in Robotics and Automation, 2001. Proceedings
	2001 IEEE International Symposium on},
  year = {2001},
  pages = { 149 - 154},
  abstract = { Software for controlling robots is often difficult to develop and
	maintain. Specialized robot programming languages make this task
	more manageable. These languages may be either created from scratch
	or incorporated within an existing language. In this paper, we demonstrate
	the latter technique: an embedded domain-specific language called
	Frob (for functional robotics) built within Haskell, a purely functional
	programming language. We use basic techniques of programming, transformation
	and composition, to create high-level controllers that are concise,
	understandable, and easily modifiable. Frob is constructed using
	functional reactive programming, a system that describes interactive
	computations using functions. In this paper, we demonstrate the use
	of Frob in the context of the Robocup robotic soccer competition.
	We develop a set of control systems for Robocup and show how these
	systems can be composed into complex soccer strategies.},
  doi = {10.1109/CIRA.2001.1013188},
  issn = { },
  keywords = { Frob; Haskell; Robocup; embedded domain specific language; functional
	programming language; functional reactive programming; functional
	robotics; robot programming languages; robotic soccer; functional
	languages; functional programming; robot programming;}
}

@INPROCEEDINGS{5204823,
  author = {Petrascu, V. and Chiorean, D. and Petrascu, D.},
  title = {ContractCML - A Contract Aware Component Modeling Language},
  booktitle = {Symbolic and Numeric Algorithms for Scientific Computing, 2008. SYNASC
	'08. 10th International Symposium on},
  year = {2008},
  pages = {273 -276},
  month = {sept.},
  abstract = {Providing software components with a four level contract specification
	- syntax, semantics, synchronization, quality of service - is important
	to their correct (re)use. The mandatory syntactic level is included
	by all current component models. Academic models also employ one
	of the others, but use different formalisms to represent it. Through
	this paper, we propose an integrated approach for handling component
	contracts. We focus on introducing ContractCML (Contract Component
	Modeling Language), a domain specific modeling language that ensures
	the basis of our proposal.},
  doi = {10.1109/SYNASC.2008.35},
  keywords = {ContractCML;academic model;component contracts;contract aware component
	modeling language;domain specific modeling language;four level contract
	specification;mandatory syntactic level;software components;formal
	specification;object-oriented programming;specification languages;}
}

@INPROCEEDINGS{5708396,
  author = {Pea, C. and Villalobos, J.},
  title = {An MDE Approach to Design Enterprise Architecture Viewpoints},
  booktitle = {Commerce and Enterprise Computing (CEC), 2010 IEEE 12th Conference
	on},
  year = {2010},
  pages = {80 -87},
  month = {nov.},
  abstract = {Enterprise Architecture (EA) has risen as a tool to support the process
	of making strategic business decisions, by achieving an integral
	vision of business and IT elements of an enterprise. An EA approach
	provides a better understanding of relationships between concepts
	from several domains, such as strategy, process, applications, and
	information. However, this heterogeneous aspect makes EA modeling
	complex because there are many domain specific modeling languages
	that were not designed to be integrated. ArchiMate is an architecture
	description language (ADL) that provides a set of predefined viewpoints
	to model an EA and enhance the communication between business and
	IT people by establishing a common language. However, ArchiMate is
	not clear about the way to customize or create new viewpoints to
	integrate new domains. This paper describes a Model Driven Engineering
	(MDE) framework to support the EA modeling activity by taking the
	ArchiMate language as its foundation. The basis of this framework,
	are tools to specify ArchiMate and its future domain specific extensions.
	It also supports the composition and customization of these domains
	to define a common EA terminology. Finally, it offers tools to define
	customized viewpoints and to build the applications that stakeholders
	need to interact with the EA models.},
  doi = {10.1109/CEC.2010.25},
  issn = {1530-1354},
  keywords = {ADL;ArchiMate;EA modeling;MDE approach;architecture description language;domain
	specific modeling languages;enterprise architecture viewpoints;model
	driven engineering;strategic business decisions;business data processing;software
	architecture;}
}

@INPROCEEDINGS{927269,
  author = {Pfahler, P. and Kastens, U.},
  title = {Configuring component-based specifications for domain-specific languages},
  booktitle = {System Sciences, 2001. Proceedings of the 34th Annual Hawaii International
	Conference on},
  year = {2001},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { The "Jacob" system supports language design processes on a very high
	level of abstraction, enabling experts from application domains to
	design their own domain-specific languages (DSLs). The system provides
	a representation of the language design space for a certain application
	domain. The user specifies his language design by composing and configuring
	language components. During this design process, the Jacob system
	checks the consistency of user decisions. The language components
	are provided by an expert in the field of computer languages and
	their implementation. Such a component consist of two parts: the
	implementation part defines how a given language feature is implemented.
	The interface part specifies relations and dependences between different
	components, e.g. to constrain the way different language features
	can be combined. This paper presents component-based DSL design using
	the Jacob system and discusses some example applications.},
  doi = {10.1109/HICSS.2001.927269},
  issn = { },
  keywords = { Jacob system; abstraction level; application domain; component-based
	specifications configuration; domain-specific language design; language
	components; language design processes; language design space; language
	feature implementation; language features combination; user decisions
	consistency checking; specification languages; subroutines;}
}

@INPROCEEDINGS{5195793,
  author = {Pfeiffer, M. and Pichler, J.},
  title = {A DSM approach for end-user programming in the automation domain},
  booktitle = {Industrial Informatics, 2009. INDIN 2009. 7th IEEE International
	Conference on},
  year = {2009},
  pages = {142 -148},
  month = {june},
  abstract = {In this paper we present an approach and a software prototype that
	enables domain experts to program control software in the automation
	domain. The approach follows the principles of domain-specific modeling
	providing a graphical domain-specific language to model the control
	cycle of an injection molding machine, a user interface to manipulate
	and monitor the control cycle as well as code generators to generate
	control code that can be executed by the machine. As result, domain
	experts like machine operators can manipulate and monitor the control
	cycle directly on the touch-screen of a machine without detailed
	software development expertise.},
  doi = {10.1109/INDIN.2009.5195793},
  issn = {1935-4576},
  keywords = {DSM;automation;code generators;control cycle;control software program;domain-specific
	modeling;end-user programming;graphical domain-specific language;injection
	molding machine;touch-screen;user interface;control engineering computing;injection
	moulding;object-oriented languages;production engineering computing;production
	equipment;program compilers;user interfaces;}
}

@INPROCEEDINGS{4304242,
  author = {Huy Pham and Ferworn, A. and Mahmoud, Q.H. and Sadeghian, A.},
  title = {Applying Model-Driven Development Techniques to the Development of
	Search and Rescue Systems},
  booktitle = {System of Systems Engineering, 2007. SoSE '07. IEEE International
	Conference on},
  year = {2007},
  pages = {1 -6},
  month = {april},
  abstract = {This paper describes our work of applying modem software engineering
	methodologies such as model-driven development and product-line engineering
	to the development and maintenance of technical supporting systems
	in the domain of search and rescue. We propose an extensible, domain-specific,
	graphical modeling language and toolset that allow software developers
	of search and rescue systems to rapidly compose and generate their
	applications from a set of predefined graphical primitives that represent
	the common building blocks of SAR applications. A case study for
	the proposed toolset is also presented.},
  doi = {10.1109/SYSOSE.2007.4304242},
  keywords = {graphical modeling language;model-driven development technique;search-and-rescue
	system;software engineering methodologies;technical supporting system;object-oriented
	programming;software engineering;specification languages;visual languages;}
}

@INPROCEEDINGS{5967121,
  author = {Piho, G. and Tepandi, J. and Parman, M. and Puusep, V. and Roost,
	M.},
  title = {Test Driven domain modelling},
  booktitle = {MIPRO, 2011 Proceedings of the 34th International Convention},
  year = {2011},
  pages = {576 -581},
  month = {may},
  abstract = {To write software we have to know requirements; to know requirements
	we have to know domain; to know the domain we have to analyze and
	model one. We propose a methodology for applying Test Driven Modelling
	in engineering of domains, requirements and software. We will restrict
	ourselves here to enterprise information systems and therefore to
	business domains. As common for Software Factories, domain models
	(as well as all other models) are software artefacts, not only documentation
	artefacts. In our approach Test Driven Modelling utilizes Test Driven
	Development for domain modelling. Domain models engineered in this
	way are used as Domain Specific Language for specifying software
	requirements. The hypothesis is that such domain models can be used
	for validation of requirements and verification of software, lead
	developments towards Software Factories, and increase dependability
	of software.},
  keywords = {business domains;documentation artefacts;domain models;domain specific
	language;domains engineering;enterprise information systems;requirements
	engineering;software artefacts;software dependability;software engineering;software
	factory;software requirements;software verification;test driven development;test
	driven domain modelling;DP industry;business data processing;formal
	specification;information systems;program testing;program verification;software
	reliability;specification languages;system documentation;}
}

@INPROCEEDINGS{5967120,
  author = {Piho, G. and Tepandi, J. and Roost, M. and Parman, M. and Puusep,
	V.},
  title = {From archetypes based domain model via requirements to software:
	Exemplified by LIMS Software Factory},
  booktitle = {MIPRO, 2011 Proceedings of the 34th International Convention},
  year = {2011},
  pages = {570 -575},
  month = {may},
  abstract = {The Archetypes Based Development (ABD) proceeds from archetypes based
	domain model via requirements to software. We give an overview of
	ABD and exemplify its application on Laboratory Information Management
	Systems (LIMS) Software Factory development. ABD is guided by Zachman
	Framework and utilizes software engineering triptych together with
	archetypes and archetype patterns. For modelling of domains the Test
	Driven Modelling (TDM) techniques are used. TDM utilizes test driven
	development techniques in domain engineering. The resultant domain
	models serve as the Domain Specific Language for prescribing requirements.
	Implementation and testing of the LIMS Software Factory proves feasibility
	of archetypes based techniques in real life systems. ABD helps developers
	to better understand business requirements, to design cost effective
	enterprise applications through systematic reuse of archetypal components,
	as well as to validate and verify requirements resulting in higher
	quality software.},
  keywords = {LIMS software factory;Zachman Framework;archetypes based domain model
	development;business requirements;domain specific language;laboratory
	information management systems software factory development;software
	engineering triptych;software requirements;test driven modelling
	techniques;business data processing;information management;laboratories;software
	quality;specification languages;}
}

@INPROCEEDINGS{951834,
  author = {Pinkwart, N. and Hoppe, U. and Gassner, K.},
  title = {Integration of domain-specific elements into visual language based
	collaborative environments},
  booktitle = {Groupware, 2001. Proceedings. Seventh International Workshop on},
  year = {2001},
  pages = {142 -147},
  abstract = {This paper presents an approach for the integration of domain related
	elements and operational semantics into collaborative environments
	based on visual languages. This integration allows for supporting
	domain specific collaborative tasks, e.g. in the area of "collaborative
	discovery learning" in science education, by integrating data modelling
	with generic discussion support. A special focus is set on flexibility
	and parameterisation of the system which is achieved through providing
	the syntax definition of the visual language as a separate resource
	file},
  doi = {10.1109/CRIWG.2001.951834},
  keywords = {collaborative discovery learning;collaborative environments;domain
	specific collaborative tasks;generic discussion support;operational
	semantics;science education;visual languages;computer aided instruction;groupware;visual
	languages;}
}

@INPROCEEDINGS{5290811,
  author = {Pinto, M. and Fuentes, L. and Valenzuela, J.A. and Pires, P.F. and
	Delicato, F.C. and Marinho, E.},
  title = {On the need of architectural patterns in AOSD for software evolution},
  booktitle = {Software Architecture, 2009 European Conference on Software Architecture.
	WICSA/ECSA 2009. Joint Working IEEE/IFIP Conference on},
  year = {2009},
  pages = {245 -248},
  month = {sept.},
  abstract = {One promising approach to tackle software evolution in AOSD is model-based
	pointcuts, where pointcuts are defined in terms of elements of a
	conceptual model, which are less susceptible to evolution than elements
	of the base model. We propose the definition of model-based pointcuts
	at the architectural level and identify three layers in the definition
	of our conceptual model: the system, the domain-specific and the
	application-specific layer. An MDD process drives the definition
	of conceptual and aspect models, their instantiation and composition.
	AO-ADL is used to implement it.},
  doi = {10.1109/WICSA.2009.5290811},
  keywords = {AOSD model;MDD process;architectural pattern;aspect-oriented software
	development;model-based pointcuts;software evolution;object-oriented
	programming;software architecture;software maintenance;}
}

@INPROCEEDINGS{4239974,
  author = {Pizka, M. and Jurgens, E.},
  title = {Automating Language Evolution},
  booktitle = {Theoretical Aspects of Software Engineering, 2007. TASE '07. First
	Joint IEEE/IFIP Symposium on},
  year = {2007},
  pages = {305 -315},
  month = {june},
  abstract = {The design and implementation of complex software systems inherently
	spans multiple levels of abstractions. The concepts of each level
	of abstractions and their interplay are represented by formal languages
	that are either implicitly known or explicitly defined. Achieving
	high productivity in software development and maintenance is thus
	strongly connected with ruling the complexity of multi-level language
	design and evolution. This paper explains the necessity for automating
	multi-level language evolution, discusses its challenges and proposes
	concepts as well as a prototypical tool that support the incremental
	co-evolution of a staged language and program generation architecture.
	This approach reduces the cost of language maintenance and paves
	the ground for an incremental and bottom-up oriented way of developing
	domain specific languages.},
  doi = {10.1109/TASE.2007.13},
  keywords = {automatic multilevel language incremental coevolution;complex software
	system design;domain specific language development;formal language;multilevel
	language design;program generation architecture;software development;software
	maintenance;software prototypical tool;automatic programming;high
	level languages;software maintenance;software prototyping;}
}

@INPROCEEDINGS{5476022,
  author = {Pjanic, E and Hasanovic, A. and Suljanovic, N. and
	Mujcic, A. and Zajc, M.},
  title = {Metaprogramming approaches to finite state machine modeling for SIP
	applications},
  booktitle = {MELECON 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference},
  year = {2010},
  pages = {592 -596},
  month = {april},
  abstract = {This paper presents a methodology to develop a complete domain specific
	language (DSL) for simple finite state machine (FSM) modeling, utilizing
	metaprogramming techniques found in Ruby programming language. Additionally,
	two libraries for FSM modeling are reviewed. A simple vending machine
	model is used to demonstrate the effectiveness of the DSL code. The
	proposed techniques together with the SIP Servlet API can be combined
	with Ruby's web development environments to develop complex converged
	telecom applications.},
  doi = {10.1109/MELCON.2010.5476022},
  keywords = {DSL code;FSM modeling;Ruby programming language;SIP Servlet API;SIP
	applications;Web development environments;domain specific language;finite
	state machine modeling;metaprogramming approaches;metaprogramming
	techniques;telecom applications;vending machine model;application
	program interfaces;finite state machines;programming languages;signalling
	protocols;specification languages;telecommunication computing;}
}

@INPROCEEDINGS{5635210,
  author = {Planas, E. and Cabot, J. and Gmez, C. and Guerra, E. and de
	Lara, J.},
  title = {Lightweight Executability Analysis of Graph Transformation Rules},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2010 IEEE
	Symposium on},
  year = {2010},
  pages = {127 -130},
  month = {sept.},
  abstract = {Domain Specific Visual Languages (DSVLs) play a cornerstone role in
	Model-Driven Engineering (MDE), where (domain specific) models are
	used to automate the production of the final application. Graph Transformation
	is a formal, visual, rule-based technique, which is increasingly
	used in MDE to express in-place model transformations like refactorings,
	animations and simulations. However, there is currently a lack of
	methods able to perform static analysis of rules, taking into account
	the DSVL meta-model integrity constraints. In this paper we propose
	a lightweight, efficient technique that performs static analysis
	of the weak executability of rules. The method determines if there
	is some scenario in which the rule can be safely applied, without
	breaking the meta-model constraints. If no such scenario exists,
	the method returns meaningful feedback that helps repairing the detected
	inconsistencies.},
  doi = {10.1109/VLHCC.2010.26},
  issn = {1943-6092},
  keywords = {domain specific visual languages;graph transformation rules;lightweight
	executability analysis;meta-model integrity constraints;model-driven
	engineering;rule-based technique;static analysis;graph grammars;knowledge
	based systems;}
}

@INPROCEEDINGS{6008966,
  author = {Playne, D.P. and Hawick, K.A.},
  title = {Auto-generation of Parallel Finite-Differencing Code for MPI, TBB
	and CUDA},
  booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW),
	2011 IEEE International Symposium on},
  year = {2011},
  pages = {1168 -1175},
  month = {may},
  abstract = {Finite-difference methods can be useful for solving certain partial
	differential equations (PDEs) in the time domain. Compiler technologies
	can be used to parse an application domain specific representation
	of these PDEs and build an abstract representation of both the equation
	and the desired solver. This abstract representation can be used
	to generate a language-specific implementation. We show how this
	framework can be used to generate software for several parallel platforms:
	Message Passing Interface (MPI), Threading Building Blocks(TBB) and
	Compute Unified Device Architecture(CUDA). We present performance
	data of the automatically-generated parallel code and discuss the
	implications of the generator in terms of code portability, development
	time and maintainability.},
  doi = {10.1109/IPDPS.2011.265},
  issn = {1530-2075},
  keywords = {CUDA;MPI;TBB;abstract representation;auto-generation;code portability;compiler
	technologies;compute unified device architecture;development time;language-specific
	implementation;maintainability;message passing interface;parallel
	finite-differencing code;parallel platforms;partial differential
	equations;threading building blocks;finite difference methods;mathematics
	computing;message passing;parallel processing;partial differential
	equations;}
}

@INPROCEEDINGS{4227526,
  author = {Plusch, M. and Fry, C. and Haase, K. and Farrell, T.},
  title = {Modeling Executable Specifications with X-Spec and Water},
  booktitle = {Integration of Knowledge Intensive Multi-Agent Systems, 2007. KIMAS
	2007. International Conference on},
  year = {2007},
  pages = {69 -72},
  month = {30 2007-may 3},
  abstract = {X-Spectrade is a business modeling tool and executable specification
	language that enables business people to describing requirements
	in their domain language. The specification, or model, can be shown
	in multiple editable views that use English and pictures. The specification
	is directly executed to deliver a fully-functional user interface
	for rich Internet applications. The model is available at run-time,
	since no code is generated. The model enforces the clean separation
	of user interface, controller logic, and services. X-Spec is built
	on the Water language. Watertrade is an open dynamic, object-based
	language that uses ConciseXMLtrade syntax. It is a multi-paradigm
	language that can represent many different modeling styles and forms
	of knowledge representation. Water can treat code as data, and is
	a meta-language for creating domain specific models. Water integrates
	many features of RDF and OWL. The Water language may be expressed
	in graphical views as well as ConciseXML.},
  doi = {10.1109/KIMAS.2007.369787},
  keywords = {ConciseXML syntax;Water language;X-Spec;business modeling tool;controller
	logic;domain language;executable specification language;executable
	specification modeling;knowledge representation;meta-language;object-based
	language;open dynamic language;rich Internet applications;user interface;XML;public
	domain software;specification languages;}
}

@INPROCEEDINGS{1039324,
  author = {Pontelli, E. and Ranjan, D. and Milligan, B. and Gupta, G.},
  title = { Phi;LOG: a domain specific language for solving phylogenetic inference
	problems},
  booktitle = {Bioinformatics Conference, 2002. Proceedings. IEEE Computer Society},
  year = {2002},
  pages = { 9 - 20},
  abstract = {Domain experts think and reason at a high level of abstraction when
	they solve problems in their domain of expertise. We present the
	design and motivation behind a domain specific language (DSL), called
	Phi;LOG, to enable biologists (domain experts) to program solutions
	to phylogenetic inference problems at a very high level of abstraction.
	The implementation infrastructure (interpreter, compiler, debugger)
	for the DSL is automatically obtained through a software engineering
	framework based on denotational semantics and logic programming.},
  doi = {10.1109/CSB.2002.1039324},
  issn = { },
  keywords = { abstraction; compiler; debugger; denotational semantics; domain experts;
	domain specific language; interpreter; logic programming; phylogenetic
	inference; software engineering; Phi;LOG; biology computing; high
	level languages; logic programming; program compilers; program debugging;
	program interpreters; programming language semantics;}
}

@INPROCEEDINGS{130646,
  author = {Popovich, S.S. and Schell, W.M. and Perry, D.E.},
  title = {Experiences with an environment generation system},
  booktitle = {Software Engineering, 1991. Proceedings., 13th International Conference
	on},
  year = {1991},
  pages = {219 -224},
  month = {may},
  abstract = {The authors report on research experience using the Gandalf environment
	generation system as a prototyping vehicle for the Inscape environment.
	A Gandalf-based environment consists of four parts: a structure editor
	kernel, which is simply linked into each executable, a set of grammar
	tables describing the language to the kernel in terms of its abstract
	syntax, one or more concrete syntax views, and a collection of action
	routines written in the extension language, ARL. Positive aspects
	of the research included experimentation, incremental evolution,
	multiple views, the coupling of semantic and editing actions, and
	the use of domain-specific facilities. Negative aspects consisted
	primarily of problems with presentation and object management},
  doi = {10.1109/ICSE.1991.130646},
  keywords = {ARL;Gandalf environment generation system;Inscape environment;abstract
	syntax;action routines;domain-specific facilities;grammar tables;incremental
	evolution;multiple views;prototyping;structure editor kernel;programming
	environments;}
}

@INPROCEEDINGS{5211094,
  author = {Pozo, S. and Varela-Vaca, A.J. and Gasca, R.M.},
  title = {AFPL2, an Abstract Language for Firewall ACLs with NAT Support},
  booktitle = {Dependability, 2009. DEPEND '09. Second International Conference
	on},
  year = {2009},
  pages = {52 -59},
  month = {june},
  abstract = {The design and management of firewall ACLs is a very hard and error-prone
	task. Part of this complexity comes from the fact that each firewall
	platform has its own low-level language with a different functionality,
	syntax, and development environment. Although high-level languages
	have been proposed to model firewall ACLs, none of them has been
	widely adopted by the industry due to a combination of factors: high
	complexity, no support of important features of firewalls, etc. In
	this paper the most important access control policy languages are
	reviewed, with special focus on the development of firewall ACLs.
	Based on this analysis, a new domain specific language for firewall
	ACLs (AFPL2) is proposed, supporting more features that other languages
	do not cover (e.g. NAT). As the result of our design methodology,
	AFPL2 is very lightweight and easy to use. AFPL2 can be translated
	to existing low-level firewall languages, or be directly interpreted
	by firewall platforms, and is an extension to a previously developed
	language.},
  doi = {10.1109/DEPEND.2009.14},
  keywords = {abstract language;access control list;access control policy languages;firewall
	design;firewall management;high-level languages;low-level language;authorisation;machine
	oriented languages;}
}

@INPROCEEDINGS{5549622,
  author = {Prahofer, H. and Hurnaus, D.},
  title = {MONACO CHARPx2014; A domain-specific language supporting hierarchical
	abstraction and verification of reactive control programs},
  booktitle = {Industrial Informatics (INDIN), 2010 8th IEEE International Conference
	on},
  year = {2010},
  pages = {908 -914},
  month = {july},
  abstract = {Domain-specific languages aim to present software in the notations
	of domain experts and allow a straightforward mapping of application
	concepts to software solutions. In this paper, we present a domain-specific
	language for programming reactive control programs. The language
	differs from other approaches mainly by its hierarchical component
	approach, in which lower-level components provide elementary operations
	and upper components rely on the operations of their subordinates
	to implement higher control tasks. Moreover, the hierarchical component
	approach is leveraged in a hierarchical verification technique in
	which component implementations are verified against dynamic contracts
	of their subcomponents. We present the principles of the verification
	technique and discuss how it can be applied in a multi-stage development
	process.},
  doi = {10.1109/INDIN.2010.5549622},
  keywords = {MONACO;domain specific language;hierarchical abstraction;hierarchical
	component approach;multistage development process;reactive control
	programming;straightforward mapping;verification technique principles;control
	engineering computing;formal verification;specification languages;}
}

@INPROCEEDINGS{4351334,
  author = {Prahofer, H. and Hurnaus, D. and Wirth, C. and Mossenbock, H.},
  title = {The Domain-Specific Language Monaco and its Visual Interactive Programming
	Environment},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {104 -110},
  month = {sept.},
  abstract = {Monaco is a domain-specific language for machine automation programming.
	It has been developed with the objective to empower domain experts
	with limited programming capabilities. Its main language features
	are an imperative notation for reactive systems, concepts for describing
	asynchronous event handling in a concise way, and a state-of-the-art
	component approach. Monaco is a programming language with a Pascal-like
	syntax, but also comes with a visual programming environment. In
	this paper we review the language Monaco, show the visual representation
	scheme, report on the programming environment and compare our visual
	notation to Statecharts.},
  doi = {10.1109/VLHCC.2007.14},
  keywords = {Monaco;Pascal-like syntax;asynchronous event handling;domain-specific
	language;language features;machine automation programming;reactive
	systems;visual interactive programming environment;visual programming
	environment;high level languages;programming environments;visual
	programming;}
}

@INPROCEEDINGS{5974321,
  author = {Prasetya, I.S.W.B. and Amorim, J. and Vos, T.E.J. and Baars, A.},
  title = {Using Haskell to script combinatoric testing of Web Services},
  booktitle = {Information Systems and Technologies (CISTI), 2011 6th Iberian Conference
	on},
  year = {2011},
  pages = {1 -6},
  month = {june},
  abstract = {The Classification Tree Method (CTM) is a popular approach in functional
	testing as it allows the testers to systematically partition the
	input domain of an SUT, and specifies the combinations they want.
	We have implemented the approach as a small domain specific language
	(DSL) embedded in the functional language Haskell. Such an embedding
	leads to clean syntax and moreover we can natively access Haskell's
	full features. This paper will explain the approach, and how it is
	applied for testing Web Services.},
  keywords = {CTM;DSL;Haskell;SUT;classification tree method;domain specific language;functional
	language Haskell;functional testing;script combinatoric testing;web
	services;Web services;pattern classification;program testing;tree
	data structures;}
}

@ARTICLE{809571,
  author = {Prechelt, L.},
  title = {Exploiting domain-specific properties: compiling parallel dynamic
	neural network algorithms into efficient code},
  journal = {Parallel and Distributed Systems, IEEE Transactions on},
  year = {1999},
  volume = {10},
  pages = {1105 -1117},
  number = {11},
  month = {nov},
  abstract = {Domain-specific constraints can be exploited to implement compiler
	optimizations that are not otherwise feasible. Compilers for neural
	network learning algorithms can achieve near-optimal colocality of
	data and processes and near-optimal balancing of load over processors,
	even for dynamically irregular problems. This is impossible for general
	programs, but restricting programs to the neural algorithm domain
	allows for the exploitation of domain-specific properties. The operations
	performed by neural algorithms are broadcasts, reductions, and object-local
	operations only; the load distribution is regular with respect to
	the (perhaps irregular) network topology; changes of network topology
	occur only from time to time. A language, compilation techniques,
	and a compiler implementation on the MasPar MP-1 are described and
	quantitative results for the effects of various optimizations used
	in the compiler are shown. Conservative experiments with weight pruning
	algorithms yield performance improvements of 27 percent due to load
	balancing and 195 percent improvement is achieved due to data locality,
	both compared to unoptimized versions. Two other optimizations-connection
	allocation and selecting the number of replicates-speed programs
	up by about 50 percent and 100 percent, respectively. This work can
	be viewed as a case study in exploiting domain-specific information;
	some of the principles presented here may apply to other domains
	as well},
  doi = {10.1109/71.809571},
  issn = {1045-9219},
  keywords = {compiler;compiler optimizations;connection allocation;neural algorithm
	domain;neural network learning algorithms;parallel dynamic neural
	network algorithms;neural nets;optimising compilers;parallel algorithms;parallel
	programming;}
}

@INPROCEEDINGS{5971283,
  author = {Qamar, A. and Wikander, J. and During, C.},
  title = {A mechatronic design infrastructure integrating heterogeneous models},
  booktitle = {Mechatronics (ICM), 2011 IEEE International Conference on},
  year = {2011},
  pages = {212 -217},
  month = {april},
  abstract = {Mechatronic system design is contemplated extensively through model-based
	approaches. To reflect the multi-domain integration inside a mechatronic
	system during the design process, integration approaches targeting
	heterogeneous models are essential. An infrastructure supporting
	the design of multi-domain systems is presented. The dependencies
	between the domain-specific models are managed by utilizing SysML
	as a common system modeling language between those models. A model
	integration framework supports model transformations between the
	system-model and the domain-specific models by utilizing Eclipse
	Modeling Framework. Structural and parameter type dependencies between
	different domain-models are handled to provide consistency between
	different views. This provides an ability to traverse between different
	views of the system, as well as maintaining consistency between those
	views. A case study on a robot system is presented to explain the
	integration of mechanical design model in Solid Edge, and dynamic
	analysis model in Simulink/SimMechanics through a SysML system model,
	all under the proposed design infrastructure. The approach is scalable
	towards other modeling formalisms by building new relations through
	the SysML model. This will support: co-evolution of domain-specific
	models, a better design of multi-domain systems, reduction in modeling
	in-consistencies, and a reduction in design time.},
  doi = {10.1109/ICMECH.2011.5971283},
  keywords = {Eclipse Modeling Framework;Simulink/SimMechanics;Solid Edge;SysML;dynamic
	analysis model;mechanical design model;mechatronic system design;multidomain
	systems;robot system;system modeling language;design engineering;mechanical
	engineering computing;mechatronics;robots;}
}

@INPROCEEDINGS{5432640,
  author = {Kan Qin and Yujiu Yang and Shiqiang Zhen and Wenhuang Liu},
  title = {A Unified Record Linkage Strategy for Web Service Data},
  booktitle = {Knowledge Discovery and Data Mining, 2010. WKDD '10. Third International
	Conference on},
  year = {2010},
  pages = {253 -256},
  month = {jan.},
  abstract = {Record linkage, also known as duplicate detection, is a key process
	that ensures the quality of data stored for Web service data. Given
	two lists of records, record linkage consists of determining all
	pairs that are similar to each other, where the overall similarity
	between two records is defined based on domain-specific similarities
	over individual attributes constituting the record. In this paper,
	we present a unified framework for recognizing clusters of near-duplicate
	records of multi-language data, specially for Chinese/English mixed
	Web data. The key ideas are: (1)Pre-processing multi-language data
	Using Chinese words segmentation and Chinese named entity recognition
	techniques; (2) Pair-wise comparison method based on domain- specific
	similarities, especially, the string kernel method; (3)a priority
	queue of duplicate clusters and representative records strategy to
	respond adaptively to the data scale. Experiments on real databases
	show that the proposed recode linkage strategy is efficiency and
	effectiveness.},
  doi = {10.1109/WKDD.2010.134},
  keywords = {Chinese named entity recognition;Chinese words segmentation;Chinese/English
	mixed web data;clusters recognition;data quality;domain-specific
	similarities;duplicate detection;pairwise comparison method;preprocessing
	multilanguage data;string kernel method;;unified framework;unified
	record linkage strategy;web service data;Web services;data analysis;natural
	language processing;}
}

@INPROCEEDINGS{5370898,
  author = {Fang-Liang Qiu and Lei Yin},
  title = {Research on Domain Requirement Analysis Method Used Ontology},
  booktitle = {Computational Intelligence and Design, 2009. ISCID '09. Second International
	Symposium on},
  year = {2009},
  volume = {1},
  pages = {299 -301},
  month = {dec.},
  abstract = {The domain requirement analysis not only is the important content
	to put domain engineering into practice, but also is the guarantee
	to realize domain specific software reuse. However, there is the
	communication gap among the domain requirement analysis because
	of the difference comprehension to domain knowledge. To resolve the
	problem, a method of domain requirement analysis based on the ontology
	is studied, which utilizes the advantages of the ontology in expressing
	domain knowledge and provides a well defined and unambiguous requirement
	definition to overcome the communication gaps among domain analysts.
	Finally, an instance of building domain ontology by UML is presented
	to depict the process of domain requirement analysis ontology-based
	method.},
  doi = {10.1109/ISCID.2009.82},
  keywords = {UML;domain engineering;domain requirement analysis;domain specific
	software reuse;ontology;Unified Modeling Language;ontologies (artificial
	intelligence);software reusability;}
}

@INPROCEEDINGS{5373500,
  author = {Quarteroni, S. and Dinarelli, M. and Riccardi, G.},
  title = {Ontology-based grounding of Spoken Language Understanding},
  booktitle = {Automatic Speech Recognition Understanding, 2009. ASRU 2009. IEEE
	Workshop on},
  year = {2009},
  pages = {438 -443},
  month = {13 2009-dec. 17},
  abstract = {Current Spoken Language Understanding models rely on either hand-written
	semantic grammars or flat attribute-value sequence labeling. In most
	cases, no relations between concepts are modeled, and both concepts
	and relations are domain-specific, making it difficult to expand
	or port the domain model. In contrast, we expand our previous work
	on a domain model based on an ontology where concepts follow the
	predicate-argument semantics and domain-independent classical relations
	are defined on such concepts. We conduct a thorough study on a spoken
	dialog corpus collected within a customer care problem-solving domain,
	and we evaluate the coverage and impact of the ontology for the interpretation,
	grounding and re-ranking of spoken language understanding interpretations.},
  doi = {10.1109/ASRU.2009.5373500},
  keywords = {domain independent classical relations;flat attribute-value sequence
	labeling;hand-written semantic grammars;ontology-based grounding;predicate-argument
	semantics;spoken dialog corpus;spoken language understanding interpretation;interactive
	systems;natural language processing;ontologies (artificial intelligence);}
}

@INPROCEEDINGS{1016490,
  author = {Quinlan, D.J. and Miller, B. and Philip, B. and Schordan, M.},
  title = {Treating a user-defined parallel library as a domain-specific language},
  booktitle = {Parallel and Distributed Processing Symposium., Proceedings International,
	IPDPS 2002, Abstracts and CD-ROM},
  year = {2002},
  pages = {105 -114},
  abstract = {Not available},
  doi = {10.1109/IPDPS.2002.1016490}
}

@INPROCEEDINGS{1046354,
  author = {Quinn, A.},
  title = {An interrogative approach to novice programming},
  booktitle = {Human Centric Computing Languages and Environments, 2002. Proceedings.
	IEEE 2002 Symposia on},
  year = {2002},
  pages = { 83 - 85},
  abstract = { Domain specific programming languages tend to be rigid in capability,
	and dependent on either a graphical interface or a scripting language.
	We present a question-oriented approach that requires no prior knowledge
	of programming and can be easily adapted to a wide range of domains.
	Interrogative programming works by "parsing" the user's intent using
	the responses to a series of closed-ended questions. Questions are
	guided by a context free grammar specified in an external file. We
	discuss the benefits, capabilities and limitations of interrogative
	programming along with the results of recent usability studies with
	our prototype.},
  doi = {10.1109/HCC.2002.1046354},
  issn = { },
  keywords = { closed ended questions; context free grammar; external file; interrogative
	programming; novice programming; question-oriented approach; usability;
	user intent parsing; automatic programming; context-free grammars;}
}

@INPROCEEDINGS{4662942,
  author = {Raedisch, T. and Weissenberg, N. and Holtkamp, B.},
  title = {Domain-Specific Individualization of Workflows},
  booktitle = {Grid and Cooperative Computing, 2008. GCC '08. Seventh International
	Conference on},
  year = {2008},
  pages = {719 -726},
  month = {oct.},
  abstract = {The virtualization of IT services by using software-as-a-service offers
	imposes the problem of correct service usage. Often an application
	level protocol has to be followed to assure logically correct workflows.
	This paper presents a domain-specific approach for the specification
	of such application level protocols. A domain ontology is combined
	with a formal protocol specification language to enable correct usage
	patterns even for dynamically adapted workflows to support individual
	needs.},
  doi = {10.1109/GCC.2008.81},
  keywords = {IT services;application level protocol;correct usage patterns;domain
	ontology;domain-specific individualization;dynamically adapted workflows;formal
	protocol specification language;software-as-a-service;virtualization;formal
	specification;ontologies (artificial intelligence);protocols;specification
	languages;workflow management software;}
}

@INPROCEEDINGS{5587025,
  author = {Rahimi, R. and Khosravi, R.},
  title = {Architecture conformance checking of multi-language applications},
  booktitle = {Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International
	Conference on},
  year = {2010},
  pages = {1 -8},
  month = {may},
  abstract = {As the development in a software project goes on, the structure of
	the implemented code diverges from the intended architecture. To
	prevent this, architecture conformance methods are used to check
	if the source code complies with the architecture. In the development
	of today's enterprise applications, general-purpose programming languages
	are used along with a number of domain specific languages. So, there
	is a need for a conformance checking method to support multi-language
	source artifacts. We present a model-based approach for checking
	cross-language architecture conformance rules. Our method is extensible,
	in the sense that it is independent of the specific set of languages
	used in the project.},
  doi = {10.1109/AICCSA.2010.5587025},
  keywords = {architecture conformance checking;cross-language architecture conformance
	rules;enterprise applications;general-purpose programming languages;multilanguage
	applications;multilanguage source artifacts;software project development;source
	code checking;conformance testing;software architecture;software
	development management;}
}

@INPROCEEDINGS{1021155,
  author = {Rahman, A. and Alam, H. and Hua Cheng and Llido, P. and Tarnikova,
	Y. and Kumar, A. and Tjabjadi, T. and Wilcox, C. and Nakatsu, C.
	and Hartono, R.},
  title = {Fusion of two parsers for a natural language processing toolkit},
  booktitle = {Information Fusion, 2002. Proceedings of the Fifth International
	Conference on},
  year = {2002},
  volume = {1},
  pages = { 228 - 234 vol.1},
  abstract = { With the rapid growth of real world applications for natural language
	processing (NLP) systems, there is a genuine demand for a general
	toolkit from which programmers with no linguistic knowledge can build
	specific NLP systems. Such a toolkit should have a parser that is
	general enough to be used across domains, and yet accurate enough
	for each specific application. In this paper, the fusion of two parsers
	to achieve both generality and accuracy in handling domain specific
	NL problems is described. Testing this combined parser on a corpus
	shows that the accuracy is significantly higher than a system that
	uses a single parser.},
  doi = {10.1109/ICIF.2002.1021155},
  issn = { },
  keywords = { accuracy; corpus; natural language processing toolkit; parser fusion;
	grammars; natural language interfaces; speech-based user interfaces;}
}

@ARTICLE{4731232,
  author = {Rajan, H. and Hosamani, M.},
  title = {Tisa: Toward Trustworthy Services in a Service-Oriented Architecture},
  journal = {Services Computing, IEEE Transactions on},
  year = {2008},
  volume = {1},
  pages = {201 -213},
  number = {4},
  month = {oct.-dec. },
  abstract = {Verifying whether a service implementation is conforming to its service-level
	agreements is important to inspire confidence in services in a service-oriented
	architecture (SoA). Functional agreements can be checked by observing
	the published interface of the service, but other agreements that
	are more non-functional in nature, are often verified by deploying
	a monitor that observes the execution of the service implementation.
	A problem is that such a monitor must execute in an untrusted environment.
	Thus, integrity of the results reported by such a monitor crucially
	depends on its integrity. We contribute an extension of the traditional
	SoA, based on hardware-based root of trust, that allows clients,
	brokers and providers to negotiate and validate the integrity of
	a requirements monitor executing in an untrusted environment. We
	make two basic claims: first, that it is feasible to realize our
	approach using existing hardware and software solutions, and second,
	that integrity verification can be done at a relatively small overhead.
	To evaluate feasibility, we have realized our approach using current
	software and hardware solutions. To measure overhead, we have conducted
	a case study using a collection of Web service implementations available
	with Apache Axis implementation.},
  doi = {10.1109/TSC.2008.18},
  issn = {1939-1374},
  keywords = {Web service;functional agreements;service-level agreements;service-oriented
	architecture;trustworthy services;Web services;security of data;software
	architecture;}
}

@ARTICLE{798319,
  author = {Ramming, J.C. and Wile, D.S.},
  title = {Guest editorial - introduction to the special section},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {289 -290},
  number = {3},
  month = {may/jun},
  abstract = {Not available},
  doi = {10.1109/TSE.1999.798319},
  issn = {0098-5589}
}

@ARTICLE{6062550,
  author = {Ranabahu, A. and Anderson, P. and Sheth, A.},
  title = {The Cloud Agnostic e-Science Analysis Platform},
  journal = {Internet Computing, IEEE},
  year = {2011},
  volume = {15},
  pages = {85 -89},
  number = {6},
  month = {nov.-dec. },
  abstract = {The amount of data being generated for e-Science domains has grown
	exponentially in the past decade, yet the adoption of new computational
	techniques in these fields hasn't seen similar improvements. The
	presented platform can exploit the power of cloud computing while
	providing abstractions for scientists to create highly scalable data
	processing workflows.},
  doi = {10.1109/MIC.2011.159},
  issn = {1089-7801},
  keywords = {cloud agnostic e-science analysis;cloud computing;data processing
	workflow;cloud computing;data handling;natural sciences computing;}
}

@INPROCEEDINGS{586466,
  author = {Rangarajan, M. and Penix, J. and Alexander, P. and Wilsey, P.A.},
  title = {Gravity: An object-oriented framework for hardware/software tool
	integration},
  booktitle = {Simulation Symposium, 1997. Proceedings. 30th Annual},
  year = {1997},
  pages = {24 -30},
  month = {apr},
  abstract = {Systems development is becoming more and more complex. It consists
	of many phases such as high-level design, low-level design, design
	validation, simulation, and so on. Throughout the design process,
	a variety of tools are used to assist and automate the various phases.
	These tools are often incompatible, forcing the design phases to
	be performed in isolation. This limits the iterative possibilities
	of the design process and forces the designer to make commitments,
	such as hardware or software implementation, early in the design
	process. The Gravity system is a tool integration framework designed
	to provide continuity throughout the design process. It is intended
	to be used to build domain-specific design environments. Gravity
	provides an easy way to construct a common store of objects involved
	in design, and to apply various tools to these objects. New languages
	and tools can be easily integrated within the framework. The system
	makes no distinction between hardware and software modules, thereby
	facilitating hardware/software co-design. Gravity is implemented
	in JAVA, providing portability networking capability and graphical
	extensibility},
  doi = {10.1109/SIMSYM.1997.586466},
  keywords = {Gravity;JAVA;design validation;domain-specific design environments;graphical
	extensibility;hardware/software codesign;hardware/software tool integration;high-level
	design;low-level design;object-oriented framework;portability networking
	capability;systems development;high level synthesis;logic design;object-oriented
	programming;programming environments;software tools;}
}

@INPROCEEDINGS{1620115,
  author = {O. Ratcliffe and S. Cimpan and F. Oquendo},
  title = {Case Study on Architecture-Centered Design for Monitoring Views at
	CERN},
  booktitle = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference
	on},
  year = {2005},
  pages = {213 -214},
  month = { },
  abstract = {The CERN #146;s Technical Control Room has implemented a method used
	to define the monitoring information that is necessary to efficiently
	restart an accelerator after a major breakdown [1]. This method is
	the basis for the development of a set of monitoring views, which
	must follow accurate rules in order to be easily understood and efficiently
	used by the control room operators.},
  doi = {10.1109/WICSA.2005.20}
}

@INPROCEEDINGS{4639089,
  author = {Rath, I. and Vago, D. and Varro, D.},
  title = {Design-time simulation of domain-specific models by incremental pattern
	matching},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {219 -222},
  month = {sept.},
  abstract = {In this paper, we present a general purpose discrete event simulation
	framework for domain-specific visual languages describing system
	behavior. In our framework, the dynamic semantics of the language
	is captured by a combination of graph transformation and abstract
	state machine rules as provided by the model transformation language
	of VIATRA2, which allows to capture complex model changes at each
	simulation step. For an efficient execution of the simulation, incremental
	graph pattern matching is used to avoid the re-computation of enabledness
	conditions of simulation rules by incrementally keeping track of
	rule contexts.},
  doi = {10.1109/VLHCC.2008.4639089},
  issn = {1943-6092},
  keywords = {abstract state machine rule;discrete event simulation;domain-specific
	model transformation language;domain-specific visual language;dynamic
	semantic language;graph transformation;incremental pattern matching;system
	behavior;discrete event simulation;finite state machines;graph grammars;graph
	theory;pattern matching;visual languages;}
}

@INPROCEEDINGS{4493315,
  author = {Ratiu, D. and Feilkas, M. and Jurjens, J.},
  title = {Extracting Domain Ontologies from Domain Specific APIs},
  booktitle = {Software Maintenance and Reengineering, 2008. CSMR 2008. 12th European
	Conference on},
  year = {2008},
  pages = {203 -212},
  month = {april},
  abstract = {Domain specific APIs offer their clients ready-to-use implementations
	of domain concepts. Beside being interfaces between the worlds of
	humans and computers, domain specific APIs contain a considerable
	amount of domain knowledge. Due to the big abstraction gap between
	the real world and today's programming languages, in addition to
	the knowledge about their domain, these APIs are cluttered with a
	considerable amount of noise in form of implementation detail. Furthermore,
	an API offers a particular view on its domain and different APIs
	regard their domains from different perspectives. In this paper we
	propose an approach for building domain ontologies by identifying
	commonalities between domain specific APIs that target the same domain.
	Besides our ontology extraction algorithm, we present a methodology
	for eliminating the noise and we sketch possible usage-scenarios
	of the ontologies for program analysis and understanding. We evaluate
	our approach through a set of case-studies on extracting domain ontologies
	from well-known domain specific APIs.},
  doi = {10.1109/CSMR.2008.4493315},
  issn = {1534-5351},
  keywords = {domain knowledge;domain ontology extraction;domain specific API;program
	analysis;program understanding;application program interfaces;ontologies
	(artificial intelligence);program diagnostics;reverse engineering;}
}

@INPROCEEDINGS{1342741,
  author = {Rauschmayer, A. and Knapp, A. and Wirsing, M.},
  title = {Consistency checking in an infrastructure for large-scale generative
	programming},
  booktitle = {Automated Software Engineering, 2004. Proceedings. 19th International
	Conference on},
  year = {2004},
  pages = {238 -247},
  month = {sept.},
  abstract = {Ubiquitous computing increases the pressure on the software industry
	to produce ever more and error-free code. Two recipes from automated
	programming are available to meet this challenge: On the one hand,
	generative programming raises the level of abstraction in software
	development by describing problems in high-level domain-specific
	languages and making them executable. On the other hand, in situations
	where one needs to produce a family of similar programs, product
	line engineering supports code reuse by composing programs from a
	set of common assets (or features). AHEAD (algebraic hierarchical
	equations for application design) is a framework for generative programming
	and product line engineering that achieves additional productivity
	gains by scaling feature composition up. Our contribution is GRAFT,
	a calculus that gives a formal foundation to AHEAD and provides several
	mechanisms for making sure that feature combinations are legal and
	that features in themselves are consistent},
  doi = {10.1109/ASE.2004.1342741},
  issn = {1068-3062},
  keywords = {AHEAD;GRAFT;algebraic hierarchical equations for application design;automated
	programming;code reuse;consistency checking;error-free code;high-level
	domain-specific language;large-scale generative programming;product
	line engineering;software development;software industry;ubiquitous
	computing;automatic programming;data integrity;formal specification;program
	compilers;program verification;software reusability;ubiquitous computing;}
}

@INPROCEEDINGS{565528,
  author = {Ravindran, E. and Hui, S.C.},
  title = {Incorporating deduction into object Petri nets},
  booktitle = {Systems, Man, and Cybernetics, 1996., IEEE International Conference
	on},
  year = {1996},
  volume = {3},
  pages = {2305 -2310 vol.3},
  month = {oct},
  abstract = {We propose a Petri net model which inherits features of object oriented
	and deductive paradigms of computing. Embedding these features at
	the modelling level facilitates specification and validation of problem
	descriptions and their attempted solutions. The proposed model extends
	the object Petri net model by incorporating domain specific constraints
	in the form of rules. Such a model can capture deducto-object oriented
	computations, wherein the modeller organises computations along object
	oriented, or deductive lines or a combination of both},
  doi = {10.1109/ICSMC.1996.565528},
  issn = {1062-922X},
  keywords = {cooperative systems;deduction;deductive object Petri nets;domain specific
	constraints;modelling;object oriented programming;Petri nets;cooperative
	systems;modelling;object-oriented programming;software agents;}
}

@INPROCEEDINGS{5254228,
  author = {Razavi, A. and Kontogiannis, K.},
  title = {ProtoTalk: A Generative Software Engineering Framework for Prototyping
	Protocols in Smalltalk},
  booktitle = {Computer Software and Applications Conference, 2009. COMPSAC '09.
	33rd Annual IEEE International},
  year = {2009},
  volume = {1},
  pages = {435 -442},
  month = {july},
  abstract = {Network protocols are complex systems implemented by collections of
	equally complex software components. In many cases, the realization
	of such protocols requires extensive prototyping and experimentation
	with different alternative implementations.In this paper, we present
	ProtoTalk, a generative, domain-specific software framework that
	utilizes model driven software engineering principles for prototyping
	state and message driven protocols with emphasis on telecommunication
	and network protocols. The framework allows first, for modeling a
	variety of common protocol features by using mappings from state
	machines, sequence diagrams and packet encodings to ProtoTalk models,
	and second, for the consequent automatic generation of prototype
	Smalltalk code from the aforementioned ProtoTalk models. In this
	respect, the paper attempts to shed light on the use of generative
	model driven programming techniques within reflective object oriented
	programming languages and environments. As a proof of concept, we
	have specified in ProtoTalk and consequently generated in Smalltalk,
	several core features of the session initiation protocol.},
  doi = {10.1109/COMPSAC.2009.197},
  issn = {0730-3157},
  keywords = {ProtoTalk;domain-specific software framework;generative model driven
	programming technique;generative software engineering framework;message
	driven protocol;model driven software engineering principles;network
	protocols;packet encodings;prototype Smalltalk code generation;prototyping
	protocols;reflective object oriented programming languages;sequence
	diagrams;session initiation protocol;state driven protocol;state
	machines;telecommunication protocol;Smalltalk;finite state machines;object-oriented
	programming;program compilers;protocols;software engineering;telecommunication
	computing;}
}

@INPROCEEDINGS{4291096,
  author = {Rebernak, D. and Mernik, M.},
  title = {A tool for compiler construction based on aspect-oriented specifications},
  booktitle = {Computer Software and Applications Conference, 2007. COMPSAC 2007.
	31st Annual International},
  year = {2007},
  volume = {2},
  pages = {11 -16},
  month = {july},
  abstract = {Aspect-oriented programming (AOP) provides a way to modularize crosscutting
	concerns. Crosscuting concerns can be found in various representations
	of software artifacts and in different steps of software life cycle
	(e.g., source code, models, requirements, language grammars). This
	paper provides an introduction to the AspectLISA tool and its aspect-oriented
	specification language for programming language definition and compiler
	construction. AspectLISA is a mature, well-tested system for automatically
	generating compilers, interpreters, and other language related tools
	from formal incremental and reusable aspect-oriented attribute grammar-based
	specifications. In the paper we discuss about the issues involved
	in the design and implementation of domain-specific aspect language
	for compiler construction, as well as some benefits of aspect-oriented
	specifications.},
  doi = {10.1109/COMPSAC.2007.46},
  issn = {0730-3157},
  keywords = {AspectLISA tool;aspect-oriented programming;aspect-oriented specification
	language;compiler construction;crosscutting concern;domain-specific
	aspect language;programming language definition;software artifact;software
	life cycle;object-oriented programming;program compilers;specification
	languages;}
}

@ARTICLE{5035596,
  author = {Rebernak, D. and Mernik, M. and Wu, H. and Gray, J.},
  title = {Domain-specific aspect languages for modularising crosscutting concerns
	in grammars},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {184 -200},
  number = {3},
  month = {june },
  abstract = {The emergence of crosscutting concerns can be observed in various
	representations of software artefacts (e.g. source code, models,
	requirements and language grammars). Although much of the focus of
	aspect-oriented programming has been on aspect languages that augment
	the descriptive power of general-purpose programming languages, there
	is also a need for domain-specific aspect languages that address
	particular crosscutting concerns found in software representations
	other than traditional source code. This study discusses the issues
	involved in the design and implementation of domain-specific aspect
	languages that are focused within the domain of language specification.
	Specifically, the study outlines the challenges and issues faced
	while designing two separate aspect languages that assist in modularising
	crosscutting concerns in grammars.},
  doi = {10.1049/iet-sen.2007.0114},
  issn = {1751-8806},
  keywords = {aspect-oriented programming;domain-specific aspect languages;general-purpose
	programming languages;grammars;language specification;software artefacts;software
	representations;source code;grammars;object-oriented languages;object-oriented
	programming;specification languages;}
}

@INPROCEEDINGS{4814171,
  author = {Reichert, T. and Klaus, E. and Schoch, W. and Meroth, A. and Herzberg,
	D.},
  title = {A language for advanced protocol analysis in automotive networks},
  booktitle = {Software Engineering, 2008. ICSE '08. ACM/IEEE 30th International
	Conference on},
  year = {2008},
  pages = {593 -602},
  month = {may},
  abstract = {The increased use and interconnection of electronic components in
	automobiles has made communication behavior in automotive networks
	drastically more complex. Both communication designs at application
	level and complex communication scenarios are often under-specified
	or out of scope of existing analysis techniques. We extend traditional
	protocol analyzers in order to capture communication at the level
	of abstraction that reflects application design and show that the
	same technique can be used to specify, monitor and test complex scenarios.
	We present CFR (channel filter rule) models, a novel approach for
	the specification of analyzers and a domain-specific language that
	implements this approach. From CFR models, we can fully generate
	powerful analyzers that extract design intentions, abstract protocol
	layers and even complex scenarios from low level communication data.
	We show that three basic concepts (channels, filters and rules) are
	sufficient to build such powerful analyzers and identify possible
	areas of application.},
  doi = {10.1145/1368088.1368171},
  issn = {0270-5257},
  keywords = {automobile;automotive network;automotive systems engineering;channel
	filter rule;communication behavior;communication data;communication
	design;complex communication;domain-specific language;electronic
	component;protocol analysis;automotive components;automotive engineering;computer
	networks;filtering theory;protocols;telecommunication channels;}
}

@INPROCEEDINGS{841038,
  author = {Reid, A. and Peterson, J. and Hager, G. and Hudak, P.},
  title = {Prototyping real-time vision systems: an experiment in DSL design},
  booktitle = {Software Engineering, 1999. Proceedings of the 1999 International
	Conference on},
  year = {1999},
  pages = {484 -493},
  month = {may},
  abstract = {Describes the enhancement of XVision, a large library of C++ code
	for real-time vision processing, into FVision (pronounced "fission"),
	a fully-featured domain-specific language (DSL) embedded in Haskell.
	The resulting prototype system substantiates the claims of increased
	modularity, effective code reuse and rapid prototyping that characterize
	the DSL approach to systems design. It also illustrates the need
	for judicious interface design: relegating computationally expensive
	tasks to XVision (pre-existing C++ components) and leaving modular
	compositional tasks to FVision (Haskell). At the same time, our experience
	demonstrates how Haskell's advanced language features (specifically,
	parametric polymorphism, lazy evaluation, higher-order functions
	and automatic storage reclamation) permit a rapid DSL design that
	is itself highly modular and easily modified. Overall, the resulting
	hybrid system exceeded our expectations: visual tracking programs
	continue to spend most of their time executing low-level image processing
	code, while Haskell's advanced features allow us to quickly develop
	and test small prototype systems within a matter of a few days, and
	to develop realistic applications within a few weeks.},
  issn = {0270-5257},
  keywords = {C++ components;C++ library;FVision;Haskell;XVision;advanced language
	features;automatic storage reclamation;code reuse;computationally
	expensive tasks;computer vision;domain-specific language design;functional
	programming;higher-order functions;hybrid system;interface design;lazy
	evaluation;low-level image processing code;modular compositional
	tasks;modularity;parametric polymorphism;rapid prototyping;real-time
	vision systems prototyping;systems design;visual tracking programs;C++
	language;computer vision;functional languages;real-time systems;software
	libraries;software prototyping;software reusability;}
}

@INPROCEEDINGS{873655,
  author = {Reveillere, L. and Merillon, F. and Consel, C. and Marlet, R. and
	Muller, G.},
  title = {A DSL approach to improve productivity and safety in device drivers
	development},
  booktitle = {Automated Software Engineering, 2000. Proceedings ASE 2000. The Fifteenth
	IEEE International Conference on},
  year = {2000},
  pages = {101 -109},
  abstract = {Although new peripheral devices are emerging at a frantic pace and
	require the fast release of drivers, little progress has been made
	to improve the development of such device drivers. Too often, this
	development consists of decoding hardware intricacies, based on inaccurate
	documentation. Then, assembly-level operations need to be used to
	interact with the device. These low-level operations reduce the readability
	of the driver and prevent safety properties from being checked. This
	paper presents an approach based on domain-specific languages (DSLs)
	to overcome these problems. We define a language, named Devil (DEVice
	Interaction Language), dedicated to defining the basic communication
	with a device. Unlike a general-purpose language, Devil allows a
	description to be checked for consistency. This not only improves
	the safety of the interaction with the device but also uncovers bugs
	early in the development process. To asses our approach, we have
	shown that Devil is expressive enough to specify a large number of
	devices. To evaluate productivity and safety improvements over traditional
	development in C, we report an experiment based on mutation testing},
  doi = {10.1109/ASE.2000.873655},
  keywords = {Device Interaction Language;Devil language;assembly-level operations;debugging;description
	consistency checking;device communication definition;device driver
	development;domain-specific language;language expressiveness;mutation
	testing;peripheral devices;productivity;readability;safety;device
	drivers;high level languages;program testing;program verification;safety;software
	engineering;}
}

@INPROCEEDINGS{1620130,
  author = {J. Revillard and S. Cimpan and E. Benoit and F. Oquendo},
  title = {Intelligent Instrument Design With ArchWare ADL},
  booktitle = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference
	on},
  year = {2005},
  pages = {249 -250},
  month = { },
  abstract = {The design of intelligent instruments, i.e., sensors and actuators,
	is a complex field. Competences in physics, mechanics, and computing
	sciences are needed for this design, and strong constraints coming
	from the instrument hardware or the chosen fieldbus have to be respected.
	One of the sensitive issues of the design is that the software design
	has to be made by instrument designers who are domain specialists,
	but not software experts. Such a designer is able to easily design
	the hardware part of the intelligent instrument but not the software
	part.},
  doi = {10.1109/WICSA.2005.40}
}

@INPROCEEDINGS{1245397,
  author = {Reyes, A.A. and Espino, J.R. and Mohan, V. and Nadkar, M.},
  title = {Ad hoc software interfacing: enterprise application integration (EAI)
	when middleware is overkill},
  booktitle = {Computer Software and Applications Conference, 2003. COMPSAC 2003.
	Proceedings. 27th Annual International},
  year = {2003},
  pages = { 570 - 575},
  month = {nov.},
  abstract = { Enterprise application integration (EAI) is cooperation of disparate
	systems and components to implement business rules in a distributed
	environment. "Systems and components" can be computer-aided design
	(CAD) or software engineering (CASE) tools, enterprise databases,
	COTS applications, or in-house software. Ad hoc software interfacing
	(AHSI) is a special kind of EAI. A tradeoff analysis classifies an
	EAI problem as an AHSI problem when middleware solutions are seen
	as heavy-handed, i.e., the planned EAI is not expected to become
	broad enough to justify the generality of a middleware solution or
	the client is unwilling to pay for a unified data model. AHSI seeks
	to "wire" extant software applications as components in new, larger
	software applications. We call applications-as-components "appliponents".
	AHSI seeks to minimize appliponent modification to the greatest extent
	possible. We demonstrate solutions to AHSI problems using XML toolkits,
	domain-specific language toolkits, and Microsoft BizTalk Server.},
  doi = {10.1109/CMPSAC.2003.1245397},
  issn = {0730-3157 },
  keywords = { CAD; CASE tools; COTS applications; Microsoft BizTalk Server; XML
	toolkits; ad hoc software interfacing; applications-as-components;
	appliponent modification; business rules implementation; disparate
	components; domain-specific language toolkits; enterprise application
	integration; enterprise databases; in-house software; middleware;
	software engineering tools; software reusability; tradeoff analysis;
	unified data model; application program interfaces; business data
	processing; integrated software; open systems; software tools;}
}

@INPROCEEDINGS{802127,
  author = {Reyes, A.A. and Richardson, D.},
  title = {Siddhartha: a method for developing domain-specific test driver generators},
  booktitle = {Automated Software Engineering, 1999. 14th IEEE International Conference
	on.},
  year = {1999},
  pages = {81 -90},
  month = {oct},
  abstract = {Siddhartha applies the domain-specific language (DSL) paradigm to
	solve difficult problems in specification-based testing (SBT). Domain-specific
	test case data specifications (TestSpecs) and difficult-to-test program
	design styles engender difficult SBT problems, which are the essential
	phenomena of interest to Siddhartha. Difficult-to-test program design
	styles are explicitly represented by domain-specific, unit test driver
	reference designs that accommodate the problematic program design
	styles. DSLs are developed to represent both TestSpecs and Driver
	reference designs. A DSL language processing tool (a translator)
	is developed that maps TestSpecs into Drivers. We developed a prototype
	implementation of Siddhartha via Reasoning SDK (formerly known as
	Software Refinery) and developed two domain-specific TestSpec rarr;Driver
	translators. Each translator generated Drivers that revealed new
	failures in a real-world digital flight control application program},
  doi = {10.1109/ASE.1999.802127},
  keywords = {DSL language processing tool;Driver reference designs;Reasoning SDK;Siddhartha;TestSpecs;difficult-to-test
	program design styles;domain-specific language paradigm;domain-specific
	test case data specifications;domain-specific test driver generator
	development;domain-specific unit test driver reference designs;real-world
	digital flight control application program;specification-based testing;translator;aerospace
	computing;automatic testing;formal specification;program interpreters;program
	testing;}
}

@ARTICLE{5332231,
  author = {Ricca, F. and Di Penta, M. and Torchiano, M. and Tonella, P. and
	Ceccato, M.},
  title = {How Developers' Experience and Ability Influence Web Application
	Comprehension Tasks Supported by UML Stereotypes: A Series of Four
	Experiments},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2010},
  volume = {36},
  pages = {96 -118},
  number = {1},
  month = {jan.-feb. },
  abstract = {In recent years, several design notations have been proposed to model
	domain-specific applications or reference architectures. In particular,
	Conallen has proposed the UML Web Application Extension (WAE): a
	UML extension to model Web applications. The aim of our empirical
	investigation is to test whether the usage of the Conallen notation
	supports comprehension and maintenance activities with significant
	benefits, and whether such benefits depend on developers ability
	and experience. This paper reports and discusses the results of a
	series of four experiments performed in different locations and with
	subjects possessing different experience-namely, undergraduate students,
	graduate students, and research associates-and different ability
	levels. The experiments aim at comparing performances of subjects
	in comprehension tasks where they have the source code complemented
	either by standard UML diagrams or by diagrams stereotyped using
	the Conallen notation. Results indicate that, although, in general,
	it is not possible to observe any significant benefit associated
	with the usage of stereotyped diagrams, the availability of stereotypes
	reduces the gap between subjects with low skill or experience and
	highly skilled or experienced subjects. Results suggest that organizations
	employing developers with low experience can achieve a significant
	performance improvement by adopting stereotyped UML diagrams for
	Web applications.},
  doi = {10.1109/TSE.2009.69},
  issn = {0098-5589},
  keywords = {UML stereotypes;Web application comprehension tasks;source code;stereotyped
	diagrams;Internet;Unified Modeling Language;}
}

@INPROCEEDINGS{6061430,
  author = {Richardson, K.D. and Bobrow, D.G. and Condoravdi, C. and Waldinger,
	R. and Das, A.},
  title = {English Access to Structured Data},
  booktitle = {Semantic Computing (ICSC), 2011 Fifth IEEE International Conference
	on},
  year = {2011},
  pages = {13 -20},
  month = {sept.},
  abstract = {We present work on using a domain model to guide text interpretation,
	in the context of a project that aims to interpret English questions
	as a sequence of queries to be answered from structured databases.
	We adapt a broad-coverage and ambiguity-enabled natural language
	processing (NLP) system to produce domain-specific logical forms,
	using knowledge of the domain to zero in on the appropriate interpretation.
	The vocabulary of the logical forms is drawn from a domain theory
	that constitutes a higher-level abstraction of the contents of a
	set of related databases. The meanings of the terms are encoded in
	an axiomatic domain theory. To retrieve information from the databases,
	the logical forms must be instantiated by values constructed from
	fields in the database. The axiomatic domain theory is interpreted
	by the first-order theorem prover SNARK to identify the groundings,
	and then retrieve the values through procedural attachments semantically
	linked to the database. SNARK attempts to prove the logical form
	as a theorem by reasoning over the theory that is linked to the database
	and returns the exemplars of the proof(s) back to the user as answers
	to the query. The focus of this paper is more on the language task,
	however, we discuss the interaction that must occur between linguistic
	analysis and reasoning for an end-to-end natural language interface
	to databases. We illustrate the process using examples drawn from
	an HIV treatment domain, where the underlying databases are records
	of temporally bound treatments of individual patients.},
  doi = {10.1109/ICSC.2011.67},
  keywords = {English access;English question;NLP system;ambiguity-enabled natural
	language processing;axiomatic domain theory;broad-coverage natural
	language processing;deductive question answering;domain-specific
	logical form;end-to-end natural language interface;first-order theorem
	prover SNARK;higher-level abstraction;language task;linguistic analysis;query;reasoning;structured
	database;vocabulary;computational linguistics;natural language processing;query
	processing;question answering (information retrieval);theorem proving;vocabulary;}
}

@INPROCEEDINGS{5380018,
  author = {Richardson, K. and Jimenez, C. and Stephens, D.R.},
  title = {Evolution of the software communication architecture standard},
  booktitle = {Military Communications Conference, 2009. MILCOM 2009. IEEE},
  year = {2009},
  pages = {1 -8},
  month = {oct.},
  abstract = {Three primary objectives of the Joint Program Executive Officer (JPEO)
	Joint Tactical Radio System (JTRS) are: (a) reduce time to field
	capability, (b) improve interoperability between services, and (c)
	decrease radio production costs. The Software Communications Architecture
	(SCA) specification is the architectural framework for all JTRS software
	artifacts that was created to maximize software application portability,
	reusability, and scalability while providing the flexibility to address
	domain specific requirements. SCA version 1.0 was published in 2000
	and the last major release (version 2.2.2) was published in 2006.
	Since release of version 2.2.2, only minor enhancements have been
	made to the specification, which have been targeted towards addressing
	items of immediate concern to the development of the Increment 1
	JTRS products. Over the course of the last two years technologies
	have evolved and there have been many SCA related lessons learned
	through the development of JTRS products such as Ground Mobile Radio
	(GMR). JPEO JTRS asserts that the SCA will continue to evolve so
	that JTRS products meet the current and emerging needs of the next
	generation warfighter. To address the requirements of the JTRS stakeholders,
	JPEO JTRS has initiated the development of a new SCA release. The
	overriding philosophy behind this revision is to position the SCA
	as a specification that is comprehensive yet flexible enough to provide
	a technical foundation for multiple generations of JTRS and industry
	products. To accomplish this flexibility, the proposed SCA enhancements
	will migrate the specification towards a technology independent representation
	and away from the current dependence on Common Object Request Broker
	Architecture (CORBA) and extensible Markup Language (XML) Document
	Type Descriptor (DTD) files. A second feature will introduce optional
	elements within the specification so that compliant products may
	be developed which better map to the functional and resource req-
	uirements of a wide array of target platforms.},
  doi = {10.1109/MILCOM.2009.5380018},
  keywords = {CORBA;Extensible Markup Language;SCA specification;XML;common object
	request broker architecture;document type descriptor;joint program
	executive officer;joint tactical radio system;software application
	portability;software artifacts;software communication architecture
	standard;software reusability;software scalability;formal specification;military
	communication;military computing;software architecture;software radio;telecommunication
	computing;}
}

@INPROCEEDINGS{366656,
  author = {Riloff, E. and Lehnert, W.},
  title = {Automated dictionary construction for information extraction from
	text},
  booktitle = {Artificial Intelligence for Applications, 1993. Proceedings., Ninth
	Conference on},
  year = {1993},
  pages = {93 -99},
  month = {mar},
  abstract = {The authors have developed a tool called AutoSlog that automatically
	constructs domain-specific dictionaries given a set of annotated
	training texts. Using AutoSlog, a first-year graduate student who
	had minimal experience with the CIRCUS sentence analyzer on which
	AutoSlog is based, created a dictionary for the domain of terrorism
	in 8 hours. In the experiments, the 8-hour AutoSlog dictionary achieved
	90% of the performance of a hand-crafted dictionary that required
	1500 person-hours of effort by 2 advanced graduate students who were
	highly skilled with the sentence analyzer},
  doi = {10.1109/CAIA.1993.366656},
  keywords = {AutoSlog;CIRCUS sentence analyzer;annotated training texts;automated
	dictionary construction;domain-specific dictionaries;information
	extraction;character recognition;feature extraction;glossaries;word
	processing;}
}

@INPROCEEDINGS{302303,
  author = {Rinaldo, F.J. and Strutz, R.E. and Evens, M.W.},
  title = {Developing a lexicon for automatic knowledge acquisition},
  booktitle = {Expert Systems for Development, 1994., Proceedings of International
	Conference on},
  year = {1994},
  pages = {74 -78},
  month = {mar},
  abstract = {The major of the bottleneck in expert system development is knowledge
	acquisition. This paper focuses on the development of a comprehensive
	lexicon to be used in an automated knowledge acquisition system.
	Previous work demonstrated the feasibility of automatically acquiring
	expert knowledge from natural language analysis of articles published
	in medical journals. Domain specific knowledge was extracted from
	English text and then converted into a rule-based knowledge representation
	for a medical expert system. Previous work used a domain specific
	sublanguage lexicon that was focused on the specific domain area.
	The current work is focused on developing a more general purpose
	lexicon that can be used for a deeper understanding of the knowledge
	contained in English text},
  doi = {10.1109/ICESD.1994.302303},
  keywords = {English text;automatic knowledge acquisition;domain specific knowledge;expert
	system development;lexicon;medical expert system;medical journals;natural
	language analysis;rule-based knowledge representation;knowledge acquisition;knowledge
	representation;natural languages;}
}

@INPROCEEDINGS{927268,
  author = {Risi, W.A. and Martinez Lopez, P.E. and Marcos, D.H.},
  title = {HyCom: a domain specific language for hypermedia application development},
  booktitle = {System Sciences, 2001. Proceedings of the 34th Annual Hawaii International
	Conference on},
  year = {2001},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { Presents HyCom, a domain-specific language (DSL) for hypermedia authoring
	embedded in the language Haskell. HyCom provides a declarative framework
	for describing hypermedia designs and also automatic application
	generation. We propose HyCom as a bridge between engineering models
	and implementation environments. HyCom is based on the principle
	of programming by combination. A hypermedia application is constructed
	by the combination and transformation of components, promoting the
	reuse of existing assets and the abstraction of common patterns.
	The resulting framework is flexible and practical - yet rigorous
	and formal - enabling the effective representation of existing engineering
	methods primitives without loss of expressiveness. We present a real
	situation in which HyCom is used in the definition of an application
	developed following systematic steps. By means of an example, we
	show the general principles underlying its use for the mapping of
	design concepts to implementation environments.},
  doi = {10.1109/HICSS.2001.927268},
  issn = { },
  keywords = { Haskell; HyCom; asset reuse; automatic application generation; common
	patterns abstraction; components transformation; declarative framework;
	design concept mapping; domain-specific language; engineering methods
	primitives; engineering models; expressiveness; hypermedia application
	development; hypermedia authoring; hypermedia designs; implementation
	environments; programming by combination; authoring languages; functional
	languages; hypermedia;}
}

@INPROCEEDINGS{4351345,
  author = {Risoldi, M. and Buchs, D.},
  title = {A domain specific language and methodology for control systems GUI
	specification, verification and prototyping},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {179 -182},
  month = {sept.},
  abstract = {A work-in-progress domain-specific language and methodology for modeling
	complex control systems GUIs is presented. MDA techniques are applied
	for language design and verification, simulation and prototyping.},
  doi = {10.1109/VLHCC.2007.21},
  keywords = {GUI modeling;complex control systems;domain specific language;language
	design;language prototyping;language simulation;language verification;control
	engineering computing;digital simulation;formal specification;graphical
	user interfaces;large-scale systems;program verification;software
	prototyping;specification languages;}
}

@INPROCEEDINGS{4384890,
  author = {Ritala, T. and Kuikka, S.},
  title = {UML Automation Profile: Enhancing the Efficiency of Software Development
	in the Automation Industry},
  booktitle = {Industrial Informatics, 2007 5th IEEE International Conference on},
  year = {2007},
  volume = {2},
  pages = {885 -890},
  month = {june},
  abstract = {The development of modern distributed automation applications is challenging.
	However, high-level modeling potentially increases the efficiency
	of development. This paper introduces a UML profile for applications
	in the automation industry. The profile is formed by extending UML
	V2 with various automation domain specific concepts. The UML profile
	mechanism and selected existing UML profiles provide a basis for
	the novel automation profile. Future work will concentrate on integration
	of the profile to the requirement standards as well as providing
	tool support for the use of the automation profile.},
  doi = {10.1109/INDIN.2007.4384890},
  issn = {1935-4576},
  keywords = {UML automation profile;Unified Modeling Language;automation industry;software
	development;Unified Modeling Language;industrial engineering;software
	engineering;}
}

@INPROCEEDINGS{5680100,
  author = {Rittenbach, T. and Satake, H. and Redding, E. and Perry, K. and Thawani,
	M. and Dietrich, C. and Thandee, R.},
  title = {GRA model driven design process},
  booktitle = {MILITARY COMMUNICATIONS CONFERENCE, 2010 - MILCOM 2010},
  year = {2010},
  pages = {1151 -1156},
  month = {31 2010-nov. 3},
  abstract = {The purpose of the Government Reference Architecture (GRA) was to
	define a modular, open systems architecture that fostered reuse and
	technology insertion refresh with modular components and product
	line variants. A graphical model-driven development (MDD) approach
	was chosen to support rapid design and development of work products
	that could be quickly leveraged by radio developers within existing
	development processes with commercial modeling tools. The GRA has
	matured its model driven methodology through recent efforts transforming
	the OSSIE-based GRA testbed from an executable UM #x2122; Platform
	Independent Model (PIM) to an executable Software Communications
	Architecture (SCA) Platform Specific Model (PSM). In GRA Phase 2,
	IBM #x00AE; Rational #x00AE; Rhapsody #x00AE; was used to develop
	the UML PIM model for the purpose of GRA interface validation. The
	third step in GRA Phase 3 was to demonstrate an executable PSM implementation
	by combining the PSM work in Rhapsody with additional tool capability
	for the SCA CORBA #x00AE; model using the SCA domain specific MDD
	tool, PrismTech Spectra CX. Although gaps were identified in the
	path to an integrated, standards-based, automated GRA modeling tool
	chain, productivity gains around improved collaboration and code
	development accelerations were realized. Evolving the GRA through
	model-driven PIM and PSM development tools, including the tool chain
	integration process, will be described in detail in this paper, along
	with analysis of MDD productivity gains achieved with this integrated
	tool chain approach.},
  doi = {10.1109/MILCOM.2010.5680100},
  issn = {2155-7578},
  keywords = {GRA Phase 3;GRA interface validation;GRA model driven design process;OSSIE-based
	GRA testbed;PrismTech Spectra CX;Rhapsody;SCA CORBA;UM platform independent
	model;automated GRA modeling tool chain;government reference architecture;graphical
	model-driven development;modular components;open systems architecture;platform
	specific model;product line variants;productivity gains;software
	communications architecture;technology insertion;Unified Modeling
	Language;government data processing;software architecture;}
}

@INPROCEEDINGS{5295300,
  author = {Rivera, J.E. and Duran, F. and Vallecillo, A.},
  title = {A graphical approach for modeling time-dependent behavior of DSLs},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {51 -55},
  month = {sept.},
  abstract = {Domain specific languages (DSLs) play a cornerstone role in Model-Driven
	Software Development for representing models and metamodels. DSLs'
	abstract syntax are usually defined by a metamodel. In-place model
	transformations provide an intuitive way to complement metamod-els
	with behavioral specifications. In this paper we extend in-place
	rules with a quantitative model of time and with mechanisms that
	allow designers to state action properties, facilitating the design
	of real-time complex systems. This approach avoids making unnatural
	changes to the DSL metamodels to represent behavioral and time aspects.
	We present the graphical modeling tool we have built for visually
	specifying these timed specifications.},
  doi = {10.1109/VLHCC.2009.5295300},
  issn = {1943-6092},
  keywords = {domain specific languages;model-driven software development;real-time
	complex systems;time-dependent behavior modeling;formal specification;object-oriented
	programming;}
}

@INPROCEEDINGS{4383991,
  author = {Rivera, J.E. and Vallecillo, A.},
  title = {Adding Behavior to Models},
  booktitle = {Enterprise Distributed Object Computing Conference, 2007. EDOC 2007.
	11th IEEE International},
  year = {2007},
  pages = {169},
  month = {oct.},
  abstract = {Domain Specific Languages (DSLs) play a cornerstone role in Model-Driven
	Software Development for representing models and metamodels. DSLs
	are usually defined in terms of their abstract and concrete syntax
	only. This allows the rapid and inexpensive development of DSLs and
	their associated tools (e.g., editors), but does not allow the representation
	of their behavioral semantics, something especially important for
	model operations like simulation and verification. In this paper
	we explore the use of Maude as a formal notation for describing models
	and metamodels, including the specification of their dynamic behavior.},
  doi = {10.1109/EDOC.2007.40},
  issn = {1541-7719},
  keywords = {Maude;behavioral semantics;domain specific languages;formal notation;metamodels;model-driven
	software development;programming language semantics;software engineering;specification
	languages;}
}

@INPROCEEDINGS{4338399,
  author = {Rizzolo, N. and Roth, D.},
  title = {Modeling Discriminative Global Inference},
  booktitle = {Semantic Computing, 2007. ICSC 2007. International Conference on},
  year = {2007},
  pages = {597 -604},
  month = {sept.},
  abstract = {Many recent advances in complex domains such as natural language processing
	(NLP) have taken a discriminative approach in conjunction with the
	global application of structural and domain specific constraints.
	We introduce LBJ, a new modeling language for specifying exact inference
	systems of this type, combining ideas from machine learning, optimization,
	first order logic (FOL), and object oriented programming (OOP). Expressive
	constraints are specified declaratively as arbitrary FOL formulas
	over functions and objects. The language's run-time library translates
	them to a mathematical programming representation from which an exact
	solution is computed. In addition, the compiler leverages an existing
	OOP language: objects and functions are grounded as the OOP objects
	and methods that encapsulate the user's data.},
  doi = {10.1109/ICSC.2007.53},
  keywords = {discriminative global inference modeling;first order logic;language
	run-time library;learning based Java modeling language;machine learning;mathematical
	programming representation;object oriented programming;optimisation;program
	compiler;Java;formal logic;inference mechanisms;learning (artificial
	intelligence);mathematical programming;object-oriented programming;program
	compilers;simulation languages;software libraries;}
}

@INPROCEEDINGS{4019612,
  author = {Robby and Dwyer, M.B. and Hatcliff, J.},
  title = {Domain-specific Model Checking Using The Bogor Framework},
  booktitle = {Automated Software Engineering, 2006. ASE '06. 21st IEEE/ACM International
	Conference on},
  year = {2006},
  pages = {369 -370},
  month = {sept.},
  abstract = {Model checking has proven to be an effective technology for verification
	and debugging in hardware and more recently in software domains.
	We believe that recent trends in both the requirements for software
	systems and the processes by which systems are developed suggest
	that domain-specific model checking engines may be more effective
	than general purpose model checking tools. To overcome limitations
	of existing tools which tend to be monolithic and non-extensible,
	we have developed an extensible and customizable model checking framework
	called Bogor. In this tutorial, we give an overview of (a) Bogor's
	direct support for modeling object-oriented designs and implementations,
	(b) its facilities for extending and customizing its modeling language
	and algorithms to create domain-specific model checking engines,
	and (c) pedagogical materials that we have developed to describe
	the construction of model checking tools built on top of the Bogor
	infrastructure},
  doi = {10.1109/ASE.2006.34},
  issn = {1527-1366},
  keywords = {Bogor infrastructure;domain-specific model checking;object-oriented
	design;program debugging;program verification;software system requirements;object-oriented
	programming;program debugging;program verification;}
}

@INPROCEEDINGS{5349876,
  author = {Robert, S. and Gerard, S. and Terrier, F. and Lagarde, F.},
  title = {A Lightweight Approach for Domain-Specific Modeling Languages Design},
  booktitle = {Software Engineering and Advanced Applications, 2009. SEAA '09. 35th
	Euromicro Conference on},
  year = {2009},
  pages = {155 -161},
  month = {aug.},
  abstract = {Off-the-shelves general purpose modeling languages cannot obviously
	cover the whole range of needs that can be encountered in current
	systems design. Therefore, putting efficiently Model-Driven Engineering
	into practice involves designing specific modeling languages. The
	goal is to cover in a more suitable manner a particular application
	domain (e.g. automotive) or specific concerns (e.g. hardware modeling)
	or even to focus on a given class of practitioners. In this respect,
	two design approaches are generally opposed which respectively propose
	to define domain-specific modeling languages from scratch or to customize
	an existing general-purpose language. This paper focuses on the latter
	approach and claims that UML profiles do provide handy and powerful
	mechanisms to design domain-specific modeling languages but are penalized
	by lacks of methodological guidelines and tool support. To cope with
	these lacks, a profile design approach is introduced, which includes
	a methodological framework to structure profiles design process and
	tool support to partly automate this process.},
  doi = {10.1109/SEAA.2009.81},
  issn = {1089-6503},
  keywords = {UML profiles;domain-specific modeling languages design;model-driven
	engineering;Unified Modeling Language;software engineering;}
}

@INPROCEEDINGS{5501481,
  author = {Romano, B.L. and Braga e Silva, G. and da Cunha, A.M. and Mouro, W.I.},
  title = {Applying MDA Development Approach to a Hydrological Project},
  booktitle = {Information Technology: New Generations (ITNG), 2010 Seventh International
	Conference on},
  year = {2010},
  pages = {1127 -1132},
  month = {april},
  abstract = {This paper describes the application of an MDA development approach
	to the Project of Amazon Integration and Cooperation for Modernization
	of Hydrological Monitoring. This project is currently under the development
	of the Brazilian Aeronautics Institute of Technology. The adopted
	approach involves among other issues the source-code generation from
	UML models using the AndroMDA Framework. This development approach
	was implemented in the four different phases of: Modeling; Stereotypes
	and Properties Setting; Documentation; and Script Generation. Besides
	these implemented phases this paper describes also the configuration
	of the development environment. At the end, some of the main results
	are reported in terms of major findings and obtained outcomes.},
  doi = {10.1109/ITNG.2010.121},
  keywords = {Amazon Integration and Cooperation for Modernization of Hydrological
	Monitoring;AndroMDA framework;Brazilian Aeronautics Institute of
	Technology;MDA development;UML models;hydrological project;model
	driven architecture;properties setting;script generation;source-code
	generation;Unified Modeling Language;aerospace computing;software
	architecture;}
}

@INPROCEEDINGS{6042080,
  author = {Romeikat, R. and Bauer, B.},
  title = {Formal Specification of Domain-Specific ECA Policy Models},
  booktitle = {Theoretical Aspects of Software Engineering (TASE), 2011 Fifth International
	Symposium on},
  year = {2011},
  pages = {209 -212},
  month = {aug.},
  abstract = {Policy-based management allows to adapt systems to changed requirements
	in a flexible and automated way. Policy development usually starts
	with the specification of high-level policies, which are then refined
	into a low-level representation. We use models to specify event-condition-action
	(ECA) policies at different levels of abstraction and consequently
	separate domain and policy aspects from each other. Domain-specific
	concepts are used within policies in their event, condition, and
	action parts. We present a formal specification of the models by
	means of a relational algebra. The algebra is used to validate the
	models at each level. Finally, executable policy code is generated
	from the low-level models.},
  doi = {10.1109/TASE.2011.29},
  keywords = {domain specific ECA policy models;event condition action;formal specification;policy
	based management;relational algebra;formal specification;relational
	algebra;}
}

@INPROCEEDINGS{5992002,
  author = {de Roo, A. and Sozer, H. and Aksit, M.},
  title = {Runtime Verification of Domain-Specific Models of Physical Characteristics
	in Control Software},
  booktitle = {Secure Software Integration and Reliability Improvement (SSIRI),
	2011 Fifth International Conference on},
  year = {2011},
  pages = {41 -50},
  month = {june},
  abstract = {Control logic of embedded systems is nowadays largely implemented
	in software. Such control software implements, among others, models
	of physical characteristics, like heat exchange among system components.
	Due to evolution of system properties and increasing complexity,
	faults can be left undetected in these models. Therefore, their accuracy
	must be verified at runtime. Traditional runtime verification techniques
	that are based on states and/or events in software execution are
	inadequate in this case. The behavior suggested by models of physical
	characteristics cannot be mapped to behavioral properties of software.
	Moreover, implementation in a general-purpose programming language
	makes these models hard to locate and verify. This paper presents
	a novel approach to explicitly specify models of physical characteristics
	using a domain-specific language, to define monitors for inconsistencies
	by detecting and exploiting redundancy in these models, and to realize
	these monitors using an aspect-oriented approach. The approach is
	applied to two industrial case studies.},
  doi = {10.1109/SSIRI.2011.14},
  keywords = {aspect-oriented approach;behavioral property;control logic;control
	software execution;domain-specific language;domain-specific model;embedded
	system;general-purpose programming language;heat exchange;physical
	characteristics;runtime verification technique;aspect-oriented programming;computational
	complexity;control engineering computing;embedded systems;heat transfer;program
	verification;specification languages;}
}

@INPROCEEDINGS{5277690,
  author = {Rosenberg, F. and Celikovic, P. and Michlmayr, A. and Leitner, P.
	and Dustdar, S.},
  title = {An End-to-End Approach for QoS-Aware Service Composition},
  booktitle = {Enterprise Distributed Object Computing Conference, 2009. EDOC '09.
	IEEE International},
  year = {2009},
  pages = {151 -160},
  month = {sept.},
  abstract = {A simple and effective composition of software services into higher-level
	composite services is still a very challenging task. Especially in
	enterprise environments, quality of service (QoS) concerns play a
	major role when building software systems following the service-oriented
	architecture (SOA) paradigm. Inthis paper we present a composition
	approach based on a domain-specific language(DSL) for specifying
	functional requirements of services and the expected QoS inform of
	constraint hierarchies by leveraging hard and soft constraints. Acomposition
	runtime will resolve the user's constraints to find an optimize dcomposition
	semi-automatically. To this end we leverage data flow analysis to
	generate a structured composition model and use two different techniques
	for the optimization, a constraint programming and an integer programming
	approach.},
  doi = {10.1109/EDOC.2009.14},
  issn = {1541-7719},
  keywords = {DSL;QoS-aware service composition;SOA;constraint programming;data
	flow analysis;domain-specific language;end-to-end approach;enterprise
	environment;functional requirement;hard constraint;integer programming
	approach;optimization;quality-of-service;service-oriented architecture;soft
	constraint;software service;structured composition model;user constraint;Web
	services;constraint handling;data flow analysis;integer programming;quality
	of service;software architecture;specification languages;}
}

@INPROCEEDINGS{4812599,
  author = {Rosenberg, F. and Leitner, P. and Michlmayr, A. and Celikovic, P.
	and Dustdar, S.},
  title = {Towards Composition as a Service - A Quality of Service Driven Approach},
  booktitle = {Data Engineering, 2009. ICDE '09. IEEE 25th International Conference
	on},
  year = {2009},
  pages = {1733 -1740},
  month = {29 2009-april 2},
  abstract = {Software as a Service (SaaS) and the possibility to compose Web services
	provisioned over the Internet are important assets for a service-oriented
	architecture (SOA). However, the complexity and time for developing
	and provisioning a composite service is very high and it is generally
	an error-prone task. In this paper we address these issues by describing
	a semi-automated "Composition as a Service'' (CaaS) approach combined
	with a domain-specific language called VCL (Vienna composition language).
	The proposed approach facilitates rapid development and provisioning
	of composite services by specifying what to compose in a constraint-hierarchy
	based way using VCL. Invoking the composition service triggers the
	composition process and upon success the newly composed service is
	immediately deployed and available. This solution requires no client-side
	composition infrastructure because it is transparently encapsulated
	in the CaaS infrastructure.},
  doi = {10.1109/ICDE.2009.153},
  issn = {1084-4627},
  keywords = {Internet;Vienna composition language;Web services;composition as a
	service;service-oriented architecture;software as a service;Web services;software
	architecture;}
}

@INPROCEEDINGS{5699164,
  author = {Rosso-Pelayo, D.A. and Trejo-Ramrez, R.A. and Gonzalez-Mendoza,
	M. and Hernandez-Gress, N.},
  title = {Business Process Mining and Rules Detection for Unstructured Information},
  booktitle = {Artificial Intelligence (MICAI), 2010 Ninth Mexican International
	Conference on},
  year = {2010},
  pages = {81 -85},
  month = {nov.},
  abstract = {In this article we show how to find evidence of incomplete or fractured
	processes in non-structured reports of known business processes,
	by means of rules, patterns and detection of cause-effect relationships.
	A priori classifications and probabilities of process activities
	are used as inputs for the analysis and rules detection. In this
	method we use a domain-specific ontology associated to process activities
	in order to improve on previous results, where occurrence of a process
	in a document set was detected by means of SLM.},
  doi = {10.1109/MICAI.2010.22},
  keywords = {SLM;business process mining;cause effect relationships;document set;domain
	specific ontology;fractured processes;nonstructured reports;priori
	classifications;process activities;rule detection;statistical language
	model;unstructured information;business data processing;data mining;document
	handling;ontologies (artificial intelligence);pattern classification;probability;statistical
	analysis;}
}

@INPROCEEDINGS{1240326,
  author = {Rosu, G. and Feng Chen},
  title = {Certifying measurement unit safety policy},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = { 304 - 309},
  month = {oct.},
  abstract = { Measurement unit safety policy checking is a topic in software analysis
	concerned with ensuring that programs do not violate basic principles
	of units of measurement. Such violations can hide significant domain-specific
	errors which are hard or impossible to find otherwise. Measurement
	unit analysis by means of automatic deduction is addressed in this
	paper. We draw general design principles for measurement unit certification
	tools and discuss our prototype for the C language, which includes
	both dynamic and static checkers. Our approach is based on assume/assert
	annotations of code, which are properly interpreted by our deduction-based
	tools and ignored by standard compilers. We do not modify the language
	in order to support units. The approach can be extended to incorporate
	other safety policies without great efforts.},
  doi = {10.1109/ASE.2003.1240326},
  issn = {1527-1366},
  keywords = { C language; automatic deduction; certifying measurement unit safety
	policy; code assert annotations; code assume annotations; deduction-based
	tools; domain-specific errors; dynamic checkers; measurement unit
	analysis; measurement unit certification tools; software analysis;
	static checkers; C language; certification; formal logic; inference
	mechanisms; program diagnostics; program verification; software architecture;
	software metrics;}
}

@INPROCEEDINGS{5210820,
  author = {Rozsnyai, S. and Schiefer, J. and Roth, H.},
  title = {SARI-SQL: Event Query Language for Event Analysis},
  booktitle = {Commerce and Enterprise Computing, 2009. CEC '09. IEEE Conference
	on},
  year = {2009},
  pages = {24 -32},
  month = {july},
  abstract = {Complex event processing (CEP) systems are capable of processing large
	amounts of events, utilizing them to monitor, steer and optimize
	business in real time. The lack of tracking events and maintaining
	the causal relationships and traceability between those events, as
	well as aggregating them to higher-level events, is a problem that
	is currently investigated by many research groups. In this paper,
	we present SARI-SQL, which is a domain-specific event-query language,
	(EQL) that is designed for business analysts to easily gain insight
	into business events. SARI-SQL enables the retrieval of near real-time
	events and can process historical events, metrics and scores for
	analytical purposes. We introduce the SARI-SQL syntax and show infrastructural
	components for the query engine. We further show examples to illustrate
	the query language, and propose a reference implementation for the
	query engine.},
  doi = {10.1109/CEC.2009.14},
  issn = {1530-1354},
  keywords = {SARI-SQL event query language;business process optimization;complex
	event processing system;domain-specific event-query language;event
	analysis;event traceability;query engine;SQL;business data processing;query
	processing;}
}

@INPROCEEDINGS{1251407,
  author = {Rubin, S.H.},
  title = {On the fusion and transference of knowledge. I},
  booktitle = {Information Reuse and Integration, 2003. IRI 2003. IEEE International
	Conference on},
  year = {2003},
  pages = { 144 - 149},
  month = {oct.},
  abstract = { Contemporary neural architectures having one or more hidden layers
	suffer from the same deficiencies that genetic algorithms and methodologies
	for non-trivial automatic programming do; namely, they cannot exploit
	inherent domain symmetries for the transference of knowledge from
	an application of lesser to greater rank, or across similar applications.
	As a direct consequence, no ensemble of contemporary neural architectures
	allows for the effective codification and transference of knowledge
	within a society of individuals (i.e., swarm knowledge). These deficiencies
	stem from the fact that contemporary neural architectures cannot
	reason symbolically using heuristic ontologies. They cannot directly
	provide symbolic explanations of what was learned for purposes of
	inspection and verification. Moreover, they do not allow the knowledge
	engineer to precondition the internal feature space through the application
	of domain-specific modeling languages. A symbolic representation
	can support the heuristic evolution of an ensemble of neural architectures.
	Each neural network in the ensemble imbues a hidden layer and for
	this reason is NP-hard in its learning performance. It may be argued
	that the internal use of a neat representation subsumes the heuristic
	evolution of a scruffy one. It follows that there is a duality of
	representation under transformation. The goal of AI then is to find
	symbolic representations, transformations, and associated heuristic
	ontologies. This paper provides an introduction to this quest. Consider
	the game of chess for example. If a neural network or symbolic heuristic
	is used to evaluate board positions, then the best found iterate
	(i.e., of weights or symbols) serves as a starting point for iterative
	refinement. This paper addresses the ordering and similarity of the
	training instances in refining subsequent iterates. If we fix the
	learning technology, then we need to focus on reducing the problem,
	composing intermediate results, and transferring the results to a
	similar domain. For example, moving just a bishop against one opposing
	piece is a reduction, moving a bishop and say a rook against one
	opposing piece a composition, and moving a queen against one or more
	opposing pieces a transference. The training sets must be mutually
	orth- ogonal, or random to maximize the learned content. Learning
	what to present and when involves self-reference and this necessarily
	implies a heuristic approach.},
  doi = {10.1109/IRI.2003.1251407},
  issn = { },
  keywords = { artificial intelligence; contemporary neural architectures; domain-specific
	modeling language; genetic algorithm; heuristic approach; heuristic
	evolution; heuristics; information codification; information fusion;
	information transference; learning; neural architecture ensemble;
	neural network; nontrivial automatic programming; swarm knowledge;
	symbolic representation; training sets; heuristic programming; learning
	(artificial intelligence); neural nets;}
}

@INPROCEEDINGS{972900,
  author = {Rubin, S.H. and Rush, R.J., Jr. and Boerke, J. and Trajkovic, L.},
  title = {On the role of informed search in veristic computing},
  booktitle = {Systems, Man, and Cybernetics, 2001 IEEE International Conference
	on},
  year = {2001},
  volume = {4},
  pages = {2301 -2308 vol.4},
  abstract = {Veristic computing is defined as computing with words. It necessarily
	entails the use of informed search in the solution of qualitatively
	constrained equations. Its use does not preclude computing with numbers.
	Veristic computing allows for the specification of higher-level programming
	languages, which can evolve domain-specific knowledge bases. The
	knowledge is evolved on a high-end computer for subsequent porting
	to a PC. The application of that knowledge to the translation of
	a higher-level program is termed expert compilation. This paper serves
	to clarify the ubiquitous role assumed by randomization in all aspects
	of software engineering-from programming language design to program
	design to program testing to knowledge transference},
  doi = {10.1109/ICSMC.2001.972900},
  issn = {1062-922X},
  keywords = {PC;computing words;domain-specific knowledge bases;expert compilation;high-end
	computer;higher-level programming languages;informed search;knowledge
	transfer;program design;program testing;programming language design;qualitatively
	constrained equations;randomization;software engineering;translation;veristic
	computing;data mining;high level languages;program compilers;software
	engineering;}
}

@INPROCEEDINGS{5773407,
  author = {Rubini, S. and Singhoff, F. and Hugues, J.},
  title = {Modeling and Verification of Memory Architectures with AADL and REAL},
  booktitle = {Engineering of Complex Computer Systems (ICECCS), 2011 16th IEEE
	International Conference on},
  year = {2011},
  pages = {338 -343},
  month = {april},
  abstract = {Real-Time Embedded systems must respect a wide range of non-functional
	properties, including safety, respect of deadlines, power or memory
	consumption. We note that correct hardware resource dimensioning
	requires taking into account the impact of the whole software, both
	the user code and the underlying run time environment. AADL allows
	one to precisely capture all of them. In this article, we evaluate
	the AADL modeling to define memory architectures, and then verification
	rules to assess that the memory is correctly dimensioned. We use
	the REAL domain-specific language to express memory requirements
	(such as layout or size) and then validate them on a case-study using
	the VxWorks real-time kernel.},
  doi = {10.1109/ICECCS.2011.40},
  keywords = {AADL modeling;REAL domain-specific language;VxWorks real-time kernel;hardware
	resource dimensioning;memory architectures;memory consumption;memory
	requirements;nonfunctional property;power consumption;real-time embedded
	systems;respect of deadlines;run time environment;safety;user code;embedded
	systems;formal verification;memory architecture;operating system
	kernels;security of data;specification languages;}
}

@INPROCEEDINGS{6068326,
  author = {Ruhroth, Thomas and Wehrheim, Heike and Ziegert, Steffen},
  title = {ReL: A Generic Refactoring Language for Specification and Execution},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2011 37th
	EUROMICRO Conference on},
  year = {2011},
  pages = {83 -90},
  month = {30 2011-sept. 2},
  abstract = {Refactoring is a powerful technique for improving the structural quality
	of software models and programs. Besides informal, example-driven
	descriptions of refactorings, a number of languages for specifying
	refactorings have been developed. Such refactoring languages are
	either specific to particular programming languages or to particular
	application purposes. In this paper, we present the generic refactoring
	language ReL. ReL is a domain-specific language which can be instantiated
	for any target language with Backus-Naur-style grammar. Thus obtained
	ReL instances are equally well suited for specifying refactorings
	and executing them. A tool chain for ReL supports the automatic instantiation
	for target languages as well as the parsing and execution of refactoring
	descriptions.},
  doi = {10.1109/SEAA.2011.22}
}

@INPROCEEDINGS{4638592,
  author = {Runde, S. and Dibowski, H. and Fay, A. and Kabitzsch, K.},
  title = {Integrated automated design approach for building automation systems},
  booktitle = {Emerging Technologies and Factory Automation, 2008. ETFA 2008. IEEE
	International Conference on},
  year = {2008},
  pages = {1488 -1495},
  month = {sept.},
  abstract = {The planning and design of building automation systems is a time consuming,
	error prone and nowadays more and more expensive task, consisting
	of a lot of repeated manual design steps done by specialized engineers.
	To reduce the engineering costs for such systems, the authors present
	a new automated top-down design approach within this paper. A knowledge-based
	system supports the planner at the requirement analysis by means
	of a guided dialog. Subsequently, the complete automation system
	is automatically designed in two steps. The abstract design proceeds
	a design based on platform- and manufacturer-independent function
	blocks via generative programming. The detailed design replaces the
	function blocks by platform- and manufacturer-specific profiles by
	means of evolutionary techniques.},
  doi = {10.1109/ETFA.2008.4638592},
  keywords = {building automation systems;evolutionary techniques;generative programming;guided
	dialog;integrated automated top-down design;knowledge-based system;manufacturer-independent
	function blocks;planning;building management systems;evolutionary
	computation;knowledge based systems;planning;}
}

@INPROCEEDINGS{4031817,
  author = {Rus, T. and Curtis, D.E.},
  title = {Application Driven Software Development},
  booktitle = {Software Engineering Advances, International Conference on},
  year = {2006},
  pages = {32},
  month = {oct. },
  abstract = {Even in its very infancy computer technology has been seen as a collection
	of tools destined to solve problems of a given application domain
	(AD) 1. The problem solving process using computers is (and has been)
	carried out within the computer environment and requires the AD experts
	to formalize their problems in computer terms. The effort put forth
	so far toward making this process easier for AD experts has generated
	a rich and well-defined information technology (IT) domain, populated
	by computer artifacts such as programming languages and program generation
	tools. Successes of this approach to problem-solving led to the development
	of current computer technology whose complexity overwhelms computer
	experts themselves. Nevertheless, the usage of current IT for problem
	solving still requires AD experts to manipulate IT domain concepts
	and tools rather than AD concepts and tools. To further help this
	process, more and more complex IT tools are generated thus increasing
	software complexity to a level where only with formidable difficulties
	can AD experts manage to develop their application systems. Among
	the side effects of this situation are the lack of efficiency in
	application system development, poor performance in computer utilization,
	and even threat to the future evolution of computer technology itself.
	Our conjecture is that in order to break this vicious circle we need
	to rethink the problem solving process. We need to abandon the requirement
	that AD experts manipulate computer terms and to allow them to manipulate
	AD specific terms using AD specific languages. The recent advances
	created by computing research makes it feasible to move the problem
	solving process from the IT domain into the AD domain.},
  doi = {10.1109/ICSEA.2006.261288}
}

@INPROCEEDINGS{5291151,
  author = {Sabo, A. and Schramm, N.},
  title = {Concurrent programming method for digital signal processing},
  booktitle = {Intelligent Systems and Informatics, 2009. SISY '09. 7th International
	Symposium on},
  year = {2009},
  pages = {267 -271},
  month = {sept.},
  abstract = {The task of programming concurrent systems is substantially more difficult
	than the task of programming sequential systems with respect to both
	correctness and efficiency. The tendency in development of embedded,
	DSP systems and processors are shifting to multi core and multiprocessor
	setups as well. The problem of easy concurrency and algorithm development
	is an important for embedded and DSP systems as well. The goal of
	this paper is to define and present a high level language that allows
	description and development of signal processing algorithms. With
	the usage of a domain specific language, we can create compact and
	easy to understand definition of algorithms. In the paper the authors
	present the advantages granted by DSL for DSP applications. The created
	definitions are hardware independent can be executed and functionally
	verified. Efficient code can be generated for various targets without
	porting. The design of the presented DSL allows code generation for
	multi-core targets in case of computing-intensive algorithms, code
	generation for multiple streams, threads. Code reuse is supported
	by merging, re-grouping, and splitting of algorithms and groups of
	algorithms.},
  doi = {10.1109/SISY.2009.5291151},
  keywords = {DSL;DSP systems;algorithm merging;algorithm regrouping;algorithm spitting;code
	generation;code reuse;computing-intensive algorithms;concurrent programming;digital
	signal processing;embedded systems;high level language;multi-core
	targets;multiple streams;multiple threads;multiprocessor;sequential
	systems;concurrency control;embedded systems;high level languages;multi-threading;program
	compilers;signal processing;}
}

@INPROCEEDINGS{5260551,
  author = {Saidani, T. and Falcou, J. and Tadonki, C. and Lacassagne, L. and
	Etiemble, D.},
  title = {Algorithmic Skeletons within an Embedded Domain Specific Language
	for the CELL Processor},
  booktitle = {Parallel Architectures and Compilation Techniques, 2009. PACT '09.
	18th International Conference on},
  year = {2009},
  pages = {67 -76},
  month = {sept.},
  abstract = {Efficiently using the hardware capabilities of the Cell processor,
	a heterogeneous chip multiprocessor that uses several levels of parallelism
	to deliver high performance, and being able to reuse legacy code
	are real challenges for application developers. We propose to use
	Generative Programming and more precisely template meta-programming
	to design an domain specific embedded language using algorithmic
	skeletons to generate applications based on a high-level mapping
	description. The method is easy to use by developers and delivers
	performance close to the performance of optimized hand-written code,
	as shown on various benchmarks ranging from simple BLAS kernels to
	image processing applications.},
  doi = {10.1109/PACT.2009.21},
  issn = {1089-795X},
  keywords = {C++ metaprogramming;Cell processor;algorithmic skeletons;domain specific
	embedded language;generative programming;hardware capability;heterogeneous
	chip multiprocessor;high-level mapping description;legacy code reuse;template
	metaprogramming;C++ language;microprocessor chips;parallel programming;}
}

@INPROCEEDINGS{5634309,
  author = {Salehi, P. and Hamoud-Lhadj, A. and Colombo, P. and Khendek, F. and
	Toeroe, M.},
  title = {A UML-Based Domain Specific Modeling Language for the Availability
	Management Framework},
  booktitle = {High-Assurance Systems Engineering (HASE), 2010 IEEE 12th International
	Symposium on},
  year = {2010},
  pages = {35 -44},
  month = {nov.},
  abstract = {The Service Availability Forum (SA Forum) is a consortium of several
	telecommunications and computing companies that defines standard
	solutions for high availability platforms. One of the most important
	SA Forum services is the Availability Management Framework (AMF)
	which is responsible for managing the availability of an application
	running under its control. To achieve this, AMF requires a complete
	configuration, which consists of several entities organized according
	to AMF rules and constraints. In this paper, we argue that AMF concepts
	form a domain for which a domain-specific modeling language can greatly
	facilitate the generation, analysis and the management of AMF configurations.
	We define such a language by extending UML through its profiling
	mechanism and we implement it. More important, we discuss the challenges
	and the lessons learned in the course of this project.},
  doi = {10.1109/HASE.2010.21},
  issn = {1530-2059},
  keywords = {AMF rules;SA forum services;UML-based domain specific modeling language;availability
	management framework;service availability forum;Unified Modeling
	Language;middleware;}
}

@INPROCEEDINGS{4685806,
  author = {Sampath, P. and Rajeev, A.C. and Ramesh, S. and Shashidhar, K.C.},
  title = {Behaviour Directed Testing of Auto-code Generators},
  booktitle = {Software Engineering and Formal Methods, 2008. SEFM '08. Sixth IEEE
	International Conference on},
  year = {2008},
  pages = {191 -200},
  month = {nov.},
  abstract = {This paper addresses the problem of testing auto-code generators.
	Auto-code generators take as input a model in certain modeling language,
	and produce as output a program that captures the execution semantics
	of the input-model. We focus on the problem of test specification
	for the purpose of automatically generating a test-suite. We propose
	a novel technique for test specification based on the execution behavior
	of models. We also propose an algorithm that uses such a behavioral
	test specification for directing test-case generation towards very
	specific behavioral patterns that we would like to exercise. We have
	implemented this technique, and have applied it for generating test-cases
	for a Stateflow auto-code generator.},
  doi = {10.1109/SEFM.2008.13},
  keywords = {auto-code generator verification;behavioral test specification;behaviour
	directed testing;execution semantics;modeling language;stateflow
	auto-code generator testing;test-case generation;formal specification;program
	compilers;program testing;program verification;programming language
	semantics;specification languages;}
}

@INPROCEEDINGS{4623308,
  author = {Sanchez, P. and Barreda, J. and Ocon, J.},
  title = {Integration of domain-specific models into a MDA framework for time-critical
	embedded systems},
  booktitle = {Intelligent Solutions in Embedded Systems, 2008 International Workshop
	on},
  year = {2008},
  pages = {1 -15},
  month = {july},
  abstract = {The model-driven architecture initiative (MDA) of the Object Management
	Group (OMG) proposes a development paradigm that can be used to deal
	with the increasing complexity of real-time embedded systems. MDA
	is based on developing both platform independent and specific models
	from which executable code can be generated in an automatic or semi-automatic
	way. In most cases, engineers use domain-specific models to describe
	the system and the challenge is to integrate these specific models
	into a general MDA methodology. Sometimes, the MDA infrastructure
	includes applications that can evaluate the real-time system performance,
	an essential aspect of the time-critical embedded system design.
	This paper presents a real-time embedded system development methodology
	based on MDA and a domain-specific model oriented to time-critical
	system modelling. The toolset supports model transformations and
	performance analysis. The performance analysis is based on the PERFidiX
	technology, a SystemC-based framework for system evaluation. The
	main contributions of this paper are the exploration of techniques
	to integrate domain-specific models into an MDA-based methodology
	and the relations of these techniques with the SystemC code generation
	and performance analysis processes.},
  doi = {10.1109/WISES.2008.4623308},
  keywords = {Object Management Group;PERFidiX technology;SystemC code generation;domain-specific
	models;model-driven architecture initiative;real-time embedded systems;semiautomatic
	way;system evaluation;time-critical embedded system design;time-critical
	system modelling;distributed object management;embedded systems;groupware;program
	compilers;software architecture;software performance evaluation;}
}

@INPROCEEDINGS{5644881,
  author = {Sanders, B.A. and Bartlett, R. and Deumens, E. and Lotrich, V. and
	Ponton, M.},
  title = {A Block-Oriented Language and Runtime System for Tensor Algebra with
	Very Large Arrays},
  booktitle = {High Performance Computing, Networking, Storage and Analysis (SC),
	2010 International Conference for},
  year = {2010},
  pages = {1 -11},
  month = {nov.},
  abstract = {Important classes of problems in computational chemistry, notably
	coupled cluster methods, consist of solutions to complicated expressions
	defined in terms of tensors. Tensors are represented by multidimensional
	arrays that are typically extremely large, thus requiring distribution
	or in some cases backing on disk. We describe a parallel programming
	environment, the Super Instruction Architecture (SIA) comprising
	a domain specific programming language SIAL and its runtime system
	SIP that are specialized for this class of problems. A novel feature
	of the programming language is that SIAL programmers express algorithms
	in terms of operations on blocks rather than individual floating
	point numbers. Efficient implementations of the block operations
	as well as management of memory, communication, and I/O are provided
	by the runtime system. The system has been successfully used to develop
	ACES III, a software package for computational chemistry.},
  doi = {10.1109/SC.2010.3},
  keywords = {ACES III;SIAL;block-oriented language;computational chemistry;coupled
	cluster methods;domain specific programming language;floating point
	numbers;multidimensional arrays;parallel programming environment;runtime
	system SIP;software package;super instruction architecture;tensor
	algebra;very large arrays;parallel programming;programming languages;software
	packages;tensors;}
}

@INPROCEEDINGS{6045368,
  author = {Sannier, N. and Baudry, B. and Thuy Nguyen},
  title = {Formalizing standards and regulations variability in longlife projects.
	A challenge for Model-driven engineering},
  booktitle = {Model-Driven Requirements Engineering Workshop (MoDRE), 2011},
  year = {2011},
  pages = {64 -73},
  month = {aug.},
  abstract = {Safety regulations and standards imposed by national regulators on
	nuclear power plant systems provide high-level requirements, recommendations
	and/or guidance expressed in natural language. In many cases, this
	leaves a large margin for interpretation, not all of which are acceptable
	to a given regulator. Currently the elements that lead to the establishment
	of acceptable/accepted practices are not always documented, nor are
	these practices formally modeled. When a new standard appears or
	when Electricite #x0301; de France (EDF) has to discuss a standard
	with another regulator, there is no systematic process to build a
	practice. Domain-specific modeling, traceability and variability
	modeling are Model-Driven Engineering (MDE) techniques that could
	address various aspects of practice formalization. This paper precisely
	defines the modeling issues that are currently faced by EDF when
	managing regulatory safety requirements, standards and practices.
	Then we review existing requirements modeling techniques to understand
	their benefits and limits according to EDF's needs.},
  doi = {10.1109/MoDRE.2011.6045368},
  keywords = {domain-specific modeling;longlife projects;model-driven engineering;nuclear
	power plant systems;regulations variability;regulatory safety requirements;requirements
	modeling;safety regulations;safety standards;formal specification;legislation;safety;standards;}
}

@INPROCEEDINGS{4626849,
  author = {Santos, A.L. and Koskimies, K. and Lopes, A.},
  title = {Automated Domain-Specific Modeling Languages for Generating Framework-Based
	Applications},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {149 -158},
  month = {sept.},
  abstract = {The adoption of Domain-Specific Modeling Languages (DSMLs) for generating
	framework-based applications has proved to be an effective way of
	enforcing the correct use of frameworks and improve the productivity
	of application developers. However, the development of the code generator
	of a DSML is typically a laborious task with difficulties in what
	concerns complexity, understandability, and maintainability. In this
	paper, we address this problem with a new approach for developing
	DSMLs for frameworks that allows to eliminate the need of implementing
	code generators. The approach relies on the extension of frameworks
	with an additional layer based on aspect-oriented programming that
	encodes a DSML. By means of a generic language workbench, framework-based
	applications can be generated from application models described in
	that DSML. The proposed language workbench was implemented in a prototype
	tool and a case study on the Eclipse Rich Client Platform was performed.},
  doi = {10.1109/SPLC.2008.17},
  keywords = {aspect-oriented programming;automated domain-specific modeling languages;code
	generator;generic language workbench;object-oriented programming;program
	compilers;simulation languages;}
}

@INPROCEEDINGS{1174889,
  author = {Saraiva, J. and Schneider, S.},
  title = {Embedding domain specific languages in the attribute grammar formalism},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { This paper presents techniques for the design and implementation
	of domain specific languages. Our techniques are based on higher-order
	attribute grammars. Formal languages are specified in the classical
	attribute formalism and domain specific languages are embedded in
	the specification via higher-order attributes. We present a domain
	specific language for pretty-printing and we show how such language
	can be easily embedded in the specification of a powerful spreadsheet-like
	tool. From such specification an incremental implementation is automatically
	derived and the first results are presented.},
  doi = {10.1109/HICSS.2003.1174889},
  issn = { },
  keywords = { attribute grammar formalism; domain specific languages; formal languages;
	higher-order attribute grammars; attribute grammars; formal specification;
	high level languages; specification languages;}
}

@INPROCEEDINGS{5479106,
  author = {Sarinho, V.T. and Apolinrio, A.L.},
  title = {A Generative Programming Approach for Game Development},
  booktitle = {Games and Digital Entertainment (SBGAMES), 2009 VIII Brazilian Symposium
	on},
  year = {2009},
  pages = {83 -92},
  month = {oct.},
  abstract = {Nowadays, due to the great distance between design and implementation
	worlds, different skills are necessary to create a game system. To
	solve this problem, a lot of strategies for game development, trying
	to increase the abstraction level necessary for the game production,
	were proposed. In this way, a lot of game engines, game frameworks
	and others, in most cases without any compatibility or reuse criteria
	between them, were developed. This paper presents a new generative
	programming approach, able to increase the production of a digital
	game by the integration of different game development artifacts,
	following a system family strategy focused on variable and common
	aspects of a computer game. As result, high level abstractions of
	games, based on a common language, can be used to configure metaprogramming
	transformations during the game production, providing a great compatibility
	level between game domain and game implementation artifacts.},
  doi = {10.1109/SBGAMES.2009.18},
  keywords = {abstraction level;computer game;game development;game engine;game
	framework;generative programming;metaprogramming transformation;computer
	games;metacomputing;software reusability;specification languages;}
}

@INPROCEEDINGS{5440239,
  author = {Sateanpattanakul, S. and Walairacht, A.},
  title = {JGroovy - an extensible Java Programming Language with Groovy},
  booktitle = {Advanced Communication Technology (ICACT), 2010 The 12th International
	Conference on},
  year = {2010},
  volume = {2},
  pages = {1139 -1144},
  month = {feb.},
  abstract = {Java is Object-Oriented Programming Languages (OOPL) that widely used
	for software development. But Java has a limitation for working with
	Domain-Specific Languages (DSLs). Java language structure and syntax
	has not more support for working with DSLs and including type of
	Java language. Static language likes Java does not flexible more
	for DSLs handle. This limitation has to solve by adding new language
	structure and syntax into Java language. Groovy is a dynamic programming
	languages that support DSLs with internal structure. In this paper,
	we introduce JGroovy which is extended Java programming language
	with Groovy programming. We are built JGroovy compilers that fully
	support Java programming language.},
  issn = {1738-9445},
  keywords = {Groovy programming;JGroovy;domain-specific languages;dynamic programming
	languages;extensible Java programming language;object-oriented programming
	languages;software development;static language;Java;object-oriented
	programming;software engineering;}
}

@INPROCEEDINGS{4473003,
  author = {Sawyer, P. and Bencomo, N. and Hughes, D. and Grace, P. and Goldsby,
	H.J. and Cheng, B.H.C.},
  title = {Visualizing the Analysis of Dynamically Adaptive Systems Using i*
	and DSLs},
  booktitle = {Requirements Engineering Visualization, 2007. REV 2007. Second International
	Workshop on},
  year = {2007},
  pages = {3},
  month = {oct.},
  abstract = {Self-adaptation is emerging as a crucial enabling capability for many
	applications, particularly those deployed in dynamically changing
	environments. One key challenge posed by dynamically adaptive systems
	(DASs) is the need to handle changes to the requirements and corresponding
	behavior of a DAS in response to varying environmental conditions.
	In this paper we propose a visual model-driven approach that uses
	the i* modeling language to represent goal models for the DAS requirements.
	Our approach applies a rigorous separation of concerns between the
	requirements for the DAS to operate in stable conditions and those
	that enable it to adapt at run-time to enable it to cope with changes
	in its environment. We further show how requirements derived from
	the i* modeling can be used by a domain-specific language to achieve
	requirements model-driven development. We describe our experiences
	with applying this approach to GridStix, an adaptive flood warning
	system, deployed on the River Ribble in North Yorkshire, England.},
  doi = {10.1109/REV.2007.10},
  keywords = {adaptive flood warning system;domain-specific language;dynamically
	adaptive system;i* modeling language;model-driven development;self-adaptation;visual
	model-driven approach;adaptive systems;data visualisation;formal
	specification;knowledge based systems;}
}

@INPROCEEDINGS{5577992,
  author = {Saxena, T. and Karsai, G.},
  title = {Towards a Generic Design Space Exploration Framework},
  booktitle = {Computer and Information Technology (CIT), 2010 IEEE 10th International
	Conference on},
  year = {2010},
  pages = {1940 -1947},
  month = {29 2010-july 1},
  abstract = {The set of all possible design alternatives for a system is referred
	to as a design-space, and design-space exploration (DSE) is the systematic
	exploration of the elements in a design-space. Various DSE techniques
	have been used for hardware/software co-design, configuration of
	software product lines and real-time software synthesis. Although
	at an abstract level DSE steps performed in these domains are similar,
	most of the current research is focused on domain specific frameworks
	which are tightly coupled with tools that evaluate point designs
	and use domain specific knowledge. There is a need for a generic
	tool that can be easily configured to model exploration problems
	from different domains as well on different levels of abstraction.
	In this paper we present Generic Design Space Exploration (GDSE)
	framework for domain independent DSE. This framework supports generic
	modeling of DSE problems from different domains using a language
	that allows a design-space to be encoded using domain-specific concepts
	and a simple constraint language that we designed. Rather than forcing
	the user to encode their design problem in a low-level constraint
	language, we advocate a higher level approach. The main contribution
	of this framework are: 1) it is able to support modeling of DSE in
	different domains 2) it supports a user-friendly constraint language
	that is expressive enough to specify constraints 3) the automated
	generation of low level constraint language code from the model alleviates
	the user from encoding the entire problem by hand 4) solver independence
	allows the user to experiment with different encodings.},
  doi = {10.1109/CIT.2010.330},
  keywords = {DSE techniques;domain specific frameworks;generic design space exploration
	framework;hardware-software codesign;higher level approach;low-level
	constraint language;real-time software synthesis;software product
	lines;hardware-software codesign;systems analysis;}
}

@INPROCEEDINGS{1698801,
  author = {Scaffidi, C.},
  title = {A Lightweight Model for End Users amp;CHARP146; Domain-Specific Data},
  booktitle = {Visual Languages and Human-Centric Computing, 2006. VL/HCC 2006.
	IEEE Symposium on},
  year = {2006},
  pages = {242 -243},
  month = {sept.},
  abstract = {Many end user programming tools lack adequate support for domain-specific
	data. We will design a lightweight representation for categories
	of data, called "topes," and develop simple methods that end users
	and system administrators can use to define new topes. To evaluate
	this approach, we will improve programming tools so end users can
	write programs that recognize data as instances of topes and manipulate
	them accordingly. We expect that these enhancements will help end
	users produce higher quality software},
  doi = {10.1109/VLHCC.2006.2},
  keywords = {domain-specific data;end user programming;lightweight model;programming
	tools;personal computing;software tools;}
}

@INPROCEEDINGS{4639050,
  author = {Schafer, W.},
  title = {Model driven development with mechatronic UML},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {9 -10},
  month = {sept.},
  abstract = {We address these challenges by the model-driven mechatronic UML development
	approach which combines domain specific modeling and refinement techniques
	with verification based on compositional model checking. The approach
	suggests modeling the software by using a refined UML 2.0 component
	model including the detailed definition of ports, connectors, and
	patterns. We further refine the component model to define a proper
	integration between discrete and continuous control such that the
	reconfiguration of hierarchical component systems can be described
	in a modular way. Compositional model checking is based on a domain
	specific decomposition of the system specification into individually
	checkable components based on a common pre-defined architectural
	model. As a basis for formal verification, a formal semantic definition
	of the concepts taken from UML 2.0 is given. For the scope of this
	presentation, this is particularly done for our notion of so-called
	real-time statecharts. Besides supporting compositional verification,
	the approach supports checking consistency between different parts
	of a systems specification by a syntax check. This check is based
	on giving a formal definition of consistency.},
  doi = {10.1109/VLHCC.2008.4639050},
  issn = {1943-6092},
  keywords = {compositional formal verification;compositional model checking;consistency
	checking;domain specific decomposition;domain specific modeling;formal
	semantic definition;hierarchical component system reconfiguration;model-driven
	mechatronic UML development approach;real-time statechart;refined
	UML 2.0 component model;refinement technique;software architectural
	modeling;syntax check;system specification;Unified Modeling Language;formal
	specification;mechanical engineering computing;mechatronics;object-oriented
	programming;program verification;software architecture;}
}

@INPROCEEDINGS{6032381,
  author = {Schatz, B.},
  title = {From Solution to Problem Spaces: Formal Methods in the Context of
	Model-Based Development and Domain-Specific Languages},
  booktitle = {Computer Software and Applications Conference (COMPSAC), 2011 IEEE
	35th Annual},
  year = {2011},
  pages = {454 -455},
  month = {july},
  abstract = {With the increased use of model-based techniques and the provision
	of domain-specific languages, the focus of the development process
	is shifting from the implementation to the analysis and the design
	phase. With this shift from the general-purpose, technical-oriented
	solution space to the application-specific, domain-oriented problem
	space, new possibilities of application open up for rigorous engineering
	techniques, both on the analysis and on the synthesis side of applications.},
  doi = {10.1109/COMPSAC.2011.112},
  issn = {0730-3157},
  keywords = {domain-oriented problem space;domain-specific languages;formal method;model-based
	development;model-based technique;rigorous engineering technique;technical-oriented
	solution space;formal specification;specification languages;}
}

@INPROCEEDINGS{4148975,
  author = {Schauerhuber, A. and Wimmer, M. and Schwinger, W. and Kapsammer,
	E. and Retschitzegger, W.},
  title = {Aspect-Oriented Modeling of Ubiquitous Web Applications: The aspectWebML
	Approach},
  booktitle = {Engineering of Computer-Based Systems, 2007. ECBS '07. 14th Annual
	IEEE International Conference and Workshops on the},
  year = {2007},
  pages = {569 -576},
  month = {march},
  abstract = {Ubiquitous Web applications (UWA) are required to be customizable,
	meaning their services need to be adaptable towards the context of
	use, e.g., user, location, time, and device. Considering UWA 's from
	a software engineering point of view, a systematic development on
	basis of models is crucial. Current Web modeling languages, however,
	often disregard the crosscutting nature of customization potentially
	affecting all parts of a Web application, and often mingle core and
	customization functionality. This leads to inefficient development
	processes, high maintenance overheads, and a low potential for reuse.
	We regard customization as a crosscutting concern in the sense of
	the aspect-oriented paradigm. As a proof of concept, we extend the
	prominent Web modeling language WebML on basis of our reference architecture
	for aspect-oriented modeling. This allows for a clear separation
	between the core and customization functionality, and - as a spin-off
	- demonstrates how to bridge existing (domain-specific) modeling
	languages with aspect-oriented concepts},
  doi = {10.1109/ECBS.2007.20},
  keywords = {Web modeling languages;aspect-oriented modeling;aspectWebML approach;customization
	functionality;ubiquitous Web application;Internet;object-oriented
	programming;specification languages;ubiquitous computing;}
}

@INPROCEEDINGS{4804563,
  author = {Schellekens, M. and Early, D. and Popovici, E.},
  title = {Designing Software for Modular Static Average-case Analysis},
  booktitle = {Future Dependable Distributed Systems, 2009 Software Technologies
	for},
  year = {2009},
  pages = {6 -10},
  month = {march},
  abstract = {MOQA is a new domain-specific programming language to design software
	for which the average-case time analysis of its programs is guaranteed
	to be modular. Time in this context refers to a broad notion of cost,
	which can be used to estimate the actual running time, but also other
	quantitative information such as power consumption, while modularity
	means that the average time of a program can be easily computed from
	the times of its constituents - something that no programming language
	of this scope has been able to guarantee so far. MOQA principles
	can be incorporated in any standard programming language. We discuss
	how MOQA can support the design of software for modular static average-case
	analysis and sketch some of the reversible features of this language.},
  doi = {10.1109/STFSSD.2009.27},
  keywords = {MOQA principle;MOQA programming language;average-case time analysis;domain-specific
	programming language;modular static average-case analysis;power consumption;reversible
	software;software design;program diagnostics;reverse engineering;software
	engineering;specification languages;}
}

@INPROCEEDINGS{926346,
  author = {Schleicher, A. and Westfechtel, B.},
  title = {Beyond stereotyping: metamodeling approaches for the UML},
  booktitle = {System Sciences, 2001. Proceedings of the 34th Annual Hawaii International
	Conference on},
  year = {2001},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { UML is being used as the universal technique for modeling object-oriented
	applications across a wide range of domains. Developing a truly adequate
	uniform modeling technique in the face of these diverse domains seems
	an unsolvable quest and contrasts domain specific software engineering
	activities. Recently, many adaptations to UML have been made to reflect
	a domain's world view. These adaptations often exceed the UML's own
	extension mechanisms and result in yet another urban UML slang. However,
	domain-specifically adapting the UML metamodel becomes increasingly
	important in the context of model checking and code generation mechanisms.
	Therefore solutions should be found to fully support metamodeling
	within the UML and UML CASE tools. The paper discusses and evaluates
	the UML's inherent as well as proprietary metamodeling approaches
	and provides domain driven ideas for a meta-modeling approach for
	a diversely used Unified Modeling Language.},
  doi = {10.1109/HICSS.2001.926346},
  issn = { },
  keywords = { CASE tools; UML; Unified Modeling Language; code generation; metamodeling
	approaches; model checking; object-oriented application modeling;
	software engineering; computer aided software engineering; formal
	specification; object-oriented methods; program compilers; specification
	languages;}
}

@INPROCEEDINGS{1385809,
  author = { Schmoelzer, G. and Mitterdorfer, S. and Kreiner, C. and Faschingbauer,
	J. and Kovacs, Z. and Teiniker, E. and Weiss, R.},
  title = {The Entity Container - An Object-Oriented and Model-Driven Persistency
	Cache},
  booktitle = {System Sciences, 2005. HICSS '05. Proceedings of the 38th Annual
	Hawaii International Conference on},
  year = {2005},
  pages = { 277b},
  month = {jan.},
  abstract = { Data persistency is a fundamental, but complex aspect of a modern
	software development process. Therefore, in order to reduce development
	costs and improve a system's quality, support for data persistency
	must be provided to common software paradigms, such as object-oriented
	programming or component based development. In this paper we present
	a new approach of an object persistency cache - the Entity Container
	(EC), based on a data model. The EC allows data and metadata management
	according to a data model independent of any specific persistency
	mechanism. We present the complete architecture, functionality and
	implementation of the system and compare our new approach with existing
	frameworks in order to point out features and major improvements
	of the EC.},
  doi = {10.1109/HICSS.2005.593},
  issn = {1530-1605 },
  keywords = { data object persistency; generative programming; model-driven development;}
}

@INPROCEEDINGS{794498,
  author = {Schneider, C.},
  title = {Executable specification for multimedia supporting refinement and
	architecture exploration},
  booktitle = {EUROMICRO Conference, 1999. Proceedings. 25th},
  year = {1999},
  volume = {1},
  pages = {394 -397 vol.1},
  abstract = {A VHDL-based methodology for top-down design, starting from an executable
	specification, supporting refinement towards RTL is proposed for
	the multimedia domain. The methodology is demonstrated using an MPEG-2
	video decoder. A key idea for writing an initial executable specification
	is to keep the modeling style as close as possible to thinking in
	the domain. The executable specification is refined by partitioning
	the initially sequential model into concurrent processes and by moving
	functionality between blocks. During partitioning, control-dominated
	parts are separated from data-intensive calculations to enable domain-specific
	refinement. Finally the timing is refined from the causal to the
	clock-related level to enable performance simulation },
  doi = {10.1109/EURMIC.1999.794498},
  keywords = {MPEG-2 video decoder;VHDL-based methodology;architecture exploration;data-intensive
	calculations;executable specification;multimedia supporting refinement;partitioning;performance
	simulation;timing;top-down design;decoding;formal specification;hardware
	description languages;multimedia systems;performance evaluation;timing;}
}

@INPROCEEDINGS{5076654,
  author = {Schneider, J.-G. and Lumpe, M.},
  title = {Component Coordination in GLoo},
  booktitle = {Software Engineering Conference, 2009. ASWEC '09. Australian},
  year = {2009},
  pages = {317 -326},
  month = {april},
  abstract = {Incorporating components from a number of different sources into a
	given application is generally considered to be a non-trivial activity.
	Over the years, various coordination mechanisms have been proposed
	to tackle this problem. However, even today, the question remains
	how to best link these coordination mechanisms with an underlying
	programming paradigm without loosing flexibility. A suitable technique
	to address this issue is language composition, enabling us to "fine-tune"
	a programming language on demand. In this compositional approach,
	we can add new features to the language as we go, within user-defined
	regions, and without polluting the underlying paradigm as a whole.
	To test the effectiveness of this technique, we explore a small stream
	processing framework and its corresponding coordination abstractions
	in this paper. More precisely, we report on our insights into using
	GLoo, a functional composition language, for the definition of extensible
	coordination abstractions that, through the composition of the concepts
	proxy, method pointer, and Pernici's "objects with roles" constitute,
	collectively, an attractive means to capture and denote inter-component
	interactions in a user-centric and domain-specific way.},
  doi = {10.1109/ASWEC.2009.33},
  issn = {1530-0803},
  keywords = {GLoo functional composition language;Pernici object composition;component
	coordination abstraction;concept proxy composition;method pointer
	composition;programming paradigm;stream processing framework;user-centric
	domain-specific intercomponent interaction;functional languages;object-oriented
	languages;object-oriented programming;}
}

@INPROCEEDINGS{5951967,
  author = {Scholz-Reiter, B. and Sowade, S. and Rippel, D.},
  title = {Configuring the Infrastructure of Autonomous Logistics' Control Systems},
  booktitle = {Parallel and Distributed Processing with Applications Workshops (ISPAW),
	2011 Ninth IEEE International Symposium on},
  year = {2011},
  pages = {159 -164},
  month = {may},
  abstract = {Autonomous control is a suitable concept in order to increase the
	flexibility and robustness of logistic systems by enabling decentralized
	decision-making and execution at the system elements. However, design
	of the corresponding control system's infrastructure is complex due
	to a high number of possible configurations. Further, logistic process
	experts have to consider several domain specific functional and non-functional
	requirements. Thereto, this paper introduces a three-phase procedure
	model, which guides logistics experts through the configuration process
	of the infrastructure of autonomous logistics' control systems. The
	model employs general design concepts of procedure models and composes
	them to a new, application area specific model that fits the needs
	of autonomously controlled logistic systems.},
  doi = {10.1109/ISPAW.2011.51},
  keywords = {autonomous logistic control system;decentralized decision making;logistic
	process;logistics experts;decision making;logistics data processing;}
}

@INPROCEEDINGS{5708394,
  author = {Schonberger, A. and Wirtz, G.},
  title = {Towards Executing ebBP-Reg B2Bi Choreographies},
  booktitle = {Commerce and Enterprise Computing (CEC), 2010 IEEE 12th Conference
	on},
  year = {2010},
  pages = {64 -71},
  month = {nov.},
  abstract = {Applying choreography and orchestration technology to Business-to-Business
	integration (B2Bi) scenarios has become a popular technique for very
	good reasons. Choreography descriptions can be used to capture B2Biscenarios
	from a global and abstract perspective while orchestrations then
	can be used to specify the local implementation of each integration
	partner. ebXML BPSS(ebBP) is a prominent B2Bi choreography standard
	with very helpful domain-specific concepts, but clear guidelines
	for creating executable choreographies are missing. In order to create
	ebBP models that are both adequateand executable, expressiveness,
	comprehensibility and standard-conformance have to be weighed up.
	In this paper, we introduce ebBP-Reg as an ebBP modeling flavor that
	is designed such that ebBP-Reg choreographies are executable as WS-BPEL
	orchestrations. At the same time, ebBP-Reg models strictly conform
	to the ebBP standard and support concurrency and decomposition. We
	characterize syntactic validity of ebBP-Reg models by means of language
	production rules and show how instances of ebBP-Reg can be implemented
	using WS-BPEL.},
  doi = {10.1109/CEC.2010.29},
  issn = {1530-1354},
  keywords = {B2Biscenarios;WS-BPEL;business-to-business integration;ebBP models;ebBP-Reg
	B2Bi choreographies;ebXML BPSS;language production rules;Web services;XML;business
	data processing;humanities;}
}

@INPROCEEDINGS{1045903,
  author = {Schwitter, R.},
  title = {English as a formal specification language},
  booktitle = {Database and Expert Systems Applications, 2002. Proceedings. 13th
	International Workshop on},
  year = {2002},
  pages = { 228 - 232},
  month = {sept.},
  abstract = { PENG is a computer-processable controlled natural language designed
	for writing unambiguous and precise specifications. PENG covers a
	strict subset of standard English and is precisely defined by a controlled
	grammar and a controlled lexicon. In contrast to other controlled
	languages, the author does not need to know the grammatical restrictions
	explicitly. ECOLE, a look-ahead text editor, indicates the restrictions
	while the specification is written. The controlled lexicon contains
	domain-specific content words that can be defined by the author on
	the fly and predefined function words. Specifications written in
	PENG can be deterministically translated into discourse representations
	structures to cope with anaphora and presuppositions and also into
	first-order predicate logic. To test the formal properties of PENG,
	we reformulated Schubert's steamroller puzzle in PENG, translated
	the resulting specification via discourse representation structures
	into first-order predicate logic with equality, and proved the steamroller's
	conclusion with OTTER, a standard theorem prover.},
  doi = {10.1109/DEXA.2002.1045903},
  issn = {1529-4188 },
  keywords = { ECOLE; English; OTTER; PENG; anaphora; computer-processable controlled
	natural language; controlled grammar; controlled lexicon; discourse
	representations structures; domain-specific content words; first-order
	predicate logic; grammatical restrictions; look-ahead text editor;
	presuppositions; steamroller puzzle; theorem prover; formal specification;
	specification languages;}
}

@INPROCEEDINGS{4208820,
  author = {Selic, B.},
  title = {A Systematic Approach to Domain-Specific Language Design Using UML},
  booktitle = {Object and Component-Oriented Real-Time Distributed Computing, 2007.
	ISORC '07. 10th IEEE International Symposium on},
  year = {2007},
  pages = {2 -9},
  month = {may},
  abstract = {UML includes special extensibility mechanisms, which are used to define
	domain-specific modeling languages that are based on UML. These mechanisms
	have been significantly improved in the latest versions of UML. Unfortunately,
	there is currently a dearth of published material on how to best
	exploit these capabilities and, consequently, many UML profiles are
	either invalid or of poor quality. In this paper, we first provide
	an overview of the new extensibility mechanisms of UML 2.1 and then
	describe a method for defining profiles that greatly increases the
	likelihood of producing technically correct quality UML profiles},
  doi = {10.1109/ISORC.2007.10},
  keywords = {UML 2.1;UML profiles;domain-specific language design;special extensibility
	mechanisms;Unified Modeling Language;}
}

@INPROCEEDINGS{4145026,
  author = {Selonen, Petri and Kettunen, Markus},
  title = {Metamodel-Based Inference of Inter-Model Correspondence},
  booktitle = {Software Maintenance and Reengineering, 2007. CSMR '07. 11th European
	Conference on},
  year = {2007},
  pages = {71 -80},
  month = {march},
  abstract = {Many software engineering processes produce series of related models.
	They describe the designed system from different viewpoints, each
	emphasizing a particular concern. Models are produced by individual
	teams and sometimes third party developers, and sometimes they are
	synthesized using tools. It is unlikely that they are developed within
	a single modeling repository or share common repository identifiers.
	To reason about the models, a correspondence relationship needs to
	be established between model elements representing the same concepts.
	Current approaches for inferring correspondence use fixed semantics
	built into the algorithms and derived from small modeling language
	subsets. This paper presents a flexible approach for inferring correspondence.
	The rules are generated based on a Meta-Object Facility compliant
	metamodel specifying modeling language of the compared model instances.
	They can be refined with additional domain specific rules. The approach
	can be used as a basis for comparing, merging and reconciling models},
  doi = {10.1109/CSMR.2007.31},
  issn = {1534-5351},
  keywords = {fixed semantics;intermodel correspondence;metamodel-based inference;metaobject
	facility compliant metamodel;modeling language;share common repository
	identifiers;single modeling repository identifiers;software engineering;Unified
	Modeling Language;inference mechanisms;programming language semantics;software
	engineering;}
}

@INPROCEEDINGS{4289919,
  author = {Selvi, P. and Gopalan, N.P.},
  title = {Automated writing Assessment of Student's Open-ended Answers Using
	the Combination of Novel Approach and Latent Semantic Analysis},
  booktitle = {Advanced Computing and Communications, 2006. ADCOM 2006. International
	Conference on},
  year = {2006},
  pages = {370 -375},
  month = {dec.},
  abstract = {In previous work we have applied the novel [7] a method originally
	designed to evaluate automatic machine translation systems, in assessing
	short essays written by students [16]. In this paper we present a
	comparative evaluation between this combination algorithms and a
	system based on BLEU and Latent Semantic Analysis. In addition we
	propose an effective combination schema for them. We study how much
	combination approach scores correlate to human scorings and other
	evaluation metrics. In spite of the simplicity of these shallow NLP
	methods, they achieve state-of-the art correlations to the teacher's
	scores while keeping the language-independence and without requiring
	any domain specific knowledge.},
  doi = {10.1109/ADCOM.2006.4289919},
  keywords = {BLEU;NLP method;automated writing assessment;latent semantic analysis;natural
	language processing;short essay;student free-text answer;computer
	aided instruction;educational administrative data processing;information
	retrieval;natural language processing;}
}

@INPROCEEDINGS{4776626,
  author = {Sen, Sagar and Vangheluwe, Hans},
  title = {Multi-domain physical system modeling and control based on meta-modeling
	and graph rewriting},
  booktitle = {Computer Aided Control System Design, 2006 IEEE International Conference
	on Control Applications, 2006 IEEE International Symposium on Intelligent
	Control, 2006 IEEE},
  year = {2006},
  pages = {69 -75},
  month = {oct.},
  abstract = {A methodology is presented which enables the specification and synthesis
	of software tools to aid in plant and controller modeling for multi-domain
	(electrical, mechanical, hydraulic, and thermal) physical systems.
	The methodology is based on meta-modeling and graph rewriting. The
	plant is modeled in a domain-specific formalism called the Real World
	Visual Model (RWVM). Such a model is successively transformed to
	an Idealized Physical Model (IPM), to an Acausal Bond Graph (ABG),
	and finally to a Causal Bond Graph (CBG). A Modelica (www.modelica.org)
	model, consisting of a Causal (algebraic and differential equation)
	Block Diagram (CBD), is generated from the CBG. All transformations
	are explicitly modeled using Graph Grammars. A PID controller model,
	specified in Modelica as a CBD is subsequently integrated with the
	plant model. AToM3 (atom3.cs.mcgill.ca), A Tool for Multi-formalism
	and Meta Modeling is used to meta-model and synthesize visual modeling
	environments for the RWVM, IPM, ABG, and CBG formalisms as well as
	for transformations between them. The entire process of modeling,
	transformation, and simulation is demonstrated by means of a hoisting
	device example. Our methodology drastically reduces development time
	(of the modeling tool an indirectly of the domain-specific models),
	integrates model checking via Bond Graph causal analysis, and facilitates
	management and reuse of meta-knowledge by explicitly modeling formalisms
	and transformations.},
  doi = {10.1109/CACSD-CCA-ISIC.2006.4776626}
}

@INPROCEEDINGS{1311104,
  author = {Sendall, S.},
  title = {Domain-driven software development - a world of transformations},
  booktitle = {Rapid System Prototyping, 2004. Proceedings. 15th IEEE International
	Workshop on},
  year = {2004},
  pages = { 110 - 112},
  month = {june},
  abstract = { Software development teams are faced with bridging the gap between
	the problem, as envisaged by the stakeholders and constrained by
	the environment, and a software solution, which is built upon the
	abstractions offered by current software technologies. Unfortunately,
	too often the abstractions offered are limited and disparate with
	respect to the problem space. Reducing this gap would facilitate
	more sophisticated problems to be tackled in software development
	projects, and it would comparatively reduce development costs and
	time-to-market, and remove errors caused by the disparity. In this
	talk, I will explore a number of techniques for improving current
	software development practice, which relate to the theme of domain-driven
	software development. Domain-driven software development is concerned
	with making use of languages that better capture the problem by using
	abstractions that are more familiar to experts in the domain. These
	domain-specific languages are made executable either directly (compilation
	or interpretation) or through tool-supported refinement/elaboration
	to computational models that can be executed, e.g., to a mainstream
	programming language where one can make use of existing frameworks,
	components, services, etc. In the later case, real value is added
	to software development only if we can automate as much as possible
	the transformation step(s). Automating these steps requires languages
	that can express such transformations in a concise and maintainable
	manner. The principles of abstraction, separation of concerns, and
	problem decomposition are essential in providing intuitive and manageable
	domain-specific languages. The practice of software modeling has
	become a significant way of applying these principles to software
	development. Over the last few years, the software development industry
	has gone through the process of standardizing visual modeling notations.
	The Unified Modeling Language (UML) is the product of this effort,
	and it unifies scores of notations that were proposed in the '80s
	and '90s. The language has gained significant industry support and
	became an object management group (OMG) standard in 1997. Nowadays,
	the majority of software modeling techniques and approaches use UML.},
  doi = {10.1109/IWRSP.2004.1311104},
  issn = {1074-6005 },
  keywords = { OMG standard; Unified Modeling Language; abstraction; domain-driven
	software development; domain-specific language; object management
	group; programming language; software modeling; tool support; Unified
	Modeling Language; software engineering; software tools;}
}

@INPROCEEDINGS{6044796,
  author = {Serot, J. and Berry, F. and Ahmed, S.},
  title = {Implementing Stream-Processing Applications on FPGAs: A DSL-Based
	Approach},
  booktitle = {Field Programmable Logic and Applications (FPL), 2011 International
	Conference on},
  year = {2011},
  pages = {130 -137},
  month = {sept.},
  abstract = {We introduce CAPH, a new domain-specific language (DSL) suited to
	the implementation of stream-processing applications on field programmable
	gate arrays (FPGA). CAPH relies upon the actor/dataflow model of
	computation. Applications are described as networks of purely dataflow
	actors exchanging tokens through unidirectional channels. The behavior
	of each actor is defined as a set of transition rules using pattern
	matching. The CAPH suite of tools currently comprises a reference
	interpreter and a compiler producing both SystemC and synthetizable
	VHDL code. We describe the implementation, with a preliminary version
	of the compiler, of a simple real-time motion detection application
	on a FPGA-based smart camera platform. The language reference manual
	and a prototype compiler are available from http://wwwlasmea.univ-bpclermont.fr/Personnel/Jocelyn.Serot/caph.html.},
  doi = {10.1109/FPL.2011.32},
  keywords = {CAPH suite;FPGA;actor-dataflow model;compiler;domain-specific language;field
	programmable gate arrays;pattern matching;real-time motion detection
	application;reference interpreter;smart camera platform;stream-processing
	applications;synthetizable VHDL code;transition rules;unidirectional
	channels;data flow computing;field programmable gate arrays;pattern
	matching;program compilers;program interpreters;specification languages;}
}

@INPROCEEDINGS{4289018,
  author = {Serrano, J.M. and Serrat, J. and Strassner, J.},
  title = {Ontology-Based Reasoning for Supporting Context-Aware Services on
	Autonomic Networks},
  booktitle = {Communications, 2007. ICC '07. IEEE International Conference on},
  year = {2007},
  pages = {2097 -2102},
  month = {june},
  abstract = {Ontology engineering has been proposed as a formal mechanism for both
	reducing the complexity of managing the information needed in network
	management and autonomic systems and for increase the portability
	of the services across homogeneous and heterogeneous networks. In
	this paper we propose an ontology for supporting the creation, delivery
	and management of context-aware services and also for the integration
	of the user's context information in service management operations
	for heterogeneous networks. This ontology provides formal semantics
	that capture concepts of context information for helping in the service
	management operations and also augments the information model for
	adding domain-specific user's context data. Using this ontology,
	we have created a "knowledge plane" that supports the reasoning needed
	by autonomic networks. We have studied the use of ontology autonomic
	elements for gathering raw context and integrating it to improve
	and/or enhance the user's context representation. Finally, we provide
	a study and analysis for ensuring the efficient handling and dissemination
	of context information to overlay applications in autonomic environments
	for self-managing or self-configuring service operations.},
  doi = {10.1109/ICC.2007.347},
  keywords = {autonomic networks;autonomic systems;complexity reduction;context-aware
	services;formal mechanism;formal semantics;heterogeneous networks;homogeneous
	networks;information management;knowledge plane;network management;ontology
	autonomic elements;ontology engineering;ontology-based reasoning;raw
	context;service management;user context information;formal languages;formal
	specification;inference mechanisms;ontologies (artificial intelligence);telecommunication
	network management;}
}

@INPROCEEDINGS{492194,
  author = {Seshadri, P. and Pirahesh, H. and Leung, T.Y.C.},
  title = {Complex query decorrelation},
  booktitle = {Data Engineering, 1996. Proceedings of the Twelfth International
	Conference on},
  year = {1996},
  pages = {450 -458},
  month = {feb-1 mar},
  abstract = {Complex queries used in decision support applications use multiple
	correlated subqueries and table expressions, possibly across several
	levels of nesting. It is usually inefficient to directly execute
	a correlated query; consequently, algorithms have been proposed to
	decorrelate the query, i.e. to eliminate the correlation by rewriting
	the query. This paper explains the issues involved in decorrelation,
	and surveys existing algorithms. It presents an efficient and flexible
	algorithm called magic decorrelation which is superior to existing
	algorithms both in terms of the generality of application, and the
	efficiency of the rewritten query. The algorithm is described in
	the context of its implementation in the Starburst Extensible Database
	System, and its performance is compared with other decorrelation
	techniques. The paper also explains why magic decorrelation is not
	merely applicable, but crucial in a parallel database system},
  doi = {10.1109/ICDE.1996.492194},
  keywords = {SQL;Starburst Extensible Database System;algorithms;complex query
	decorrelation;correlated query;decision support;magic decorrelation;multiple
	correlated subqueries;nesting;parallel database system;performance;relational
	database;rewritten query;table expressions;SQL;database theory;decision
	support systems;distributed databases;query processing;relational
	databases;software performance evaluation;}
}

@INPROCEEDINGS{4736164,
  author = {Setavoraphan, K. and Grant, F.H.},
  title = {Conceptual simulation modeling: The structure of domain specific
	simulation environment},
  booktitle = {Simulation Conference, 2008. WSC 2008. Winter},
  year = {2008},
  pages = {975 -986},
  month = {dec.},
  abstract = {This study focuses on the development of a conceptual simulation modeling
	tool that can be used to structure a domain specific simulation environment.
	The issues in software engineering and knowledge engineering such
	as object-oriented concepts and knowledge representations are addressed
	to identify and analyze modeling frameworks and patterns of a specific
	problem domain. Thus, its structural and behavioral characteristics
	can be conceptualized and described in terms of simulation architecture
	and context. Moreover, symbols, notations, and diagrams are developed
	as a communication tool that creates a blueprint to be seen and recognized
	by both domain experts and simulation developers, which leads to
	the effectiveness and efficiency in the simulation development of
	any specific domains.},
  doi = {10.1109/WSC.2008.4736164},
  keywords = {conceptual simulation modeling;domain specific simulation;knowledge
	representations;object-oriented concepts;knowledge representation;object-oriented
	methods;simulation languages;}
}

@ARTICLE{1353202,
  author = {Niraj Shah and Plishker, W. and Kaushik Ravindran and Keutzer, K.},
  title = {NP-Click: a productive software development approach for network
	processors},
  journal = {Micro, IEEE},
  year = {2004},
  volume = {24},
  pages = { 45 - 54},
  number = {5},
  month = {sept.-oct.},
  abstract = { Application-specific integrated circuit (ASIC) design is too risky
	and prohibitively expensive for many applications. This trend, combined
	with increasing silicon capability on a die, is fueling the emergence
	of application-specific programmable architectures. This focus on
	architecture design for network processors has made programming them
	an arduous task. Current network processors require in-depth knowledge
	of the architecture just to begin programming the device. However,
	for network processors to succeed, programmers must efficiently implement
	high-performance applications on them. Writing high-performance code
	for modern network processors is difficult because of their complexity.
	NP-Click is a simple programming model that permits programmers to
	reap the benefits of a domain specific language while still allowing
	for target-specific optimizations. Results for the Intel IXP1200
	indicate that NP-Click delivers a large productivity gain at a slight
	performance expense.},
  doi = {10.1109/MM.2004.53},
  issn = {0272-1732},
  keywords = { NP-Click; application-specific programmable architecture; network
	processor; programming model; software development; specific language;
	computer architecture; multi-threading; software engineering; specification
	languages;}
}

@INPROCEEDINGS{4803005,
  author = {Shams, R. and Elsayed, A.},
  title = {A Corpus-based evaluation of lexical components of a domain-specific
	text to Knowledge Mapping prototype},
  booktitle = {Computer and Information Technology, 2008. ICCIT 2008. 11th International
	Conference on},
  year = {2008},
  pages = {242 -247},
  month = {dec.},
  abstract = {The aim of this paper is to evaluate the lexical components of a text
	to knowledge mapping (TKM) prototype. The prototype is domain-specific,
	the purpose of which is to map instructional text onto a knowledge
	domain. The context of the knowledge domain of the prototype is physics,
	specifically DC electrical circuits. During development, the prototype
	has been tested with a limited data set from the domain. The prototype
	now reached a stage where it needs to be evaluated with a representative
	linguistic data set called corpus. A corpus is a collection of text
	drawn from typical sources which can be used as a test data set to
	evaluate NLP systems. As there is no available corpus for the domain,
	we developed a representative corpus and annotated it with linguistic
	information. The evaluation of the prototype considers one of its
	two main components-lexical knowledge base. With the corpus, the
	evaluation enriches the lexical knowledge resources like vocabulary
	and grammar structure. This leads the prototype to parse a reasonable
	amount of sentences in the corpus.},
  doi = {10.1109/ICCITECHN.2008.4803005},
  keywords = {NLP system;corpus-based evaluation;domain-specific text;grammar;instructional
	text;knowledge domain;lexical component;lexical knowledge resources;linguistic
	data set;text to knowledge mapping;vocabulary;grammars;linguistics;natural
	language processing;text analysis;vocabulary;}
}

@INPROCEEDINGS{6031212,
  author = {Shams, R. and Shahnawaz Chowdhury, M.S.A. and Abu Saleh Shawon, S.M.},
  title = {Domain-specific textual commonsense concept acquisition using a corpus},
  booktitle = {Communications, Computing and Control Applications (CCCA), 2011 International
	Conference on},
  year = {2011},
  pages = {1 -6},
  month = {march},
  abstract = {In this paper, we present a textual commonsense concept acquisition
	system named SenCept. It works on text of DC electrical circuits
	and provides commonsense concepts associated with them for better
	contextualization. SenCept uses a manually developed commonsense
	knowledge-base that is built upon linguistic information of a domain-specific
	corpus. We selected representative commonsense knowledge by using
	several parameters like knowledge weight, average commonsensical
	distances among knowledge, and normalized mean. To identify commonsense
	concepts for any sentence, SenCept concentrates on mean of distances
	between normalized weights of representative sentences and average
	commonsensical distances among knowledge. We fed 100 sentences to
	five human subjects and SenCept to evaluate its performance. Results
	showed that concepts produced by SenCept are originated from textual
	commonsense in contrast to human analysis that produces concepts
	from domain knowledge. Moreover, SenCept's Common Concept Rate (CCR)
	is 43 percent- which is better than that of human analysis.},
  doi = {10.1109/CCCA.2011.6031212},
  keywords = {DC electrical circuit text;SenCept common concept rate;commonsense
	knowledge-base;commonsensical distance;domain knowledge;domain-specific
	corpus;domain-specific textual commonsense concept acquisition;human
	analysis;knowledge weight;linguistic information;normalized mean;representative
	commonsense knowledge;electrical engineering computing;knowledge
	acquisition;knowledge representation;natural language processing;text
	analysis;}
}

@INPROCEEDINGS{5532548,
  author = {Shani, Uri and Sela, Aviad},
  title = {Integrating Domain-Specific Programming into Software Design},
  booktitle = {Software Science, Technology and Engineering (SWSTE), 2010 IEEE International
	Conference on},
  year = {2010},
  pages = {1 -6},
  month = {june},
  abstract = {Domain-specific languages (DSLs) have recently become a focus of attention
	in the software engineering community. We look at domain-specific
	modeling (DSM) methods that drive modeling languages for specific
	domains with a strong emphasis on visual tools and suggest a method
	for integrating them into common software design methodologies. We
	demonstrate a practical approach, whereby components of software
	are designed to be externalized as specific domain-oriented tasks.
	The logic in such tasks is intended to be developed by skilled personnel,
	different from those required to implement the main application.
	Furthermore, the application will become adaptable to a large class
	of solutions that do not require new version releases when business
	logic changes. Unlike application customization via configuration
	parameters, the logic implemented in DSL languages requires a meaningful
	imperative expressive power. Our method starts with the common software
	design methodologies based on UML and uses the Eclipse Modeling Framework
	(EMF) tools to externalize a selected subset of the design.},
  doi = {10.1109/SwSTE.2010.9}
}

@INPROCEEDINGS{4426552,
  author = {Shashirekha, H.L. and Murali, S.},
  title = {Ontology Based Structured Representation for Domain Specific Unstructured
	Documents},
  booktitle = {Conference on Computational Intelligence and Multimedia Applications,
	2007. International Conference on},
  year = {2007},
  volume = {1},
  pages = {50 -54},
  month = {dec.},
  abstract = {Extracting information from unstructured, brief and short text composed
	of short phrases, incomplete sentences, unordered sequence of words
	and words in short form not falling into any regular syntax is a
	challenging task. This paper describes an approach to automatically
	extract information from data rich unstructured text documents based
	on a domain dependent ontology and populate a database. Here, we
	apply pattern matching in terms of keywords/constants to extract
	the patterns and generate a structured text representation with respect
	to a domain specific ontology. The approach is illustrated on one
	such unstructured, short and brief text -classified matrimonial advertisement.
	The performance analysis of the approach on this case study is presented.},
  doi = {10.1109/ICCIMA.2007.255},
  keywords = {classified matrimonial advertisement;domain dependent ontology;domain
	specific unstructured documents;information extraction;ontology based
	structured representation;pattern matching;structured text representation;text
	documents;data structures;document handling;information retrieval;ontologies
	(artificial intelligence);text analysis;}
}

@INPROCEEDINGS{5708808,
  author = {Dan Shen and Ruvini, J. and Mukherjee, R. and Sundaresan, N.},
  title = {A Study of Smoothing Algorithms for Item Categorization on e-Commerce
	Sites},
  booktitle = {Machine Learning and Applications (ICMLA), 2010 Ninth International
	Conference on},
  year = {2010},
  pages = {23 -28},
  month = {dec.},
  abstract = {One central issue in a long-tail online marketplace such as eBay is
	to automatically put user self-input items into a catalog in real
	time. This task is extremely challenging when the inventory scales
	up, the items become ephemeral, and the user input remains noisy.
	Indeed, catalog learning has emerged as a key technical property
	for other major online ecommerce applications including search and
	recommendation. We formulate the item cataloging task as a Bayesian
	classification problem, which shall scale well in very large data
	set and have good online prediction performance. The inherent data
	sparseness issue, especially for those tail categories, is key to
	the overall model performance. We address the data sparseness issue
	by adapting statistically sound smoothing methods well studied in
	language modeling tasks. However, there are data characteristics
	specific to the ecommerce domain, including short yet focused item
	description, very large and hierarchical catalog taxonomy, and highly
	skewed distribution over types of items. We investigate these domain-specific
	regularities empirically, and report practically significant results
	with real-world true-scale data.},
  doi = {10.1109/ICMLA.2010.11},
  keywords = {Bayesian classification problem;adapting statistically sound smoothing
	methods;catalog learning;data sparseness;e-commerce sites;hierarchical
	catalog taxonomy;highly skewed distribution;inventory;item cataloging
	task;item categorization;language modeling tasks;long-tail online
	marketplace;online ecommerce;online prediction;real-world true-scale
	data;smoothing algorithms;Web sites;belief networks;cataloguing;electronic
	commerce;}
}

@INPROCEEDINGS{1316736,
  author = {Shetty, S. and Neema, S. and Bapty, T.},
  title = {Model based self adaptive behavior language for large scale real
	time embedded systems},
  booktitle = {Engineering of Computer-Based Systems, 2004. Proceedings. 11th IEEE
	International Conference and Workshop on the},
  year = {2004},
  pages = { 478 - 483},
  month = {may},
  abstract = {At Fermi lab, high energy physics experiments require very large number
	of real time computations. With thousands of processors (around sim;1000
	FPGA's, sim;2500 embedded processors, sim;2500 PC's and sim;25,000,000
	detector channels) involved in performing event filtering on a trigger
	farm, there is likely to be a large number of failures within the
	software and hardware systems. Historically, physicists have developed
	their own software and hardware for experiments such as BTeV [J.N.
	Buttler (2002)]. However, their time is best spent working on physics
	and not software development. The target users of this tool are the
	physicists. The tool should be user-friendly and the physicists should
	be able to introduce custom self-adaptive behaviors, since they can
	best define how the system should behave in fault conditions. The
	BTeV trigger system is being used as a model for researching tools
	for defining fault behavior and automatically generating the software.
	This paper presents a language to define the behaviors and an application
	scenario for the BTeV system and its expected fault scenarios. These
	self adaptive system tools are implemented using model integrated
	computing. The domain specific graphical language (DSL) is implemented
	within the generic modeling environment (GME) tool, which is a meta-programmable
	modeling environment developed at Vanderbilt University.},
  doi = {10.1109/ECBS.2004.1316736},
  issn = { },
  keywords = { BTeV trigger system; Fermi lab; domain specific graphical language;
	embedded processors; event filtering; generic modeling environment
	tool; high energy physics experiments; large scale real time embedded
	systems; meta-programmable modeling environment; model based self
	adaptive behavior language; model integrated computing; embedded
	systems; fault diagnosis; large-scale systems; self-adjusting systems;
	specification languages; visual languages;}
}

@INPROCEEDINGS{1409952,
  author = {Shetty, S. and Nordstrom, S. and Ahuja, S. and Di Yao and Bapty,
	T. and Neema, S.},
  title = {Systems integration of large scale autonomic systems using multiple
	domain specific modeling languages},
  booktitle = {Engineering of Computer-Based Systems, 2005. ECBS '05. 12th IEEE
	International Conference and Workshops on the},
  year = {2005},
  pages = { 481 - 489},
  month = {april},
  abstract = { Software design, development and maintenance for large scale systems
	has been one of the most difficult and expensive phases of the software
	development life cycle. Design and maintenance is especially difficult
	when the system includes autonomic features. As the system size and
	variety of autonomic behaviors scale up, it increases the chance
	of many unexpected and unwanted interactions. Separate design tools
	can hide these potential interactions. To face these challenges,
	we propose an autonomic system integration platform where holistic
	design models capture system structure, target system resources,
	and autonomic behavior. The fault mitigative, autonomic behavior
	can be explicitly coupled to the components and underlying resources
	of the system. System generation technology is used to create the
	software that implements these coupled specifications, including
	communication between components with custom data type marshalling
	and demarshalling, system startup and configuration, fault tolerant
	behavior, and autonomic procedures for self-correction. This modeling
	schema, along with the tools to generate the various system components
	are described in this paper.},
  doi = {10.1109/ECBS.2005.65},
  keywords = { custom data type demarshalling; custom data type marshalling; fault
	mitigative autonomic behavior; large scale autonomic system integration;
	multiple domain specific modeling languages; software design; software
	development; software development life cycle; software maintenance;
	system component generation; formal specification; software architecture;
	software prototyping; software tools; specification languages;}
}

@INPROCEEDINGS{1303186,
  author = {Shi, W. and Corriveau, J.-P.},
  title = {An executable model for a family of election algorithms},
  booktitle = {Parallel and Distributed Processing Symposium, 2004. Proceedings.
	18th International},
  year = {2004},
  pages = { 178},
  month = {april},
  abstract = { Summary form only given. We present an executable model for a family
	of algorithms dealing with leader election in a ring topology. We
	follow the traditional approach of system family engineering. That
	is, we develop a feature model that captures variability across these
	algorithms. We then proceed to produce a generator. This generator
	receives as inputs specific values for each of the variation points
	(i.e., features) we identify. And it produces the behavior corresponding
	to the specific configuration of features at hand. Contrary to existing
	generative programming literature, we do not resort to C++ meta-programming
	but instead develop an executable model using Rational Rose RT. More
	precisely, we have designed a single state chart that can model all
	the algorithms of the family we studied. We focus here on how to
	obtain such a state chart, rather than on the identification of the
	features we used, or on ROSE-RT semantics. We do believe however
	that our approach can be reused to provide a semantically unified
	and executable modelling approach for other families of algorithms.},
  doi = {10.1109/IPDPS.2004.1303186},
  keywords = { C++ meta-programming; ROSE-RT semantics; election algorithms; generative
	programming; ring topology; state chart; system family engineering;
	distributed algorithms; systems engineering; topology;}
}

@INPROCEEDINGS{4416807,
  author = {Sierla, S. and Christensen, J. and Koskinen, K. and Peltola, J.},
  title = {Educational approaches for the industrial acceptance of IEC 61499},
  booktitle = {Emerging Technologies and Factory Automation, 2007. ETFA. IEEE Conference
	on},
  year = {2007},
  pages = {482 -489},
  month = {sept.},
  abstract = {Professionals' assessments on the usefulness of IEC 61499 depend on
	several factors that are outside of the scope of the standard. Automation
	design in the industry is largely routine work that is based on copying
	and modifying existing domain-specific solutions, so the lack of
	legacy software exacerbates learning difficulties. Other factors
	such as team organization and the development environment also strongly
	affect the experience of using IEC 61499. The goal of this paper
	is to identify how a designer's framework needs to change before
	IEC 61499 can be applied successfully. Two differently organized
	IEC 61499 courses for professionals and researchers are described,
	and the impact of the educational approach on a positive experience
	for professionals is discussed.},
  doi = {10.1109/EFTA.2007.4416807},
  keywords = {IEC 61499 course;automation design;educational approach;professional
	assessment;IEC standards;educational computing;industrial engineering;industrial
	training;}
}

@INPROCEEDINGS{1425175,
  author = {Sierra, J.L. and Fernandez-Manjon, B. and Fernandez-Valmayor, A.
	and Navarro, A.},
  title = {Document-oriented software construction based on domain-specific
	markup languages},
  booktitle = {Information Technology: Coding and Computing, 2005. ITCC 2005. International
	Conference on},
  year = {2005},
  volume = {2},
  pages = { 392 - 397 Vol. 2},
  month = {april},
  abstract = { In this paper we present ADDS (approach to document-oriented development
	of software), our solution to software construction based on domain-specific
	languages (DSLs). DSLs in ADDS are formulated as descriptive domain-specific
	markup languages (DSMLs) that are used for marking up the documents
	that describe the relevant aspects of the applications (e.g. data
	and some aspects of the behavior). Final running applications are
	obtained by the processing of these documents with suitable processors.
	ADDS promotes the incremental development of DSMLs and their processors,
	so they can evolve according to the authoring needs of the different
	participants in the development process (domain experts and developers).
	The incremental nature of ADDS is eased by its document orientation.
	Thus ADDS palliates the high costs of formulation, operationalization
	and maintenance of DSLs exhibited by other approaches.},
  doi = {10.1109/ITCC.2005.134},
  keywords = { ADDS; DSML; XML; document-oriented software construction; domain-specific
	markup languages; XML; object-oriented programming; software maintenance;
	software prototyping; specification languages; system documentation;}
}

@INPROCEEDINGS{1652489,
  author = {Sierra, J.L. and Fernandez-Valmayor, A. and Guinea, M.},
  title = {Exploiting Author-Designed Domain-Specific Descriptive Markup Languages
	in the Production of Learning Content},
  booktitle = {Advanced Learning Technologies, 2006. Sixth International Conference
	on},
  year = {2006},
  pages = {515 -519},
  month = {july},
  abstract = {In this paper we describe an approach to the production of learning
	resources where authors (students and instructors) are actively involved
	in the production process. This active involvement is achieved by
	using descriptive markup technologies. Authors are compelled to produce
	learning resources in the form of documents, and to make the structure
	of these documents explicit by creating and using descriptive markup
	languages. This lets developers formalize these author-designed markup
	languages and provide suitable transformation specifications for
	translating these marked documents into their final presentations.
	We exemplify this approach with the production of DHTML pages by
	Ph.D. students in archaeology, oriented to be used as resources owned
	by reusable learning objects in Chasqui, an authoring and deployment
	tool used in the virtualization of academic museums at the Complutense
	University of Madrid (Spain)},
  doi = {10.1109/ICALT.2006.1652489},
  keywords = {active learning;author-designed domain-specific descriptive markup
	languages;learning content production;learning resources;reusable
	learning objects;educational computing;page description languages;}
}

@INPROCEEDINGS{1508657,
  author = {Sierra, J.L. and Fernandez-Valmayor, A. and Guinea, M. and Hernanz,
	H. and Navarro, A.},
  title = {Building repositories of learning objects in specialized domains:
	the Chasqui approach},
  booktitle = {Advanced Learning Technologies, 2005. ICALT 2005. Fifth IEEE International
	Conference on},
  year = {2005},
  pages = { 225 - 229},
  month = {july},
  abstract = { In this paper we describe the Chasqui approach to the construction
	of repositories of learning objects (LO) in specific knowledge areas.
	This approach is the result of our experiences in the virtualization
	of academic museums at the Complutense University of Madrid (Spain)
	as well as of some experiences in the visualization of the campus
	of this university. This approach promotes a close collaboration
	between two kinds of actors: domain experts (e.g., researchers and
	lecturers) and developers. This collaboration results in (i) the
	definition of a suitable model for the LO in the domain of interest,
	(ii) the construction by developers of a domain-specific tool for
	authoring LO conforming this model, and (iii) the population of the
	repository by domain experts using this tool. The domain specific
	LO models and the domain specific tools based on these models facilitate
	the production and maintenance of the repositories and the exploitation
	of the educational potential of the resources used by the experts
	in their daily work.},
  doi = {10.1109/ICALT.2005.77},
  keywords = { Chasqui approach; Complutense University of Madrid; Spain; academic
	museum virtualization; campus virtualization; domain experts; domain-specific
	tool for; learning object authoring; learning object repository building;
	specialized domains; virtual campus; virtual museum; distributed
	object management; educational computing; humanities; object-oriented
	programming; virtual reality;}
}

@INPROCEEDINGS{4278774,
  author = {da Silva Santos, L.O.B. and Ramparany, F. and Costa, P.D. and Vink,
	P. and Etter, R. and Broens, T.},
  title = {A Service Architecture for Context Awareness and Reaction Provisioning},
  booktitle = {Services, 2007 IEEE Congress on},
  year = {2007},
  pages = {25 -32},
  month = {july},
  abstract = {Context awareness has emerged as an important element in distributed
	computing. It offers mechanisms allowing applications to be aware
	of their environment and enabling them to adjust their behavior to
	the current context. In order to keep track of the relevant context
	information, a flexible service mechanism should be available for
	the client applications. In this paper we present a service architecture
	to provide context- awareness capabilities to users and client applications.
	Moreover, the service is able to react depending on the user's preferences
	and context. The conditions for the reaction and the reaction itself
	are defined in rules the users submit to the service by means of
	a convenient rule language.},
  doi = {10.1109/SERVICES.2007.10},
  keywords = {context awareness;convenient rule language;distributed computing;reaction
	provisioning;service architecture;software architecture;ubiquitous
	computing;}
}

@INPROCEEDINGS{1231564,
  author = {da Silva, A.R.},
  title = {The XIS approach and principles},
  booktitle = {Euromicro Conference, 2003. Proceedings. 29th},
  year = {2003},
  pages = { 33 - 40},
  month = {sept.},
  abstract = { XIS is a R D project which mission is to analyze, develop and evaluate
	mechanisms and tools to produce information systems from a more efficient
	and productive way than it is done currently. XIS project is influenced
	by MDA reference model, and is mainly based on three principles:
	it is based on high-level models specification; it is based on generative
	programming techniques; and it is component-based architecture-centric.
	XIS is not a conceptual research plan, it is a working on project
	with concrete results and produced systems. Here we overview the
	XIS project by introducing its main elements, such as the XIS approach;
	XIS platform; XIS/UML profile; and XIS/XML language. Finally, we
	present the main conclusions and the work that will be handled in
	the near future.},
  doi = {10.1109/EURMIC.2003.1231564},
  issn = {1089-6503},
  keywords = { MDA reference model; R D project; UML profile; XIS approach; XIS
	language; XIS platform; XIS profile; XML language; component-based
	architecture-centric; generative programming; high-level model specification;
	information system; XML; formal specification; information systems;
	object-oriented programming; software architecture; software development
	management; specification languages;}
}

@INPROCEEDINGS{1245347,
  author = {Silva, A.R. and Lemos, G. and Matias, T. and Costa, M.},
  title = {The XIS generative programming techniques},
  booktitle = {Computer Software and Applications Conference, 2003. COMPSAC 2003.
	Proceedings. 27th Annual International},
  year = {2003},
  pages = { 236 - 241},
  month = {nov.},
  abstract = { XIS is a R D project which main mission is to analyze, develop and
	evaluate mechanisms and tools to produce information systems from
	a more abstract, high-level, efficient and productive way than it
	is done currently. XIS project is influenced by MDA reference model,
	and is mainly based on three principles: namely high-level models
	specification; generative programming techniques; and it is component-based
	architecture-centric. XIS is not a conceptual research plan, it is
	a working on project with concrete results and produced systems.
	In this paper we detail the generative programming techniques used
	in the XIS project as well as the discussions and main decisions
	tackled on. Finally, we present the main conclusions, the relationship
	between XIS and MDA, and the work that will be handled in the near
	future.},
  doi = {10.1109/CMPSAC.2003.1245347},
  issn = {0730-3157 },
  keywords = { XIS; component-based architecture centric; generative programming
	techniques; high-level models specification; information systems;
	automatic programming; information systems; object-oriented programming;
	software architecture; specification languages;}
}

@ARTICLE{4435107,
  author = {da Silva, A.R. and Saraiva, J. and Ferreira, D. and Silva, R. and
	Videira, C.},
  title = {Integration of RE and MDE paradigms: the ProjectIT approach and tools},
  journal = {Software, IET},
  year = {2007},
  volume = {1},
  pages = {294 -314},
  number = {6},
  month = {december },
  abstract = {The suggestion that in software development projects the emphasis
	must be on the project management (RE), requirements engineering,
	and design activities, and consequently efforts in production activities
	- such as traditional software programming and testing - should be
	minimised and performed as automatically as possible is discussed.
	The Project IT approach that integrates contributions from the RE
	and model-driven engineering communities is also discussed. The goal
	with requirement specification is not just in managing textual specifications,
	but also to obtain a consistent requirements document that is in
	conformance with a domain- specific language, and that can be re-used
	to increase the design and development activities in the context
	of model driven and code generation techniques. Furthermore, the
	feasibility and benefits of this approach by presenting a proof-of-concept
	case study are discussed, in which the orchestration of the concepts
	and concrete components related with the Project IT approach, the
	PIT-RSL, XIS and PIT-TSL languages and the Project lT-Studio CASE
	tool is emphasised. A practical demonstration of the approach including
	the description of the system requirements, the design of the system,
	the use of code generation techniques, and how they integrate to
	improve and accelerate the software engineering lifecycle is presented.},
  doi = {10.1049/iet-sen:20070012},
  issn = {1751-8806},
  keywords = {ProjectIT;code generation techniques;design activities;project management;requirement
	specification;requirements engineering;software development projects;software
	engineering lifecycle;formal specification;product life cycle management;program
	compilers;project management;software development management;}
}

@INPROCEEDINGS{390473,
  author = {Simard, R.J. and Zeigler, B.P. and Couretas, J.M.},
  title = {Verb phrase model specification via system entity structures},
  booktitle = {AI, Simulation, and Planning in High Autonomy Systems, 1994. 'Distributed
	Interactive Simulation Environments'., Proceedings of the Fifth Annual
	Conference on},
  year = {1994},
  pages = {192 -198},
  month = {dec},
  abstract = {In investigating front end model development, an environment is described
	that allows for model construction through pruning a domain specific
	system entity structure. The preformal stages of the model will be
	represented by a verb phrase. This representation is sufficiently
	derailed to serve as the basis for model construction and yet sufficiently
	ldquo;soft rdquo; to support knowledge acquisition during model construction.
	This paper establishes the adequacy of this representation},
  doi = {10.1109/AIHAS.1994.390473},
  keywords = {discrete event model;front end model development;knowledge acquisition;knowledge
	representation;model construction;natural language;noun taxonomy;system
	entity structures;verb phrase model specification;computational linguistics;discrete
	event simulation;formal specification;knowledge acquisition;knowledge
	representation;natural languages;}
}

@INPROCEEDINGS{5393565,
  author = {Singh, S.K. and Gupta, R. and Sabharwal, S. and Gupta, J.P.},
  title = {Automatic extraction of events from Textual Requirements specification},
  booktitle = {Nature Biologically Inspired Computing, 2009. NaBIC 2009. World Congress
	on},
  year = {2009},
  pages = {415 -420},
  month = {dec.},
  abstract = {Events give important information about the behavior of a system in
	a summarized form. In the past, events have played an important role
	in breaking the functional requirements of the system in the event
	partitioning approach. Our previous work has shown that events
	can be a starting point in object-oriented analysis of requirements.
	Every event triggers a use case in the system, hence should get a
	priority in identifying and analyzing requirements over use cases.
	In any system there is plethora of events happening, some are important
	to be recorded, while others are to be ignored. Moreover, there are
	various perspectives to define events. Thus, it becomes important
	to have an automated process that could help not only in extracting
	events but also analyze and classify them into various types. A study
	on various existing event extraction tools shows that they are either
	domain specific or take events as actions that occur at a particular
	time. There is no tool which extracts events that represent system
	behavior and at the same time gives a result that can be reused for
	application in multiple domains. This paper presents, a domain independent
	tool, developed in Java that automates the process of extraction,
	analysis and classification of events from textual requirements expressed
	in English as a natural language. This tool also assists the analysts
	in further refining identified events and to add some new events
	in the application domain. Tool has been tested on several case studies
	from different domains and has given very promising results.},
  doi = {10.1109/NABIC.2009.5393565},
  keywords = {English natural language;Java;domain independent tool;event automatic
	extraction;event extraction tools;event partitioning approach;object-oriented
	analysis;textual requirements specification;Java;information filtering;natural
	language processing;systems analysis;text analysis;}
}

@INPROCEEDINGS{5329456,
  author = {Singh, S.K. and Sabharwal, S. and Gupta, J.P.},
  title = {E-XTRACT: A Tool for Extraction, Analysis and Classification of Events
	from Textual Requirements},
  booktitle = {Advances in Recent Technologies in Communication and Computing, 2009.
	ARTCom '09. International Conference on},
  year = {2009},
  pages = {306 -308},
  month = {oct.},
  abstract = {This paper presents, a domain independent tool, developed in JAVA
	that automates the process of extraction, analysis and classification
	of events from textual requirements, expressed in English as a natural
	language. A study on various existing event extraction tools shows
	that they are either domain specific or take events as actions that
	occur at a particular time. There is no tool which extracts events
	that represent system behavior and at the same time gives a result
	that can be reused for application in multiple domains. This tool
	also assists the analysts in further refining events that are identified
	or to add some new events relevant to application domain. Tool has
	been tested on several case studies from different domains and has
	given very promising results.},
  doi = {10.1109/ARTCom.2009.120},
  keywords = {E-XTRACT;English;JAVA;event analysis;event classification;event extraction;natural
	language;Java;knowledge acquisition;natural language processing;text
	analysis;}
}

@INPROCEEDINGS{1251041,
  author = {Sinha, A. and Smidts, C.S. and Moran, A.},
  title = {Enhanced testing of domain specific applications by automatic extraction
	of axioms from functional specifications},
  booktitle = {Software Reliability Engineering, 2003. ISSRE 2003. 14th International
	Symposium on},
  year = {2003},
  pages = { 181 - 190},
  month = {nov.},
  abstract = { Adequate testing is necessary and important to ensure reliability
	of software. Most test models are specification-based and fail to
	capture implicit domain specific properties. This paper presents
	a technique, which uses a HaskellDB specification of the software
	to extract domain specific properties and embed them into the test
	generation model. HaskellDB is an embedded domain specific functional
	and strongly typed language for database related applications. Specifying
	using HaskellDB ensures that a set of axioms based on type safeness
	of the database queries hold for the specification. The implementation
	of the application should also satisfy these properties and should
	be tested accordingly. We therefore propose a technique that extracts
	the axioms automatically from the HaskellDB specification and embeds
	additional test paths in the test model leading to an enriched test
	suite. We present an example application of the technique and compare
	the results against a manual testing technique.},
  doi = {10.1109/ISSRE.2003.1251041},
  issn = {1071-9458},
  keywords = { HaskellDB specification; automatic axiom extraction; database queries;
	database related applications; domain specific application testing;
	functional specifications; software reliability; software testing;
	specification-based test models; test generation model; test path
	embedding; type safeness; database management systems; formal specification;
	functional languages; program testing; software reliability; type
	theory;}
}

@INPROCEEDINGS{4626873,
  author = {Sivonen, S.},
  title = {DSML for Developing Repository-Based Eclipse Plug-Ins},
  booktitle = {Software Product Line Conference, 2008. SPLC '08. 12th International},
  year = {2008},
  pages = {356},
  month = {sept.},
  abstract = {Summary form only given. This paper presents a successful case of
	utilising DSM in software product line development: DSML and code
	generator for creating repository-based Eclipse plug-ins.},
  doi = {10.1109/SPLC.2008.47},
  keywords = {DSML;MySQL database;code generation;code generator;domain-specific
	modelling languages;embedded Java database;open source platform;repository-based
	Eclipse plug-ins;software product line development:;program compilers;public
	domain software;simulation languages;software engineering;}
}

@INPROCEEDINGS{1240311,
  author = {Skene, J. and Emmerich, W.},
  title = {A model-driven approach to non-functional analysis of software architectures},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = { 236 - 239},
  month = {oct.},
  abstract = { We present an approach to managing formal models using model driven
	architecture (MDA) technologies that deliver analysis techniques
	through integration with the design tools and repositories that practitioners
	use. Expert modeling knowledge is captured in domain-specific languages
	and meta-model constraints. These are represented using UML (Unified
	Modeling Language) and collocated with designs and analysis models,
	providing a flexible and visible approach to managing semantic associations.
	The approach relies on standards to permit deployment in multiple
	tools. We demonstrate our approach with an example in which queuing-network
	models are associated with UML design models to predict average case
	performance.},
  doi = {10.1109/ASE.2003.1240311},
  issn = {1527-1366},
  keywords = { MDA technologies; UML; Unified Modeling Language; average case performance;
	domain-specific languages; expert modeling knowledge; formal model
	management; meta-model constraints; model driven architecture; model-driven
	approach; multiple tools; nonfunctional analysis; queueing network;
	queuing-network models; software architectures; formal logic; queueing
	theory; software architecture; specification languages;}
}

@ARTICLE{5210121,
  author = {Skene, J. and Raimondi, F. and Emmerich, W.},
  title = {Service-Level Agreements for Electronic Services},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2010},
  volume = {36},
  pages = {288 -304},
  number = {2},
  month = {march-april },
  abstract = {The potential of communication networks and middleware to enable the
	composition of services across organizational boundaries remains
	incompletely realized. In this paper, we argue that this is in part
	due to outsourcing risks and describe the possible contribution of
	Service-Level Agreements (SLAs) to mitigating these risks. For SLAs
	to be effective, it should be difficult to disregard their original
	provisions in the event of a dispute between the parties. Properties
	of understandability, precision, and monitorability ensure that the
	original intent of an SLA can be recovered and compared to trustworthy
	accounts of service behavior to resolve disputes fairly and without
	ambiguity. We describe the design and evaluation of a domain-specific
	language for SLAs that tend to exhibit these properties and discuss
	the impact of monitorability requirements on service-provision practices.},
  doi = {10.1109/TSE.2009.55},
  issn = {0098-5589},
  keywords = {SLA;communication networks;domain specific language;electronic services;middleware;outsourcing
	risks;service level agreements;Internet;client-server systems;high
	level languages;outsourcing;}
}

@INPROCEEDINGS{5474729,
  author = {Sledziewski, K. and Bordbar, B. and Anane, R.},
  title = {A DSL-Based Approach to Software Development and Deployment on Cloud},
  booktitle = {Advanced Information Networking and Applications (AINA), 2010 24th
	IEEE International Conference on},
  year = {2010},
  pages = {414 -421},
  month = {april},
  abstract = {With the advent of Cloud computing massively scalable and cost effective
	IT resources can be accessed and used seamlessly. Various APIs are
	made available for manipulating the infrastructure of the Cloud and
	its data models and for applying the deployment tools. Cloud computing
	promotes a new approach to software development. In particular, the
	development team must bridge the gap between the requirement of the
	clients and the available facilities on the Cloud. This complexity
	might inevitably result in higher cost and potentially unsatisfactory
	results. In this paper a method for bridging the gap between the
	clients view and software development on the Cloud is proposed. It
	is based on the introduction of Domain Specific Languages (DSL) into
	the process of Cloud based application development and deployment.
	Domain Specific Languages facilitate the development of applications
	by easing the design of high level models and specifications that
	the client can understand and even produce. The automated method
	described in the paper implements and deploys software for the Cloud.
	A preliminary evaluation shows that the proposed approach improves
	the process of developing and deploying applications on the Cloud.},
  doi = {10.1109/AINA.2010.81},
  issn = {1550-445X},
  keywords = {cloud computing;domain specific languages;software deployment;software
	development;distributed programming;software engineering;specification
	languages;}
}

@INPROCEEDINGS{994492,
  author = {Sloane, A.M.},
  title = {Post-design domain-specific language embedding: a case study in the
	software engineering domain},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = { 3647 - 3655},
  month = {jan.},
  abstract = { Experiences are presented from a new case study of embedding domain-specific
	languages in the lazy functional language Haskell. The domain languages
	come from the Odin software build system. Thus, in contrast to most
	previous embedding projects, a design and implementation of the domain
	languages existed when the project began. Consequently, the design
	could not be varied to suit the target language and it was possible
	to evaluate the success or otherwise of the embedding process in
	more detail than if the languages were designed from scratch. Experiences
	were mostly positive. The embedded implementation is significantly
	smaller than its Odin equivalent. Many benefits are obtained from
	having the full power of an expressive programming language available
	to the domain programmer The project also demonstrates in a practical
	software engineering setting the utility of modern functional programming
	techniques such as lazy evaluation and monads for structuring programs.
	On the down side, the efficiency of the embedded version compares
	unfavourably to the original system.},
  doi = {10.1109/HICSS.2002.994492},
  keywords = { Haskell; Odin software build system; expressive programming language;
	functional programming techniques; lazy evaluation; lazy functional
	language; monads; post-design domain-specific language embedding;
	program structuring; software engineering; functional languages;
	functional programming; software engineering;}
}

@INPROCEEDINGS{1607380,
  author = {Sochos, P. and Riebisch, M. and Philippow, I.},
  title = {The feature-architecture mapping (FArM) method for feature-oriented
	development of software product lines},
  booktitle = {Engineering of Computer Based Systems, 2006. ECBS 2006. 13th Annual
	IEEE International Symposium and Workshop on},
  year = {2006},
  pages = {9 pp. -318},
  month = {march},
  abstract = {Software product lines (PLs) are large, complex systems, demanding
	high maintainability and enhanced flexibility. Nonetheless, in the
	state of the art PL methods, features are scattered and tangled throughout
	the system components, leading to poor maintainability. Additionally,
	the majority of PL methods support manual product composition, while
	the implementation of feature-level variability in PL products influences
	the system's conceptual integrity. Generative programming techniques
	do enhance flexibility, but on the cost of maintainability. The feature-architecture
	mapping (FArM) method provides a stronger mapping between features
	and the architecture. It is based on a series of transformations
	on the initial PL feature model. During these transformations, architectural
	components are derived, encapsulating the business logic of each
	transformed feature and having interfaces reflecting the feature
	interactions. The flexibility of FArM architectures is supported
	through the explicit integration of plug-in mechanisms. The methodology
	is evaluated in the context of a wireless handheld device PL},
  doi = {10.1109/ECBS.2006.69},
  keywords = {FArM architecture;PL feature model;architectural component;business
	logic;complex system;feature-architecture mapping;feature-oriented
	development;plug-in mechanism;software product line;wireless handheld
	device PL;object-oriented programming;software architecture;software
	maintenance;}
}

@INPROCEEDINGS{1714322,
  author = {Haitao Song and Zhumei Song and Shixiong Zheng},
  title = {Mapping Aspect-Oriented Domain-Specific Model to Code for Real Time
	System},
  booktitle = {Intelligent Control and Automation, 2006. WCICA 2006. The Sixth World
	Congress on},
  year = {2006},
  volume = {2},
  pages = {6426 -6431},
  month = {0-0 },
  abstract = {Model-integrated computing (MIC) benefits from aspect-oriented programming
	(AOP) during real-time system modeling process. However, crosscutting
	concerns, which MIC wants to eliminate, still appear because of lacking
	the elements to describe aspects directly in ordinary C++ language
	when the model is interpreted to source code. This paper presents
	an approach to interpret the aspect-oriented domain-specific model
	to AOP source code. Firstly, crosscutting concerns are defined as
	separated aspects during aspect-oriented (AO) modeling. Secondly,
	AOP model interpreter traverses these aspects and generates AOP code
	in Aspect C++. Subsequently, the weaver of Aspect C++ language weaves
	the aspects into a real system. This process fits well with the OMG's
	model driven architecture (MDA). The AOP model interpreter can be
	integrated into generic modeling environment (GME) conveniently,
	so as to support the whole process of system development using AO.
	Furthermore, the weaving is postponed from model interpreting to
	AOP language interpreting, which focuses the model-designers' attention
	on the construction of domain model itself and integrates the ability
	of AOP weaver in stock into the mapping procedure from AO model to
	AO application smoothly. Finally, several examples are described
	to illustrate the encapsulation of crosscutting concerns with the
	support of AOP model interpreter},
  doi = {10.1109/WCICA.2006.1714322},
  keywords = {AOP model interpreter;Aspect C++ language;aspect-oriented model;aspect-oriented
	modeling;aspect-oriented programming;domain-specific model;generic
	modeling environment;model driven architecture;model-integrated computing;real-time
	system modeling;source code interpretation;system development;C++
	language;digital simulation;object-oriented programming;program interpreters;real-time
	systems;software engineering;}
}

@INPROCEEDINGS{6068347,
  author = {Sosa, Josune De and Diaz, Oscar and Trujillo, Salvador},
  title = {Defining DSL Expressions Collaboratively in Multidisciplinary Embedded
	Engineering},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2011 37th
	EUROMICRO Conference on},
  year = {2011},
  pages = {217 -220},
  month = {30 2011-sept. 2},
  abstract = {To a larger extent than in other software applications, embedded systems
	commonly require the participation of a mixture of engineers that
	collaboratively produce a piece of software. This makes this area
	particularly prone to Domain Specific Languages (DSLs). By raising
	the abstraction level, DSLs facilitate the understanding of a DSL
	specification by engineers with different backgrounds. By being domain-specific,
	DSLs makes possible the separation of concerns that are not possible
	to separate at code level, and this in turn, facilitates the collaborative
	specification of DSL expressions. However, "these DSL views" are
	rarely orthogonal, and dependencies commonly exist among them. In
	some cases, task serialization along those dependencies might be
	a solution but at the cost of reducing task parallelization. Rather,
	this paper introduces "an assertive approach": all DSL view developments
	are launched from the start, and engineers can request from their
	mates, who are working on a different view, to prioritize some tasks
	so that they can continue. Realizing this vision implies: (1) explicitly
	stating DSL dependencies and (2), the existence of view-aware editors
	that interpret such dependencies during the collaborative specification
	of DSL expressions. This approach is borne out by MUVIE, a view-aware
	DSL editor implemented on top of GMF.},
  doi = {10.1109/SEAA.2011.41}
}

@ARTICLE{5420789,
  author = {Spinellis, Diomidis},
  title = {Software Tracks},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {10 -11},
  number = {2},
  month = {march-april },
  abstract = {Railroad tracks offer guidance and support. There are various tools
	that can give our software the same handling. The main tool for guiding
	the code's direction is the language's type system. For values, the
	type system can help us by establishing a separate type for each
	distinct class; for code, interfaces and abstract classes ensure
	that we won't forget some crucial methods when we add functionality
	through a new class. With domain-specific languages or even suitably
	initialized data structures we can efficiently express exactly what
	the designer intended and nothing more. At a higher level, architectures
	that enforce a particular open-ended but well-defined interface will
	also guide a software's progress. Finally, the most flexible track-laying
	approach is a tool-supported software development process.},
  doi = {10.1109/MS.2010.56},
  issn = {0740-7459}
}

@ARTICLE{4814952,
  author = {Spinellis, Diomidis},
  title = {Drawing Tools},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {12 -13},
  number = {3},
  month = {may-june },
  abstract = {A drawing is often the best way to describe a large, complex artifact.
	In software development, you can easily derive pictures from code
	through tools that automate diagram creation. With the Graphviz tools
	you can draw directed and undirected relations between elements using
	a simple declarative language. Pic features a procedural drawing
	language that lets you define your own domain-specific drawing language.
	Gnuplot can plot data and functions in a wide variety of 2D and 3D
	styles. Finally, you can plot geographical data through the Generic
	Mapping Tools (GMT) or by generating KML files. You can obtain additional
	leverage by having one graphics tool or script generate output for
	another and by using the appropriate output format.},
  doi = {10.1109/MS.2009.63},
  issn = {0740-7459}
}

@ARTICLE{4420074,
  author = {Spinellis, D.},
  title = {Rational Metaprogramming},
  journal = {Software, IEEE},
  year = {2008},
  volume = {25},
  pages = {78 -79},
  number = {1},
  month = {jan.-feb. },
  abstract = {Metaprogramming, using programs to manipulate other programs, is as
	old as programming. From self-modifying machine code in early computers
	to expressions involving partially applied functions in modern functional-programming
	languages, metaprogramming is an essential part of an advanced programmer's
	arsenal. Everyday metaprogramming involves on-the-fly code production.
	Representative examples include dynamically generated SQL statements
	and code created for evaluation at runtime in interpreted languages.
	Metaprogramming also occurs in programs that spew out HTML or XML.
	Although we can't classify these markup languages as code, their
	rich syntactic structure qualifies their generation as metaprogramming.
	Unfortunately, we commonly produce code on the fly by simply pasting
	together character strings. This means that it's difficult to verify
	essential properties of the generated code - such as validity, correctness,
	and safety - at compile time.},
  doi = {10.1109/MS.2008.15},
  issn = {0740-7459},
  keywords = {HTML;SQL statements;XML;code generation;functional-programming languages;program
	verification;rational metaprogramming;self-modifying machine code;SQL;object-oriented
	programming;program compilers;program verification;software tools;}
}

@ARTICLE{4267595,
  author = {Spinellis, D.},
  title = {The Tools We Use},
  journal = {Software, IEEE},
  year = {2007},
  volume = {24},
  pages = {20 -21},
  number = {4},
  month = {july-aug. },
  abstract = {What's the state of the art in the tools we use to build software?
	To answer this question, I let a powerful server build from source
	code about 7,000 open source packages over a period of a month. The
	packages I built form a subset of the FreeBSD operating system ports
	collection, comprising a wide spectrum of application domains: from
	desktop utilities and biology applications to databases and development
	tools. The collection is representative of modern software because,
	unlike say a random sample of SourceForge.net projects, FreeBSD developers
	have found these programs useful enough to port to FreeBSD.},
  doi = {10.1109/MS.2007.121},
  issn = {0740-7459},
  keywords = {FreeBSD operating system ports collection;SourceForge.net projects;open
	source packages;software tools;operating systems (computers);public
	domain software;software tools;}
}

@ARTICLE{1657941,
  author = {Spinellis, D.},
  title = {Choosing a programming language},
  journal = {Software, IEEE},
  year = {2006},
  volume = {23},
  pages = {62 -63},
  number = {4},
  month = {july-aug. },
  abstract = {This paper evaluates the use of a functional language for implementing
	domain-specific functionality. The factors we consider when choosing
	a programming language are programmer productivity, maintainability,
	efficiency, portability, tool support, and software and hardware
	interfaces. The choice of programming language is a fine balancing
	act. Modern object-oriented languages such as Java and C# are more
	orthogonal and hide fewer surprises for the programmer, although
	the inevitable accumulation of features makes this statement less
	true with every new version of each language},
  doi = {10.1109/MS.2006.97},
  issn = {0740-7459},
  keywords = {domain-specific functionality;functional language;hardware interface;programmer
	productivity;programming language;software interface;object-oriented
	languages;specification languages;}
}

@INPROCEEDINGS{1194795,
  author = {Sprinkle, J. and Agrawal, A. and Levendovszky, T. and Feng Shi and
	Karsai, G.},
  title = {Domain model translation using graph transformations},
  booktitle = {Engineering of Computer-Based Systems, 2003. Proceedings. 10th IEEE
	International Conference and Workshop on the},
  year = {2003},
  pages = { 159 - 168},
  month = {april},
  abstract = { The implementation of computer based systems (CBSs) is commonly guided
	by constraints imposed by the particular domain of the CBS. Domain-specific
	programming is a convenient way to provide a domain expert with a
	language that is customized to the particular constraints and assumptions
	of the domain.. The careful thought and design that precede the development
	of any domain-specific visual language restrict the programmer from
	illegal formalisms, and allow for the rapid determination of the
	validity of the "program". Usually, the domain-specific visual language
	is designed and produced using a metamodel of some sort. Occasionally,
	similar domains can benefit from models created according to the
	ontology of this original metamodel, but usually some amount of model-transformation
	is required to give validity of the transformed models. This paper
	presents a visual language for transforming domain-models that can
	express the mapping between the meta-models of the "input" (i.e.
	the "old" domain) and the "output" (i.e. the "new" domain), and uses
	graph-rewriting techniques to transform the "old" domain-models into
	the appropriate "new" form.},
  doi = {10.1109/ECBS.2003.1194795},
  keywords = { computer based systems; domain model translation; domain-specific
	programming; domain-specific visual language; graph transformations;
	graph-rewriting techniques; metamodel; model transformation; ontology;
	formal specification; graph grammars; rewriting systems; visual languages;}
}

@ARTICLE{5076453,
  author = {Sprinkle, J. and Mernik, M. and Tolvanen, J.-P. and Spinellis, D.},
  title = {Guest Editors' Introduction: What Kinds of Nails Need a Domain-Specific
	Hammer?},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {15 -18},
  number = {4},
  month = {july-aug. },
  abstract = {Domain-specific techniques, languages, tools, and models, such as
	Fortran and Cobol can easily be viewed as domain-specific languages
	for scientific and business computing, respectively. Their domain
	is just very wide. What has changed is the technology for creating
	domain-specific languages (DSLs). Now it is easier to define languages
	and get tool support for narrower domains. Such focus offers increased
	abstraction, making development faster and easier. In domain-specific
	approaches, developers construct solutions from concepts representing
	things in the problem domain, not concepts of a given general-purpose
	programming language. Ideally, a DSL follows the domain abstractions
	and semantics as closely as possible, letting developers perceive
	themselves as working directly with domain concepts. The created
	specifications might then represent simultaneously the design, implementation,
	and documentation of the system, which can be generated directly
	from them. The mapping from the high-level domain concepts to implementation
	is possible because of the domain specificity: the language and code
	generators fit the requirements of a narrowly defined domain.},
  doi = {10.1109/MS.2009.92},
  issn = {0740-7459},
  keywords = {domain abstraction;domain semantics;domain specific language and modelling;software
	tools;specification languages;}
}

@INPROCEEDINGS{839396,
  author = {Srinivasan, J. and Murthy, R. and Sundara, S. and Agarwal, N. and
	DeFazio, S.},
  title = {Extensible indexing: a framework for integrating domain-specific
	indexing schemes into Oracle8i},
  booktitle = {Data Engineering, 2000. Proceedings. 16th International Conference
	on},
  year = {2000},
  pages = {91 -100},
  abstract = {Extensible indexing is a SQL-based framework that allows users to
	define domain-specific indexing schemes, and integrate them into
	the Oracle8i server. Users register a new indexing scheme, the set
	of related operators, and additional properties through SQL data
	definition language extensions. The implementation for an indexing
	scheme is provided as a set of Oracle Data Cartridge Interface (ODCIIndex)
	routines for index-definition, index-maintenance, and index-scan
	operations. An index created using the new indexing scheme, referred
	to as domain index, behaves and performs analogous to those built
	natively by the database system. The Oracle8i server implicitly invokes
	user-supplied index implementation code when domain index operations
	are performed, and executes user-supplied index scan routines for
	efficient evaluation of domain-specific operators. This paper provides
	an overview of the framework and describes the steps needed to implement
	an indexing scheme. The paper also presents a case study of Oracle
	Cartridges (intermedia text, spatial, and visual information retrieval),
	and Daylight (Chemical compound searching) Cartridge, which have
	implemented new indexing schemes using this framework and discusses
	the benefits and limitations},
  doi = {10.1109/ICDE.2000.839396},
  keywords = {Daylight Cartridge;Oracle Cartridges;Oracle Data Cartridge Interface
	routines;Oracle8i server;SQL data definition language extension;SQL-based
	framework;domain index;domain-specific indexing schemes;extensible
	indexing;index-definition operations;index-maintenance operations;index-scan
	operations;operators;user-supplied index implementation code;user-supplied
	index scan routines;SQL;database indexing;query processing;relational
	databases;}
}

@INPROCEEDINGS{853839,
  author = {Srinivasan, S. and Ponceleon, D. and Petkovic, D. and Viswanathan,
	M.},
  title = {Query expansion for imperfect speech: applications in distributed
	learning},
  booktitle = {Content-based Access of Image and Video Libraries, 2000. Proceedings.
	IEEE Workshop on},
  year = {2000},
  pages = {50 -54},
  abstract = {Advances in speech recognition technology have shown encouraging results
	for spoken document retrieval where the average precision often approaches
	70% of that achieved for perfect text transcriptions. Typical applications
	of spoken document retrieval pertain to retrieval of stories from
	archived video/audio assets. In the CueVideo project, our application
	focus is spoken document retrieval from a video database for just-in-time
	training/distributed learning. Typical content is not pre-segmented,
	has no predefined structure, is of varying audio quality, and may
	not have domain specific data available. For such content, we propose
	a two level search, namely, a first level search across the entire
	video collection, and a second level search within a specific video.
	At both search levels, we perform an experimental evaluation of a
	combination of new and existing query expansion methods, intended
	to offset retrieval errors due to misrecognition},
  doi = {10.1109/IVL.2000.853839},
  keywords = {CueVideo project;distributed learning;imperfect speech;just-in-time
	training;misrecognition;query expansion;retrieval errors;search levels;speech
	recognition;spoken document retrieval;stories;text transcriptions;two
	level search;video database;distance learning;information retrieval;natural
	languages;speech recognition;video databases;}
}

@INPROCEEDINGS{1342516,
  author = {Steen, M.W.A. and Akehurst, D.H. and ter Doest, H.W.L. and Lankhorst,
	M.M.},
  title = {Supporting viewpoint-oriented enterprise architecture},
  booktitle = {Enterprise Distributed Object Computing Conference, 2004. EDOC 2004.
	Proceedings. Eighth IEEE International},
  year = {2004},
  pages = { 201 - 211},
  month = {sept.},
  abstract = { Increasingly, organisations establish what is called an enterprise
	architecture. The enterprise architecture combines and relates all
	architectures describing some aspect of the organization, such as
	the business process architecture, the information architecture,
	and the application architecture. It is a blueprint of the organisation,
	which serves as a starting point for analysis, design and decision
	making. Viewpoints define abstractions on the set of models representing
	the enterprise architecture, each aimed at a particular type of stakeholder
	and addressing a particular set of concerns. The use of viewpoints
	is widely advocated for managing the inherent complexity in enterprise
	architecture. Viewpoints can both be used to view certain aspects
	in isolation, and for relating two or more aspects. However, in order
	to make such a viewpoint-oriented approach practically feasible,
	architects require a tool environment, which supports the definition,
	generation, editing and management of architectural views. Moreover,
	such an environment should work in concert with existing domain-specific
	modelling tools. We present the design of such a tool environment
	for viewpoint-oriented enterprise architecture.},
  doi = {10.1109/EDOC.2004.1342516},
  issn = {1541-7719 },
  keywords = { business process architecture; decision making; information architecture;
	organisational structures; viewpoint-oriented enterprise architecture;
	Unified Modeling Language; corporate modelling; decision making;
	distributed processing; formal specification; organisational aspects;}
}

@INPROCEEDINGS{5598074,
  author = {Stefanescu, A. and Wieczorek, S. and Wendland, M.-F.},
  title = {Using the UML Testing Profile for Enterprise Service Choreographies},
  booktitle = {Software Engineering and Advanced Applications (SEAA), 2010 36th
	EUROMICRO Conference on},
  year = {2010},
  pages = {12 -19},
  month = {sept.},
  abstract = {In this paper we present an approach of using model-driven technologies
	for testing of service component interactions. We report on an industrial
	experiment with a novel combination of existing UML standards, i.e.,
	the UML Testing Profile (U2TP), in conjunction with proprietary domain
	specific languages (DSLs). Many model-based testing (MBT) approaches
	use the UML 2 standard, but very few of them use also U2TP. Moreover,
	in practice UML coexists with DSLs which makes the overall integration
	not easy. We present our experiences and challenges of a U2TP-enabled
	MBT approach for a DSL for enterprise service choreographies, which
	describe the communication protocols between service components.
	The proposed workflow directly translates choreographies into UML
	models augmented with U2TP stereotypes, which are further loaded
	into our FOKUS!MBT tool chain. The tool provides an implementation
	of the U2TP standalone meta-model along with test case and test data
	generators to describe a holistic test process within one dedicated
	meta-model for testing concerns.},
  doi = {10.1109/SEAA.2010.44},
  issn = {1089-6503},
  keywords = {FOKUS!MBT tool chain;UML 2 standard;UML testing profile;communication
	protocol;dedicated meta-model;domain specific languages;enterprise
	service choreography;holistic test process;model-based testing approach;model-driven
	technology;service component interaction testing;test data generators;Unified
	Modeling Language;object-oriented programming;program testing;protocols;}
}

@INPROCEEDINGS{4402764,
  author = {Stephenson, Z. and McDermid, J.},
  title = {Using Model Checking to Validate Style-Specific Architectural Refactoring
	Patterns},
  booktitle = {Software Engineering Workshop, 2007. SEW 2007. 31st IEEE},
  year = {2007},
  pages = {53 -62},
  month = {6 2007-feb. 8},
  abstract = {When developing a new domain-specific architectural style, there can
	be uncertainty about the feasibility of using that style. In particular,
	the HADES architectural style contains refactoring patterns intended
	to remove undesirable scheduling features such as deadlock and livelock,
	but these patterns have not yet been fully validated. We report on
	the translation between the HADES structure and the input languages
	for two popular model checkers (SPIN and NuSMV) to help validate
	these patterns. We found model checking to be a valuable asset in
	confirming the presence of undesirable features.},
  doi = {10.1109/SEW.2007.36},
  issn = {1550-6215},
  keywords = {HADES architectural style;NuSMV;SPIN;model checking;style-specific
	architectural refactoring pattern;program verification;software architecture;}
}

@INPROCEEDINGS{1174804,
  author = {Stoffler, D. and Coon, S.I. and Huey, R. and Olson, A.J. and Sanner,
	M.F.},
  title = {Integrating biomolecular analysis and visual programming: flexibility
	and interactivity in the design of bioinformatics tools},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { One of the challenges in bio-computing is to enable the efficient
	use of a wide variety of rapidly evolving computational methods to
	simulate, analyze and understand complex interactions of molecular
	systems. Our laboratory is interested in the development of novel
	computational technologies and in the application of these technologies
	to the analysis and understanding of complex biological systems.
	We have been using the Python programming language as a platform
	to develop reusable and interoperable components dealing with different
	aspects of structural bioinformatics. These components are the basic
	building blocks from which several domain specific applications have
	been developed. In this paper we describe the integration of two
	applications developed in our laboratory: PMV and a visual-programming
	environment. PMV is a general purpose, command-driven molecular visualization
	and manipulation program built from reusable software components.
	The visual-programming environment enables a user to build interactively
	networks describing novel combinations of computational methods.
	We describe several applications demonstrating the synergy created
	by combining these two programs.},
  doi = {10.1109/HICSS.2003.1174804},
  issn = { },
  keywords = { Python programming language; bioinformatics tool design; biomolecular
	analysis; integrated software; visual programming; biology computing;
	data visualisation; integrated software; molecular biophysics; object-oriented
	programming; open systems; software reusability; visual programming;}
}

@INPROCEEDINGS{4637548,
  author = {Stone, A. and Strout, M. and Behere, S.},
  title = {Automatic Determination of May/Must Set Usage in Data-Flow Analysis},
  booktitle = {Source Code Analysis and Manipulation, 2008 Eighth IEEE International
	Working Conference on},
  year = {2008},
  pages = {153 -162},
  month = {sept.},
  abstract = {Data-flow analysis is a common technique to gather program information
	for use in transformations such as register allocation, dead-code
	elimination, common subexpression elimination, scheduling, and others.
	Tools for generating data-flow analysis implementations remove the
	need for implementers to explicitly write code that iterates over
	statements in a program, but still require them to implement details
	regarding the effects of aliasing, side effects, arrays, and user-defined
	structures. This paper presents the DFAGen Tool, which generates
	implementations for locally separable (e.g. bit-vector) data-flow
	analyses that are pointer, side-effect, and aggregate cognizant from
	an analysis specification that assumes only scalars. Analysis specifications
	are typically seven lines long and similar to those in standard compiler
	textbooks. The main contribution of this work is the automatic determination
	of may and must set usage within automatically generated data-flow
	analysis implementations.},
  doi = {10.1109/SCAM.2008.28},
  keywords = {DFAGen tool;common sub expression elimination;compiler textbooks;data-flow
	analysis;dead-code elimination;may set usage;must set usage;register
	allocation;data flow analysis;program compilers;software tools;}
}

@INPROCEEDINGS{5295261,
  author = {Storrle, H.},
  title = {VMQL: A generic visual model query language},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {199 -206},
  month = {sept.},
  abstract = {Shifting the focus from code to models in software development brings
	into view model-related tasks such as querying which are not very
	well supported by current CASE tools. Existing textual query languages
	like OCL are often not acceptable for domain modelers. Also, most
	query languages suffer from a mismatch between models, queries, and
	results. The visual model query language (VMQL) tries to overcome
	this by using a modeling language also as the query language and
	result presentation language.},
  doi = {10.1109/VLHCC.2009.5295261},
  issn = {1943-6092},
  keywords = {CASE tools;generic visual model query language;software development;textual
	query languages;query languages;software engineering;}
}

@INPROCEEDINGS{4283831,
  author = {Strahonja, V.},
  title = {The Evaluation Criteria of Workflow Metamodels},
  booktitle = {Information Technology Interfaces, 2007. ITI 2007. 29th International
	Conference on},
  year = {2007},
  pages = {553 -558},
  month = {june},
  abstract = {This paper defines the evaluation framework of workflow metamodels,
	based on a list of evaluation criteria. The presented evaluation
	criteria combine domain specific evaluation approach and some of
	the existing quality metrics, defined in the field of software. Well
	established evaluation criteria of workflow metamodels should enable
	their comparison, selection and proper use of methods and tool built
	upon them. The final result is a list of nine categories of evaluation
	criteria, decomposed into more than forty subcategories, but the
	list of subcategories is extendable. The practical validation of
	proposed evaluation highlighted some topics for future research.},
  doi = {10.1109/ITI.2007.4283831},
  issn = {1330-1012},
  keywords = {quality metrics;workflow metamodel;software metrics;workflow management
	software;}
}

@INPROCEEDINGS{5626417,
  author = {Strasser, T. and Peters, T. and Jagle, H. and Zrenner, E. and Wilke,
	R.},
  title = {An integrated domain specific language for post-processing and visualizing
	electrophysiological signals in Java},
  booktitle = {Engineering in Medicine and Biology Society (EMBC), 2010 Annual International
	Conference of the IEEE},
  year = {2010},
  pages = {4687 -4690},
  month = {31 2010-sept. 4},
  abstract = {Electrophysiology of vision - especially the electroretinogram (ERG)
	- is used as a non-invasive way for functional testing of the visual
	system. The ERG is a combined electrical response generated by neural
	and non-neuronal cells in the retina in response to light stimulation.
	This response can be recorded and used for diagnosis of numerous
	disorders. For both clinical practice and clinical trials it is important
	to process those signals in an accurate and fast way and to provide
	the results as structured, consistent reports. Therefore, we developed
	a freely available and open-source framework in Java (http://www.eye.uni-tuebingen.de/project/idsI4sigproc).
	The framework is focused on an easy integration with existing applications.
	By leveraging well-established software patterns like pipes-and-filters
	and fluent interfaces as well as by designing the application programming
	interfaces (API) as an integrated domain specific language (DSL)
	the overall framework provides a smooth learning curve. Additionally,
	it already contains several processing methods and visualization
	features and can be extended easily by implementing the provided
	interfaces. In this way, not only can new processing methods be added
	but the framework can also be adopted for other areas of signal processing.
	This article describes in detail the structure and implementation
	of the framework and demonstrate its application through the software
	package used in clinical practice and clinical trials at the University
	Eye Hospital Tuebingen one of the largest departments in the field
	of visual electrophysiology in Europe.},
  doi = {10.1109/IEMBS.2010.5626417},
  issn = {1557-170X},
  keywords = {ERG;Java;University Eye Hospital Tuebingen;application programming
	interfaces;electrophysiological signals;electroretinogram;fluent
	interfaces;integrated domain specific language;light stimulation;neural
	cells;pipes-and-filters;post-processing;smooth learning curve;vision;visual
	electrophysiology;visualization features;Java;application program
	interfaces;data visualisation;electroretinography;medical signal
	processing;neurophysiology;vision;}
}

@INPROCEEDINGS{5207863,
  author = {Sturmer, G. and Mangler, J. and Schikuta, E.},
  title = {A Domain Specific Language and Workflow Execution Engine to Enable
	Dynamic Workflows},
  booktitle = {Parallel and Distributed Processing with Applications, 2009 IEEE
	International Symposium on},
  year = {2009},
  pages = {653 -658},
  month = {aug.},
  abstract = {Workflow engines often being based on WS-BPEL, currently rely on a
	mix of recovery/modification strategies that are either part of the
	workflow description, part of the workflow engine, or realized as
	plugins to the workflow engine. To foster the development of distributed
	cloud-based workflow engines and novel repair algorithms, workflow
	engines have to be modularized in order to overcome the static and
	inflexible APIs provided by these workflow engines. Dynamic features
	gained by a modularization include the creation of external modules
	to monitor as well as modify a workflow to provide error handling
	in conjunction with service level agreement (SLA) constraints. The
	aim of this paper is to present a flexible workflow execution engine
	to facilitate the development of a new dynamic infrastructure to
	realize dynamic workflow engines with a focus on cloud-based environments.},
  doi = {10.1109/ISPA.2009.106},
  keywords = {API;WS-BPEL;application program interfaces;cloud-based environments;domain
	specific language;dynamic infrastructure;dynamic workflows;recovery/modification
	strategies;service level agreement;service oriented architecture;workflow
	execution engine;application program interfaces;software architecture;workflow
	management software;}
}

@INPROCEEDINGS{1265639,
  author = {Subramonian, V. and Gill, C.},
  title = {A generative programming framework for adaptive middleware},
  booktitle = {System Sciences, 2004. Proceedings of the 37th Annual Hawaii International
	Conference on},
  year = {2004},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { Component middleware technologies such as the CORBA component model
	(CCM), J2EE (Alur et al., 2001), and .NET, were developed to address
	many limitations like interdependencies between services and object
	interfaces, limited re-use, of first-generation middleware technologies
	such as CORBA 2.x, XML, and SOAP (Snell and McLeod, 2001). These
	component technologies have addressed a wide range of application
	domains, but unfortunately for distributed real-time and embedded
	(DRE) systems, the focus of these technologies has been primarily
	on functional and not quality of service (QoS) properties. Research
	on QoS-aware component models such as the CIAO project (Wang et al.
	2003) shows that there is a fundamental difference between configuration
	of functional and QoS properties even within such a unified component
	model: the dominant decomposition of functional properties is essentially
	object-oriented, while the dominant decomposition of QoS properties
	is essentially aspect-oriented. In this paper, we describe how a
	focus on aspect frameworks for configuring QoS properties both complements
	and extends QoS-aware component models. This paper makes three main
	contributions to the state of the art in DRE systems middleware.
	First, it describes a simple but representative problem for configuring
	QoS aspects that cut across architectural layers, system and distribution
	boundaries, which motivates our focus on aspect frameworks. Second,
	it provides a formalization of that problem using first order logic
	nfrastructure configuration logic - which both guides the design
	of aspect configuration infrastructure, and offers a way to connect
	these techniques with model-integrated computing (Ledeczi et al.
	2001) approaches to further reduce the programming burden on DRE
	system developers. Third, it describes alternative mechanisms to
	ensure correct configuration of the aspects involved, and notes the
	phases of the DRE system lifecycle at which each such configuration
	mechanism is most appropriate.},
  doi = {10.1109/HICSS.2004.1265639},
  keywords = { DRE system middleware; QoS-aware component model; adaptive middleware;
	aspect configuration infrastructure; first order logic; generative
	programming; infrastructure configuration logic; model-integrated
	computing; reflective middleware; system aspects; configuration management;
	formal logic; middleware; object-oriented programming; quality of
	service;}
}

@INPROCEEDINGS{933659,
  author = {Succi, G. and Pedrycz, W. and Yip, J. and Kaytazov, I.},
  title = {Intelligent design of product lines in Holmes},
  booktitle = {Electrical and Computer Engineering, 2001. Canadian Conference on},
  year = {2001},
  volume = {1},
  pages = {75 -80 vol.1},
  abstract = {Software product lines are a promising approach to develop multiple
	products by providing reduction in rework and a systematic way to
	exploit the synergistic relationships between products. Many software
	product line efforts risk failure by ignoring non-reuse aspects of
	software product lines. One should also consider how to increase
	the perceived value of each product in the line compared to competitors.
	Product characterization refers to the analysis of existing and potential
	products of a firm and their comparison with existing and potential
	products in the market place. The results of product characterization
	are essential for defining appropriate goals for a software product
	line. Holmes is a software product line support tool that addresses
	the full software product line life cycle by supporting all phases
	of the Sherlock method. This includes specific targeted and integrated
	support for product characterization},
  doi = {10.1109/CCECE.2001.933659},
  keywords = {Holmes;Sherlock method;intelligent design;life cycle;product lines;software
	product lines;product development;programming environments;software
	tools;}
}

@INPROCEEDINGS{5695619,
  author = {Sugiki, A. and Kato, K. and Ishii, Y. and Taniguchi, H. and Hirooka,
	N.},
  title = {Kumoi: A High-Level Scripting Environment for Collective Virtual
	Machines},
  booktitle = {Parallel and Distributed Systems (ICPADS), 2010 IEEE 16th International
	Conference on},
  year = {2010},
  pages = {322 -329},
  month = {dec.},
  abstract = {We have designed and implemented a scripting environment called "Kumoi"
	for managing collective VMs in a large-scale data center. Kumoi is
	unlike other scripting environments because it exploits strong typing
	with type inference and high-level description. Kumoi introduces
	several advancements, including treating virtual machines as first-class
	objects and decoupling the scripting model and its execution for
	hiding as many details as possible. We implemented Kumoi as an embedded
	domain-specific language based on Scala along with distributed agents
	running on each physical machine. Evaluation using example scripts
	showed that an administrator can more concisely write the instructions
	for performing complex VM lifecycle management tasks. Use of this
	environment should improve management efficiency and agility.},
  doi = {10.1109/ICPADS.2010.71},
  issn = {1521-9097},
  keywords = {Kumoi;Scala;cloud computing;collective virtual machines;distributed
	agent;embedded domain-specific language;first-class objects;high-level
	description;high-level scripting environment;large-scale data center;scripting
	model decoupling;type inference;virtual machine lifecycle management
	task;computer centres;distributed processing;embedded systems;reasoning
	about programs;software agents;type theory;virtual machines;}
}

@INPROCEEDINGS{5362278,
  author = {Yueheng Sun and Weijie Ni and Rui Men},
  title = {An Automatic Approach for Domain-Specific Dictionary Expansion Based
	on Web Mining},
  booktitle = {Knowledge Acquisition and Modeling, 2009. KAM '09. Second International
	Symposium on},
  year = {2009},
  volume = {2},
  pages = {96 -99},
  month = {30 2009-dec. 1},
  abstract = {This paper proposes an automatic expansion approach for an existing
	domain-specific dictionary based on Web mining. Using the terminology
	pairs in a dictionary as queries, we first extract snippet fragments
	that potentially contain the Chinese translations of current English
	phrases. Based on matching patterns extracted from the Web, we can
	get the most likely translation for a new English phrase. Finally
	we use the vector space model to filter out the translation equivalents
	belong to our expected domain, and a domain-specific dictionary expanded
	by these new terminology pairs is thereby built. The performance
	of our approach is verified on a dictionary of finance and accounting,
	and a precision between 85-90% is achieved on considering different
	thresholds.},
  doi = {10.1109/KAM.2009.54},
  keywords = {Chinese translations;English phrases;Web mining;automatic expansion
	approach;domain-specific dictionary;domain-specific dictionary expansion;pattern
	matching;queries;snippet fragment extraction;vector space model;Internet;data
	mining;dictionaries;language translation;natural language processing;query
	processing;}
}

@INPROCEEDINGS{5693213,
  author = {Sureka, A. and Jalote, P.},
  title = {Detecting Duplicate Bug Report Using Character N-Gram-Based Features},
  booktitle = {Software Engineering Conference (APSEC), 2010 17th Asia Pacific},
  year = {2010},
  pages = {366 -374},
  month = {30 2010-dec. 3},
  abstract = {We present an approach to identify duplicate bug reports expressed
	in free-form text. Duplicate reports needs to be identified to avoid
	a situation where duplicate reports get assigned to multiple developers.
	Also, duplicate reports can contain complementary information which
	can be useful for bug fixing. Automatic identification of duplicate
	reports (from thousands of existing reports in a bug repository)
	can increase the productivity of a Triager by reducing the amount
	of time a Triager spends in searching for duplicate bug reports of
	any incoming report. The proposed method uses character N-gram-based
	model for the task of duplicate bug report detection. Previous approaches
	are word-based whereas this study investigates the usefulness of
	low-level features based on characters which have certain inherent
	advantages (such as natural-language independence, robustness towards
	noisy data and effective handling of domain specific term variations)
	over word-based features for the problem of duplicate bug report
	detection. The proposed solution is evaluated on a publicly-available
	dataset consisting of more than 200 thousand bug reports from the
	open-source Eclipse project. The dataset consists of ground-truth
	(pre-annotated dataset having bug reports tagged as duplicate by
	the Triager). Empirical results and evaluation metrics quantifying
	retrieval performance indicate that the approach is effective.},
  doi = {10.1109/APSEC.2010.49},
  issn = {1530-1362},
  keywords = {Triager;automatic identification;character n-gram based features;duplicate
	bug report detection;evaluation metrics;free form text;ground truth;low-level
	features;open source Eclipse project;publicly available dataset;retrieval
	performance;information retrieval;program debugging;program testing;public
	domain software;software maintenance;text analysis;}
}

@INPROCEEDINGS{5501172,
  author = {Sushkov, N. and Zykov, S.},
  title = {Message system refactoring using DSL},
  booktitle = {Software Engineering Conference in Russia (CEE-SECR), 2009 5th Central
	and Eastern European},
  year = {2009},
  pages = {153 -158},
  month = {oct.},
  abstract = {This article covers the message delivery system refactoring using
	Domain Driven Development (DDD) and Domain Specific Language (DSL)
	approach. First it explains the main concepts of Domain Driven Development
	and Domain Specific Language. After that it describes the steps of
	development process based on Domain Specific Language including domain
	model design and development of DSL notation by the example of message
	delivery system. In conclusion it overviews key benefits provided
	by DDD and DSL approach (compare with previous version of message
	delivery system).},
  doi = {10.1109/CEE-SECR.2009.5501172},
  keywords = {DSL;domain driven development;domain model design;domain specific
	language;message delivery system refactoring;message passing;}
}

@INPROCEEDINGS{1530778,
  author = {Syeda-Mahmood, T. and Shah, G. and Akkiraju, R. and Ivan, A.-A. and
	Goodwin, R.},
  title = {Searching service repositories by combining semantic and ontological
	matching},
  booktitle = {Web Services, 2005. ICWS 2005. Proceedings. 2005 IEEE International
	Conference on},
  year = {2005},
  pages = { 13 - 20 vol.1},
  month = {july},
  abstract = { In this paper, we explore the use of domain-independent and domain-specific
	ontologies to find matching service descriptions. The domain-independent
	relationships are derived using an English thesaurus after tokenization
	and part-of-speech tagging. The domain-specific ontological similarity
	is derived by an inference on the semantic annotations associated
	with Web service descriptions. Matches due to the two cues are combined
	to determine an overall semantic similarity score. By combining multiple
	cues, we show that better relevancy results can be obtained for service
	matches from a large repository, than could be obtained using any
	one cue alone.},
  doi = {10.1109/ICWS.2005.102},
  keywords = { English thesaurus; Web service description; domain-independent ontology;
	domain-specific ontology; ontological matching; part-of-speech tagging;
	searching service repository; semantic annotation; semantic matching;
	tokenization; information retrieval; natural languages; ontologies
	(artificial intelligence); semantic Web; speech processing; thesauri;}
}

@INPROCEEDINGS{1409894,
  author = {Sztipanovits, J.},
  title = {Model integrated computing: foundations and applications},
  booktitle = {Engineering of Computer-Based Systems, 2005. ECBS '05. 12th IEEE
	International Conference and Workshops on the},
  year = {2005},
  pages = { xii},
  month = {april},
  abstract = { Summary form only given. The goal of this article is to describe
	our approach to model-based design, which is based on an integrated
	framework called model-integrated computing (MIC). MIC includes theoretical
	foundations for specifying the syntax and semantics of domain-specific
	modeling languages (DSML), and provides a meta-programmable tool
	suite for modeling, model transformation, code generation and tool
	integration. The approaches and tools discussed are used in a wide
	range projects focusing on different categories of computer-based
	systems.},
  doi = {10.1109/ECBS.2005.50},
  keywords = { DSML; MIC; code generation; computer-based systems; domain-specific
	modeling language; meta-programmable tool; model transformation;
	model-based design; model-integrated computing; formal specification;
	metacomputing; programming language semantics; software tools; specification
	languages;}
}

@INPROCEEDINGS{494548,
  author = {Sztipanovits, J. and Karsai, G. and Franke, H.},
  title = {Model-integrated program synthesis environment},
  booktitle = {Engineering of Computer-Based Systems,1996. Proceedings., IEEE Symposium
	and Workshop on},
  year = {1996},
  pages = {348 -355},
  month = {mar},
  abstract = {The paper describes a model-integrated program synthesis environment
	for computer-based system applications. In model-integrated program
	synthesis (MIPS), domain-specific, multiple-view models represent
	the software, its environment and their relationships. Model interpreters
	translate the models into the input languages of static and dynamic
	analysis tools, and application specific model interpreters synthesize
	software applications. The components of the system are built in
	the framework of the layered multigraph architecture, which separates
	the generic and domain/application specific components, and defines
	interfaces for expandability},
  doi = {10.1109/ECBS.1996.494548},
  keywords = {application specific components;application specific model interpreters;computer-based
	system applications;domain specific components;domain-specific multiple-view
	models;dynamic analysis tools;expandability interfaces;generic components;input
	languages;layered multigraph architecture;model interpreters;model
	translation;model-integrated program synthesis environment;software;software
	application synthesis;static analysis tools;program interpreters;programming
	environments;software tools;system monitoring;}
}

@INPROCEEDINGS{5172612,
  author = {Tagiew, R.},
  title = {Multi-Agent Petri-Games},
  booktitle = {Computational Intelligence for Modelling Control Automation, 2008
	International Conference on},
  year = {2008},
  pages = {130 -135},
  month = {dec.},
  abstract = {We introduce a language for the representation of a subset of strategic
	interactions. The representation is based on Petri nets. Representable
	games are restricted to have a finite number of states and actions.
	The language is additionally able to define time critical processes
	with discrete time periods. It can be used for both tasks of practical
	game computing: definition of game servers and calculating game theoretical
	or heuristic solutions. Theoretical analyzes, syntactical details,
	algorithms and concrete examples are given.},
  doi = {10.1109/CIMCA.2008.15},
  keywords = {Petri nets;discrete time periods;game computing;language;multi-agent
	Petri-games;representable games;strategic interactions;Petri nets;game
	theory;high level languages;multi-agent systems;}
}

@INPROCEEDINGS{4772953,
  author = {Taha, W.},
  title = {Plenary talk III Domain-specific languages},
  booktitle = {Computer Engineering Systems, 2008. ICCES 2008. International Conference
	on},
  year = {2008},
  pages = {xxiii -xxviii},
  month = {nov.},
  abstract = {Computer science is undergoing a revolution today, in which language
	designers are shifting attention from general purpose programming
	languages to so-called domain-specific languages (DSLs). General-purpose
	languages like Java, C#, C++, and C have long been the primary focus
	of language research. The idea was to create one language that would
	be better suited for programming than any other language. Ironically,
	we now have so many different general purpose languages that it is
	hard to imagine how this goal could be attained. Instead of aiming
	to be the best for solving any kind of computing problem, DSLs aim
	to be particularly good for solving a specific class of problems,
	and in doing so they are often much more accessible to the general
	public than traditional programming languages.},
  doi = {10.1109/ICCES.2008.4772953},
  keywords = {computer science;domain-specific language;general purpose programming
	language;programming languages;}
}

@INPROCEEDINGS{5069055,
  author = {Talby, D.},
  title = {The perceived value of authoring and automating acceptance tests
	using a model driven development toolset},
  booktitle = {Automation of Software Test, 2009. AST '09. ICSE Workshop on},
  year = {2009},
  pages = {154 -157},
  month = {may},
  abstract = {One approach to applying keyword driven testing in a model-driven
	development environment is by defining a domain specific language
	for test cases. The toolset then provides test editors, versioning,
	validation, reporting and hyperlinks across models - in addition
	to enabling automated test execution. This case study evaluates the
	effectiveness of such a solution as perceived by two teams of professional
	testers, who used it to test several products over a two year period.
	The results suggest that in addition to the expected benefits of
	automation, the solution reduces the time and effort required to
	write tests, maintain tests and plan the test authoring and execution
	efforts - at the expense of requiring longer training and a higher
	bar for recruiting testers.},
  doi = {10.1109/IWAST.2009.5069055},
  keywords = {acceptance tests;automated test execution;domain specific language;keyword
	driven testing;model driven development toolset;perceived value;test
	authoring;authoring systems;program testing;specification languages;}
}

@INPROCEEDINGS{5552681,
  author = {Talpin, J. and Ouy, J. and Gautier, T. and Besnard, L. and Cortier,
	A.},
  title = {Modular Interpretation of Heterogeneous Modeling Diagrams into Synchronous
	Equations Using Static Single Assignment},
  booktitle = {Application of Concurrency to System Design (ACSD), 2010 10th International
	Conference on},
  year = {2010},
  pages = {137 -146},
  month = {june},
  abstract = {The ANR project SPaCIFY develops a domain-specific programming environment,
	Synoptic, to engineer embedded software for space applications. Synoptic
	is an Eclipse-based modeling environment which supports all aspects
	of aerospace software design. As such, it is a domain-specific environment
	consisting of heterogeneous modeling and programming principles defined
	in collaboration with the industrial partners and end users of the
	project : imperative synchronous programs, data-flow diagrams, mode
	automata, blocks, components, scheduling, mapping and timing. This
	article focuses on the essence and distinctive features of its behavioral
	or programming aspects : actions, flows and automata, for which we
	use the code generation infrastructure of the synchronous modeling
	environment SME. It introduces an efficient method for transforming
	a hierarchy of blocks consisting of actions (sequential Esterel-like
	programs), data-flow diagrams (to connect and time modules) and mode
	automata (to schedule or mode blocks) into a set of synchronous equations.
	This transformation significantly reduces the needed control states
	and block synchronizations. It consists of an inductive static-single
	assignment transformation algorithm across a hierarchy of blocks
	that produces synchronous equations. The impact of this new transformation
	technique is twofold. With regards to code generation objectives,
	it reduces the needed resynchronization of each block in the system
	with respects to its parents, potentially gaining substantial performance
	from way less synchronizations. With regards to verification requirements,
	it also reduces the number of states across a hierarchy of automata
	and hence maximizes model checking performances.},
  doi = {10.1109/ACSD.2010.14},
  issn = {1550-4808},
  keywords = {Eclipse-based modeling environment;Synoptic;aerospace software design;behavioral
	aspect;code generation infrastructure;data-flow diagram;domain-specific
	environment;domain-specific programming environment;embedded software;heterogeneous
	modeling diagram;imperative synchronous program;inductive static-single
	assignment transformation;mapping;mode automata;mode block;model
	checking;programming aspect;programming principle;scheduling;sequential
	Esterel-like program;space application;synchronous equation;synchronous
	modeling environment;timing;verification requirement;aerospace computing;automata
	theory;data flow analysis;embedded systems;formal specification;formal
	verification;program compilers;programming environments;}
}

@INPROCEEDINGS{4839227,
  author = {Tambe, S. and Dabholkar, A. and Gokhale, A.},
  title = {CQML: Aspect-Oriented Modeling for Modularizing and Weaving QoS Concerns
	in Component-Based Systems},
  booktitle = {Engineering of Computer Based Systems, 2009. ECBS 2009. 16th Annual
	IEEE International Conference and Workshop on the},
  year = {2009},
  pages = {11 -20},
  month = {april},
  abstract = {Current domain-specific modeling (DSM) frameworks for designing component-based
	systems often consider the system's structural and behavioral concerns
	as the two dominant concerns of decomposition while treating nonfunctional
	or quality of service (QoS) concerns as an after thought. Such frameworks
	lack a strong decoupling between the modeling of the system's structural
	composition and their QoS requirements. This lack of QoS modularization
	limits (1) reusability of such frameworks, (2) ease of maintenance
	when new non-functional characteristics are added, and (3) independent
	evolution of the modeling frameworks along both the structural and
	non-functional dimensions. This paper describes component QoS modeling
	language (CQML), which is a reusable, extensible, and aspect-oriented
	modeling approach that provides strong separation between the structural
	and non-functional dimensions. CQML supports independent evolution
	of structural as well as QoS metamodel of composition modeling languages.
	The join point model of CQML enables declarative QoS aspect modeling
	and supports automatic weaving of structural changes effected by
	QoS requirements. We evaluate the capabilities of CQML for a variety
	of structural modeling languages and provide quantitative results
	indicating the modeling effort saved in automating the weaving of
	QoS concerns.},
  doi = {10.1109/ECBS.2009.24},
  keywords = {QoS;aspect-oriented modeling;component QoS modeling language;component-based
	systems;domain-specific modeling frameworks;quality of service;object-oriented
	programming;quality of service;software reusability;}
}

@INPROCEEDINGS{5231976,
  author = {Tambe, S. and Dabholkar, A. and Gokhale, A.},
  title = {Fault-Tolerance for Component-Based Systems - An Automated Middleware
	Specialization Approach},
  booktitle = {Object/Component/Service-Oriented Real-Time Distributed Computing,
	2009. ISORC '09. IEEE International Symposium on},
  year = {2009},
  pages = {47 -54},
  month = {march},
  abstract = {General-purpose middleware, by definition, cannot readily support
	domain-specific semantics without significant manual efforts in specializing
	the middleware. This paper presents GRAFT (GeneRative Aspects for
	Fault Tolerance), which is a model-driven, automated, and aspects-based
	approach for specializing general-purpose middleware with failure
	handling and recovery semantics imposed by a domain.Model-driven
	techniques are used to specify the special fault tolerance requirements,
	which are then transformed into middleware-level code artifacts using
	generative programming. Since the resulting fault tolerance semantics
	often crosscut the middleware architecture, GRAFT uses aspect-oriented
	programming to weave them into the original fabric of the general-purpose
	middleware. We evaluate the capabilities of GRAFT using a representative
	case study.},
  doi = {10.1109/ISORC.2009.50},
  issn = {1555-0885},
  keywords = {aspect-oriented programming;automated middleware specialization approach;component-based
	systems;domain-specific semantics;failure handling;fault tolerance
	requirements;fault tolerance semantics;general-purpose middleware;generative
	programming;middleware architecture;middleware-level code artifacts;model-driven
	techniques;recovery semantics;middleware;object-oriented programming;software
	fault tolerance;}
}

@INPROCEEDINGS{4814998,
  author = {Tambe, S. and Dabholkar, A. and Gokhale, A. and Kavimandan, A.},
  title = {Towards a QoS Modeling and Modularization Framework for Component-based
	Systems},
  booktitle = {Enterprise Distributed Object Computing Conference Workshops, 2008
	12th},
  year = {2008},
  pages = {43 -49},
  month = {sept.},
  abstract = {Current domain-specific modeling (DSM) frameworks for designing component-based
	systems provide modeling support for system's structural as well
	as non-functional or quality of service (QoS) concerns. However,
	the focus of such frameworks on system's non-functional concerns
	is an after-thought and their support is at best adhoc. Further,
	such frameworks lack strong decoupling between the modeling of the
	system's structural composition and their QoS requirements. This
	lack of QoS modularization limits (1) reusability of such frameworks,
	(2) ease of their maintenance when new non-functional characteristics
	are added, and (3) independent evolution of the modeling frameworks
	along both the structural and non-functional dimensions. This paper
	describes Component QoS modeling language (CQML), which is a reusable,
	extensible, and platform-independent QoS Modeling Language that provides
	strong separation between the structural and non-functional dimensions.
	CQML supports independent evolution of structural metamodel of composition
	modeling languages as well as QoS metamodel. To evaluate, we superimpose
	CQML on a purely structural modeling language and automatically generate,
	configure, and deploy componentbased fault-monitoring infrastructure
	using aspect-oriented modeling (AOM) techniques.},
  doi = {10.1109/EDOCW.2008.60},
  keywords = {component QoS modeling language;component-based system;domain-specific
	modeling;independent modeling framework evolution;modularization
	framework reusability;platform-independent QoS modeling language;quality
	of service;software maintenance;structural meta model;object-oriented
	programming;quality of service;software architecture;software maintenance;software
	prototyping;software reusability;specification languages;}
}

@INPROCEEDINGS{1467891,
  author = {Peiyi Tang},
  title = {Formal methods to generate parallel iterative codes for PDE-based
	applications},
  booktitle = {Engineering of Complex Computer Systems, 2005. ICECCS 2005. Proceedings.
	10th IEEE International Conference on},
  year = {2005},
  pages = { 106 - 115},
  month = {june},
  abstract = { Developing parallel software is far more complex than traditional
	sequential software. An effective approach to deal with the complexity
	of parallel software is domain-specific programming in an abstraction
	higher than general-purpose programming languages. In this paper,
	we focus on the domain of the applications based on partial differential
	equations (PDE) and provide a formal framework and methods for PDE
	compilers to generate parallel iterative codes for the domain. We
	also provide a PDE compiler optimization to minimize the number of
	messages between parallel processors. Our framework and methods can
	be used to build PDE compilers to generate efficient parallel software
	for PDE-based applications automatically.},
  doi = {10.1109/ICECCS.2005.46},
  keywords = { PDE compiler; PDE-based applications; abstraction; compiler optimization;
	domain-specific programming; formal method; message minimization;
	parallel iterative code generation; parallel processors; parallel
	software complexity; partial differential equations; programming
	languages; formal specification; formal verification; message passing;
	minimisation; optimising compilers; parallel programming; parallelising
	compilers; partial differential equations;}
}

@INPROCEEDINGS{5370324,
  author = {Xinhuai Tang and Xiangfeng Luo and Xueqiang Mi and Xiaozhou Yuan
	and Delai Chen},
  title = {DSL Route: An Efficient Integration Solution for Message Routing},
  booktitle = {Semantics, Knowledge and Grid, 2009. SKG 2009. Fifth International
	Conference on},
  year = {2009},
  pages = {436 -437},
  month = {oct.},
  abstract = {As the core of current enterprise integration solution, messaging
	systems provide important functionalities for reliable message delivery
	and complicated service routing. This paper introduces a domain specific
	language (DSL) route to improve current messaging solution. DSL route
	provides fluent and graceful route definition. With DSL routes, the
	integration solutions are more agile and configurable since enterprise
	integration patterns (EIPs) are naturally supported in DSL route
	model.},
  doi = {10.1109/SKG.2009.66},
  keywords = {DSL route;domain specific language;enterprise integration patterns;message
	delivery;message routing;messaging systems;business process re-engineering;message
	passing;specification languages;}
}

@INPROCEEDINGS{1548572,
  author = {Taylor, J.M. and Poliakov, D. and Mazlack, L.J.},
  title = {Domain-specific ontology merging for the semantic Web},
  booktitle = {Fuzzy Information Processing Society, 2005. NAFIPS 2005. Annual Meeting
	of the North American},
  year = {2005},
  pages = { 418 - 423},
  month = {june},
  abstract = { Natural language understanding is needed to intelligently handle
	the large volumes of information that are processed by computers.
	Most of the data handled by computers is text; the data exists in
	emails, on Web pages, and in databases. The data is semi-structured;
	it is not rigorously, unambiguously organized, and constrained. Ontologies
	may help with analyzing and understanding text. Ontologies provide
	a capability to represent objects, concepts and other entities and
	the relationships between them. Ontologies may be used as a tool
	for finding possible meanings of words in text, and meanings of text
	in general. We need to be able to merge ontologies from different,
	sometimes unrelated, sources. Ontologies may be inconsistent, incomplete
	or imprecise. We consider how to potentially merge two domain-specific
	ontologies. We assume that both ontologies contain concepts and instances,
	and both ontologies are large. We suggest to use the information
	available on the Web using search engines, as well as other methods,
	such as lexical, semantic and heuristics, to merge two ontologies.},
  doi = {10.1109/NAFIPS.2005.1548572},
  keywords = { World Wide Web; data handling; domain-specific ontology; natural
	language; search engine; semantic Web; semi-structured data; text
	understanding; natural languages; ontologies (artificial intelligence);
	semantic Web; text analysis;}
}

@INPROCEEDINGS{5945329,
  author = {Temate, S. and Broto, L. and Tchana, A. and Hagimont, D.},
  title = {A High Level Approach for Generating Model's Graphical Editors},
  booktitle = {Information Technology: New Generations (ITNG), 2011 Eighth International
	Conference on},
  year = {2011},
  pages = {743 -749},
  month = {april},
  abstract = {Domain Specific Languages (DSL) are increasingly used in software
	engineering and other domains. The result is an increasing need of
	appropriate DSL's tools, especially platform for building, editors
	and runtime associated with DSLs. Different experiences show that
	existent DSL tools are generally not user friendly enough, or simply
	unadapted for the generation of graphical DSL editors. In this paper
	we present a higher level and effortless framework for generating
	graphical DSL editors. This framework was designed and experienced
	in the context of an autonomic management system based on a component
	model.},
  doi = {10.1109/ITNG.2011.131},
  keywords = {DSL tools;autonomic management system;domain specific language;graphical
	DSL editor;model graphical editor;software engineering;user friendliness;computer
	graphics;software engineering;specification languages;ubiquitous
	computing;}
}

@INPROCEEDINGS{5572413,
  author = {Temprado-Battad, B. and Sarasa-Cabezuelo, A. and Sierra, J.L.},
  title = {Managing the Production and Evolution of e-learning Tools with Attribute
	Grammars},
  booktitle = {Advanced Learning Technologies (ICALT), 2010 IEEE 10th International
	Conference on},
  year = {2010},
  pages = {427 -431},
  month = {july},
  abstract = {Many e-learning tools are based on domain-specific languages (DSLs)
	targeted to the educational domain. Thus, methods and techniques
	from the programming language community can help in developing these
	tools. In this paper, we show how attribute grammars, a well-known
	declarative specification method for the syntax and semantics of
	programming languages, can facilitate the production and subsequent
	evolution of e-learning tools. We also describe how we produced and
	extended, a courseware system supporting an XML-based DSL, by using
	XLOP (XML Language-Oriented Processing), a meta-tool supporting attribute
	grammars for the development of XML processing applications.},
  doi = {10.1109/ICALT.2010.124},
  keywords = {XLOP;XML processing;XML-based DSL;attribute grammars;courseware system;declarative
	specification method;domain-specific languages;e-learning tools;educational
	domain;language-oriented processing;production management;programming
	language community semantic;XML;attribute grammars;courseware;educational
	aids;intelligent tutoring systems;programming language semantics;}
}

@INPROCEEDINGS{4906779,
  author = {Zhi Teng and Ye Liu and Fuji Ren},
  title = {Notice of Violation of IEEE Publication Principles A foundation for
	knowledge system with application in information retrieval and knowledge
	acquisition},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2008. NLP-KE
	'08. International Conference on},
  year = {2008},
  pages = {1 -7},
  month = {oct.},
  abstract = {Notice of Violation of IEEE Publication Principles
	
	"A Foundation for Knowledge System with Application in Information
	Retrieval and Knowledge Acquisition," by Zhi Teng, Ye Liu, Fuji Ren,
	in the Proceedings of the International Conference on Natural Language
	Processing and Knowledge Engineering, 2008. NLP-KE '08, Oct. 2008
	
	After careful and considered review of the content and authorship
	of this paper by a duly constituted expert committee, this paper
	has been found to be in violation of IEEE's Publication Principles.
	
	This paper contains significant portions of original text from the
	papers cited below. The original text was copied with insufficient
	attribution.
	
	"Beyond SumBasic: Task-focused Summarization with Sentence Simplification
	and Lexical Expansion" by L. Vanderwende, H. Suzuki, C. Brockett,
	A. Nenkova in Information Processing and Management, Vol 43, Elsevier,
	2007, pp 1606-1618
	
	"The Use of Domain-specific Concepts in Biomedical Text Summarization"
	by L. Reeve, H. Han, A. Brooks, in Information Processing and Management,
	Vol 43, Elsevier 2007, pp. 1765-1776
	
	"Sentences Optimum Selection for Multi-document Summarization" by
	B. Qin, T. Liu, S. Chen, S. Li in the Journal of Computer Research
	and Development, Institute of Computer Technology, Dec 2007, pp.
	67-74
	
	"READING: A Self Sufficient Internet News System with Applications
	in Information and Knowledge Mining" by D. Bracewell, F. Ren, K.
	Hisazumi, Z. Teng, Y. Furose in the Proceedings of the International
	Conference on Natural Language Processing and Knowledge Engineering,
	2007. NLP-KE 2007, Aug. 2007, pp. 190-196
	
	"A Foundation for Japanese-English Cross-lingual Information Retrieval
	and Knowledge Acquisition from News" by D. Bracewell in his Ph.D
	Thesis, University of Tokushima, 2007
	
	In this paper we present a knowledge system, which is a self sufficient
	system for aggregating, extraction and classifying new- s from around
	the world. This paper present some algorithms for monitoring and
	gathering news from RSS and Web sites, creating special domain corpora.
	The system, however, goes beyond the traditional news aggregation
	sites by mining information and knowledge from the news articles
	to aid in such tasks as decision making. The system is able to collect,
	classify and mine knowledge automatically without the need for an
	editor or intervention by an administrator. As knowledge system,
	average users, news professionals and researchers alike will find
	that the system offers something for them. Our work shows that better
	performance is achieved and the result showed that this method has
	potential in this field.},
  doi = {10.1109/NLPKE.2008.4906779},
  keywords = {RSS;Web sites;aggregation sites;classification;decision making;information
	retrieval;knowledge acquisition;knowledge system;mining information;self
	sufficient system;special domain corpora;Web sites;classification;data
	mining;decision making;information retrieval;}
}

@INPROCEEDINGS{1336696,
  author = {Teranishi, K. and Raghavan, P. and Zi-Kui Liu},
  title = {Towards a Grid enabled system for multicomponent materials design},
  booktitle = {Cluster Computing and the Grid, 2004. CCGrid 2004. IEEE International
	Symposium on},
  year = {2004},
  pages = { 664 - 669},
  month = {april},
  abstract = { We are developing a portal for multicomponent materials design using
	Grid-enabled large-scale simulations. In this paper we report on
	our services based application architecture which allows integration
	of simulation software with Grid services to provide a Web-based
	computational laboratory for the modeling of Al-Cu-Mg-Si alloys.
	We examine user requirements and describe the design of our framework.
	Our architecture is implemented using existing middleware such as
	the Globus and Java CoG toolkits. An interesting feature of our design
	is the separation of the high-level specification of the materials
	modeling system from the implementation through the use of a markup
	language such as XML. We use markup languages with domain-specific
	extensions to specify (in architecture-independent form) rules and
	constraints that allow meaningful composition of simulation tasks
	and experimentally determined material properties. In addition, we
	use them to specify application code interfaces so that our simulation
	server can be dynamically reconfigured to include new software and
	constraints.},
  doi = {10.1109/CCGrid.2004.1336696},
  keywords = { Al-Cu-Mg-Si; Al-Cu-Mg-Si alloys; Globus; Grid enabled system; Grid
	services; Java CoG toolkit; Web-based computational laboratory; XML;
	application code interfaces; high-level specification; large-scale
	simulations; markup language; middleware; multicomponent materials
	design; portal; services based application architecture; simulation
	software; Web sites; XML; aluminium alloys; copper alloys; grid computing;
	magnesium alloys; materials properties; middleware; physics computing;
	portals; silicon alloys; simulation;}
}

@ARTICLE{5725239,
  author = {Terrel, A.R.},
  title = {From Equations to Code: Automated Scientific Computing},
  journal = {Computing in Science Engineering},
  year = {2011},
  volume = {13},
  pages = {78 -82},
  number = {2},
  month = {march-april },
  abstract = {Using domain-specific languages, scientific codes can let users work
	directly with equations and benefit from optimizations not available
	with general compilers.},
  doi = {10.1109/MCSE.2011.31},
  issn = {1521-9615},
  keywords = {automated scientific computing;domain-specific languages;optimizations;scientific
	codes;optimisation;programming languages;specification languages;}
}

@INPROCEEDINGS{5628615,
  author = {Thapa, V. and Eunjee Song and Hanil Kim},
  title = {An Approach to Verifying Security and Timing Properties in UML Models},
  booktitle = {Engineering of Complex Computer Systems (ICECCS), 2010 15th IEEE
	International Conference on},
  year = {2010},
  pages = {193 -202},
  month = {march},
  abstract = {In this paper, we present an approach to verify whether a UML design
	model satisfies its domain-specific security and time-related requirements
	in an integrated tool environment. This approach is based on a UML
	metamodel extension mechanism given as profiles. As a model verification
	tool, we chose the USE (UML-based Specification Environment) since
	additional functional and non-functional constraints in a UML model
	should be formally specified using the OCL (Object Constraint Language).
	In order to address both security and timing properties together
	in a model, we combine two profiles, UMLsec for security and MARTE
	(UML profile for Modeling and Analysis of Real-Time and Embedded
	systems) for time, into the UML metamodel. Then, this combined metamodel
	is converted to a form of USE specification so that it can be used
	for verifying models using USE. In this approach, however, this combined
	metamodel is considered as a large class model in USE because USE
	does not support profiles. Therefore, models to be verified are created
	as object models that are instances of the given class model, i.e.
	the extended metamodel in our case. Our approach is illustrated with
	a distributed, interoperable wireless communications-based railroad
	control system called the Positive Train Control (PTC) System.},
  doi = {10.1109/ICECCS.2010.10},
  keywords = {MARTE;OCL;UML models;UML-based specification environment;USE;distributed
	wireless communications;domain-specific security;interoperable wireless
	communications;modeling and analysis of real-time and embedded systems;object
	constraint language;positive train control system;railroad control
	system;security verification;timing properties;Unified Modeling Language;distributed
	processing;formal specification;formal verification;object-oriented
	languages;open systems;railway engineering;security of data;}
}

@INPROCEEDINGS{5556840,
  author = {Thein, N.L.L.},
  title = {Automation of ontology development from domain specific concept taxonomy},
  booktitle = {Computer and Communication Engineering (ICCCE), 2010 International
	Conference on},
  year = {2010},
  pages = {1 -5},
  month = {may},
  abstract = {Currently, much of the information on the Web is described using only
	natural language, which can be seen as a major obstacle in developing
	the Semantic Web. Since the annotations describing different resources
	are one of the key components of the Semantic Web, we need to create
	them easy to use and cost-effective ways. However, various systems
	for creating annotations have been developing; there seem to be a
	lack of systems that can be easily used by annotators unfamiliar
	with the technical side of the Semantic Web. In this paper, extraction
	possible concepts relating to a particular domain are stored in concept
	taxonomy and describe how agents can contribute to develop ontology
	automatically. We have developed an annotation system supporting
	distributed collaboration with concept ontology, and hiding the complexity
	of the annotation scheme. Given concept taxonomy for a particular
	application domain, software agent can represent and organize data
	pertinent to this domain more effectively than without any prior
	knowledge of the relationships between concepts. This system adapts
	flexibly to different metadata schemas, which makes it suitable for
	different applications. Support for using ontologies is based on
	agents and concept searching.},
  doi = {10.1109/ICCCE.2010.5556840},
  keywords = {annotation system;domain specific concept taxonomy;metadata schemas;ontology
	development;semantic Web;software agent;meta data;ontologies (artificial
	intelligence);semantic Web;software agents;}
}

@INPROCEEDINGS{740484,
  author = {Thibault, S. and Consel, C. and Muller, G.},
  title = {Safe and efficient active network programming},
  booktitle = {Reliable Distributed Systems, 1998. Proceedings. Seventeenth IEEE
	Symposium on},
  year = {1998},
  pages = {135 -143},
  month = {oct},
  abstract = {Active networks are aimed at incorporating programmability into the
	network to achieve extensibility. One approach to obtaining extensibility
	is to download router programs into network nodes. This programmability
	is critical to allow multipoint distributed systems to adapt to network
	conditions and individual clients' needs. Although promising, this
	approach raises critical issues such as safety to achieve reliability
	despite the complexity of a distributed system, security to protect
	shared resources, and efficiency to maximize usage of bandwidth.
	This paper proposes the use of a domain-specific language, PLAN-P,
	to address all of the above issues. To address safety and security,
	we give examples of properties of PLAN-P programs that can be automatically
	checked due to the use of a restricted language. For efficiency,
	we show that an automatically generated run-time compiler for PLAN-P
	produces code which outperforms an equivalent compiled Java program.
	Additionally, we present performance results on a real application
	(learning bridge) where we obtain 100% of the maximum possible throughput},
  doi = {10.1109/RELDIS.1998.740484},
  keywords = {active network programming;automatically generated run-time compiler;domain-specific
	language;equivalent compiled Java program;extensibility;learning
	bridge;multipoint distributed systems;network nodes;programmability;router
	programs;shared resources;client-server systems;distributed programming;}
}

@INPROCEEDINGS{776525,
  author = {Thibault, S. and Marant, J. and Muller, G.},
  title = {Adapting distributed applications using extensible networks},
  booktitle = {Distributed Computing Systems, 1999. Proceedings. 19th IEEE International
	Conference on},
  year = {1999},
  pages = {234 -243},
  abstract = {Active networks have been proposed to allow the dynamic extension
	of network behavior by downloading application-specific protocols
	(ASPs) into network routers. We demonstrate the feasibility of the
	use of ASPs in an active network for the adaptation of distributed
	software components. We have implemented three examples which show
	that ASPs can be used to easily extend distributed applications,
	and furthermore, that such adaptation can be safe, portable and efficient.
	Safety and efficiency is obtained by implementing the ASPs in PLAN-P,
	a domain-specific language and run-time system for active networking.
	The presented examples illustrate three different applications: audio
	broadcasting with bandwidth adaptation in routers; an extensible
	HTTP server with load-balancing facilities; and a multipoint MPEG
	server derived from a point-to-point server},
  doi = {10.1109/ICDCS.1999.776525},
  keywords = {HTTP server;PLAN-P;active networks;application-specific protocols;audio
	broadcasting;bandwidth adaptation;distributed applications;distributed
	software components;domain-specific language;extensible networks;load
	balancing;multipoint MPEG server;network routers;point-to-point server;run-time
	system;distributed processing;high level languages;protocols;resource
	allocation;telecommunication network routing;}
}

@ARTICLE{798325,
  author = {Thibault, S.A. and Marlet, R. and Consel, C.},
  title = {Domain-specific languages: from design to implementation application
	to video device drivers generation},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1999},
  volume = {25},
  pages = {363 -377},
  number = {3},
  month = {may/jun},
  abstract = {Domain-specific languages (DSL) have many potential advantages in
	terms of software engineering, ranging from increased productivity
	to the application of formal methods. Although they have been used
	in practice for decades, there has been little study of methodology
	or implementation tools for the DSL approach. We present our DSL
	approach and its application to a realistic domain: the generation
	of video display device drivers. The article focuses on the validation
	of our proposed framework for domain-specific languages, from design
	to implementation. The framework leads to a flexible design and structure,
	and provides automatic generation of efficient implementations of
	DSL programs. Additionally, we describe an example of a complete
	DSL for video display adaptors and the benefits of the DSL approach
	for this application. This demonstrates some of the generally claimed
	benefits of using DSLs: increased productivity, higher-level abstraction,
	and easier verification. This DSL has been fully implemented with
	our approach and is available. Compose project URL: http://www.irisa.fr/compose/gal},
  doi = {10.1109/32.798325},
  issn = {0098-5589},
  keywords = {DSL approach;automatic program generation;domain-specific languages;formal
	methods;higher-level abstraction;implementation application;software
	engineering;video device driver generation;video display adaptors;video
	display device drivers;application generators;device drivers;high
	level languages;video equipment;}
}

@ARTICLE{1293067,
  author = {Thomas, D.},
  title = {MDA: revenge of the modelers or UML utopia?},
  journal = {Software, IEEE},
  year = {2004},
  volume = {21},
  pages = { 15 - 17},
  number = {3},
  month = {may-june},
  abstract = { Modeling is at the core of many disciplines, but it is especially
	important in engineering because it facilitates communication and
	constructs complex things from smaller parts. Model engineering or
	model-driven development, treats software development as a set of
	transformations between successive models from requirements to analysis,
	to design, to implementation, to deployment. We discuss UML and MDA
	tools for developing software models. We also discuss domain specific
	languages, domain-oriented programming, platform-specific models
	and model engineering.},
  doi = {10.1109/MS.2004.1293067},
  issn = {0740-7459},
  keywords = { MDA tool; Model Driven Architecture; UML; domain specific language;
	domain-oriented programming; model engineering; model-driven development;
	platform-specific model; software development; software model; object-oriented
	methods; object-oriented programming; program compilers; software
	architecture; specification languages;}
}

@ARTICLE{1411763,
  author = {Thramboulidis, K.},
  title = {Model-integrated mechatronics - toward a new paradigm in the development
	of manufacturing systems},
  journal = {Industrial Informatics, IEEE Transactions on},
  year = {2005},
  volume = {1},
  pages = { 54 - 61},
  number = {1},
  month = {feb.},
  abstract = { The traditional approach for the development of manufacturing systems
	considers the constituent parts of the system, i.e., mechanical,
	electronic, and software, to be developed independently and then
	integrated to form the final system. This approach is being criticized
	as inappropriate for the complexity and the dynamics of today's systems.
	This paper proposes an architecture that promotes model integration
	not only for implementation space artifacts but also in artifacts
	of the early analysis and design phases of the development process.
	The proposed architecture, which promotes reuse and significantly
	decreases development and validation time, is at the heart of a new
	paradigm called model-integrated mechatronics (MIM). MIM applies
	domain-specific modeling languages for the concurrent engineering
	of mechanical, electronic and software components of mechatronic
	systems. It simplifies the integrated development process of manufacturing
	systems by using as basic construct the mechatronic component. The
	MIM paradigm was utilized to define "Archimedes," a system platform
	that supports the engineer through a methodology, a framework, and
	a set of tools to automate the development process of agile mechatronic
	manufacturing systems.},
  doi = {10.1109/TII.2005.844427},
  issn = {1551-3203},
  keywords = { Archimedes; agile mechatronic manufacturing systems; concurrent engineering;
	domain-specific modeling languages; electronic components; integrated
	development process; manufacturing systems development; mechanical
	components; mechatronic component; mechatronic systems; model evolution;
	model integration; model-driven development; model-integrated mechatronics;
	software components; agile manufacturing; control engineering computing;
	integrated manufacturing systems; mechatronics; simulation languages;}
}

@INPROCEEDINGS{1240301,
  author = {Tilevich, E. and Urbanski, S. and Smaragdakis, Y. and Fleury, M.},
  title = {Aspectizing server-side distribution},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = { 130 - 141},
  month = {oct.},
  abstract = { We discuss how a collection of domain-specific and domain-independent
	tools can be combined to "aspectize" the distributed character of
	server-side applications, to a much greater extent than with prior
	efforts. Specifically, we present a framework that can be used with
	a large class of unaware applications to turn their objects into
	distributed objects with minimal programming effort. Our framework
	is developed on top of three main components: AspectJ (a high-level
	aspect language), XDoclet (a low-level aspect language), and NRMI
	(a middleware facility that makes remote calls behave more like local
	calls). We discuss why each of the three components offers unique
	advantages and is necessary for an elegant solution, why our approach
	is general, and how it constitutes a significant improvement over
	past efforts to isolate distribution concerns.},
  doi = {10.1109/ASE.2003.1240301},
  issn = {1527-1366},
  keywords = { AspectJ; NRMI; XDoclet; aspectizing; distributed objects; domain-independent
	tools; domain-specific tools; high-level aspect language; low-level
	aspect language; middleware facility; programming effort; remote
	calls; server-side distribution; distributed object management; distributed
	programming; high level languages; middleware; object-oriented programming;
	remote procedure calls;}
}

@INPROCEEDINGS{6018709,
  author = {Huey Ling Toh and Hawkes, L.W. and Lacher, R.C.},
  title = {Adaptive query-based model for improved ranking in closed domain
	factoid question answering},
  booktitle = {Information Society (i-Society), 2010 International Conference on},
  year = {2010},
  pages = {260 -265},
  month = {june},
  abstract = {The closed domain question answering QA systems achieve precision
	and recall at the cost of complex language processing techniques
	to parse the answer corpus. The task of locating the search phrase
	in the small answer corpus is non-trivial, as there are fewer answers
	to search from. We propose a query-based model for indexing answers
	in a closed domain factoid question answering system. Further, we
	use a phrase term inference method for improving the ranking order
	of related questions. Our solution offers an adaptive, lightweight
	approach to a factoid question answering system for domain specific
	knowledge bases with significantly simplified language processing
	techniques.},
  keywords = {adaptive query based model;answer indexing;closed domain factoid question
	answering;complex language processing techniques;domain specific
	knowledge bases;phrase term inference method;indexing;inference mechanisms;knowledge
	based systems;natural language processing;query processing;}
}

@INPROCEEDINGS{1691611,
  author = {Tolvanen, J.},
  title = {Domain-Specific Modeling and Code Generation for Product Lines},
  booktitle = {Software Product Line Conference, 2006 10th International},
  year = {2006},
  pages = { 229},
  month = {aug.},
  abstract = { Current modeling languages provide surprisingly little, if any, support
	for product line development. They are either based on the code world
	using the semantically well-defined concepts of programming languages
	(e.g. UML, SA/SD) or based on an architectural view using a simple
	component-connector concept. In both cases, the languages themselves
	say nothing about a product family or its variants. This situation
	could be compared to that of a programmer being asked to write object-oriented
	programs where the language does not support any object-oriented
	concepts.},
  doi = {10.1109/SPLINE.2006.1691611}
}

@INPROCEEDINGS{6030092,
  author = {Tolvanen, J.-P. and Kelly, S.},
  title = {Creating Domain-Specific Modeling Languages for Product Lines},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {358},
  month = {aug.},
  abstract = {This tutorial teaches how to define Domain-Specific Modeling languages
	for product lines: how to identify domain concepts and capture them
	in the language specification, how to enforce the architecture and
	coding rules, what options are available for code generation, and
	what are the industry experiences from companies. The tutorial includes
	exercises allowing participants to apply the language definition
	skills learned.},
  doi = {10.1109/SPLC.2011.9},
  keywords = {code generation;coding rules;domain specific modeling language;language
	specification;product lines;formal specification;high level languages;}
}

@INPROCEEDINGS{5284945,
  author = {Tomassen, Stein L. and Strasunskas, Darijus},
  title = {Construction of Ontology Based Semantic-Linguistic Feature Vectors
	for Searching: The Process and Effect},
  booktitle = {Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT
	'09. IEEE/WIC/ACM International Joint Conferences on},
  year = {2009},
  volume = {3},
  pages = {133 -138},
  month = {sept.},
  abstract = {Search is among the most frequent activities on the Web. However,
	the search activity still requires extra efforts in order to get
	satisfactory results. One of the reasons is heterogeneous information
	resources and exponential growth of information. The problem of heterogeneity
	arises as a result of discipline specific language used even in domain
	specific documents. This particular problem we tackle in this paper.
	We propose an approach to construct semantic-linguistic feature vectors
	(FV). The FVs are built based on domain semantics encoded in an ontology
	and enhanced by a relevant terminology from documents on the Web.
	Semantic information from the ontologies is also used to expand the
	user queries and the FVs are used to filter and rank the retrieved
	documents. The strength of this approach is twofold. First, it is
	grounded on relevant semantics from an ontology, and second, it accounts
	for statistically significant collocations of other terms and phrases
	in relation to the ontology entities. In this paper, we explain how
	these FVs are constructed and what effect they have on search performance.},
  doi = {10.1109/WI-IAT.2009.248}
}

@INPROCEEDINGS{5954393,
  author = {Torens, C. and Ebrecht, L. and Lemmer, K.},
  title = {Inverse Model Based Testing -- Generating Behavior Models from Abstract
	Test Cases},
  booktitle = {Software Testing, Verification and Validation Workshops (ICSTW),
	2011 IEEE Fourth International Conference on},
  year = {2011},
  pages = {75 -78},
  month = {march},
  abstract = {Test cases contain a huge amount of domain specific knowledge of experts.
	Engineers may not use this knowledge only for validation purposes,
	e.g., applying test cases to software and hardware units, it can
	be also a starting point for modeling the product prototypically.
	This paper addresses the issue of generating a functional behavior
	model from abstract test cases. The test cases belong to the conformity
	and interoperability test standard for train-borne control units
	of the European Train Control System (ETCS). The contribution illustrates
	the approach and its advantages.},
  doi = {10.1109/ICSTW.2011.97},
  keywords = {European train control system;abstract test case;conformity standard;functional
	behavior model;interoperability test standard;inverse model based
	testing;product prototype modeling;control engineering computing;open
	systems;program testing;rail traffic;}
}

@INPROCEEDINGS{6032227,
  author = {Torsel, A.-M.},
  title = {Automated Test Case Generation for Web Applications from a Domain
	Specific Model},
  booktitle = {Computer Software and Applications Conference Workshops (COMPSACW),
	2011 IEEE 35th Annual},
  year = {2011},
  pages = {137 -142},
  month = {july},
  abstract = {Model-based testing is a promising technique for test case design
	that is used in an increasing number of application domains. However,
	to fully gain efficiency advantages, intuitive domain-specific notations
	with comfortable tool support as well as a high degree of automation
	in the whole testing process are required. In this paper, a model-based
	testing approach for web application black box testing is presented.
	A notation for web application control flow models augmented with
	data flow information is introduced. The described research prototype
	demonstrates the fully automated generation of ready to use test
	case scripts for common test automation tools including test oracles
	from the model.},
  doi = {10.1109/COMPSACW.2011.32},
  keywords = {Web application black box testing;Web application control flow models;automated
	test case generation;data flow information;domain specific model;domain
	specific notations;model based testing approach;test automation tools;test
	case design;test oracles;tool support;Internet;program testing;}
}

@INPROCEEDINGS{18010,
  author = {Tortora, G. and Leoncini, P.},
  title = {A model for the specification and interpretation of visual languages},
  booktitle = {Visual Languages, 1988., IEEE Workshop on},
  year = {1988},
  pages = {52 -60},
  month = {oct},
  abstract = {A model for the specification of icon systems is proposed, and a general-purpose
	icon interpreter is presented, based on attribute grammars. In the
	model, the underlying context-free grammar is a picture grammar that
	expresses the syntactic aspects of the icon systems. An attribute
	evaluator computes the meaning of a given icon sentence by evaluating
	the designated s-attribute of the nonterminal on the root of the
	parse tree. As semantic rules are actually domain-independent rule
	schemata, during the attribute evaluation a domain-specific knowledge
	base is consulted. The meaning of the ionic sentence is expressed
	in terms of conceptual tree graphs, which are well-suited for later
	execution. The design of the model is based on the theory of generalized
	icons. The system diagram of the icon interpreter is presented. The
	icon dictionary, the domain specific knowledge base, and the attribute
	grammar are described},
  doi = {10.1109/WVL.1988.18010},
  keywords = {attribute grammars;conceptual tree graphs;context-free grammar;domain-independent
	rule schemata;domain-specific knowledge base;icon interpreter;icon
	systems;interpretation;picture grammar;specification;visual languages;formal
	specification;high level languages;program interpreters;user interfaces;}
}

@INPROCEEDINGS{5380847,
  author = {Townsend, J.A. and Downing, J. and Murray-Rust, P.},
  title = {CHIC - Converting Hamburgers into Cows},
  booktitle = {e-Science, 2009. e-Science '09. Fifth IEEE International Conference
	on},
  year = {2009},
  pages = {337 -343},
  month = {dec.},
  abstract = {We have developed a methodology and workflow (CHIC) for the automatic
	semantification and structuring of legacy textual scientific documents.
	CHIC imports common document formats (PDF, DOCX and (X)HTML) and
	uses a number of toolkits to extract components and convert them
	into SciXML. This is sectioned into text-rich and data-rich streams
	and stand-off annotation (SAF) is created for each. Embedded domain
	specific objects can be converted into XML (chemical markup language).
	The different workflow streams can then be recombined and typically
	converted into RDF (resource description format).},
  doi = {10.1109/e-Science.2009.54},
  keywords = {(X)HTML;CHIC;DOCX;PDF;SciXML;automatic semantification;chemical markup
	language;data-rich streams;legacy textual scientific documents;resource
	description format;stand-off annotation;XML;document handling;scientific
	information systems;software maintenance;}
}

@INPROCEEDINGS{5715316,
  author = {Tran, H. and Holmes, T. and Oberortner, E. and Mulo, E. and Cavalcante,
	A.B. and Serafinski, J. and Tluczek, M. and Birukou, A. and Daniel,
	F. and Silveira, P. and Zdun, U. and Dustdar, S.},
  title = {An End-to-End Framework for Business Compliance in Process-Driven
	SOAs},
  booktitle = {Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),
	2010 12th International Symposium on},
  year = {2010},
  pages = {407 -414},
  month = {sept.},
  abstract = {It is significant for companies to ensure their businesses conforming
	to relevant policies, laws, and regulations as the consequences of
	infringement can be serious. Unfortunately, the divergence and frequent
	changes of different compliance sources make it hard to systematically
	and quickly accommodate new compliance requirements due to the lack
	of an adequate methodology for system and compliance engineering.
	In addition, the difference of perception and expertise of multiple
	stakeholders involving in system and compliance engineering further
	complicates the analyzing, implementing, and assessing of compliance.
	For these reasons, in many cases, business compliance today is reached
	on aper-case basis by using ad hoc, hand-crafted solutions for specific
	rules to which they must comply. This leads in the long run to problems
	regarding complexity, understandability, and maintainability of compliance
	concerns in a SOA. To address the aforementioned challenges, we present
	in this invited paper a comprehensive SOA business compliance software
	framework that enables a business to express, implement, monitor,
	and govern compliance concerns.},
  doi = {10.1109/SYNASC.2010.52},
  keywords = {ad hoc hand-crafted solutions;aper-case basis;compliance engineering;compliance
	requirements;comprehensive SOA business compliance software framework;end-to-end
	framework;multiple stakeholders;process-driven SOA;commerce;service-oriented
	architecture;}
}

@INPROCEEDINGS{5990574,
  author = {Tranoris, C.},
  title = {Adopting the DSM paradigm: Defining federation scenarios through
	resource brokers for experimentally driven research},
  booktitle = {Integrated Network Management (IM), 2011 IFIP/IEEE International
	Symposium on},
  year = {2011},
  pages = {1140 -1147},
  month = {may},
  abstract = {Federation scenarios for experimentally driven research usually involve
	resources offered by a diverse pool of organizations. Federation
	can be done by a resource broker, which has no resources of its own.
	Instead the broker matches customer's requested services and providers
	resources based on the SLA required by the end-user. The end-user
	has no knowledge that the broker does not control the resources.
	This work considers the concepts of modeling and meta-modeling to
	define a resource broker and to specify federation scenarios by applying
	the Domain Specific Modeling (DSM) paradigm. Moreover, we acknowledge
	the fact that resource models already exist and we adopt model to
	model transformations. We argue that defining a resource broker meta-model,
	focusing on the federation domain and applying DSM practices is necessary
	in order to: i) create formal description of a resource broker and
	its resource providers with its offered services and resources, ii)
	have valid, comprehensible and unambiguous configurations that support
	federation scenarios, iii) simplify the combination of services and
	resources from third parties that are non-conformant to the meta-model
	and iv) have common definitions and understanding by the resource
	federation domain, thus being efficient and practicable. We present
	a family of Domain Specific Languages (DSLs) having the meta-model
	as their abstract syntax, for defining model entities, describing
	a resource broker and federation scenarios between organizations.
	Additionally, prototype tooling supports the DSLs and the proposed
	framework.},
  doi = {10.1109/INM.2011.5990574},
  keywords = {DSM paradigm;domain specific language;domain specific modeling;experimentally
	driven research;federation scenario;resource broker formal description;resource
	broker meta model;resource brokers;unambiguous configuration;Internet;formal
	concept analysis;formal languages;}
}

@INPROCEEDINGS{6030091,
  author = {Trask, B. and Roman, A.},
  title = {Leveraging Model Driven Engineering in Software Product Line Architectures},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {356 -357},
  month = {aug.},
  abstract = {Model Driven Engineering (MDE) is a promising recent innovation in
	the software industry that has proven to work synergistically with
	Software Product Line Architectures (SPLA). It can provide the tools
	necessary to fully harness the power of Software Product Lines. The
	major players in the software industry including commercial companies
	such as IBM, Microsoft, standards bodies including the Object Management
	Group and leading Universities such as the ISIS group at Vanderbilt
	University are embracing this MDE/SPLA combination fully. IBM is
	spearheading the Eclipse Foundation including its MDE tools like
	the Eclipse Modeling Framework (EMF) and the Graphical Modeling Framework.
	Microsoft has also launched their Software Factories and DSL Toolkit
	into the MDE space. Top software groups such as the ISIS group at
	Vanderbilt are using these MDE techniques in combination with SPLAs
	for very complex systems. The Object Management Group is working
	on standardizing the various facets of MDE. All of these groups are
	capitalizing on the perfect storm of critical innovations today that
	allow such an approach to finally be viable. To further emphasize
	the timeliness of this technology is the complexity ceiling the software
	industry find itself facing wherein the platform technologies have
	increased far in advance of the language tools necessary to deal
	with them. This complexity ceiling is evident in today's Software
	Product Lines. As more and more software products are evolving into
	families of systems, it is vital to formally capture the commonalities
	and variabilities, the abstractions and the refinements, the frameworks
	and the framework extension points and completion code associated
	with a particular family. Model Driven Engineering has shown to be
	a very promising approach for capturing these aspects of software
	systems and families of systems which thereafter can be integrated
	systematically into Software Product Lines and Product Line Architectures.
	The process of Dev- - eloping Software Product Line Architectures
	can be a complex task. However, the use of Model Driven Engineering
	(MDE) techniques can facilitate the development of SPLAs by introducing
	Domain Specific Languages, Graphical Editors, and Generators. Together
	these are considered the sacred triad of MDE. Key to understanding
	MDE and how it fits into SPLAs is to know exactly what each part
	of the trinity means, how it relates to the other parts, and what
	the various implementations are for each. This tutorial will demonstrate
	the use of the Eclipse Modeling Framework (EMF) and Eclipse's Graphical
	Modeling Framework (GMF) to create an actual MDE solution as applied
	to a sample SPLA. During this tutorial we will also illustrate how
	to model the visual artifacts of our Domain Model and generate a
	Domain Specific Graphical Editor using GMF. This tutorial continues
	to be updated each year to include recent and critical innovations
	in MDE and SPL. This year will include information on key Model Transformation,
	Constraints and Textual Modeling Languages targeted at Software Product
	Lines. Additionally, it will cover advances in Software Product Line
	migration technologies. The goal of this tutorial is to educate attendees
	on what MDE technologies are, how exactly they relate synergistically
	to Software Product Line Architectures, and how to actually apply
	them using an existing Eclipse implementation.The benefits of the
	technology are so far reaching that we feel the intended audience
	spans technical managers, developers and CTOs. In general the target
	audience includes researchers and practitioners who are working on
	problems related to the design and implementation of SPLAs and would
	like to understand the benefits of applying MDE techniques towards
	SPLAs and leverage Eclipse as a framework to develop MDE solutions.
	The first half will be less technical than the second half where
	we cover the details of SPLA and MDE in action in complete detail
	showing patterns and code.},
  doi = {10.1109/SPLC.2011.63},
  keywords = {DSL toolkit;IBM;MDE tools;Microsoft;commercial companies;complexity
	ceiling;constraint modeling language;domain specific graphical editor;domain
	specific languages;eclipse modeling framework;graphical modeling
	framework;language tools;model driven engineering;object management
	group;software factories;software industry;software product line
	architectures;textual modeling language;programming languages;software
	architecture;software houses;}
}

@INPROCEEDINGS{5999351,
  author = {Truchat, S. and Kohlein, A. and Vollmar, J. and Lowen, U.},
  title = {Model Driven Plant Modernization: A Vision of Model Based Industrial
	Engineering},
  booktitle = {Management and Service Science (MASS), 2011 International Conference
	on},
  year = {2011},
  pages = {1 -4},
  month = {aug.},
  abstract = {To support modernization of industrial plants, we proposed a component
	oriented modeling method, based on a central model sustaining domain
	specific views. Dependency management should allow predictive re-engineering.
	We formalized the meta-model in a UML profile, and used OCL to express
	dependencies. We verified these concepts in an industrial case study
	with the tool suite that has been developed based on these studies.
	Investigations about a model driven approach for plant engineering
	seem to be promising.},
  doi = {10.1109/ICMSS.2011.5999351},
  keywords = {OCL;UML profile;component oriented modeling method;dependency management;meta
	model;model driven industrial plant modernization;model driven plant
	engineering approach;object constraint language;predictive re-engineering;Unified
	Modeling Language;industrial engineering;industrial plants;object-oriented
	languages;production engineering computing;}
}

@INPROCEEDINGS{1300391,
  author = {Shiu Lun Tsang and Clarke, S. and Baniassad, E.},
  title = {An evaluation of aspect-oriented programming for Java-based real-time
	systems development},
  booktitle = {Object-Oriented Real-Time Distributed Computing, 2004. Proceedings.
	Seventh IEEE International Symposium on},
  year = {2004},
  pages = {291 -300},
  month = {may},
  abstract = {Some concerns, such as debugging or logging functionality, cannot
	be captured cleanly, and are often tangled and scattered throughout
	the code base. These concerns are called crosscutting concerns. Aspect-oriented
	programming (AOP) is a paradigm that enables developers to capture
	crosscutting concerns in separate aspect modules. The use of aspects
	has been shown to improve understandability and maintainability of
	systems. It has been shown that real-time concerns, such as memory
	management and thread scheduling, are crosscutting concerns [A. Corsaro
	et al., (2002), M.Deters et al., (2001), A. Gal et al., (2002)].
	However it is unclear whether encapsulating these concerns provides
	benefits. We were interested in determining whether using AOP to
	encapsulate real-time crosscutting concerns afforded benefits in
	system properties such as understandability and maintainability.
	This paper presents research comparing the system properties of two
	systems: a real-time sentient traffic simulator and its aspect-oriented
	equivalent. An evaluation of AOP is presented indicating both benefits
	and drawbacks with this approach},
  doi = {10.1109/ISORC.2004.1300391},
  keywords = {Java-based real-time system;aspect-oriented programming;memory management;real-time
	sentient traffic simulator;system maintainability;system understandability;thread
	scheduling;Java;data encapsulation;multi-threading;object-oriented
	programming;real-time systems;storage management;}
}

@INPROCEEDINGS{4262713,
  author = {Tsymbal, A. and Zillner, S. and Huber, M.},
  title = {Feature Ontology for Improved Learning from Large-Dimensional Disease-Specific
	Heterogeneous Data},
  booktitle = {Computer-Based Medical Systems, 2007. CBMS '07. Twentieth IEEE International
	Symposium on},
  year = {2007},
  pages = {595 -600},
  month = {june},
  abstract = {Nowadays, ontologies and machine learning constitute two major technologies
	for domain-specific knowledge extraction. While the aim of these
	two technologies is the same - the extraction of useful knowledge
	- little is known about how the two sources of knowledge can be integrated.
	This problem is especially important for biomedicine where relevant
	data are often naturally complex having large dimensionality and
	including heterogeneous features. In this paper we propose an approach
	for improving the performance of machine learning by integrating
	the knowledge provided by ontologies for large-dimensional disease-specific
	heterogeneous data. The basic idea is to redefine the concept of
	similarity by incorporating available ontological knowledge. Benefits
	and difficulties of this integration are discussed and an example
	from the field of paediatric cardiology is described.},
  doi = {10.1109/CBMS.2007.50},
  issn = {1063-7125},
  keywords = {biomedicine;domain-specific knowledge extraction;knowledge integration;large-dimensional
	disease-specific heterogeneous data;machine learning;ontology;paediatric
	cardiology;similarity;cardiology;knowledge acquisition;knowledge
	based systems;learning (artificial intelligence);medical computing;ontologies
	(artificial intelligence);paediatrics;}
}

@INPROCEEDINGS{4959068,
  author = {Rongjun Tu and Weidong Liu},
  title = {A Study of Virtual Enterprise Modeling Based on Domain Specific Software
	Architecture},
  booktitle = {Education Technology and Computer Science, 2009. ETCS '09. First
	International Workshop on},
  year = {2009},
  volume = {2},
  pages = {415 -418},
  month = {march},
  abstract = {Virtual enterprise model affords the valid instruction for rapid establishing
	and successful running of virtual enterprise. However, authors perceive
	that low quality and low efficiency are serious restriction factor
	to the development of virtual enterprise model. In order to overcome
	above-mentioned embarrassment in virtual enterprise modeling, authors
	put forward applying software reuse technology and domain engineering
	theory to establishing the domain specific software architecture
	of virtual enterprise, then develop application system and establish
	the reusable component library in terms of domain specific software
	architecture of virtual enterprise. On the one hand, the quality
	and efficiency of modeling can be promoted remarkably. On the other
	hand, the model of virtual enterprise can be reused in the same domain.},
  doi = {10.1109/ETCS.2009.354},
  keywords = {UML;agile manufacturing;component library;domain engineering theory;domain
	specific software architecture;software reuse technology;virtual
	enterprise modeling;Unified Modeling Language;agile manufacturing;object-oriented
	programming;software architecture;software libraries;software reusability;virtual
	enterprises;virtual manufacturing;}
}

@INPROCEEDINGS{5298635,
  author = {Tunaoglu, D. and Alan, O. and Sabuncu, O. and Akpinar, S. and Cicekli,
	N.K. and Alpaslan, F.N.},
  title = {Event Extraction from Turkish Football Web-casting Texts Using Hand-crafted
	Templates},
  booktitle = {Semantic Computing, 2009. ICSC '09. IEEE International Conference
	on},
  year = {2009},
  pages = {466 -472},
  month = {sept.},
  abstract = {In this paper, we present a domain specific information extraction
	approach. We use manually formed templates to extract information
	from unstructured documents where grammatical and syntactical errors
	occur frequently. We applied our approach to primarily Turkish unstructured
	soccer Web-casting texts. Compared to automated approaches we achieve
	high precision-recall rates (97% - 85%). In addition to that, unlike
	automated approaches we do not use part-of-speech taggers, parsers,
	phrase chunkers or that kind of a linguistic tool. As a result, our
	approach can be applied to any domain or any language without the
	necessity of successful linguistic tools. The drawback of our approach
	is the time spent on crafting the templates. We also propose the
	means to decrease that time.},
  doi = {10.1109/ICSC.2009.16},
  keywords = {Turkish football Web-casting text;domain specific information extraction;event
	extraction;grammatical error;hand-crafted template;syntactical error;Internet;information
	retrieval;sport;text analysis;}
}

@INPROCEEDINGS{4588910,
  author = {Tundrea, E.},
  title = {SmartModels CHARPx2014; an MDE platform for the management of software
	product lines},
  booktitle = {Automation, Quality and Testing, Robotics, 2008. AQTR 2008. IEEE
	International Conference on},
  year = {2008},
  volume = {3},
  pages = {193 -199},
  month = {may},
  abstract = {In software engineering everything evolves very fast: user requirements,
	technologies, methodologies and applications. Can we foresight and
	strengthen our approaches to build software to confront these more
	and more complex challenges? While there are key issues to solve,
	it is also noteworthy to know that we are very close to exciting
	innovations. Software Product Lines (SPL) - modeling technology together
	with source-code generative tools seem to make it easier to manage
	diverse environments with complex, constantly changing relationships.
	In the context of SPL, this paper presents an approach - SmartModels
	[1] [2], validated by a prototype - SmartFactory. It reviews the
	state-of-the-art of SmartModels briefly introducing its principles,
	basic entities and main elements when defining a business-model.
	It also addresses the Meta-Object Protocol (MOP) which lays the foundation
	of SmartModelspsila mechanism to fill the gap between the semantics
	and the reification of a model entity.},
  doi = {10.1109/AQTR.2008.4588910},
  keywords = {MDE platform;MOP;SPL;SmartFactory;SmartModels;meta-object protocol;software
	engineering;software product lines management;source code generative
	tools;product development;software development management;software
	reusability;}
}

@INPROCEEDINGS{5700816,
  author = {Tur, G. and Hakkani-Tur, D. and Heck, L.},
  title = {What is left to be understood in ATIS?},
  booktitle = {Spoken Language Technology Workshop (SLT), 2010 IEEE},
  year = {2010},
  pages = {19 -24},
  month = {dec.},
  abstract = {One of the main data resources used in many studies over the past
	two decades for spoken language understanding (SLU) research in spoken
	dialog systems is the airline travel information system (ATIS) corpus.
	Two primary tasks in SLU are intent determination (ID) and slot filling
	(SF). Recent studies reported error rates below 5% for both of these
	tasks employing discriminative machine learning techniques with the
	ATIS test set. While these low error rates may suggest that this
	task is close to being solved, further analysis reveals the continued
	utility of ATIS as a research corpus. In this paper, our goal is
	not experimenting with domain specific techniques or features which
	can help with the remaining SLU errors, but instead exploring methods
	to realize this utility via extensive error analysis. We conclude
	that even with such low error rates, ATIS test set still includes
	many unseen example categories and sequences, hence requires more
	data. Better yet, new annotated larger data sets from more complex
	tasks with realistic utterances can avoid over-tuning in terms of
	modeling and feature design. We believe that advancements in SLU
	can be achieved by having more naturally spoken data sets and employing
	more linguistically motivated features while preserving robustness
	due to speech recognition noise and variance due to natural language.},
  doi = {10.1109/SLT.2010.5700816},
  keywords = {airline travel information system;data resources;discriminative machine
	learning;error analysis;error rates;intent determination;linguistically
	motivated features;natural language;naturally spoken data sets;slot
	filling;speech recognition noise;spoken dialog systems;spoken language
	understanding;computational linguistics;error analysis;natural language
	processing;speech recognition;}
}

@INPROCEEDINGS{4639384,
  author = {Ubayashi, N. and Otsubo, G. and Noda, K. and Yoshida, J. and Tamai,
	T.},
  title = {AspectM: UML-Based Extensible AOM Language},
  booktitle = {Automated Software Engineering, 2008. ASE 2008. 23rd IEEE/ACM International
	Conference on},
  year = {2008},
  pages = {501 -502},
  month = {sept.},
  abstract = {AspectM, a UML-based aspect-oriented modeling (AOM) language, provides
	not only basic modeling constructs but also an extension mechanism
	called metamodel access protocol (MMAP) that allows a modeler to
	extend the AspectM metamodel. MMAP enables a modeler to construct
	domain-specific AOM languages at relatively low cost. In this paper,
	we show the overview of an AspectM support tool consisting of a reflective
	model editor and a verifying model weaver.},
  doi = {10.1109/ASE.2008.91},
  issn = {1527-1366},
  keywords = {AspectM metamodel;AspectM support tool;MMAP;UML;extensible aspect-oriented
	modeling language;metamodel access protocol;model weaver verification;reflective
	model editor;Unified Modeling Language;object-oriented languages;program
	verification;software tools;}
}

@INPROCEEDINGS{4273252,
  author = {Ubayashi, N. and Sano, S. and Otsubo, G.},
  title = {A Reflective Aspect-Oriented Model Editor Based on Metamodel Extension},
  booktitle = {Modeling in Software Engineering, 2007. MISE '07: ICSE Workshop 2007.
	International Workshop on},
  year = {2007},
  pages = {12},
  month = {may},
  abstract = {AspectM, an aspect-oriented modeling language, provides not only basic
	modeling constructs but also an extension mechanism called metamodel
	access protocol (MMAP) that allows a modeler to modify the metamodel.
	MMAP consists of metamodel extension points, extension operations,
	and primitive predicates for defining pointcut designators. In this
	paper, a reflective model editor for supporting MMAP is proposed.
	A new modeling construct can be introduced by extending the metamodel.
	This mechanism, a kind of edit-time structural reflection, enables
	a modeler to represent domain-specific crosscutting concerns.},
  doi = {10.1109/MISE.2007.3},
  keywords = {aspect-oriented modeling language;edit-time structural reflection;metamodel
	access protocol;metamodel extension points;reflective aspect-oriented
	model editor;object-oriented languages;simulation languages;}
}

@INPROCEEDINGS{919093,
  author = {Uchitel, S. and Kramer, J.},
  title = {A workbench for synthesising behaviour models from scenarios},
  booktitle = {Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International
	Conference on},
  year = {2001},
  pages = { 188 - 197},
  month = {may},
  abstract = { Scenario-based specifications such as Message Sequence Charts (MSCs)
	are becoming increasingly popular as part of a requirements specification.
	Our objective is to facilitate the development of behaviour models
	in conjunction with scenarios. In this paper, we first present an
	MSC language with semantics in terms of labelled transition systems
	and parallel composition. The language integrates existing languages
	based on the use of high-level MSCs (hMSCs) and on identifying component
	states. This integration allows stakeholders to break up scenario
	specifications into manageable parts using hMCSs and to explicitly
	introduce additional information and domain-specific or other assumptions
	using state labels. Secondly, we present an algorithm, implemented
	in Java, which translates scenarios into a specification in the form
	of Finite Sequential Processes. This can then be fed to the labelled
	transition system analyser for model checking and animation. Finally
	we show how many of the assumptions embedded in existing synthesis
	approaches can be translated into our approach. Thus we provide the
	basis of a common workbench for supporting MSC specifications, behaviour
	synthesis and analysis.},
  doi = {10.1109/ICSE.2001.919093},
  issn = {0270-5257 },
  keywords = { MSC language; animation; behaviour models; behaviour models synthesis;
	behaviour synthesis; component states; labelled transition system
	analyser; labelled transition systems; message sequence charts; model
	checking; parallel composition; requirements specification; scenario-based
	specifications; semantics; state labels; workbench; Java; computer
	animation; formal specification;}
}

@ARTICLE{1178048,
  author = {Uchitel, S. and Kramer, J. and Magee, J.},
  title = {Synthesis of behavioral models from scenarios},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2003},
  volume = {29},
  pages = { 99 - 115},
  number = {2},
  month = {feb.},
  abstract = { Scenario-based specifications such as Message Sequence Charts (MSCs)
	are useful as part of a requirements specification. A scenario is
	a partial story, describing how system components, the environment,
	and users work concurrently and interact in order to provide system
	level functionality. Scenarios need to be combined to provide a more
	complete description of system behavior. Consequently, scenario synthesis
	is central to the effective use of scenario descriptions. How should
	a set of scenarios be interpreted? How do they relate to one another?
	What is the underlying semantics? What assumptions are made when
	synthesizing behavior models from multiple scenarios? In this paper,
	we present an approach to scenario synthesis based on a clear sound
	semantics, which can support and integrate many of the existing approaches
	to scenario synthesis. The contributions of the paper are threefold.
	We first define an MSC language with sound abstract semantics in
	terms of labeled transition systems and parallel composition. The
	language integrates existing approaches based on scenario composition
	by using high-level MSCs (hMSCs) and those based on state identification
	by introducing explicit component state labeling. This combination
	allows stakeholders to break up scenario specifications into manageable
	parts and reuse scenarios using hMCSs; it also allows them to introduce
	additional domain-specific information and general assumptions explicitly
	into the scenario specification using state labels. Second, we provide
	a sound synthesis algorithm which translates scenarios into a behavioral
	specification in the form of Finite Sequential Processes. This specification
	can be analyzed with the Labeled Transition System Analyzer using
	model checking and animation. Finally, we demonstrate how many of
	the assumptions embedded in existing synthesis approaches can be
	made explicit and modeled in our approach. Thus, we provide the basis
	for a common approach to scenario-based specification, synthesis,
	and analysis.},
  doi = {10.1109/TSE.2003.1178048},
  issn = {0098-5589},
  keywords = { behavioral specification; explicit component state labeling; finite
	sequential processes; labeled transition system analyzer; labeled
	transition systems; message sequence charts; multiple scenarios;
	parallel composition; requirements specification; scenario synthesis;
	scenario-based specifications; semantics; synthesis algorithm; formal
	specification; specification languages;}
}

@INPROCEEDINGS{4052941,
  author = {Unruh, A. and Bailey, J. and Ramamohanarao, K.},
  title = {A Logging-Based Approach for Building More Robust Multi-agent Systems},
  booktitle = {Intelligent Agent Technology, 2006. IAT '06. IEEE/WIC/ACM International
	Conference on},
  year = {2006},
  pages = {342 -349},
  month = {dec.},
  abstract = {In an agent system, the ability to handle problems and recover from
	them is important in sustaining stability and providing robustness.
	We claim that execution logging is essential to support agent system
	robustness, and that agents should have architectural-level support
	for logging and recovery methods. We describe an infrastructure-level,
	default methodology for agent problem-handling, based on logging,
	and supported by declaratively encoding domain-specific knowledge
	related to changes in goal status and semantic compensations. Via
	logging, the approach allows repair of already-completed as well
	as current goals. We define a language, APLR, to support and constrain
	incremental specification of problem-handling information, with the
	agents' problem-handling behaviour increasing in so phistication
	as more knowledge is added to the system. The approach is implemented
	by mapping the methodology and domain knowledge to 3APL-like plan
	rules extended to support logging.},
  doi = {10.1109/IAT.2006.12},
  keywords = {APLR language;agent programming language;domain-specific knowledge;logging-based
	approach;problem-handling behaviour;robust multi-agent system;multi-agent
	systems;software agents;}
}

@INPROCEEDINGS{185276,
  author = {Vahid, F. and Gajski, D.D.},
  title = {Obtaining functionally equivalent simulations using VHDL and a time-shift
	transformation},
  booktitle = {Computer-Aided Design, 1991. ICCAD-91. Digest of Technical Papers.,
	1991 IEEE International Conference on},
  year = {1991},
  pages = {362 -365},
  month = {nov},
  abstract = {It is pointed out that many translation schemes from domain-specific
	languages to supposedly functionally equivalent VDHL (VHSIC hardware
	description language) have been developed as an approach to simulation.
	However, due to a subtle theoretical limitation to this approach,
	functionally equivalent VHDL cannot be created for the general case,
	making such translations an unsound technique. The authors propose
	an alternative approach which strives instead for functionally equivalent
	simulation, while still taking advantage of VHDL simulators. This
	method uses a novel time-shift transformation in conjunction with
	any translation scheme, making correct simulations easily obtainable.
	This bridges the gap to a sound and advantageous use of VHDL as a
	tool for simulating domain-specific languages},
  doi = {10.1109/ICCAD.1991.185276},
  keywords = { VHDL; VHSIC hardware description language; domain-specific languages;
	functionally equivalent simulations; time-shift transformation; VLSI;
	circuit analysis computing; specification languages;}
}

@INPROCEEDINGS{4839229,
  author = {Vajk, T. and Kereskenyi, R. and Levendovszky, T. and Ledeczi, A.},
  title = {Raising the Abstraction of Domain-Specific Model Translator Development},
  booktitle = {Engineering of Computer Based Systems, 2009. ECBS 2009. 16th Annual
	IEEE International Conference and Workshop on the},
  year = {2009},
  pages = {31 -37},
  month = {april},
  abstract = {Model-based development methodologies are gaining ground as software
	applications are getting more and more complex while the pressure
	to decrease time-to-market continually increase. Domain-specific
	modeling tools that support system analysis, simulation, and automatic
	code generation can increase productivity. However, most domain-specific
	model translators are still manually written. This paper presents
	a technique that automatically generates a domain-specific application
	programming interface from the same metamodels that are used to define
	the domain-specific modeling language itself. This facilitates the
	creation of domain-specific model translators by providing a high-level
	abstraction hiding all the cumbersome modeling tool-specific implementation
	details from the developer. The approach is illustrated using the
	generic modeling environment and the Microsoft .NET C# language.},
  doi = {10.1109/ECBS.2009.30},
  keywords = {Microsoft .NET C# language;automatic code generation;domain-specific
	application programming interface;domain-specific model translator
	development;generic modeling environment;model-based development
	methodologies;system analysis;application program interfaces;program
	interpreters;software engineering;systems analysis;}
}

@INPROCEEDINGS{1318493,
  author = {Valsan, Z. and Emele, M.},
  title = {Thematic text clustering for domain specific language model adaptation},
  booktitle = {Automatic Speech Recognition and Understanding, 2003. ASRU '03. 2003
	IEEE Workshop on},
  year = {2003},
  pages = { 513 - 518},
  month = {nov.-3 dec.},
  abstract = { We propose a new approach for thematic text clustering. The text
	clusters are used to generate domain specific language models in
	order to address the problem of language model adaptation. The method
	relies on a new discriminative n-gram based term selection process
	(n>l), which reduces the influence of the corpus inhomogeneity, and
	outputs only semantically focused n-grams as being the most representative
	key terms in the corpus. These key terms are then used to automatically
	cluster the whole document collection and generate LM out of these
	text clusters. Different key term selection methods are evaluated
	using perplexity as a measure. Automatically computed clusters are
	compared with manually assigned labelling according to genre information.
	The results of these experimental studies are presented and discussed.
	Compared to the manual clustering a significant performance improvement
	between 21.87 % and 53.12 % is observed depending on the chosen key
	term selection method.},
  doi = {10.1109/ASRU.2003.1318493},
  keywords = { automatic clustering; discriminative n-gram process; document collection;
	domain specific language models; language model adaptation; perplexity
	measure; term selection process; thematic text clustering; pattern
	clustering; speech recognition; text analysis;}
}

@INPROCEEDINGS{4396972,
  author = {Van Cutsem, T. and Mostinckx, S. and Boix, E.G. and Dedecker, J.
	and De Meuter, W.},
  title = {AmbientTalk: Object-oriented Event-driven Programming in Mobile Ad
	hoc Networks},
  booktitle = {Chilean Society of Computer Science, 2007. SCCC '07. XXVI International
	Conference of the},
  year = {2007},
  pages = {3 -12},
  month = {nov.},
  abstract = {In this paper, we describe AmbientTalk: a domain- specific language
	for orchestrating service discovery and composition in mobile ad
	hoc networks. AmbientTalk is a distributed object-oriented language
	whose actor-based, event-driven concurrency model makes it highly
	suitable for composing service objects across a mobile network. The
	language is a so-called ambient-oriented programming language which
	treats network partitions as a normal mode of operation. We describe
	AmbientTalk's object model, concurrency model and distributed communication
	model in detail. We also highlight influences from other languages
	and middleware that have shaped AmbientTalk's design.},
  doi = {10.1109/SCCC.2007.12},
  issn = {1522-4902},
  keywords = {AmbientTalk;ambient-oriented programming language;distributed communication
	model;distributed object-oriented language;domain- specific language;mobile
	ad hoc networks;object-oriented event-driven programming;service
	discovery;ad hoc networks;mobile radio;object-oriented languages;object-oriented
	programming;specification languages;telecommunication computing;}
}

@INPROCEEDINGS{4076906,
  author = {Van Wyk, E. and Johnson, E.},
  title = {Composable Language Extensions for Computational Geometry: A Case
	Study},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {258c},
  month = {jan. },
  abstract = {This paper demonstrates how two different sets of powerful domain
	specific language features can be specified and deployed as composable
	language extensions. These extensions incorporate analyses and transformations
	that simplify the process of writing efficient and robust computational
	geometry programs and can be automatically added to a host language
	and used simultaneously. This is not possible in domain-specific
	language and library-based implementations of these features. One
	extension relies on characteristics of geometric algorithms to implement
	efficient exact-precision integers; the other employs a technique
	that symbolically perturbs geometric coordinates to safely and automatically
	handle degeneracies in the input data. These language extensions
	are implemented in an extensible language framework based on higher-order
	attribute grammars and forwarding. Attribute evaluation on the new
	language extension constructs is used to implement the static analysis
	and code transformations that enable the generation of efficient
	code.},
  doi = {10.1109/HICSS.2007.139},
  issn = {1530-1605}
}

@INPROCEEDINGS{4026865,
  author = {Van Wyk, E. and Krishnan, L. and Bodin, D. and Johnson, E. and Schwerdfeger,
	A. and Russell, P.},
  title = {Tool Demonstration: Silver Extensible Compiler Frameworks and Modular
	Language Extensions for Java and C},
  booktitle = {Source Code Analysis and Manipulation, 2006. SCAM '06. Sixth IEEE
	International Workshop on},
  year = {2006},
  pages = {161},
  month = {sept. },
  abstract = {In this tool demonstration of Silver extensible compiler frameworks
	for Java and C we illustrate how new languages that are adapted to
	specific problem domains can be easily created, by their users, by
	importing a set of desired domain-specific language extensions into
	an extensible host language compiler. Language extensions for computational
	geometry and database access will be shown. We also show extensions
	that provide general purpose language features such as algebraic
	types and pattern matching can be imported into an extensible language
	compiler. Each Silver extensible compiler framework supports the
	development of language extensions that have two important facets.
	First, language extensions should satisfy a completeness requirement.
	That is, they should be as welldeveloped as host language features
	and fit seamlessly into the host language. In particular, the language
	feature designer should be able to specify new language constructs
	together with their domain-specific semantic analyses and techniques
	for their optimization. One aspect of this requirement is that language
	extension should report a useful error message when they are used
	incorrectly. Second, the extensions should be modular so that a programmer
	can extend his or her language by choosing from a collection of previously
	defined features knowing only the functionality they provide and
	with no implementation-level knowledge or a detailed analysis of
	their interactions. Thus we draw a distinction between the programmer
	importing an extension and the feature designer who implements it.
	We will show extensible compilers for both C and Java. These compilers
	are defined by an attribute grammar written in the Silver attribute
	grammar language. A Silver compiler analyses attribute grammar specifications
	and generates an executable compiler for the defined (extended) language
	by translating the Silver specifications into an efficient Haskell
	representation. Language extensions are also specified as Silv- er
	attribute grammar fragments and the framework tools automatically
	compose specifications of the host language and chosen language extensions
	into a specification for the custom extended language.},
  doi = {10.1109/SCAM.2006.32}
}

@INPROCEEDINGS{5540750,
  author = {Vanderbauwhede, W. and Margala, M. and Chalamalasetti, S. R. and
	Purohit, S.},
  title = {A C } # x002B;  # {x002B;-embedded Domain-Specific Language for programming
	the MORA soft processor array},
  booktitle = {Application-specific Systems Architectures and Processors (ASAP),
	2010 21st IEEE International Conference on},
  year = {2010},
  pages = {141 -148},
  month = {july},
  abstract = {MORA is a novel platform for high-level FPGA programming of streaming
	vector and matrix operations, aimed at multimedia applications. It
	consists of soft array of pipelined low-complexity SIMD processors-in-memory
	(PIM). We present a Domain-Specific Language (DSL) for high-level
	programming of the MORA soft processor array. The DSL is embedded
	in C #x002B; #x002B;, providing designers with a familiar language
	framework and the ability to compile designs using a standard compiler
	for functional testing before generating the FPGA bitstream using
	the MORA toolchain. The paper discusses the MORA-C #x002B; #x002B;
	DSL and the compilation route into the assembly for the MORA machine
	and provides examples to illustrate the programming model and performance.},
  doi = {10.1109/ASAP.2010.5540750},
  issn = {1063-6268}
}

@INPROCEEDINGS{5706239,
  author = {Vanderbauwhede, W. and Nabi, S.W.},
  title = {A high-level language for programming a NoC-based Dynamic Reconfiguration
	Infrastructure},
  booktitle = {Design and Architectures for Signal and Image Processing (DASIP),
	2010 Conference on},
  year = {2010},
  pages = {7 -14},
  month = {oct.},
  abstract = {We present an infrastructure for dynamic reconfiguration of heterogeneous
	coarse-grained reconfigurable architectures (CGRAs) based on our
	Gannet SoC platform. We introduce the infrastructure and in particular
	its domain-specific high-level programming language Gannet-C and
	discuss the language features that support dynamic reconfiguration
	and the way they are supported by the compiler and the hardware.
	We illustrate our approach with simulation results obtained using
	a cycle-approximate SystemC model of the Gannet platform.},
  doi = {10.1109/DASIP.2010.5706239},
  keywords = {Gannet SoC platform;Gannet-C;NoC based dynamic reconfiguration;SystemC
	model;coarse grained reconfigurable architecture;domain specific
	high level programming language;dynamic compiler;C++ language;network-on-chip;reconfigurable
	architectures;}
}

@INPROCEEDINGS{1210276,
  author = {VanderMeer, D. and Navathe, S.B. and Datta, A. and Dutta, K. and
	Thomas, H. and Ramamritham, K.},
  title = {FUSION: a system allowing dynamic Web service composition and automatic
	execution},
  booktitle = {E-Commerce, 2003. CEC 2003. IEEE International Conference on},
  year = {2003},
  pages = { 399 - 404},
  month = {june},
  abstract = { Service portals are systems which expose a bundle of Web services
	to the user, allowing the specification and subsequent execution
	of complex tasks defined over these individual services. Examples
	of situations where service portals would be valuable include making
	travel plans or purchasing a home. Service portals must be capable
	of converting an abstract user goal into a correct and optimal concrete
	execution plan, executing according to the plan, verifying the result
	against a user's stated satisfaction criteria, and in the case of
	satisfaction failure, initiating the appropriate recovery procedures.
	The basic framework needed to support this functionality, from gathering
	the input to generating an optimal plan and executing that plan,
	is a prerequisite for all service portals, yet there are currently
	no such commercial systems in existence, and the research literature
	has given only cursory treatment to some of these issues. In this
	paper, we describe FUSION, a comprehensive software system which
	provides the underlying framework for a service portal. We show how
	using the elements of this framework, service portal designers and
	architects can easily create domain-specific service portals, e.g.,
	a travel service portal. We also present the Web services execution
	specification language (WSESL), a language that we have developed
	to describe execution plans in the context of the FUSION services
	model. Finally, we develop a set of data structures and algorithms
	for generating correct and optimal execution plans.},
  doi = {10.1109/COEC.2003.1210276},
  issn = { },
  keywords = { FUSION service model; FUSION system; WSESL language; Web services
	execution specification language; abstract user goal; automatic execution;
	commercial system; complex tasks; comprehensive software system;
	data structures; domain specific service portal; dynamic Web service
	composition; framework elements; functionality support; home purchasing;
	input gathering; optimal concrete execution plan; optimal plan generation;
	plan execution; recovery procedure initiation; satisfaction failure;
	service portal architect; service portal designer; task execution;
	task specification; travel service portal; user stated satisfaction
	criteria; abstract data types; computer communications software;
	data mining; optimal control; parallel algorithms; portals; specification
	languages;}
}

@INPROCEEDINGS{5741253,
  author = {Vangheluwe, H.},
  title = {Invited Talk: Promises and Challenges of Model-Driven Engineering},
  booktitle = {Software Maintenance and Reengineering (CSMR), 2011 15th European
	Conference on},
  year = {2011},
  pages = {3 -4},
  month = {march},
  abstract = {The complexity of (software-intensive) systems we build as well as
	the demands that are put on quality, safety, and maintainability
	of these systems has grown drastically over the last decades. To
	tackle this complexity, Model-Driven Engineering (MDE) treats models,
	in various formalisms, as first-class artifacts. Such models may
	be obtained by reverse-engineering of existing software artifacts,
	for the purpose of analysis, optimization, and evolution. Increasingly,
	however, software is no longer the primary artifact but rather synthesized
	from more abstract models. In an attempt to minimize "accidental
	complexity", the most appropriate modeling languages or formalisms
	are used for each specific (sub-)problem and phase in the development
	process. Domain-Specific Modeling (DSM) in particular tries to bridge
	the gap between the problem domain and the technical solution domain.
	This has led to a proliferation of the number of (software) modeling
	languages. Software artifacts, models, but also modeling languages,
	may and will evolve. This has important repercussions on related
	artifacts such as instance models and transformations. If MDE and
	DSM are to be usable at an industrial scale, modeling language evolution
	has to be dealt with, ideally by (semi-)automatically co-evolving
	artifacts. This talk introduces MDE concepts and techniques as well
	as the challenges these introduce. Particular attention is be paid
	to modeling language engineering and language evolution.},
  doi = {10.1109/CSMR.2011.62},
  issn = {1534-5351},
  keywords = {domain-specific modeling;language engineering;language evolution;model-driven
	engineering;reverse engineering;software artifacts;software-intensive
	systems;software engineering;}
}

@INPROCEEDINGS{1371323,
  author = {Vangheluwe, H. and de Lara, J.},
  title = {Computer automated multi-paradigm modelling for analysis and design
	of traffic networks},
  booktitle = {Simulation Conference, 2004. Proceedings of the 2004 Winter},
  year = {2004},
  volume = {1},
  pages = { 2 vol. (xliv+2164)},
  month = {dec.},
  abstract = {Computer automated multiparadigm modelling (CAMPaM) is an enabler
	for domain-specific analysis and design. Traffic, a new untimed visual
	formalism for vehicle traffic networks, is introduced. The syntax
	of traffic models is meta-modelled in the entity-relationship diagrams
	formalism. From this, augmented with concrete syntax information,
	a visual modelling environment is synthesized using our CAMPaM tool
	AToM3, a tool for multiformalism and meta-modelling. The semantics
	of the traffic formalism is subsequently modelled by mapping traffic
	models onto Petri net models. As models' abstract syntax is graph-like,
	graph rewriting can be used to transform models. The advantages of
	a domain-specific formalism such as traffic as opposed to a generic
	formalism such as Petri nets are presented. We demonstrate how mapping
	onto Petri nets allows one to employ the vast array of Petri net
	analysis techniques. A coverability graph is generated and conservation
	analysis is automated by transforming this graph into an integer
	linear programming specification.},
  doi = {10.1109/WSC.2004.1371323},
  issn = { },
  keywords = { Petri net model; computer automated multiparadigm modelling; domain-specific
	analysis; entity-relationship diagrams formalism; graph rewriting;
	integer linear programming specification; meta-modelled; vehicle
	traffic networks; visual formalism; visual modelling; Petri nets;
	entity-relationship modelling; formal specification; graph grammars;
	integer programming; linear programming; programming language semantics;
	rewriting systems; traffic engineering computing; visual languages;}
}

@INPROCEEDINGS{880058,
  author = {Varga, L. and Kozma, R. and Kun, A. and Hosszu, G. and Kovacs, F.
	and Schneider, C.},
  title = {VHDL-based system-level design methodology for multimedia signal
	processing applications},
  booktitle = {Electrotechnical Conference, 2000. MELECON 2000. 10th Mediterranean},
  year = {2000},
  volume = {2},
  pages = { 814 - 817 vol.2},
  abstract = { We propose an efficient system-level design methodology for multimedia
	signal processing applications, and demonstrate its applicability
	on a case study of MPEG-2 video decoder. The methodology offers to
	start from an executable specification, and supports refinement toward
	the register transfer level. Its primary objective is to provide
	the system designer with a method to design, simulate and evaluate
	a system through a series of abstraction levels using a VHDL based
	top-down modeling methodology. Sequential transformations are performed,
	and the initial model is partitioned in order to make the domain-specific
	refinements possible. The sequential model is transformed into concurrent
	processes. Different partitions can be quickly evaluated by moving
	functionality between processes. Inter-process communications via
	abstract communication channels are introduced. Finally the timing
	is refined by introducing propagation delay, and the earlier casual
	communication is replaced by clock-based communication.},
  doi = {10.1109/MELCON.2000.880058},
  keywords = { MPEG-2 video decoder; VHDL; abstract communication channel; casual
	communication; clock timing; concurrent process; functional partitioning;
	inter-process communication; multimedia signal processing; propagation
	delay; register transfer level; sequential transformation; system-level
	design; top-down model; decoding; hardware description languages;
	high level synthesis; multimedia communication; video coding;}
}

@INPROCEEDINGS{6061371,
  author = {Vasquez, R. and Verma, K. and Kass, A.},
  title = {Distributing Computationally Expensive Matching of Requirements to
	Capability Models},
  booktitle = {Semantic Computing (ICSC), 2011 Fifth IEEE International Conference
	on},
  year = {2011},
  pages = {554 -558},
  month = {sept.},
  abstract = {In this paper, we present a distributed way to automatically map users'
	requirements to reference process models. In a prior paper [9], we
	presented a tool called Process Model Requirements Gap Analyzer (ProcGap),
	which combines natural language processing, information retrieval,
	and semantic reasoning to automatically match and map textual requirements
	to domain-specific process models. Although the tool proved beneficial
	to users in reusing prior knowledge, by making it easy to use process
	models, the tool has one main drawback. It takes a long time to compare
	a very large requirements document, one that has a few thousand requirements,
	to a process model hierarchy with a few thousand capabilities. In
	this paper, we present how we solved this problem using Apache Hadoop.
	Apache Hadoop allows ProcGap to distribute matching task across several
	machines, increasing the tool's performance and usability. We present
	the performance comparison of running ProcGap on a single-machine,
	and our distributed version.},
  doi = {10.1109/ICSC.2011.54},
  keywords = {Apache Hadoop;ProcGap;capability model;information retrieval;natural
	language processing;process model requirement gap analyzer;semantic
	reasoning;document handling;grammars;information retrieval;natural
	language processing;}
}

@INPROCEEDINGS{1046361,
  author = {Vass, M. and Schoenhoff, P.},
  title = {Error detection support in a cellular modeling end-user programming
	environment},
  booktitle = {Human Centric Computing Languages and Environments, 2002. Proceedings.
	IEEE 2002 Symposia on},
  year = {2002},
  pages = { 104 - 106},
  abstract = { Debugging tools for end-user programming have largely concentrated
	on generic and general-domain applications, such as spreadsheets.
	Complex, domain specific tasks such as cellular modeling, however,
	introduce new types of errors, non-existent in a general domain.
	These errors may be generated at typographical, semantic, modeling,
	or cognitive levels. Our work explores requirements and potential
	for error detection and debugging systems within an end-user programming
	(EUP) environment geared toward modeling cellular biology.},
  doi = {10.1109/HCC.2002.1046361},
  issn = { },
  keywords = { EUP environment; cellular modeling end-user programming environment;
	cognitive errors; complex domain-specific tasks; debugging tools;
	end-user programming; end-user programming environment; error detection
	support; modeling cellular biology; modeling errors; semantic errors;
	spreadsheets; typographical errors; program debugging; programming
	environments;}
}

@INPROCEEDINGS{5501141,
  author = {Vassiliev, S.A.},
  title = {Using microsoft DSL technology to planning of the drug therapy course
	and diagnostic process},
  booktitle = {Software Engineering Conference in Russia (CEE-SECR), 2009 5th Central
	and Eastern European},
  year = {2009},
  pages = {289 -293},
  month = {oct.},
  abstract = {Medical mistakes at creation of the process of diagnostics and therapy
	(further-course) can lead to the most fatal consequences for the
	patient. It is necessary for doctor to keep in mind the wide range
	of diverse factors in a context of the current status of the patient.
	In this report the innovative method of creation of the course of
	drug therapy is presented. The course is considered as the program
	in Domain-Specific Language. Are described base entities of this
	language (the Patient, Therapy, Diagnostics) and course engineering
	process (creation, translation, debug and deployment of the final
	programs at mobile devices). The integrated development environment
	has been developed with Microsoft Visual Studio Shell technology.
	The analysis of compatibility of the medicaments has been implemented
	by the logic programming (Prolog). Set of predicates is generated
	from the drug's descriptions. The analysis of efficiency of this
	approach for elimination of the typical errors is presented.},
  doi = {10.1109/CEE-SECR.2009.5501141},
  keywords = {diagnostic process;domain specific language;drug therapy course;drugs
	descriptions;fatal consequences;innovative method;logic programming;medical
	mistakes;microsoft DSL technology;microsoft visual studio shell technology;drugs;logic
	programming;patient diagnosis;patient treatment;}
}

@INPROCEEDINGS{5090916,
  author = {Vecchie, E. and Talpin, J.-P. and Schneider, K.},
  title = {Separate compilation and execution of imperative synchronous modules},
  booktitle = {Design, Automation Test in Europe Conference Exhibition, 2009. DATE
	'09.},
  year = {2009},
  pages = {1580 -1583},
  month = {april},
  abstract = {The compilation of imperative synchronous languages like Esterel has
	been widely studied, the separate compilation of synchronous modules
	has not, and remains a challenge. We propose a new compilation method
	inspired by traditional sequential code generation techniques to
	produce coroutines whose hierarchical structure reflects the control
	flow of the original source code. A minimalistic runtime system executes
	separately compiled modules.},
  issn = {1530-1591},
  keywords = {Esterel language;imperative synchronous language;imperative synchronous
	modules;minimalistic runtime system;module compilation;module execution;sequential
	code generation;source code control flow;data flow analysis;program
	compilers;}
}

@INPROCEEDINGS{4668086,
  author = {Vepsalainen, T. and Hastbacka, D. and Kuikka, S.},
  title = {Tool Support for the UML Automation Profile - For Domain-Specific
	Software Development in Manufacturing},
  booktitle = {Software Engineering Advances, 2008. ICSEA '08. The Third International
	Conference on},
  year = {2008},
  pages = {43 -50},
  month = {oct.},
  abstract = {The development of modern distributed automation applications is challenging
	and present development practices contain manual transferring of
	informal information from one phase to another. Our research aims
	to overcome some of these challenges by integrating concepts from
	modern object-oriented design, model-driven development and high-level
	modeling potential of the UML automation profile into a seamless
	development path from PI-diagrams to control software. This paper
	presents a prototype of a control engineering tool that supports
	the UML automation profile and is intended to cover part of the development
	chain. The tool was implemented on the Eclipse platform and it utilizes
	various open source tools and frameworks to enable also usage of
	UML and SysML in modeling work. The implemented tool can be extended
	by transformation tools capable of processing requirements of the
	control system and PIM-model of the designed control software.},
  doi = {10.1109/ICSEA.2008.22},
  keywords = {Eclipse platform;PI-diagrams;SysML;UML automation profile;control
	engineering tool;control software;distributed automation;domain-specific
	software development;high-level modeling;manufacturing automation;model-driven
	development;object-oriented design;tool support;Unified Modeling
	Language;control engineering computing;formal specification;manufacturing
	systems;object-oriented programming;}
}

@ARTICLE{1576656,
  author = {Verheecke, B. and Vanderperren, W. and Jonckers, V.},
  title = {Unraveiliny crossoutting concerns in Web services middleware},
  journal = {Software, IEEE},
  year = {2006},
  volume = {23},
  pages = {42 -50},
  number = {1},
  month = {jan.-feb. },
  abstract = {Service-oriented architectures are designed to support loose coupling
	between interacting software applications. Using Web services technology,
	SOAs support the creation of distributed applications in a heterogeneous
	environment. The ultimate SOA goal is to let developers write applications
	that are independent of the specific services they use - applications
	that select and integrate services on the fly. Currently, service
	developers use the Web services description language to describe
	their services and publish the documentation in a registry. Service
	clients can browse these registries to find a service that matches
	their need and to determine how to communicate with it. By analyzing
	the WSDL documentation, the client can integrate the service and
	invoke it through XML-based SOAP communication. The Web Services
	Management Layer provides adaptive middleware that uses dynamic AOP
	to solve several crosscutting concerns in service-oriented architectures},
  doi = {10.1109/MS.2006.31},
  issn = {0740-7459},
  keywords = {AOP;WSDL documentation;Web service description language;XML-based
	SOAP communication;aspect-oriented programming;middleware;service-oriented
	architecture;Internet;XML;middleware;object-oriented programming;}
}

@INPROCEEDINGS{4695871,
  author = {Verma, Shireesh and Atluri, Srinath and Bertacco, Valeria and Glasser,
	Mark and Gopalan, Badri and Rosenberg, Sharon},
  title = {Panel: Software practices for verification/testbench management},
  booktitle = {High Level Design Validation and Test Workshop, 2008. HLDVT '08.
	IEEE International},
  year = {2008},
  pages = {35 -37},
  month = {nov.},
  abstract = {The ever rising complexity of current hardware designs and the huge
	cost penalty of delivering a faulty product have led to a growing
	investment in functional verification and in the development of new
	technologies and methodologies in this area. The traditional HDL
	based testbenches are not proving sufficient for verification. Verification
	teams are switching to languages such as C++, SystemC or HVLs (High
	Level Verification Languages) such as Vera, e, SystemVerilog where
	they can manage the complexity in a more efficient manner. They are
	adopting concepts such as Object and Aspect Oriented Programming
	to impart structure to their respective verification infrastructures.
	In fact building a well equipped testbench for an industrial scale
	design is equivalent to developing quite complex software. However,
	such an extensive borrowing from software arena heralds the spillover
	of problems associated with traditional complex software development.
	The issues at hand are further aggravated with hardware domain specific
	issues such as concurrency, timing etc.},
  doi = {10.1109/HLDVT.2008.4695871},
  issn = {1552-6674}
}

@INPROCEEDINGS{4656427,
  author = {Vermolen, S.},
  title = {Software Language Evolution},
  booktitle = {Reverse Engineering, 2008. WCRE '08. 15th Working Conference on},
  year = {2008},
  pages = {323 -326},
  month = {oct.},
  abstract = {By abstraction and factoring out domain specific knowledge, model
	driven engineering addresses the problem of increasing software complexity.
	Both models and meta models are generally subject to evolution, yet
	evolution of a meta model can cause conforming models to no longer
	conform and thereby no longer be usable. Therefore, models need to
	be migrated to reflect changes to their meta models. As evolution
	is typically frequent and reoccurring, manual migration of models
	is cumbersome and holds back the development process, yet automatic
	support is generally lacking. In this research we identify the problems
	caused by meta model evolution and develop methodologies and tools
	to solve these by supporting meta model evolution generically and
	automatically.},
  doi = {10.1109/WCRE.2008.42},
  issn = {1095-1350},
  keywords = {domain specific knowledge;meta model evolution;model driven engineering;software
	complexity;software language evolution;computational complexity;metacomputing;programming
	languages;software engineering;}
}

@ARTICLE{806980,
  author = {Vetter, J. and Schwan, K.},
  title = {Techniques for high-performance computational steering},
  journal = {Concurrency, IEEE},
  year = {1999},
  volume = {7},
  pages = {63 -74},
  number = {4},
  month = {oct-dec},
  abstract = {Computational steering lets researchers investigate, calibrate and
	control long-running, resource-intensive applications at run-time.
	Magellan, a prototype computational steering system, uses a domain-specific
	language called ACSL to intelligently control multi-threaded asynchronous
	steering servers that cooperatively steer applications},
  doi = {10.1109/4434.806980},
  issn = {1092-3063},
  keywords = {ACSL;Magellan;calibration;cooperative application steering;domain-specific
	language;high-performance computational steering;intelligent control;long-running
	resource-intensive applications;multi-threaded asynchronous steering
	servers;run-time control;cooperative systems;intelligent control;multi-threading;multiprocessing
	programs;network servers;}
}

@INPROCEEDINGS{1690153,
  author = {Videira, C. and Ferreira, D. and da Silva, A.R.},
  title = {A Linguistic Patterns Approach for Requirements Specification},
  booktitle = {Software Engineering and Advanced Applications, 2006. SEAA '06. 32nd
	EUROMICRO Conference on},
  year = {2006},
  pages = {302 -309},
  month = {29 2006-sept. 1},
  abstract = {Despite the efforts made to overcome the problems associated with
	the development of information systems, we must consider that it
	is still an immature activity, with negative consequences in time,
	budget and quality. One of the root causes for this situation is
	the fact that many projects do not follow a structured, standard
	and systematic approach, like the methodologies and best practices
	proposed by software engineering. In this paper, we present a requirements
	specification language, called ProjectIT-RSL, based on the identification
	of the most frequently used linguistic patterns in requirements documents,
	written in natural language. To guarantee the consistency of the
	written requirements and the integration with generative programming
	tools, the requirements are analyzed by parsing tools, and immediately
	validated according with the syntactic and semantic rules of the
	language},
  doi = {10.1109/EUROMICRO.2006.8},
  issn = {1089-6503},
  keywords = {ProjectIT-RSL;generative programming tools;information systems development;linguistic
	patterns approach;natural language;parsing tools;requirements specification
	language;semantic rules;software engineering;syntactic rules;formal
	specification;natural languages;programming language semantics;software
	tools;specification languages;}
}

@INPROCEEDINGS{5532356,
  author = {Vignaga, Andrs},
  title = {Typing Textual Entities and M2T/T2M Transformations in a Model Management
	Environment},
  booktitle = {Chilean Computer Science Society (SCCC), 2009 International Conference
	of the},
  year = {2009},
  pages = {115 -122},
  month = {nov.},
  abstract = {Global Model Management (GMM) is a model-based approach for managing
	large sets of interrelated heterogeneous and complex MDE artifacts.
	Such artifacts are usually represented as models, however as many
	Domain Specific Languages have a textual concrete syntax, GMM also
	supports textual entities and model-to-text/text-to-model transformations
	which are projectors that bridge the MDE technical space and the
	Grammarware technical space. As the transformations supported by
	GMM are executable artifacts, typing is critical for preventing type
	errors during execution. We proposed the cGMM calculus which formalizes
	the notion of typing in GMM. In this work, we extend cGMM with new
	types and rules for supporting textual entities and projectors. With
	such an extension, those artifacts may participate in transformation
	compositions addressing larger transformation problems. We illustrate
	the new constructs in the context of an interoperability case study.},
  doi = {10.1109/SCCC.2009.25},
  issn = {1522-4902}
}

@INPROCEEDINGS{4068269,
  author = {Villanueva-Pena, P.E. and Kunz, T.},
  title = {GP-pro: the generative programming protocol generator for routing
	in mobile ad hoc networks},
  booktitle = {Wireless Mesh Networks, 2006. WiMesh 2006. 2nd IEEE Workshop on},
  year = {2006},
  pages = {129 -131},
  month = {sept.},
  abstract = {Routing in mobile ad hoc networks (MANETs) where network topology
	is highly dynamic is not a trivial task. Routing protocols have been
	profoundly researched but only three of them have reached the RFC
	status (AODV[7], OLSR[5] and TBPRF[6]). On the other hand, the constantly
	increasing network requirements in terms of bandwidth, robustness,
	reliability and quality of service for a broad range of multiplatform
	scenarios demand for fast development and implementation of routing
	protocols that satisfy specific user and network requirements. However,
	current practices for protocol development and implementation are
	costly error-prone and time-consuming, especially when existing knowledge
	is not properly reused. Generative Programming is an attractive solution
	that makes use of reusable components and is also empowered with
	the knowledge to automatically assemble them. This paper discusses
	the design and development of the GP- Pro protocol generator (based
	on generative programming), for automatic generation of ad hoc routing
	protocols, according to user requirements expressed by means of a
	specification language. GP-Pro is designed to be extensible, with
	the explicit goal of generating a large number of different protocols
	by different component combinations. GP-Pro addresses the generation
	of proactive, reactive and position-based routing protocols.},
  doi = {10.1109/WIMESH.2006.288628},
  keywords = {GP-Pro;MANET;generative programming protocol generator;mobile ad hoc
	networks;network bandwidth;network reliability;network robustness;network
	topology;networks routing;quality of service;routing protocols;specification
	language;ad hoc networks;mobile radio;quality of service;routing
	protocols;specification languages;telecommunication computing;telecommunication
	network reliability;telecommunication network topology;}
}

@INPROCEEDINGS{5775163,
  author = {Villar, Eugenio and Flake, Peter},
  title = {LBSD4: Synthesis for SoC and beyond},
  booktitle = {Specification Design Languages, 2010. IC 2010. Forum on},
  year = {2010},
  pages = {1},
  month = {sept.},
  abstract = {This session presents very different concept from the large field
	of system synthesis. The first paper addresses the synthesis of complex
	glue logic for integration of components of Systems-on-Chip. The
	second contribution demonstrates a synthesis concept for a rich subset
	of Haskell into synthesisable VHDL. In contrast to domain specific
	languages like ForSyDe or Lava it supports plain Haskell. Reversible
	logic has been described more than 30 years ago and might offer high
	potential for very efficient solutions. However, it still lacks efficient
	design methodology support. SyReC is the subject of the third paper
	and describes a synthesisable programming language for this type
	of target logic.},
  doi = {10.1049/ic.2010.0168}
}

@ARTICLE{1237164,
  author = {Virirakis, L.},
  title = {GENETICA: A computer language that supports general formal expression
	with evolving data structures},
  journal = {Evolutionary Computation, IEEE Transactions on},
  year = {2003},
  volume = {7},
  pages = { 456 - 481},
  number = {5},
  month = {oct.},
  abstract = { This paper presents a general problem-solving method combining the
	principles of artificial intelligence and evolutionary computation.
	The problem-solving method is based on the computer language GENETICA,
	which stands for "Genetic Evolution of Novel Entities Through the
	Interpretation of Composite Abstractions." GENETICAs programming
	environment includes a computational system that evolves data abstractions,
	viewed as genotypes of data generation scenarios for a GENETICA program,
	with respect to either confirmation or optimization goals. A problem
	can be formulated as a GENETICA program, while the solution is represented
	as a data structure resulting from an evolved data generation scenario.
	This approach to problem solving offers: 1) generality, since it
	concerns virtually any problem stated in formal logic; 2) effectiveness,
	since formally expressed problem-solving knowledge can be incorporated
	in the problem statement; and 3) creativity, since unpredictable
	solutions can be obtained by evolved data structures. It is shown
	that domain specific languages, including genetic programming ones,
	that inherit GENETICAs features can be developed in GENETICA. The
	language G-CAD, specialized to problem solving in the domain of architectural
	design, is presented as a case study followed by experimental results.},
  doi = {10.1109/TEVC.2003.816581},
  issn = {1089-778X},
  keywords = { G-CAD; GENETICA; architectural design; artificial intelligence; computer
	language; data abstractions; data generation scenarios; data structure;
	domain specific languages; evolutionary computation; evolving data
	structures; experimental results; formal logic; general formal expression;
	genetic programming; genotypes; optimization; problem-solving; problem-solving
	method; programming environment; architectural CAD; data structures;
	evolutionary computation; formal logic; high level languages; problem
	solving; programming environments;}
}

@INPROCEEDINGS{1541153,
  author = {Visser, E.},
  title = {Transformations for abstractions},
  booktitle = {Source Code Analysis and Manipulation, 2005. Fifth IEEE International
	Workshop on},
  year = {2005},
  pages = { 3 - 12},
  month = {sept.-1 oct.},
  abstract = { The transformation language Stratego provides high-level abstractions
	for implementation of a wide range of transformations. Our aim is
	to integrate transformation in the software development process and
	make it available to programmers. This requires the transformations
	provided by the programming environment to be extensible. This paper
	presents a case study in the implementation of extensible programming
	environments using Stratego, by developing a small collection of
	language extensions and several typical transformations for these
	languages.},
  doi = {10.1109/SCAM.2005.26},
  keywords = { Stratego; program transformations; programming environments; transformation
	language; program compilers; programming environments;}
}

@INPROCEEDINGS{138659,
  author = {Vo, K.-P.},
  title = {IFS: a software tool to build end user systems},
  booktitle = {Systems Integration, 1990. Systems Integration '90., Proceedings
	of the First International Conference on},
  year = {1990},
  pages = {26 -35},
  month = {apr},
  abstract = {The salient features of IFS (Interpretive Frame System), a tool for
	building end-user systems with sophisticated, yet easy to use interfaces,
	are reviewed. IFS promotes the idea of separating design programming
	from computational programming. It provides a programming language
	and environment suitable for implementing the design aspects of an
	application system, and its task structure and user interface. Domain-specific
	computations are assumed to be carried out by other tools and processes.
	The language provides powerful constructs for executing and interfacing
	with application-specific subroutines and processes for such computations.
	By restricting the program interface at the subroutine and process
	level, IFS both makes it easy to reuse existing tools and encourages
	the construction of new tools reusable in other contexts. Being able
	to directly execute subroutines for critical computations, application
	systems can achieve their required levels of efficiency or robustness.
	More important, however, is the ability to quickly prototype system
	designs and user interfaces and to allow early involvement of users
	in the system development cycle, ensuring accurate requirements},
  doi = {10.1109/ICSI.1990.138659},
  keywords = {Interpretive Frame System;application system;application-specific
	subroutines;computational programming;critical computations;design
	programming;end-user systems;process level;program interface;programming
	language;reusable;software tool;system designs;system development
	cycle;task structure;user interface;programming environments;software
	reusability;software tools;user interfaces;}
}

@ARTICLE{56448,
  author = {Vo, K.-P.},
  title = {IFS: a tool to build application systems},
  journal = {Software, IEEE},
  year = {1990},
  volume = {7},
  pages = {29 -36},
  number = {4},
  month = {jul},
  abstract = {The interpretive frame system (IFS), a tool for building application
	systems, is presented. IFS separates high-level design and user-interface
	programming from domain-specific programming. It offers a language
	suitable for implementing systems of interconnected tasks, simplifies
	the construction of sophisticated but easy-to-use user interfaces,
	and increases tool reuse in system construction. A system built with
	IFs is called a frame system, and it consists of four layers-the
	user interface, system structure, computational functions, and data
	architecture-which are described. The IFS language and programming
	environment are also described. A programming example is given. Reuse
	and prototyping are discussed},
  doi = {10.1109/52.56448},
  issn = {0740-7459},
  keywords = {IFS;IFS language;application systems;computational functions;data
	architecture;domain-specific programming;frame system;high-level
	design;interconnected tasks;interpretive frame system;programming
	environment;prototyping;system construction;system structure;tool
	reuse;user-interface programming;high level languages;programming
	environments;software engineering;software reusability;software tools;}
}

@ARTICLE{5204065,
  author = {Voelter, M},
  title = {Architecture As Language},
  journal = {Software, IEEE},
  year = {2009},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {This article showcases and explains the use of domain-specific languages
	to express software architectures. Specifically, I report about project
	experiences where we created system-specific textual DSLs that were
	used to describe the system in a tool-processable way, to reason
	about the system, and to generate the majority of technical implementation
	code. The article starts off with a quick look at software architecture
	today and introduces the concept of architecture DSLs. Then follows
	an extensive example that shows an example language and how it evolved
	together with the architecture it describes. The next major section
	looks at the benefits of the approach, discusses why we used textual
	languages as opposed to graphical ones, and discusses issues around
	tooling, model validation, code generation, and the importance of
	standard modeling languages. The article concludes by taking a brief
	look at some of the challenges of the approach.},
  doi = {10.1109/MS.2009.110},
  issn = {0740-7459}
}

@ARTICLE{5232796,
  author = {Voelter, M},
  title = {Architecture As Language},
  journal = {Software, IEEE},
  year = {2009},
  volume = {PP},
  pages = {1},
  number = {99},
  month = { },
  abstract = {This article showcases and explains the use of domain-specific languages
	to express software architectures. Specifically, I report about project
	experiences where we created system-specific textual DSLs that were
	used to describe the system in a tool-processable way, to reason
	about the system, and to generate the majority of technical implementation
	code. The article starts off with a quick look at software architecture
	today and introduces the concept of architecture DSLs. Then follows
	an extensive example that shows an example language and how it evolved
	together with the architecture it describes. The next major section
	looks at the benefits of the approach, discusses why we used textual
	languages as opposed to graphical ones, and discusses issues around
	tooling, model validation, code generation, and the importance of
	standard modeling languages. The article concludes by taking a brief
	look at some of the challenges of the approach.},
  doi = {10.1109/MS.2009.149},
  issn = {0740-7459}
}

@INPROCEEDINGS{6030048,
  author = {Voelter, M. and Visser, E.},
  title = {Product Line Engineering Using Domain-Specific Languages},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {70 -79},
  month = {aug.},
  abstract = {This paper investigates the application of domain-specific languages
	in product line engineering (PLE). We start by analyzing the limits
	of expressivity of feature models. Feature models correspond to context-free
	grammars without recursion, which prevents the expression of multiple
	instances and references. We then show how domain-specific languages
	(DSLs) can serve as a middle ground between feature modeling and
	programming. They can be used in cases where feature models are too
	limited, while keeping the separation between problem space and solution
	space provided by feature models. We then categorize useful combinations
	between configuration with feature model and construction with DSLs
	and provide an integration of DSLs into the conceptual framework
	of PLE. Finally we show how use of a consistent, unified formalism
	for models, code, and configuration can yield important benefits
	for managing variability and trace ability. We illustrate the concepts
	with several examples from industrial case studies.},
  doi = {10.1109/SPLC.2011.25},
  keywords = {context free grammar;domain specific language;feature model;feature
	modeling;feature programming;product line engineering;context-free
	grammars;high level languages;production engineering computing;}
}

@INPROCEEDINGS{6030088,
  author = {Volter, M.},
  title = {DSLs for Product Lines: Approaches, Tools, Experiences},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {353},
  month = {aug.},
  abstract = {Domain-Specific Languages are languages narrowly focused on a particular
	problem domain. Compared to feature models they are more expressive,
	but possibly not as easy to use. Compared to source code, they are
	usually simpler, more targeted and hence easier to use - although
	not quite as flexible. DSLs can play an important role in PLE, filling
	the gap between configuration via feature models and low-level programming.
	This tutorial covers approaches, tools and experiences of using DSLs
	in PLE. I will start with briefly introducing DSLs and their role
	in PLE. We will then look at a real-world DSL that has been used
	to describe a product line of electrical devices. We will then spend
	some time extending a DSL built with Eclipse Xtext to get some hands-on
	tool experience. Then we will look at another DSL, one for robot
	control, that integrates nicely with feature models. We will then
	finally spend some time extending a DSL build with JetBrains MPS,
	before we wrap up the tutorial.},
  doi = {10.1109/SPLC.2011.58},
  keywords = {DSL;Eclipse Xtext;JetBrains MPS;PLE;domain specific language;electrical
	device product line;feature model;low-level programming;product line;robot
	control;software development management;software tools;specification
	languages;}
}

@ARTICLE{5420798,
  author = {Volter, M.},
  title = {Architecture as Language},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {56 -64},
  number = {2},
  month = {march-april },
  abstract = {In this paper, software architects can develop domain specific languages
	that express the desired architecture during the definition process
	and use them to describe systems based on the architecture.},
  doi = {10.1109/MS.2010.38},
  issn = {0740-7459},
  keywords = {definition process;domain specific languages;software architecture;software
	architecture;specification languages;}
}

@INPROCEEDINGS{5967191,
  author = {Vorobyeva, O.P. and Cherkashin, E.A. and Ipatov, S.A. and Paramonov,
	V.V.},
  title = {Information modeling of business processes in X-ray fluorescent analysis},
  booktitle = {MIPRO, 2011 Proceedings of the 34th International Convention},
  year = {2011},
  pages = {941 -944},
  month = {may},
  abstract = {The article is devoted to the formalization of investigation processes
	of substances by means of the X-ray fluorescence analysis (XRF).
	The research is aimed at solving the problem of automation of the
	analytical investigations in the XRF, which suppose to increases
	the productivity of the analyst and the accuracy of determining the
	concentration of elements in samples. Decomposition of information
	processes has been represented in the IDEF0 standard as a hierarchy
	of the core activities. Based on the decomposition the information
	model of the domain is developed. The model is represented as UML-diagrams
	adapted to MDA (Model Driven Architecture) based generative programming
	tools. These tools allow one to transform the model automatically
	into a skeleton of information system for XRF techniques automation.},
  keywords = {IDEFO standard;MDA;UML-diagram;X-ray fluorescent analysis;XRF;XRF
	technique automation;business process information modeling;concentration
	determination;generative programming tools;information process decomposition;model
	driven architecture;Unified Modeling Language;X-ray fluorescence
	analysis;chemistry computing;software architecture;}
}

@INPROCEEDINGS{4907655,
  author = {Voronenko, Y. and de Mesmay, F. and Puschel, M.},
  title = {Computer Generation of General Size Linear Transform Libraries},
  booktitle = {Code Generation and Optimization, 2009. CGO 2009. International Symposium
	on},
  year = {2009},
  pages = {102 -113},
  month = {march},
  abstract = {The development of high-performance libraries has become extraordinarily
	difficult due to multiple processor cores, vector instruction sets,
	and deep memory hierarchies. Often, the library has to be reimplemented
	and reoptimized, when a new platform is released. In this paper we
	show how to automatically generate general input-size libraries for
	the domain of linear transforms. The input to our generator is a
	formal specification of the transform and the recursive algorithms
	the library should use; the output is a library that supports general
	input size, is vectorized and multithreaded, provides an adaptation
	mechanism for the memory hierarchy, and has excellent performance,
	comparable to or better than the best human-written libraries. Further,
	we show that our library generator enables various customizations;
	one example is the generation of Java libraries.},
  doi = {10.1109/CGO.2009.33},
  keywords = {Java libraries;computer generation;deep memory hierarchies;formal
	specification;general size linear transform libraries;human-written
	libraries;multiple processor cores;vector instruction sets;Java;software
	libraries;transforms;}
}

@INPROCEEDINGS{4020194,
  author = {Wada, H. and Suzuki, J.},
  title = {A Domain Specific Modeling Framework for Secure Network Applications},
  booktitle = {Computer Software and Applications Conference, 2006. COMPSAC '06.
	30th Annual International},
  year = {2006},
  volume = {2},
  pages = {353 -355},
  month = {sept.},
  abstract = {Domain specific languages (DSLs) provide a promising solution to directly
	represent and implement domain concepts (G. Cook, 2004). DSLs are
	visual or textual languages targeted to particular problem domains,
	rather than general-purpose languages that are aimed at any software
	problems. Various DSLs have been proposed and used for describing,
	for example, security aspects of network applications (e.g., role-based
	access control, data encryption and secure network links) (T. Lodderstedt
	et al., 2002). Although many experience reports have demonstrated
	DSLs can improve software development productivity (e.g., by M. Vokac,
	2005), existing DSLs are supported only by specific tools and frameworks;
	there are few generic frameworks supporting arbitrary DSLs. This
	Ph.D. research investigates a generic model-driven development (MDD)
	framework that supports arbitrary DSLs, and empirically evaluates
	a series of techniques to develop such a framework. Steps towards
	creating the proposed framework include investigating a generic foundation
	to handle arbitrary DSLs; strategies, principles and tradeoffs in
	different DSL designs (e.g., DSL syntax and semantics); building
	blocks for modeling and programming domain concepts; transformation
	strategies from domain concepts to the final (compilable) source
	code; and development processes to leverage the proposed framework
	well},
  doi = {10.1109/COMPSAC.2006.93},
  issn = {0730-3157},
  keywords = {domain concept modeling;domain concept programming;domain specific
	languages;domain specific modeling;general-purpose languages;model-driven
	development;network application security aspects;secure network applications;software
	development productivity;software problems;textual languages;visual
	languages;specification languages;visual languages;}
}

@INPROCEEDINGS{1284047,
  author = {Wagelaar, D.},
  title = {Towards a context-driven development framework for ambient intelligence},
  booktitle = {Distributed Computing Systems Workshops, 2004. Proceedings. 24th
	International Conference on},
  year = {2004},
  pages = { 304 - 309},
  month = {march},
  abstract = { Portable and embedded devices form an increasingly large group of
	computers, often referred to as ambient intelligence (AmI). This
	new variety in computing platforms cause a corresponding diversity
	in software/hardware platforms and other context factors. Component-based
	middleware platforms offer a uniform environment for software, but
	they do not take away specific context differences, such as hardware
	resources, user identity/role and logical/physical location. Specialised
	component versions and/or configurations have to be made for each
	computing context if that computing context is to be used to its
	full extent. This is because the fine differences between component
	versions cannot be separated into finer components with the current
	component models. Aspect-oriented programming and generative programming
	technologies can be used to provide the fine-grained modularity that
	is necessary. In addition, the diversity of component-based platforms
	themselves form an extra reason for different component versions.
	We propose using a context-driven framework for the development of
	AmI components, which is based upon a gradual refinement mechanism.
	This refinement mechanism can cope with the course-grained differences
	between component models as well as the fine-grained differences
	between computing configurations.},
  doi = {10.1109/ICDCSW.2004.1284047},
  issn = { },
  keywords = { ambient intelligence; aspect-oriented programming; component version;
	component-based middleware platform; computing platform; context-driven
	development framework; course-grained difference; embedded device;
	fine-grained difference; generative programming technology; hardware
	resource; refinement mechanism; user identity; middleware; object-oriented
	methods; object-oriented programming;}
}

@INPROCEEDINGS{4076903,
  author = {D'Arcy Walsh and Francis Bordeleau and Bran Selic},
  title = {A Constrained Executable Model of Dynamic System Reconfiguration},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {257c},
  month = {jan. },
  abstract = {Explicit metaclass programming techniques are presented that enable
	domain-specific objects to dynamically change their run-time properties.
	The domain-specific objects are instantiations of a domain model
	of dynamic system reconfiguration. The domain model is the product
	of a model-based domain analysis that identified a set of concepts
	that reflect the types of reconfigurations possible and the system
	integrity characteristics that must be maintained during such reconfigurations.
	It is expressed using the Unified Modeling Language (UML) as a constrained
	representation of the domain-level specification and then realized
	as an executable model using a programming environment that supports
	explicit metaclass programming},
  doi = {10.1109/HICSS.2007.11},
  issn = {1530-1605},
  keywords = {Unified Modeling Language;domain-level specification;domain-specific
	object;dynamic system reconfiguration;metaclass programming technique;model-based
	domain analysis;programming environment;Unified Modeling Language;object-oriented
	programming;programming environments;}
}

@ARTICLE{5551015,
  author = {Wampler, Dean and Clark, Tony},
  title = {Guest Editors' Introduction: Multiparadigm Programming},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {20 -24},
  number = {5},
  month = {sept.-oct. },
  abstract = {The guest editors of this special issue on multiparadigm programming
	explore the field's recent explosive growth and how the articles
	(plus email roundtable) they've selected for this issue exemplify
	its current status.},
  doi = {10.1109/MS.2010.119},
  issn = {0740-7459}
}

@ARTICLE{5551016,
  author = {Wampler, D. and Clark, T. and Ford, N. and Goetz, B.},
  title = {Multiparadigm Programming in Industry: A Discussion with Neal Ford
	and Brian Goetz},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {61 -64},
  number = {5},
  month = {sept.-oct. },
  abstract = {Using multiparadigm programming (MPP) has costs as well as benefits.
	Over email, guest editors Dean Wampler and Tony Clark discussed with
	Neal Ford and Brian Goetz the practical issues for MPP in industrial
	software development teams. What follows is a transcript.},
  doi = {10.1109/MS.2010.121},
  issn = {0740-7459},
  keywords = {industrial software development team;multiparadigm programming;software
	development management;}
}

@INPROCEEDINGS{4666072,
  author = {Miao Wan and Song Liu and Jian-Yi Liu and Cong Wang},
  title = {Automatic Technical Term Extraction Based on Term Association},
  booktitle = {Fuzzy Systems and Knowledge Discovery, 2008. FSKD '08. Fifth International
	Conference on},
  year = {2008},
  volume = {2},
  pages = {19 -23},
  month = {oct.},
  abstract = {This paper proposes a new automatic Chinese term extracting algorithm
	combining both statistics-based and rule-based methods. This algorithm
	firstly uses a statistical method to extract two-word candidates
	from raw corpus, and then extends these candidates forward to obtain
	multi-word candidate terms. We propose a new metric named term association
	(TA) that can measure the combining degree between words in a string
	very well. In the second subsystem it filters these candidates to
	get domain-specific technical terms based on defined rules. Our purpose
	is to achieve a higher precision of the domain-specific Chinese term
	extraction task by the hybrid method than the previous approaches.
	This algorithm implements an extractor with an unprocessed corpus
	as input for technical papers of ethanol fuels. The results of experiments
	are analyzed and evaluated, and the precision and recall are 84.26%
	and 63.86% respectively.},
  doi = {10.1109/FSKD.2008.40},
  keywords = {Chinese term extracting algorithm;automatic technical term extraction;rule-based
	methods;statistics-based methods;term association;data mining;statistical
	analysis;}
}

@INPROCEEDINGS{5521517,
  author = {Wang, D.S.},
  title = {A Domain-Specific Question Answering System Based on Ontology and
	Question Templates},
  booktitle = {Software Engineering Artificial Intelligence Networking and Parallel/Distributed
	Computing (SNPD), 2010 11th ACIS International Conference on},
  year = {2010},
  pages = {151 -156},
  month = {june},
  abstract = {The rapid growth in the development of Internet-based information
	systems increases the demand for natural language interfaces that
	are easy to set up and maintain. Unfortunately, the problem of deep
	understanding natural language queries is far from being solved.
	In this paper, an automated question answering system based on domain
	ontology and question template is proposed, which does not need deep
	understanding of user's questions. Ontology was employed to model
	the interesting domain, and properties of concepts are described
	by a collection of question templates. The system captures a user's
	intention by matching his questions to predefined templates, and
	return answers corresponding to template's query focus. The system
	has been applied to mobile service consulting area, and experimental
	results show that the system has high practicability as well as maintainability.},
  doi = {10.1109/SNPD.2010.31},
  keywords = {Internet-based information systems;automated question answering system;domain
	ontology;domain-specific question answering system;mobile service
	consulting area;natural language interfaces;natural language query;question
	template query;Internet;natural language interfaces;ontologies (artificial
	intelligence);query processing;}
}

@INPROCEEDINGS{1342502,
  author = {Wang, G. and Chen, A. and Wang, C. and Fung, C. and Uczekaj, S.},
  title = {Integrated quality of service (QoS) management in service-oriented
	enterprise architectures},
  booktitle = {Enterprise Distributed Object Computing Conference, 2004. EDOC 2004.
	Proceedings. Eighth IEEE International},
  year = {2004},
  pages = { 21 - 32},
  month = {sept.},
  abstract = { One of the significant challenges for making service-oriented architectures
	(SOA) effective for enterprise systems is quality of service (QoS)
	management because of the dynamic, flexible, and compositional nature
	of SOA. QoS management must be integrated into service-oriented enterprise
	architectures. It must support a set of common QoS characteristics
	and provide comprehensive QoS services end to end, from application,
	to middleware, and to network and from source hosts to destination
	hosts across a network. We describe such an integrated QoS management
	architecture and its services. We classify QoS characteristics into
	four categories and each of which is decomposed into a set of measurable
	attributes. We integrate these characteristics into an XML-based
	language for applications and QoS providers to express QoS requirements
	and contracts. We model an integrated QoS management architecture
	based on standard specifications from organizations like ISO and
	OMG. We implement a comprehensive set of QoS management services
	with innovation resource management techniques and adaptation mechanisms.
	We provide test data to validate our architecture and solution first
	in a publish/subscribe style of enterprise SOA. In comparison with
	other work in QoS management, our architecture and solution provide
	innovative techniques, extensions, and generalizations beyond traditional
	task-oriented QoS management in object-oriented middleware and domain
	specific applications.},
  doi = {10.1109/EDOC.2004.1342502},
  issn = {1541-7719 },
  keywords = { XML-based language; innovation resource management; object-oriented
	middleware; quality of service management; service-oriented enterprise
	architecture; task-oriented management; XML; distributed object management;
	innovation management; middleware; object-oriented programming; open
	systems; organisational aspects; quality of service;}
}

@INPROCEEDINGS{1336107,
  author = {Jenq-Haur Wang and Jei-Wen Teng and Pu-Jen Cheng and Wen-Hsiang Lu
	and Lee-Feng Chien},
  title = {Translating unknown cross-lingual queries in digital libraries using
	a Web-based approach},
  booktitle = {Digital Libraries, 2004. Proceedings of the 2004 Joint ACM/IEEE Conference
	on},
  year = {2004},
  pages = { 108 - 116},
  month = {june},
  abstract = { Users' cross-lingual queries to a digital library system might be
	short and not included in a common translation dictionary (unknown
	terms). In this paper, we investigate the feasibility of exploiting
	the Web as the corpus source to translate unknown query terms for
	cross-language information retrieval (CLIR) in digital libraries.
	We propose a Web-based term translation approach to determine effective
	translations for unknown query terms by mining bilingual search-result
	pages obtained from a real Web search engine. This approach can enhance
	the construction of a domain-specific bilingual lexicon and benefit
	CLIR services in a digital library that only has monolingual document
	collections. Very promising results have been obtained in generating
	effective translation equivalents for many unknown terms, including
	proper nouns, technical terms and Web query terms.},
  doi = {10.1109/JCDL.2004.1336107},
  issn = { },
  keywords = { Web search engine; Web-based approach; bilingual search-result page
	mining; cross-language information retrieval; digital library; domain-specific
	bilingual lexicon; monolingual document collection; translation dictionary;
	user cross-lingual query; Internet; digital libraries; query processing;
	search engines;}
}

@INPROCEEDINGS{6057527,
  author = {Wang, Liming and Wang, Guonv and Wang, Wei and Zhan, Xiaolu and Liu,
	Xiyang and Chen, Ping},
  title = {MBD-DSP: A model based design solution for DSP},
  booktitle = {Electrical and Control Engineering (ICECE), 2011 International Conference
	on},
  year = {2011},
  pages = {4561 -4564},
  month = {sept.},
  abstract = {The general process of DSP software development maybe like this: i)
	create and simulate models in Simulink or other tools; ii) write
	C or assembly code in VisualDSP #x002B; #x002B;, then generate executable
	files. In this development mode, the models are used to verify the
	design, the needed executable files have nothing to do with these
	models. So the developers suffer from the extra burden, which leads
	the low efficiency. Besides, some modeling tools do support code
	generation from created models, but the generated code is difficult
	to understand, this is because they are general purpose tools. With
	the above problems in mind, MBD-DSP is presented in this paper. MBD-DSP
	is not used to develop general purpose applications. Instead, we
	focus on domain specific development, especially on radar development.},
  doi = {10.1109/ICECENG.2011.6057527}
}

@INPROCEEDINGS{5968061,
  author = {Mingwei Wang and Shan Li and Jingtao Zhou},
  title = {An Ontology Model for Manufacturing Grid Service by Extending OWL-S},
  booktitle = {Future Computer Sciences and Application (ICFCSA), 2011 International
	Conference on},
  year = {2011},
  pages = {212 -215},
  month = {june},
  abstract = {Manufacturing grid is a new pattern developed to answer unprecedented
	challenges in manufacturing industry, one of the most important foundations
	of which is encapsulating or modeling manufacturing resources into
	grid service. But this kind of grid service is restricted by ambiguous
	and insufficient service description. This paper presents an ontology
	model, called M-service profile ontology, which provides the framework
	for explicitly encoding semantics of domain-specific engineering
	information. The model is generated by extending OWL-S with two classes.
	One is ManufacturingProfile class which provides formal engineering
	indexes needed during collaborative work. Anther is the QoSProfile
	class which provides a mechanism to represent quality of service.
	This ontology model enriches semantic descriptions of grid service
	which will be very conducive for later service discovery and management.},
  doi = {10.1109/ICFCSA.2011.55},
  keywords = {Anther;M-service profile ontology;OWL-S;QoSprofile class;Web service;agile
	manufacturing;domain-specific engineering information;manufacturing
	grid service;manufacturing industry;manufacturingprofile class;ontology
	model;quality of service;service-oriented architecture;Web services;agile
	manufacturing;grid computing;knowledge representation languages;ontologies
	(artificial intelligence);production engineering computing;quality
	of service;service-oriented architecture;}
}

@INPROCEEDINGS{1517870,
  author = {Ting Wang and Maynard, D. and Peters, W. and Bontcheva, K. and Cunningham,
	H.},
  title = {Extracting a domain ontology from linguistic resource based on relatedness
	measurements},
  booktitle = {Web Intelligence, 2005. Proceedings. The 2005 IEEE/WIC/ACM International
	Conference on},
  year = {2005},
  pages = { 345 - 351},
  month = {sept.},
  abstract = { Creating domain-specific ontologies is one of the main bottlenecks
	in the development of the semantic Web. Learning an ontology from
	linguistic resources is helpful to reduce the costs of ontology creation.
	In this paper, we describe a method to extract the most related concepts
	from HowNet, a Chinese-English bilingual knowledge dictionary, in
	order to create a customized ontology for a particular domain. We
	introduce a new method to measure relatedness (rather than similarity
	between concepts), which overcomes some of the traditional problems
	associated with similar concepts being far apart in the hierarchy.
	Experiments show encouraging results.},
  doi = {10.1109/WI.2005.63},
  keywords = { Chinese-English bilingual knowledge dictionary; HowNet; domain ontology;
	linguistic resource; relatedness measurement; semantic Web; computational
	linguistics; dictionaries; natural languages; ontologies (artificial
	intelligence);}
}

@INPROCEEDINGS{4666069,
  author = {Wei Wang and Kunhui Lin and Changle Zhou},
  title = {A Hybrid Statistical Language Model Applied to the Domain Specific
	Information Retrieval},
  booktitle = {Fuzzy Systems and Knowledge Discovery, 2008. FSKD '08. Fifth International
	Conference on},
  year = {2008},
  volume = {2},
  pages = {3 -7},
  month = {oct.},
  abstract = {The traditional language model takes the multi-topics document corpus
	as the research target. In order to avoid the interference brought
	by the multi-topics problem, this paper focuses on the domain specific
	Information Retrieval (IR). In domain specific IR, different terms
	are considered to take different contribution degrees to the final
	query result. So the terms in a document can be divided into different
	categories according to their contribution degrees. And the statistical
	information of a term, mainly its probabilities, is computed by different
	methods and smooth strategies according to its category. This paper
	proposed an improved hybrid statistical language model used in the
	Domain Specific IR. This new model has about 9%~10% performance increment
	in the experimental result. In the end, some challenges and research
	orientation of the statistical language model research are presented.},
  doi = {10.1109/FSKD.2008.240},
  keywords = {domain specific information retrieval;multitopics document corpus;probability;query
	language;smooth strategy;statistical information;statistical language
	model;information retrieval;probability;query languages;statistical
	analysis;}
}

@INPROCEEDINGS{5953654,
  author = {Yan Wang and Gaspes, V.},
  title = {A compositional implementation of Modbus in Protege},
  booktitle = {Industrial Embedded Systems (SIES), 2011 6th IEEE International Symposium
	on},
  year = {2011},
  pages = {123 -131},
  month = {june},
  abstract = {Network protocols today play a major role in embedded software for
	industrial automation, with constant efforts to adapt existing device
	software to new emerging standards. In earlier work, we have proposed
	a compilation-based approach using a domain-specific language, Protege,
	which automatically generates protocol stack implementations in C
	from modular high-level descriptions. In this paper, we provide a
	case study of the Protege language in an industrial setting. We have
	implemented the Modbus protocol over TCP/IP and over serial line,
	and tested it using an industrial gateway. Our implementation demonstrates
	Protege's advantages for software productivity, easy maintenance
	and code reuse, and it achieves many desirable properties of industrial
	embedded network software.},
  doi = {10.1109/SIES.2011.5953654},
  keywords = {C language;Modbus protocol;Protege language;TCP/IP;compilation-based
	approach;domain-specific language;industrial automation;industrial
	embedded network software;industrial gateway;modular high-level descriptions;network
	protocols;software productivity;factory automation;field buses;internetworking;network
	interfaces;protocols;transport protocols;}
}

@INPROCEEDINGS{5254252,
  author = {Yingbo Wang and Yali Wu and Allen, A. and Espinoza, B. and Clarke,
	P.J. and Yi Deng},
  title = {Towards the Operational Semantics of User-Centric Communication Models},
  booktitle = {Computer Software and Applications Conference, 2009. COMPSAC '09.
	33rd Annual IEEE International},
  year = {2009},
  volume = {1},
  pages = {254 -262},
  month = {july},
  abstract = {The pervasiveness of complex communication services and the need for
	end-users to play a greater role in developing communication services
	have resulted in the creation of the Communication Virtual Machine
	(CVM) technology. The CVM technology consists of a Communication
	Modeling Language (CML) and the CVM. CML is a declarative modeling
	language that can be used to specify domain-specific communication
	services and the CVM is the platform used to realize the CML models.In
	this paper we explicitly define the operational semantics of CML
	to support (1) the synthesis of CML models into executable control
	scripts and (2) the handling of negotiation and media transfer events
	during communication. We specify the semantics of CML using label
	transition systems and describe in detail an algorithm that is essential
	for the interpretation of CML models. A case study is presented showing
	how the semantics support the rapid realization of a scenario from
	the healthcare domain.},
  doi = {10.1109/COMPSAC.2009.41},
  issn = {0730-3157},
  keywords = {communication modeling language;communication virtual machine;complex
	communication services;declarative modeling language;executable control
	scripts;health care domain;label transition systems;operational semantics;user-centric
	communication model;specification languages;ubiquitous computing;}
}

@INPROCEEDINGS{5622538,
  author = {Yujuan Wang and Li Zhou and QiuHua Zheng and Zhen Zhang and Guohua
	Wu},
  title = {An approach of code generation based on Model Integrated Computing},
  booktitle = {Computer Application and System Modeling (ICCASM), 2010 International
	Conference on},
  year = {2010},
  volume = {15},
  pages = {V15-114 -V15-117},
  month = {oct.},
  abstract = {Model Integrated Computing (MIC) is a theory of domain-specific modeling,
	using meta-model as a domain-specific modeling language (DSML), and
	constructing a component library for domain model. DSML abstracts
	the commonness and individuality in a domain, the developer employs
	it to construct domain model to represent the system. The final cross-platform
	code is automatically generated by code interpreter. In this paper,
	we propose an approach for designing code interpreter, it can transform
	domain model into a Platform Independent Model (PIM), and parsing
	PIM through code template, finally generating the code. This approach
	can shorten development time, promote reusability of code, improve
	work efficiency, and accomplish system rapidly.},
  doi = {10.1109/ICCASM.2010.5622538},
  keywords = {code generation;code interpreter;code reusability;cross platform code
	template;domain model construction;domain specific modeling language;domain
	specific modeling theory;metamodel;model integrated computing;platform
	independent model;metacomputing;program compilers;program interpreters;simulation
	languages;}
}

@INPROCEEDINGS{1034645,
  author = {Ye-Yi Wang and Acero, A.},
  title = {Grammar learning for spoken language understanding},
  booktitle = {Automatic Speech Recognition and Understanding, 2001. ASRU '01. IEEE
	Workshop on},
  year = {2001},
  pages = { 292 - 295},
  abstract = { Many state-of-the-art conversational systems use semantic-based robust
	understanding and manually derived grammars, a very time-consuming
	and error-prone process. This paper describes a machine-aided grammar
	authoring system that enables a programmer to develop rapidly a high
	quality grammar for conversational systems. This is achieved with
	a combination of domain-specific semantics, a library grammar, syntactic
	constraints and a small number of example sentences that have been
	semantically annotated. Our experiments show that the learned semantic
	grammars consistently outperform manually authored grammars, requiring
	much less authoring load.},
  doi = {10.1109/ASRU.2001.1034645},
  issn = { },
  keywords = { context free grammar; conversational systems; grammar learning; machine-aided
	grammar authoring; natural language understanding; semantic-based
	understanding; spoken language understanding; syntactic constraints;
	context-free grammars; learning (artificial intelligence); natural
	language interfaces; speech recognition;}
}

@INPROCEEDINGS{5765205,
  author = {Zhixue Wang and Qingchao Dong and Hongyue He and Fengke Fu},
  title = {Imprecise domain-specific modeling for C4ISR capability requirements
	analysis},
  booktitle = {Information Science and Technology (ICIST), 2011 International Conference
	on},
  year = {2011},
  pages = {33 -38},
  month = {march},
  abstract = {The paper proposes an approach to model the uncertain and vague requirements
	information of the C4ISR capability requirements. It suggests that
	C4ISR requirements elicitation be initiated with capability meta
	concept framework (CMCF) construction according to the Meta-Model
	of DoDAF. With the semantic confinement of CMCF, the approach extends
	the Fuzzy-UML and declares a C4ISR domain-specific modeling language
	(C4ISR DSL). Compared with classic UML and Fuzzy-UML, the C4ISR DSL
	is more specialized in C4ISR domain knowledge elicitation and reuse.
	The experiment shows the modeling language can better express both
	precise and vague concepts of the C4ISR capability model.},
  doi = {10.1109/ICIST.2011.5765205},
  keywords = {C4ISR DSL;C4ISR capability requirements analysis;C4ISR domain-specific
	modeling language;CMCF;DoDAF;capability meta concept framework;domain
	knowledge elicitation;fuzzy-UML;meta model;unified modeling language;Unified
	Modeling Language;fuzzy set theory;military computing;software architecture;systems
	analysis;}
}

@INPROCEEDINGS{4641445,
  author = {Zhonglei Wang and Herkersdorf, A. and Merenda, S. and Tautschnig,
	M.},
  title = {A model driven development approach for implementing reactive systems
	in hardware},
  booktitle = {Specification, Verification and Design Languages, 2008. FDL 2008.
	Forum on},
  year = {2008},
  pages = {197 -202},
  month = {sept.},
  abstract = {To deal with the increasing complexity of digital systems, the model
	driven development approach has proven to be beneficial. This paper
	presents a model driven hardware design process that is dedicated
	to reactive embedded systems. The approach is based on the component
	language (COLA), a synchronous data flow language with formal semantics.
	COLA follows the hypothesis of perfect synchrony. Models thus do
	not assume specific timing properties and remain deterministic as
	long as data flow requirements are retained. This is an essential
	feature for modeling safety-critical systems. Further, the well-defined
	semantics not only allows that the resulting models can be formally
	reasoned about, but is also the key to translation to domain-specific
	languages. This paper describes the approach of translating the models
	to VHDL descriptions from their graphical representations. As COLA
	is well-adapted to both data flow description and control automata,
	the generated VHDL code can be synthesized to very efficient FPGA
	circuits, comparable to that synthesized from hand-written VHDL code
	according to our case study.},
  doi = {10.1109/FDL.2008.4641445},
  keywords = {FPGA circuits;VHDL descriptions;component language;data flow requirements;domain-specific
	languages;formal semantics;model driven hardware design process;reactive
	embedded systems;safety-critical systems;synchronous data flow language;data
	flow analysis;embedded systems;hardware description languages;parallel
	languages;safety-critical software;}
}

@INPROCEEDINGS{1318500,
  author = {Zhong-Hua Wang},
  title = {Name entity recognition using language models},
  booktitle = {Automatic Speech Recognition and Understanding, 2003. ASRU '03. 2003
	IEEE Workshop on},
  year = {2003},
  pages = { 554 - 559},
  month = {nov.-3 dec.},
  abstract = { The paper presents a new statistical name entity recognition algorithm,
	which does not require the collection and manual annotation of domain-specific
	sentences to train the models. The models of the name entities are
	domain-independent and could be directly applied to other domains
	of applications. This technique can also be applied to decode a set
	of raw sentences iteratively, if available, and use the decoded output
	to improve the statistical models. Applied to the mutual fund trading
	application, this new technique achieves a performance comparable
	to that using the decision tree model, which is trained from an annotated
	corpus. Iterative decoding of a set of natural language utterances
	and training of the general language model decreases the sentence
	error rate by 11%.},
  doi = {10.1109/ASRU.2003.1318500},
  keywords = { Markov chain; Viterbi algorithm; decision tree model; domain-specific
	sentences; iterative decoding; language models; mutual fund trading;
	name entity recognition; natural language utterances; statistical
	algorithm; Markov processes; Viterbi decoding; iterative decoding;
	learning (artificial intelligence); natural languages; speech recognition;
	statistical analysis;}
}

@INPROCEEDINGS{1510025,
  author = {Ward, M. and Zedan, H.},
  title = {MetaWSL and meta-transformations in the FermaT transformation system},
  booktitle = {Computer Software and Applications Conference, 2005. COMPSAC 2005.
	29th Annual International},
  year = {2005},
  volume = {1},
  pages = { 233 - 238 Vol. 2},
  month = {july},
  abstract = { A program transformation is an operation which can be applied to
	any program (satisfying the transformations applicability conditions)
	and returns a semantically equivalent program. In the FermaT transformation
	system program transformations are carried out in a wide spectrum
	language, called WSL, and the transformations themselves are written
	in an extension of WSL called MetaWSL which was specifically designed
	to be a domain-specific language for writing program transformations.
	As a result, FermaT is capable of transforming its own source code
	via meta-transformations. This paper introduces MetaWSL and describes
	some applications of meta-transformations in the FermaT system.},
  doi = {10.1109/COMPSAC.2005.107},
  issn = {0730-3157},
  keywords = { FermaT transformation system; MetaWSL; WSL; domain-specific language;
	metatransformation; program transformation; semantically equivalent
	program; source code transformation; formal specification; programming
	language semantics; specification languages;}
}

@INPROCEEDINGS{117798,
  author = {Wauchope, K.},
  title = {Structural domain modeling for understanding equipment failure messages},
  booktitle = {Southeastcon '90. Proceedings., IEEE},
  year = {1990},
  pages = {188 -193 vol.1},
  month = {apr},
  abstract = {Demonstrates that domain-specific knowledge is required to recover
	implicit references to causality from narrative texts that describe
	equipment failures. Understanding texts that discuss complex pieces
	of equipment requires the possession not only of general knowledge
	about the types of objects and predicates in the domain, but also
	detailed knowledge about the particular equipment in question. This
	more expert level of knowledge is needed to dereference the names
	and descriptions of equipment referred to in the text and to infer
	their causal relationships and operational states when these are
	only implicitly expressed by the message writer. Knowing the structural
	configuration of the equipment is useful in both tasks, and a structural
	domain model can be extracted readily from equipment manuals and
	their accompanying parts lists, thus easing the knowledge acquisition
	bottleneck problem and making practical applications more feasible},
  doi = {10.1109/SECON.1990.117798},
  keywords = {causal relationships;causality;description dereferencing;domain-specific
	knowledge;equipment failure messages;equipment manuals;implicit references;inference;knowledge
	acquisition bottleneck;name dereferencing;narrative texts;operational
	states;parts lists;structural configuration;structural domain modelling;text
	understanding;failure analysis;knowledge acquisition;knowledge based
	systems;mechanical engineering computing;natural languages;}
}

@INPROCEEDINGS{4639085,
  author = {Weaver, C.},
  title = {Coordinated queries: A domain specific language for exploratory development
	of multiview visualizations},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {197 -200},
  month = {sept.},
  abstract = {The development of information visualization user interfaces suffers
	from a persistent gap between toolkit-based programming and interactive
	design, between the visualization experts who build tools and the
	information experts who cultivate analytic utility. Coordinated queries
	is a declarative language for live design of information visualization
	user interfaces. The combination of a visualization specific language
	with an extensive library of views and queries gives designers flexible,
	yet precise control over the appearance and behavior of data in response
	to interaction. Through live, integrated editing of essentially infinite
	variations on well-known patterns of visual data representation and
	multiple view coordination, it is practical to iteratively grow and
	vary visualizations much more quickly than by traditional means,
	thereby facilitating open-ended visual exploration and analysis in
	a variety of domains.},
  doi = {10.1109/VLHCC.2008.4639085},
  issn = {1943-6092},
  keywords = {coordinated queries;information visualization user interface;multiple
	view coordination;multiview visualizations;visual data representation;data
	visualisation;user interfaces;}
}

@INPROCEEDINGS{4677370,
  author = {Weaver, C.},
  title = {Multidimensional visual analysis using cross-filtered views},
  booktitle = {Visual Analytics Science and Technology, 2008. VAST '08. IEEE Symposium
	on},
  year = {2008},
  pages = {163 -170},
  month = {oct.},
  abstract = {Analysis of multidimensional data often requires careful examination
	of relationships across dimensions. Coordinated multiple view approaches
	have become commonplace in visual analysis tools because they directly
	support expression of complex multidimensional queries using simple
	interactions. However, generating such tools remains difficult because
	of the need to map domain-specific data structures and semantics
	into the idiosyncratic combinations of interdependent data and visual
	abstractions needed to reveal particular patterns and distributions
	in cross-dimensional relationships. This paper describes: (1) a method
	for interactively expressing sequences of multidimensional set queries
	by cross-filtering data values across pairs of views, and (2) design
	strategies for constructing coordinated multiple view interfaces
	for cross-filtered visual analysis of multidimensional data sets.
	Using examples of cross-filtered visualizations of data from several
	different domains, we describe how cross-filtering can be modularized
	and reused across designs, flexibly customized with respect to data
	types across multiple dimensions, and incorporated into more wide-ranging
	multiple view designs. The demonstrated analytic utility of these
	examples suggest that cross-filtering is a suitable design pattern
	for instantiation in a wide variety of visual analysis tools.},
  doi = {10.1109/VAST.2008.4677370},
  keywords = {complex multidimensional set query expression;cross-filtered coordinated
	multiple view interface;data semantics;data visualization;design
	strategy;domain-specific data structure mapping;idiosyncratic combination;multidimensional
	visual data analysis tool;visual abstraction;data analysis;data structures;data
	visualisation;query processing;visual databases;}
}

@ARTICLE{1492372,
  author = {Weber, S. and Chan, H. and Degenaro, L. and Diament, J. and Fokoue-Nkoutche,
	A. and Rouvellou, I.},
  title = {Fusion: a system for business users to manage program variability},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2005},
  volume = {31},
  pages = { 570 - 587},
  number = {7},
  month = {july},
  abstract = { In order to make software components more flexible and reusable,
	it is desirable to provide business users with facilities to assemble
	and control them without their needing programming knowledge. This
	paper describes a fully functional prototype middleware system where
	variability is externalized so that core applications need not be
	altered for anticipated changes. In this system, application behavior
	modification is fast and easy, making this middleware suitable for
	frequently changing programs.},
  doi = {10.1109/TSE.2005.82},
  issn = {0098-5589},
  keywords = { Web site development tool; Web site management tool; business logic;
	human factors; middleware system; program variability management;
	software architecture; software component flexibility; software component
	reusability; software design; user interface; Internet; business
	data processing; middleware; object-oriented programming; software
	architecture; software development management; software reusability;
	user interfaces;}
}

@INPROCEEDINGS{5280146,
  author = {Tongming Wei and Ruisheng Zhang and Xianrong Su and Shilin Chen and
	Lian Li},
  title = {GaussianScriptEditor: An Editor for Gaussian Scripting Language for
	Grid Environment},
  booktitle = {Grid and Cooperative Computing, 2009. GCC '09. Eighth International
	Conference on},
  year = {2009},
  pages = {39 -44},
  month = {aug.},
  abstract = {More and more chemists carry out scientific research using computation.
	In this process, computational chemistry software has played a very
	important role. Among these computational chemistry software, Gaussian
	is very prominent. The most essential user interface of Gaussian
	is scripting language, but software-oriented scripting language is
	a great burden to chemists. Meanwhile, chemists are increasingly
	using grid environment to do scientific research. So, it is significant
	to build a user-friendly and chemist-oriented Gaussian scripting
	language editing environment in grid. In this paper, we introduce
	GaussianScriptEditor to solve these problems. GaussianScriptEditor
	is on the basis of grid platform and its implementation is related
	to domain specific language (DSL), knowledge in computational chemistry,
	technology of compiling, ANTLR and DLTK.},
  doi = {10.1109/GCC.2009.49},
  keywords = {ANTLR technology;DLTK technology;DSL;GaussianScriptEditor;chemist-oriented
	Gaussian scripting language editing environment;compiling technology;computational
	chemistry software;domain specific language;grid environment;scientific
	research;software-oriented scripting language;user interface;user-friendly
	application;authoring languages;chemistry computing;grid computing;human
	computer interaction;program compilers;programming environments;text
	editing;user interfaces;}
}

@INPROCEEDINGS{4291052,
  author = {Weigold, T. and Kramp, T. and Buhler, P.},
  title = {ePVM - An Embeddable Process Virtual Machine},
  booktitle = {Computer Software and Applications Conference, 2007. COMPSAC 2007.
	31st Annual International},
  year = {2007},
  volume = {1},
  pages = {557 -564},
  month = {july},
  abstract = {At the heart of every business process management system resides a
	workflow engine, here termed process execution engine. Yet despite
	playing such a central role, contemporary business process engines
	generally still leave much to be desired in terms of interoperability,
	versatility, and programmability. Therefore, this paper introduces
	ePVM, an embeddable process execution engine aimed at solving these
	issues. Basically, ePVM is built upon two core concepts. Firstly,
	an execution model which is deeply rooted in the theoretical framework
	of communicating state machines. Secondly, whereas many efforts have
	been made to create the ultimate process language, ePVM provides
	in contrast a low-level run-time environment based on a JavaScript
	interpreter where higher-level domain specific process definition
	languages can be mapped to. Our work explores both concepts in more
	detail and also positions ePVM in the current business process and
	workflow domain.},
  doi = {10.1109/COMPSAC.2007.110},
  issn = {0730-3157},
  keywords = {JavaScript interpreter;business process management system;definition
	language;embeddable process virtual machine;low-level run-time environment;process
	execution engine;workflow engine;Java;business data processing;virtual
	machines;workflow management software;}
}

@INPROCEEDINGS{5718965,
  author = {Weiss, B. and Winkelmann, A.},
  title = {A Metamodel Based Perspective on the Adaptation of a Semantic Business
	Process Modeling Language to the Financial Sector},
  booktitle = {System Sciences (HICSS), 2011 44th Hawaii International Conference
	on},
  year = {2011},
  pages = {1 -10},
  month = {jan.},
  abstract = {Process modeling is an important prerequisite to process reorganization
	and management. As a result, many companies have spent much effort
	on process documentation, while hardly gaining equivalent benefits
	in the analysis and usage of the resulting process models. To balance
	the cost-benefit-ratio of process modeling projects with respect
	to their later usage, especially regarding automatic process model
	analysis (i.e. for optimization purposes etc.), new domain-specific
	and thus semantic business process modeling languages (SBPML) have
	been proposed for selected domains. In this paper we investigate
	the adaptability of SBPML from the public sector to the banking sector
	from the perspective of the language's metamodel, since banks are
	currently highly involved in modeling initiatives to industrialize
	and optimize their process landscapes and are unsatisfied with existing
	modeling approaches regarding the cost-benefit-ratio of modeling.
	While taking a metamodel perspective on the language artifact itself,
	we derive requirements for process modeling from the domain of financial
	institutions and present findings on the adaptation of SBPML, giving
	a complete conceptual model of the SBPML method for banks.},
  doi = {10.1109/HICSS.2011.16},
  issn = {1530-1605},
  keywords = {SBPML;banking sector;cost benefit ratio;financial sector;metamodel
	based perspective;process management;process modeling;process reorganization;public
	sector;semantic business process modeling language;bank data processing;business
	process re-engineering;financial management;simulation languages;}
}

@INPROCEEDINGS{5306352,
  author = {Weiss, D.M.},
  title = {Architecture of product lines},
  booktitle = {Software Maintenance, 2009. ICSM 2009. IEEE International Conference
	on},
  year = {2009},
  pages = {6},
  month = {sept.},
  abstract = {A product line is a family of products designed to take advantage
	of their common aspects (commonalities) and predicted variabilities.
	A product line may be software only, e.g., a family of GUIs; software
	+ hardware, e.g., a family of televisions; or hardware only. Where
	software is a part of the product line, the variability accommodated
	by the product line is an economic decision and strongly affects
	the technology and the architecture used in the design and implementation
	of the product line. For example, where variability is narrowly bounded,
	a domain specific language may be used to define the product line
	and generate members of it, but the market for it may be relatively
	narrow. On the other hand, initial investment cost may be high, production
	cost very low, and time to market very short. Where variability is
	broadly bounded, a complex software architecture may be needed, production
	of products may be only semi-automated, time to market may increase,
	but market appeal may be much wider. Empirical studies, using baseline
	techniques, suggest that applying product line engineering produces
	a factor of three to five improvement in product development cost
	or product development speed. This talk will focus on the architectural
	considerations in defining and designing a product line, particularly
	questions such as "What are the attributes of a good software product
	line architecture?" and "How might a product line architecture change
	the economics of software development?", introducing an open market
	both in architecture and in software components. I will illustrate
	points with examples taken from Lucent Technologies and Avaya, from
	the Software Product Line Hall of Fame, from building architecture,
	and from other industries.},
  doi = {10.1109/ICSM.2009.5306352},
  issn = {1063-6773},
  keywords = {complex software architecture;product development cost;product development
	speed;product line engineering;software development economics;software
	product line architecture;software architecture;}
}

@INPROCEEDINGS{6030045,
  author = {Wende, C. and Assmann, U. and Zivkovic, S. and Kuhn, H.},
  title = {Feature-Based Customisation of Tool Environments for Model-Driven
	Software Development},
  booktitle = {Software Product Line Conference (SPLC), 2011 15th International},
  year = {2011},
  pages = {45 -54},
  month = {aug.},
  abstract = {Model-driven software development (MDSD) bridges the gap between domain-specific
	abstractions and general purpose implementation languages and promises
	enhanced productivity for software engineering. The availability
	and appropriateness of tool environments supporting the developer
	is a crucial factor for such productivity promises. The widespread
	use of MDSD on various domains means a special challenge for the
	development of MDSD environments. Tool users expect advanced tools
	customised for the very special domain they are working in. However,
	tool development and customisation is a complex and expensive task.
	To address these challenges we propose to apply the principles of
	software product line engineering (SPLE) for feature-based customisation
	of MDSD tool environments. This paper is an experience report for
	the development of a product-line of MDSD tool environments that
	employ ontology technology to advance MDSD. Finally, we discuss the
	lessons learned as well as the benefits and challenges observed for
	feature-based tool customisation.},
  doi = {10.1109/SPLC.2011.29},
  keywords = {domain-specific abstractions;feature-based tool customisation;general
	purpose implementation languages;model-driven software development;ontology
	technology;software engineering;software product line engineering;tool
	development;ontologies (artificial intelligence);software tools;}
}

@INPROCEEDINGS{6042458,
  author = {Wenger, M. and Melik-Merkumians, M. and Hegny, I. and Hametner, R.
	and Zoitl, A.},
  title = {Utilizing IEC 61499 in an MDA control application development approach},
  booktitle = {Automation Science and Engineering (CASE), 2011 IEEE Conference on},
  year = {2011},
  pages = {495 -500},
  month = {aug.},
  abstract = {Traditional control application engineering techniques tend to mix
	logical functionality with hardware access methods. This greatly
	impedes reusability. Through separation of the logical control application
	domain and the specific hardware domain the MDA (Model-Driven Architecture)
	proposes a solution to this problem. In the domain of embedded systems
	development this approach brought a great advantage in reducing the
	complexity of the development process. In this paper we investigate
	if and how IEC 61499 can be utilized as a DSL (Domain Specific Language)
	for control application development in the domain of industrial automation
	systems. We will show the potentials of our work by developing a
	sample control application using the suggested methods.},
  doi = {10.1109/CASE.2011.6042458},
  issn = {2161-8070},
  keywords = {DSL;IEC 61499;MDA control application development approach;domain
	specific language;embedded systems;industrial automation systems;logical
	control application;manufacturing industries;model-driven architecture;control
	engineering computing;manufacturing industries;software engineering;specification
	languages;}
}

@INPROCEEDINGS{4362623,
  author = {Wenzel, S. and Hutter, H. and Kelter, U.},
  title = {Tracing Model Elements},
  booktitle = {Software Maintenance, 2007. ICSM 2007. IEEE International Conference
	on},
  year = {2007},
  pages = {104 -113},
  month = {oct.},
  abstract = {In model-driven engineering developers work mainly or only with models,
	which exist in many versions. This paper presents an approach to
	trace single model elements or groups of elements within a version
	history of a model. It also offers analysis capabilities such as
	detection of logical coupling between model elements. The approach
	uses a differencing algorithm blown as SiDiff to identify similar
	elements in different versions of a model. SiDiff is highly configurable
	and thus our tracing approach can be adapted to all diagram types
	of the UML and to a large set of domain specific languages. The approach
	has been implemented as an Eclipse plug-in that visualizes all relevant
	information about the traces and it allows developers to interactively
	explore details. It has been evaluated by several groups of test
	persons; they considered most of the functions of the tool to be
	very useful.},
  doi = {10.1109/ICSM.2007.4362623},
  issn = {1063-6773},
  keywords = {Eclipse plug-in;SiDiff;UML;differencing algorithm;domain specific
	language;information visualization;model element tracing;model-driven
	engineering developer;tracing approach;Unified Modeling Language;program
	diagnostics;software engineering;}
}

@INPROCEEDINGS{4814203,
  author = {Wenzel, S. and Kelter, U.},
  title = {Analyzing model evolution},
  booktitle = {Software Engineering, 2008. ICSE '08. ACM/IEEE 30th International
	Conference on},
  year = {2008},
  pages = {831 -834},
  month = {may},
  abstract = {Model-driven development leads to development processes in which a
	large number of different versions of models are produced. We present
	FAME, a tool environment which enables fine-grained analysis of the
	version history of a model. The tool is generic in the sense that
	it can work with various model types including UML and domain-specific
	languages.},
  doi = {10.1145/1368088.1368214},
  issn = {0270-5257},
  keywords = {FAME;UML;development process;domain-specific languages;fine-grained
	analysis;model evolution;model-driven development;tool environment;version
	history;configuration management;formal specification;software tools;}
}

@INPROCEEDINGS{4384867,
  author = {Werner, J. and Mathe, J.L. and Duncavage, S. and Malin, B. and Ledeczi,
	A. and Jirjis, J.N. and Sztipanovits, J.},
  title = {Platform-Based Design for Clinical Information Systems},
  booktitle = {Industrial Informatics, 2007 5th IEEE International Conference on},
  year = {2007},
  volume = {2},
  pages = {749 -754},
  month = {june},
  abstract = {Clinical information systems (CIS) have emerged as a new critical
	infrastructure that influence affordability and security of health
	care delivery. Complex and conflicting societal requirements, such
	as providing control for patients over their personal health information
	and requiring health organizations to assure the security and privacy
	of patient-specific information, create significant technical challenges
	for the design of CIS. This paper presents a novel approach that
	is based on the principles and tools of model integrated computing
	(MIC), platform-based design (PBD) and service-oriented architectures
	(SOA). We present a domain-specific, graphical design environment
	and show how formal system specifications can be mapped to different
	service-oriented architecture execution platforms through a set of
	standard languages, such as WSBPEL and XACML. The model-integrated
	clinical information systems (MICIS) design environment includes
	a suite of domain-specific modeling languages capturing essential
	aspects of CIS design, model transformation tools that map the domain
	models onto the standard specification languages of SOA platforms
	and static model analysis tools checking the consistency and wellformedeness
	of the multiple-view models. The MICIS design tool is tested in modeling
	the MyHealth@Vanderbilt patient portal of the Vanderbilt University
	Medical Center.},
  doi = {10.1109/INDIN.2007.4384867},
  issn = {1935-4576},
  keywords = {WSBPEL;XACML;clinical information system;formal system specification;graphical
	design environment;model integrated computing;platform-based design;service-oriented
	architecture;specification language;data privacy;health care;medical
	information systems;software architecture;specification languages;}
}

@INPROCEEDINGS{5290805,
  author = {Weyns, D.},
  title = {A pattern language for multi-agent systems},
  booktitle = {Software Architecture, 2009 European Conference on Software Architecture.
	WICSA/ECSA 2009. Joint Working IEEE/IFIP Conference on},
  year = {2009},
  pages = {191 -200},
  month = {sept.},
  abstract = {Developing architectural support for self-adaptive systems, i.e. systems
	that are able to autonomously adapt to changes in their operating
	conditions, is a key challenge for software engineers. Multi-agent
	systems are a class of decentralized systems that are known for realizing
	qualities such as adaptability and scalability. In this paper, we
	present a pattern language for multi-agent systems. The pattern language
	distills domain-specific architectural knowledge derived from extensive
	experiences with developing various multi-agent systems. The pattern
	language, consisting of the five interrelated patterns, supports
	architects with designing software architectures for a family of
	self-adaptive systems. We illustrate the patterns for a case study
	in the domain of automated transportation systems.},
  doi = {10.1109/WICSA.2009.5290805},
  keywords = {automated transportation systems;decentralized systems;domain-specific
	architectural knowledge;multi-agent systems;pattern language;self-adaptive
	systems;software architectures;multi-agent systems;software architecture;}
}

@ARTICLE{5076458,
  author = {White, J. and Hill, J.H. and Gray, J. and Tambe, S. and Gokhale,
	A.S. and Schmidt, D.C.},
  title = {Improving Domain-Specific Language Reuse with Software Product Line
	Techniques},
  journal = {Software, IEEE},
  year = {2009},
  volume = {26},
  pages = {47 -53},
  number = {4},
  month = {july-aug. },
  abstract = {Complex software systems, such as traffic management systems and shipboard
	computing environments, raise several concerns (such as performance,
	reliability, and fault tolerance) that developers must manage throughout
	the software life cycle. Domain-specific languages (DSLs) have emerged
	as a powerful mechanism for capturing and reasoning about these diverse
	concerns. For each system concern, you can design a DSL to precisely
	capture key domain-level information while shielding developers and
	users from the technical solution's implementation-level details.},
  doi = {10.1109/MS.2009.95},
  issn = {0740-7459},
  keywords = {complex software systems;domain-specific language reuse;software life
	cycle;software product line techniques;software reusability;specification
	languages;}
}

@ARTICLE{5035595,
  author = {White, J. and Schmidt, D.C.},
  title = {Automating deployment planning with an aspect weaver},
  journal = {Software, IET},
  year = {2009},
  volume = {3},
  pages = {167 -183},
  number = {3},
  month = {june },
  abstract = {Deployment has emerged as a major challenge in distributed real-time
	and embedded (DRE) systems. Application deployment planners must
	integrate numerous functional and non-functional constraints, such
	as security and performance, to produce correct deployment plans.
	The numerous deployment constraints and their complex interactions
	make manually deducing correct/efficient deployments hard. Four contributions
	to the study of automated deployment processes are presented. First,
	it shows that a deployment planner and an aspect weaver accomplish
	the same abstract problem - that is, mapping items from a source
	set (advice or components) to items in a target set (joinpoints or
	nodes) according to a set of rules - and uses this abstract definition
	of deployment planning to automate it with an aspect weaver. Second,
	this paper describes how the ScatterML domain-specific aspect language
	incorporates complex global constraints for specifying deployment
	pointcuts. Third, we show how static aspect weaving problems can
	be reduced to a constraint satisfaction problem and a constraint
	solver used to deduce a correct weaving. Fourth, we show that phrasing
	weaving as a constraint satisfaction problem and automating deployment
	through a constraint solver-based weaver yields several key benefits,
	ranging from guaranteed deployment plan correctness to bounds on
	worst-case solution quality.},
  doi = {10.1049/iet-sen.2007.0123},
  issn = {1751-8806},
  keywords = {ScatterML domain-specific aspect language;constraint solver-based
	weaver;deployment planning automation;distributed real-time and embedded
	systems;static aspect weaving;distributed processing;embedded systems;object-oriented
	languages;planning;software quality;}
}

@INPROCEEDINGS{1498097,
  author = {White, J. and Schmidt, D. and Aniruddha Gokhale},
  title = {The J3 Process for Building Autonomic Enterprise Java Bean Systems},
  booktitle = {Autonomic Computing, 2005. ICAC 2005. Proceedings. Second International
	Conference on},
  year = {2005},
  pages = {363 -364},
  month = {june},
  abstract = {Autonomic computer systems aim to reduce the configuration, operational,
	and maintenance costs of distributed enterprise applications. This
	paper provides two contributions to the development of autonomic
	computing systems using Enterprise Java Beans (EJBs). First, we describe
	a model-driven development (MDD) tool that formally captures the
	design of EJB systems, their quality of service (QoS) requirements,
	and the autonomic properties that will be applied to the EJBs to
	support the rapid development of autonomic EJB applications. Second,
	we describe how this MDD tool can generate code to plug EJBs into
	a Java component framework that provides an autonomic structure to
	monitor, configure, and execute EJBs and their adaptation strategies
	at run-time},
  doi = {10.1109/ICAC.2005.60},
  keywords = {EJB applications;EJB systems;J2EEML;J3 process;Java component;QoS
	requirements;adaptation strategy;autonomic Enterprise Java bean systems;autonomic
	computer systems;autonomic structure;code generation;distributed
	enterprise applications;domain-specific modeling language;quality
	of service;system configuration;system maintenance;Java;distributed
	object management;formal specification;object-oriented programming;program
	compilers;quality of service;specification languages;}
}

@INPROCEEDINGS{5934826,
  author = {Whitsitt, S. and Sprinkle, J.},
  title = {Message Modeling for the Joint Architecture for Unmanned Systems
	(JAUS)},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {251 -259},
  month = {april},
  abstract = {The Joint Architecture for Unmanned Systems (JAUS) is a standard for
	sensing, control, and computational communication of components for
	unmanned systems. This paper presents a modeling environment capable
	of producing a domain-specific prototype of the software necessary
	for inter-computer communications. A metamodel is used to provide
	the domain-specific modeling language to model both the messages
	used in JAUS, and the shell interfaces for components that transmit
	and receive those messages. The produced artifacts are C and C++
	code that can be used in unmanned systems and simulations of such
	systems, including tests that validate the structure and behavior
	of the generated code. The generated code is compatible with standard
	JAUS implementations, and is validated using the Open JAUS open source
	API and framework. Future work describes the second spiral of features
	and behaviors (currently in the design phase). The case study and
	test environment for the software generated by this project is an
	autonomous ground vehicle, modeled on a Ford Escape Hybrid that is
	used in laboratory experiments.},
  doi = {10.1109/ECBS.2011.17},
  keywords = {C code;C++ code;Ford Escape Hybrid;Open JAUS;autonomous ground vehicle;domain-specific
	modeling language;inter-computer communications;joint architecture
	for unmanned systems;message modeling;open source API;C++ language;application
	program interfaces;control engineering computing;mobile robots;public
	domain software;remotely operated vehicles;simulation languages;}
}

@INPROCEEDINGS{5655567,
  author = {Wieczorek, S. and Stefanescu, A. and Roth, A.},
  title = {Model-Driven Service Integration Testing - A Case Study},
  booktitle = {Quality of Information and Communications Technology (QUATIC), 2010
	Seventh International Conference on the},
  year = {2010},
  pages = {292 -297},
  month = {29 2010-oct. 2},
  abstract = {This paper presents a case study for the modeling and model-based
	testing (MBT) of enterprise service choreographies. Our proposed
	MBT approach uses proprietary models called Message Choreography
	Models (MCM) as test models. The case study illustrates how MCM-based
	service integration testing allows to formalize design decisions
	and enables full integration into an existing industrial test infrastructure
	by using the concepts of domain specific languages and model transformations.
	Further, the MBT tools integrated into the testing framework have
	been compared based on one concrete use case.},
  doi = {10.1109/QUATIC.2010.49},
  keywords = {domain specific languages;enterprise service choreographies;industrial
	test infrastructure;message choreography models;model transformations;model-based
	testing;model-driven service integration testing;testing framework;formal
	specification;program testing;}
}

@INPROCEEDINGS{5641070,
  author = {Wien, T. and Reichenbach, F. and Carlson, E. and Stlhane, T.},
  title = {Reducing development costs in industrial safety projects with CESAR},
  booktitle = {Emerging Technologies and Factory Automation (ETFA), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {1 -4},
  month = {sept.},
  abstract = {The demand for high Safety levels in industrial applications is growing.
	New certification and documentation requirements increase the product
	cost significantly. New or improved methods for high level specification
	and design may help to do part of the development process more automatically.
	In the CESAR project ABB, NTNU and SINTEF investigates if the Boiler
	Plate and Domain Specific Language approach for specification can
	be used to facilitate automatic generation of safety code and help
	to automatically document the process as required for safety authorization.},
  doi = {10.1109/ETFA.2010.5641070},
  issn = {1946-0740},
  keywords = {CESAR;NTNU;SINTEF;boiler plate and domain specific language;development
	costs;industrial safety projects;safety authorization;authorisation;costing;safety;}
}

@INPROCEEDINGS{1174893,
  author = {Wile, D.},
  title = {Lessons learned from real DSL experiments},
  booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International
	Conference on},
  year = {2003},
  pages = { 10 pp.},
  month = {jan.},
  abstract = { Over the years, our group, led by Bob Baker, designed and implemented
	three domain-specific languages for use by outside people in real
	situations. The first language described the communication format
	of messages used by NATO to specify command-and-control messages
	between people and equipment; the processor we generated checked
	these messages for consistency. The second language was in part graphical,
	part textual, and was used to demonstrate how naval ship formations
	were constituted and the constrained movements they could undergo.
	The last language was a mixture of graphics, text, and declarative
	information specified using three different COTS products. It was
	used to describe census survey "instruments," used to collect census
	data in the field. The code generated was to be installed in the
	survey takers' laptops. Each of these was actually a prototype for
	what would have taken more time to engineer and polish before putting
	into practice. Although each effort was essentially successful, none
	of the languages was ever followed up with the subsequent engineering
	efforts that we expected or at least hoped for. The first two were
	demonstrated and reviewed informally. The last effort was more seriously
	reviewed, in that training sessions and a formal review process were
	undertaken to evaluate the potential effectiveness of the product.
	Herein I elaborate where these language efforts succeeded and where
	they failed, gleaning lessons for others who take the somewhat risky
	step of committing to develop a DSL for a particular user community.},
  doi = {10.1109/HICSS.2003.1174893},
  issn = { },
  keywords = { COTS products; DSL experiments; NATO; census data; census survey
	instruments; code generation; command-and-control messages; communication
	format; declarative information; domain-specific languages; graphics
	information; naval ship formations; text information; command and
	control systems; formal specification; specification languages; visual
	programming;}
}

@INPROCEEDINGS{610346,
  author = {Wile, D.S.},
  title = {Abstract Syntax from Concrete Syntax},
  booktitle = {Software Engineering, 1997., Proceedings of the 1997 (19th) International
	Conference on},
  year = {1997},
  pages = {472 -480},
  month = {may},
  abstract = {Not available},
  doi = {10.1109/ICSE.1997.610346},
  issn = {0270-5257}
}

@INPROCEEDINGS{111269,
  author = {Williamson, J.S. and Jensen, P.S. and Ogata, L. and Graves, W.H.},
  title = {Automatic programming technologies for avionics software (APTAS)
	},
  booktitle = {Digital Avionics Systems Conference, 1990. Proceedings., IEEE/AIAA/NASA
	9th},
  year = {1990},
  pages = {96 -100},
  month = {oct},
  abstract = {The APTAS software development environment, which provides automatic
	programming support for both rapid prototyping and target software
	system development in Ada, is described. APTAS utilizes domain-specific
	knowledge base support for automatic code synthesis from high-level
	system specifications. Each domain knowledge base includes both generic
	system architectures, in the form of templates, and design rules
	which capture carefully developed implementation strategies of various
	high-level specifications. Early users of this system will be systems
	and software engineers whose responsibility is to develop avionics
	system architectures. With the APTAS system, engineers will be able
	to quickly compare and contrast various architectural designs, including
	hardware and software tradeoffs, by testing instrumented, operational
	prototypes. The engineer will specify the system in a high-level
	specification language tailored to the functional area. Drawing from
	a knowledge base germane to the specifications APTAS will then interpret
	the specification in order to synthesize executable code. The high-level
	design language CIDL is a key element of APTAS, since it provides
	all the resources required to specify and test real-time, parallel
	processing systems needed for avionics applications},
  doi = {10.1109/DASC.1990.111269},
  keywords = {APTAS;Ada;architectural designs;automatic code synthesis;automatic
	programming;avionics software;domain-specific knowledge base;high-level
	design language;high-level system specifications;parallel processing;rapid
	prototyping;target software system development;aerospace computing;automatic
	programming;formal specification;knowledge based systems;parallel
	programming;specification languages;}
}

@INPROCEEDINGS{972658,
  author = {Windisch, A. and Monjau, D.},
  title = {An operational framework for the multi-lingual system simulation
	based on pi;-calculus},
  booktitle = {Computer Science Society, 2001. SCCC 2001. Proceedings. XXI Internatinal
	Conference of the Chilean},
  year = {2001},
  pages = {282 -291},
  abstract = {Complex heterogeneous systems are usually specified at system level
	by a set of interacting cores each of which can be implemented in
	a different, domain-specific language. The dynamic verification of
	such systems requires a coupling of a set of language-specific simulators
	to a multilanguage simulation system such that the simulation semantics
	of each individual language are respected. In order to aid a semantics-preserving
	coupling this paper introduces an operational framework based upon
	which the simulation semantics of different languages can be formally
	captured and their correct co-simulation semantics can be derived.
	This formalisation of the simulation semantics is founded on a fixed
	set of semantic primitives for capturing model structure, behaviour
	communication, timing, and scheduling. All parts of the presented
	framework are defined in the single unifying notation of the pi;-calculus
	process algebra},
  doi = {10.1109/SCCC.2001.972658},
  keywords = { pi;-calculus;behaviour;co-simulation semantics;communication;complex
	heterogeneous systems-on-chip;domain-specific language;dynamic verification;heterogeneous
	components;heterogeneous systems;language-specific simulators;microelectronic
	circuits;model structure;multi-lingual system simulation;multilanguage
	simulation system;operational framework;process algebra;scheduling;semantic
	primitives;semantics-preserving coupling;simulation semantics;timing;circuit
	simulation;hardware description languages;integrated circuit design;pi
	calculus;}
}

@INPROCEEDINGS{6061365,
  author = {Witherell, P. and Narayanan, A. and JaeHyun Lee},
  title = {Using Metamodels to Improve Product Models and Facilitate Inferencing},
  booktitle = {Semantic Computing (ICSC), 2011 Fifth IEEE International Conference
	on},
  year = {2011},
  pages = {506 -513},
  month = {sept.},
  abstract = {Increasing information requirements are causing domain models to become
	more complex and difficult to manage. Domain-specific languages are
	developed with consideration for domain experts, and therefore are
	meant to be domain-friendly. However, their effectiveness in domain-specific
	models, when developed for knowledge management applications, is
	often limited by their expressiveness and implementation. In this
	paper we discuss current domain modeling practices, specifically
	the use of OWL (Web Ontology Language) and SWRL (Semantic Web Rule
	Language) within the context of product development, and how they
	often do not consider their intended application. To address this,
	we (1) recommend a set of best practices to account for domain context
	while promoting application-specific domain modeling, (2) propose
	that a metamodel be used to incorporate these practices early on
	in domain modeling and review how similar information has been represented
	in the past, and (3) discuss what factors should be considered in
	the development of such a metamodel in the future.},
  doi = {10.1109/ICSC.2011.59},
  keywords = {OWL;SWRL;Web ontology language;application-specific domain modeling;domain
	context;domain experts;domain modeling practices;domain models;domain-friendly;domain-specific
	languages;domain-specific models;inferencing;information requirements;knowledge
	management applications;metamodels;product development;product models;semantic
	Web rule language;expert systems;inference mechanisms;knowledge management;knowledge
	representation languages;ontologies (artificial intelligence);product
	development;semantic Web;specification languages;}
}

@ARTICLE{1377189,
  author = {Wong, E.Y.C. and Chan, A.T.S. and Hong Va Leong},
  title = {Xstream: a middleware for streaming XML contents over wireless environments},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2004},
  volume = {30},
  pages = { 918 - 935},
  number = {12},
  month = {dec.},
  abstract = { XML (extensible Markup Language) has been developed and deployed
	by domain-specific standardization bodies and commercial companies.
	Studies have been conducted on a wide variety of issues encompassing
	XML. In the use of XML for wireless computing, the focus has been
	on investigating ways to efficiently represent XML data for transmission
	over a wireless environment. We propose a middleware, Xstream (XML
	Streaming), for efficiently streaming XML contents over a wireless
	environment by leveraging the rich semantics and structural characteristics
	of XML documents and by flexibly managing units containing fragments
	of data into autonomous units, known as XDU (Xstream Data Unit) fragments.
	The concept of an XDU is fundamental to the operation of Xstream.
	It provides for the efficient transfer of documents across a wireless
	link and allows other issues and challenges pertaining to wireless
	transmission to be addressed. By fragmenting and organizing an XML
	document into XDU fragments, we are able to incrementally send fragments
	across a wireless link, while the receiver is able to perform look-ahead
	processing of the document without having to wait for the entire
	document to be downloaded. We propose a fragmenting strategy based
	on the value of the wireless link's Maximum Transfer Units (MTUs).
	In addition, we present and evaluate several packetizing strategies,
	i.e., strategies wherein a collection of XDUs are grouped into a
	packet to optimize packet delivery and processing. At the receiving
	end of this process, a reassembly strategy incrementally reconstructs
	the XML document as XDU fragments are being received, thereby facilitating
	client application implementation of look-ahead processing.},
  doi = {10.1109/TSE.2004.108},
  issn = {0098-5589},
  keywords = { Maximum Transfer Units; XML content streaming; XML documents; Xstream
	Data Unit fragments; Xstream middleware; domain-specific standardization;
	extensible Markup Language; packetizing strategies; wireless computing;
	wireless environments; XML; formal specification; middleware; mobile
	computing; multimedia computing;}
}

@INPROCEEDINGS{1245326,
  author = {Wong, E.Y.C. and Chan, A.T.S. and Hong-Va Leong},
  title = {Semantic-based approach to streaming XML contents using Xstream},
  booktitle = {Computer Software and Applications Conference, 2003. COMPSAC 2003.
	Proceedings. 27th Annual International},
  year = {2003},
  pages = { 91 - 96},
  month = {nov.},
  abstract = { XML (eXtensible Markup Language) has been developed and deployed
	by domain-specific standardization bodies and commercial companies.
	We investigate the possibilities and issues encompassing the use
	of generalized XML in a wireless computing environment. Current approaches
	of fragmenting data do not take into account of the semantics and
	structure of the data, therefore ignoring the specific needs of individual
	application. We propose a middleware, Xstream (XML Streaming) for
	augmenting XML contents by leveraging on the rich semantics and structural
	characteristics of the XML document into autonomous units, which
	are known as XDU (Xstream Data Unit). In this paper we describe the
	framework and the techniques involved and study the performance of
	the techniques.},
  doi = {10.1109/CMPSAC.2003.1245326},
  issn = {0730-3157 },
  keywords = { World Wide Web; XDU; XML Streaming; XML contents; Xstream Data Unit;
	autonomous unit; commercial companies; data structure; domain-specific
	standardization; eXtensible Markup Language; middleware; mobile computing;
	semantic-based approach; wireless computing environment; XML; middleware;
	mobile computing; semantic Web;}
}

@INPROCEEDINGS{823078,
  author = {Wood, B. and Tumay, K.},
  title = {MODSIM III and CACI's applications},
  booktitle = {Simulation Conference Proceedings, 1999 Winter},
  year = {1999},
  volume = {1},
  pages = {234 -238 vol.1},
  abstract = {This tutorial introduces CACI's MODSIM III language, showing how its
	simulation ldquo;world view rdquo; together with its object-oriented
	architecture and built-in graphics contribute to successful simulation
	model building. The tutorial also provides an overview of CACI's
	domain-specific simulation tools, namely SIMPROCESS and COMNET III,
	that are developed using MODSIM III},
  doi = {10.1109/WSC.1999.823078},
  keywords = {CACI;COMNET III;MODSIM III;SIMPROCESS;built-in graphics;domain-specific
	simulation tools;object-oriented language;simulation language;simulation
	model;simulation world view;computer graphics;digital simulation;object-oriented
	languages;simulation languages;}
}

@ARTICLE{4585363,
  author = {Wood, S.K. and Akehurst, D.H. and Uzenkov, O. and Howells, W.G.J.
	and McDonald-Maier, K.D.},
  title = {A Model-Driven Development Approach to Mapping UML State Diagrams
	to Synthesizable VHDL},
  journal = {Computers, IEEE Transactions on},
  year = {2008},
  volume = {57},
  pages = {1357 -1371},
  number = {10},
  month = {oct. },
  abstract = {With the continuing rise in the complexity of embedded systems, there
	is an emerging need for a higher level modelling environment that
	facilitates efficient handling of this complexity. The aim here is
	to produce such a high level environment using Model Driven Development
	(MDD) techniques that maps a high level abstract description of an
	electronic embedded system into its low level implementation details.
	The Unified Modelling Language (UML) is a high level graphical based
	language that is broad enough in scope to model embedded systems
	hardware circuits. The authors have developed a framework for deriving
	Very High Speed Integrated Circuits Hardware Description Language
	(VHDL) code from UML state diagrams and defined a set of rules that
	enable automated generation of synthesisable VHDL code from UML specifications
	using MDD techniques. By adopting the techniques and tools described
	in this paper the design and implementation of complex state-based
	systems is greatly simplified.},
  doi = {10.1109/TC.2008.123},
  issn = {0018-9340},
  keywords = {UML specification;UML state diagram mapping;electronic embedded system;embedded
	system hardware circuit model;high-level abstract description;high-level
	graphical-based language;model-driven development approach;synthesizable
	VHDL code generation;very high speed integrated circuit hardware
	description language;Unified Modeling Language;embedded systems;hardware
	description languages;object-oriented programming;program compilers;}
}

@INPROCEEDINGS{6037575,
  author = {Wouters, L. and Gervais, M.},
  title = {xOWL: An Executable Modeling Language for Domain Experts},
  booktitle = {Enterprise Distributed Object Computing Conference (EDOC), 2011 15th
	IEEE International},
  year = {2011},
  pages = {215 -224},
  month = {29 2011-sept. 2},
  abstract = {Nowadays, modeling complex domains such those involving the description
	of human behaviors is still a challenge. An answer is to apply the
	Domain Specific Languages principle, which advocates that Domain
	Experts should model themselves their knowledge in order to avoid
	misunderstanding or loss of information during the knowledge elicitation
	phase. But Domain Experts must then be provided a modeling language
	enabling them to describe such complex domains. Moreover, in order
	to help them build models, immediate feedbacks would have to be available
	so that they can revise their modeling choices in earlier steps.
	Model execution is a way to address this issue. We provide xOWL,
	a language that can be used as a backend for multiple domain-specific
	syntaxes enabling Domain Experts to model themselves the structural
	as well as behavioral knowledge of their domain. xOWL comes with
	an interpreter integrated in an environment offering models execut
	ability in such way that Domain Experts can work in an iterative
	and incremental way using a trial and error approach. The implemented
	prototype is currently in use at EADS.},
  doi = {10.1109/EDOC.2011.13},
  issn = {1541-7719},
  keywords = {domain experts;domain specific languages principle;domain-specific
	syntaxes;executable modeling language;information loss;knowledge
	elicitation phase;model-driven development;semantic Web;trial and
	error approach;xOWL;knowledge acquisition;knowledge representation
	languages;semantic Web;software engineering;specification languages;}
}

@INPROCEEDINGS{5666227,
  author = {Ke Wu and Jiangsheng Yu and Hanpin Wang and Fei Cheng},
  title = {Unsupervised text pattern learning using minimum description length},
  booktitle = {Universal Communication Symposium (IUCS), 2010 4th International},
  year = {2010},
  pages = {161 -166},
  month = {oct.},
  abstract = {The knowledge of text patterns in a domain-specific corpus is valuable
	in many natural language processing (NLP) applications such as information
	extraction, question-answering system, and etc. In this paper, we
	propose a simple but effective probabilistic language model for modeling
	the in-decomposability of text patterns. Under the minimum description
	length (MDL) principle, an efficient unsupervised learning algorithm
	is implemented and the experiment on an English critical writing
	corpus has shown promising coverage of patterns compared with human
	summary.},
  doi = {10.1109/IUCS.2010.5666227},
  keywords = {English;critical writing corpus;minimum description length;natural
	language processing;probabilistic language model;text pattern;unsupervised
	learning;computational linguistics;learning (artificial intelligence);natural
	language processing;probability;text analysis;}
}

@INPROCEEDINGS{655214,
  author = {Peng Wu and Bhatnagar, R. and Epshtein, L. and Bhandaru, M. and Zhongwen
	Shi},
  title = {Alarm correlation engine (ACE)},
  booktitle = {Network Operations and Management Symposium, 1998. NOMS 98., IEEE},
  year = {1998},
  volume = {3},
  pages = {733 -742 vol.3},
  month = {feb},
  abstract = {Networks are growing in size and complexity, resulting in increased
	alarm volume and number of unfamiliar alarms. Often, there is no
	proportional increase in monitoring personnel and response time to
	faults suffers. GTE deployed Telephone Operations Network Integrated
	Control System (TONICS) in 1993 to support its network management
	operations. To stay competitive in the face of continued staff reductions,
	increase in network size, and monitoring complications related to
	deregulation of the telephone industry, GTE is introducing artificial
	intelligence techniques into TONICS. Alarm Correlation Engine (ACE),
	the system described in this paper, is part of the effort. ACE aids
	network management by correlating alarms on the basis of common cause
	to provide alarm compression, filtering, and suppression. In conjunction
	with its ability to carry out prescribed responses, it improves response
	times and increases productivity. ACE was developed with the following
	requirements: reliability, speed, versatility (handle alarms from
	different switches and networks), ease of knowledge engineering (field
	technicians must be able to construct, test, and modify correlation
	patterns), handle in real time multiple network problems, and finally,
	interface smoothly with GTE's TONICS system. ACE's strength lies
	in its domain specific correlation language which facilitates knowledge
	engineering and in its asynchronous processing core that enables
	integration into a real-time monitoring system},
  doi = {10.1109/NOMS.1998.655214},
  keywords = {ACE;Alarm Correlation Engine;Telephone Operations Network Integrated
	Control System;knowledge engineering;network management;real-time
	monitoring system;alarm systems;telecommunication network management;}
}

@INPROCEEDINGS{6009024,
  author = {Xing Wu and Mueller, F. and Pakin, S.},
  title = {Automatic Generation of Executable Communication Specifications from
	Parallel Applications},
  booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW),
	2011 IEEE International Symposium on},
  year = {2011},
  pages = {2089 -2092},
  month = {may},
  abstract = {Portable parallel benchmarks are widely used and highly effective
	for (a) the evaluation, analysis and procurement of high-performance
	computing (HPC) systems and (b) quantifying the potential benefits
	of porting applications for new hardware platforms. Yet, past techniques
	to synthetically parametrized hand-coded HPC benchmarks prove insufficient
	for today's rapidly-evolving scientific codes particularly when subject
	to multi-scale science modeling or when utilizing domain-specific
	libraries. To address these problems, this work contributes novel
	methods to automatically generate highly portable and customizable
	communication benchmarks from HPC applications. We utilize ScalaTrace,
	a lossless, yet scalable, parallel application tracing framework
	to collect selected aspects of the run-time behavior of HPC applications.
	We subsequently generate benchmarks with identical run-time behavior
	from the collected traces in the CONCEPTUAL language, a domain-specific
	language that enables the expression of sophisticated communication
	patterns using a rich and easily understandable grammar yet compiles
	to ordinary C+MPI. Experimental results demonstrate that the generated
	benchmarks are able to preserve the run-time behavior of the original
	applications. This ability to automatically generate performance-accurate
	benchmarks from parallel applications is novel and without any precedence,
	to our knowledge.},
  doi = {10.1109/IPDPS.2011.384},
  issn = {1530-2075},
  keywords = {CONCEPTUAL language;ScalaTrace application tracing framework;executable
	communication specification;high-performance computing system;parallel
	application;porting application;application program interfaces;formal
	specification;message passing;parallel processing;programming languages;}
}

@INPROCEEDINGS{6032329,
  author = {Yali Wu and Hernandez, F. and Clarke, P.J. and France, R.},
  title = {A DSML for Coordinating User-Centric Communication Services},
  booktitle = {Computer Software and Applications Conference (COMPSAC), 2011 IEEE
	35th Annual},
  year = {2011},
  pages = {93 -102},
  month = {july},
  abstract = {Rapid advances in electronic communication devices and technologies
	have resulted in a shift in the way communication applications are
	being developed. The emerging development strategies provide end-users
	with a greater ability to manipulate the underlying communication
	technologies by providing the appropriate level of abstraction, referred
	to as user-centric communication. In communication-intensive domains
	such as telemedicine and disaster management, the user-centric communication
	strategies still lack the ability to coordinate the various communication
	services in collaborative processes. In this paper, we present a
	domain-specific modeling language (DSML), Workflow Communication
	Modeling Language (WF-CML), that supports the rapid realization of
	collaborative user-centric communication applications. WF-CML is
	an extension of CML with communication specific abstractions of workflow
	concepts. To realize WF-CML models, the dynamic synthesis process
	in the Communication Virtual Machine (CVM) prototype was extended
	to coordinate the negotiation and media transfer processes based
	on events generated during the collaboration. We also present a comparative
	study to show the advantage of using WF-CML over a general-purpose
	workflow language and execution environment.},
  doi = {10.1109/COMPSAC.2011.20},
  issn = {0730-3157},
  keywords = {collaborative processes;communication intensive domains;communication
	virtual machine;disaster management;domain specific modeling language;electronic
	communication devices;general purpose workflow language;telemedicine;user
	centric communication services coordination;workflow communication
	modeling language;electronic messaging;emergency services;groupware;mobile
	computing;simulation languages;telemedicine;virtual machines;}
}

@INPROCEEDINGS{5730363,
  author = {Wang Wusheng and Li Weiping and Wu Zhonghai and Chu Weijie and Mo
	Tong},
  title = {An Ontology-Based Context Model for Building Context-Aware Services},
  booktitle = {Intelligent Systems, Modelling and Simulation (ISMS), 2011 Second
	International Conference on},
  year = {2011},
  pages = {296 -299},
  month = {jan.},
  abstract = {One of the most important points in context-aware computing is to
	build a reasonable context model. Currently, many models were not
	built in a common structure. It is hard to build other models depending
	on the existing ones. In this paper, we build a common ontology-based
	context model with Web Ontology Language (OWL), which provides an
	efficient and convenient way of ontology-building by adding domain-specific
	concepts into this common model. We show the structure of the model
	and explain the main concepts of it. With the common model, we propose
	our solutions to solve the problems of position and context history
	that may exist in the process of modeling a specific domain. Finally,
	we show a context model of Beijing Capital International Airport
	(BCIA), which was built on the common model. A simple reasoning test
	is run on the BCIA model with Semantic Web Rule Language (SWRL).},
  doi = {10.1109/ISMS.2011.52},
  keywords = {Beijing Capital International Airport;Web ontology language;context-aware
	computing;context-aware services;ontology-based context model;semantic
	Web rule language;ontologies (artificial intelligence);semantic Web;ubiquitous
	computing;}
}

@INPROCEEDINGS{5572346,
  author = {Wei Xia and Xiaodong Mu and Yiping Yao and Bing Wang and Fei Xing},
  title = {Research on technologies of Event Graph based Parallel Discrete Event
	Simulation},
  booktitle = {Networked Computing and Advanced Information Management (NCM), 2010
	Sixth International Conference on},
  year = {2010},
  pages = {69 -74},
  month = {aug.},
  abstract = {Event Graph (EG) formalism is a simple, powerful and language-independent
	way of representing Discrete Event Simulation (DES) models. Parallel
	Discrete Event Simulation (PDES), which leverages the power of parallel
	processing, can significantly improve the performance and capacity
	of DES. However, most of the state-of-the-art parallel simulators
	are based on the logical process (LP) paradigm, and implemented in
	general purpose programming language like Java. This makes the modeling
	process more prone to error and untraceable for domain experts to
	benefit from this field. The paper presents an approach to transforming
	an EG model of a DES to an equivalent LP based model, in order to
	combine the benefit from a modeling language that uses the graphical
	advantages and the PDES of LP paradigm with performance gain. This
	work is done on a flexible modeling and simulation platform JAMES
	II. The experiments show that EG models are successfully transformed
	to LP models with domain-specific languages recognizer ANTLR, and
	this approach endows JAMES II's LP paradigm with the capacity to
	support EG based model.},
  keywords = {ANTLR;EG model;JAMES II;domain-specific language recognizer;event
	graph formalism;flexible modeling;general purpose programming language;logical
	process paradigm;modeling language;parallel discrete event simulation;parallel
	processing;parallel simulator;simulation platform;discrete event
	simulation;graph theory;parallel processing;simulation languages;}
}

@INPROCEEDINGS{5231555,
  author = {Peng Xiaoming and Fang Qiqing and Hu Yahui and Zhu Bingjian},
  title = {A User Requirements Oriented Dynamic Web Service Composition Framework},
  booktitle = {Information Technology and Applications, 2009. IFITA '09. International
	Forum on},
  year = {2009},
  volume = {1},
  pages = {173 -177},
  month = {may},
  abstract = {A novel two staged and user requirements oriented Dynamic Web Service
	composition framework is presented in this paper. Firstly, we separate
	the end user requirements into two parts:functional and non-functional
	requirements. Consequently, with user requirements in OWL-S, the
	composition firstly uses JSHOP2 technique to generate an Abstract
	Service Plan to satisfy user functional requirement. In the second
	stage, the Web Service Instance Selection problem is converted into
	a multi-objective optimization problem, using the Multi-Objective
	Ant Colony Optimization (MOACO), the Abstract Service Plan is concretized
	into an Concrete Workflow based on non-functional requirements, and
	then being transformed into BPEL4WS. Theoretical analysis indicates
	the proposed solution which separately dealing with the functional
	requirements and the non-functional requirements leads to a significant
	reduction of the search space. The prototype in a domain-specific
	scenario indicates the feasibility of the framework.},
  doi = {10.1109/IFITA.2009.95},
  keywords = {abstract service plan;dynamic Web service composition framework;multiobjective
	ant colony optimization;nonfunctional requirement;search space;user
	requirement;Web services;knowledge representation languages;optimisation;}
}

@INPROCEEDINGS{5599721,
  author = {Fei Xie and Xindong Wu and Xuegang Hu},
  title = {Keyphrase extraction based on semantic relatedness},
  booktitle = {Cognitive Informatics (ICCI), 2010 9th IEEE International Conference
	on},
  year = {2010},
  pages = {308 -312},
  month = {july},
  abstract = {Keyphrase extraction is a fundamental research task in natural language
	processing and text mining. A limitation of previous keyphrase extraction
	methods based on semantic analysis is that the acquisition of the
	semantic features within phrases is restricted by the constructed
	thesaurus and language. An approach to the acquisition of the semantic
	features within phrases from a single document is proposed in this
	paper, which is used to extract document keyphrases. Semantic relatedness
	degrees between phrases are computed using word co-occurrence information
	in the document, and the document is represented as a relatedness
	graph. Keyphrases are extracted based on the semantic relatedness
	features acquired from the graph. Our experiments demonstrate that
	the proposed keyphrase extraction method always outperforms the baseline
	methods TFIDF and Kea. Furthermore, our approach is not domain-specific
	and the method generalizes well when it is trained on one domain
	(journal articles) and tested on another (news web pages).},
  doi = {10.1109/COGINF.2010.5599721},
  keywords = {keyphrase extraction methods;natural language processing;semantic
	analysis;semantic features acquisition;semantic relatedness;text
	mining;data mining;natural language processing;text analysis;word
	processing;}
}

@INPROCEEDINGS{5341669,
  author = {Lei Xu and Jennings, B.},
  title = {A Framework for Automated Creation and Deployment of Consolidated
	Charging Schemes for Service Compositions},
  booktitle = {Web Services, 2009. ECOWS '09. Seventh IEEE European Conference on},
  year = {2009},
  pages = {49 -57},
  month = {nov.},
  abstract = {If environments offering facilities for service composition are to
	be commercially successful then it will be important that service
	providers can avail of efficient, automated processes by which composed
	services can be metered, charged and billed for. In many industries,
	for example telecommunications, sophisticated systems are employed
	to provide usage and content based charging for services. In these
	systems charging schemes are typically manually configured and verified
	prior to services being made available to customers. For composed
	services we require a more dynamic charging approach, in which charging
	schemes for composed services can be automatically generated and
	deployed. In this paper we describe a framework that meets this need.
	In it, charging schemes for services making up a composed services
	are collected, consolidated and deployed onto multiple rating engines,
	which then coordinate to calculate a charge for an composed service
	invocation.},
  doi = {10.1109/ECOWS.2009.24},
  keywords = {Web service composition;consolidated charging scheme creation;consolidated
	charging scheme deployment;dynamic charging approach;Web services;}
}

@INPROCEEDINGS{4258619,
  author = {Lei Xu and Jennings, B.},
  title = {Automating the Generation, Deployment and Application of Charging
	Schemes for Composed IMS Services},
  booktitle = {Integrated Network Management, 2007. IM '07. 10th IFIP/IEEE International
	Symposium on},
  year = {2007},
  pages = {856 -859},
  month = {21 2007-yearly 25},
  abstract = {Providers of communications services aim to specify, realise and deploy
	services as quickly as possible in order to gain a competitive edge
	in meeting evolving consumer demands. One means of creating new service
	offerings is the composition of pre-existing services which, when
	orchestrated in a particular manner, provide novel functionality.
	Indeed, many industry analysts predict the emergence of virtual services
	providers, who do not themselves deploy and offer individual services,
	but instead orchestrate services offered by other providers. Crucial
	to the success of providers offering composed services is an efficient,
	automated process by which such services are charged and billed for.
	In this paper we present a process for automated generation of charging
	schemes for composed IMS services, based on analysis of the charging
	schemes associated with services comprising those composed services.
	Charging schemes are specified using a Domain Specific Language (DSL),
	so that they can then be mapped to platform specific representations
	that can be deployed onto one or more rating engines. Semi- automated
	configuration of charging schemes in this manner obviates the need
	for expensive manual configuration of accounting components every
	time a new composed services is specified and activated.},
  doi = {10.1109/INM.2007.374728},
  keywords = {Domain Specific Language;automated generation;consumer demands;virtual
	services providers;office automation;}
}

@INPROCEEDINGS{1342989,
  author = {Peng Xu and Deters, R.},
  title = {Using event-streams for fault-management in MAS},
  booktitle = {Intelligent Agent Technology, 2004. (IAT 2004). Proceedings. IEEE/WIC/ACM
	International Conference on},
  year = {2004},
  pages = { 433 - 436},
  month = {sept.},
  abstract = { Dependability is a key issue in the deployment of every multi-agent
	system (MAS). Only if its services are perceived as dependable (e.g.
	available, reliable, secure and safe) will the MAS be considered
	useful to ensure the domain specific dependability requirements,
	it is essential to enable the MAS to detect and react to critical
	states of agents. This can be done by either enabling the agent to
	deal with unexpected situations or by adding a fault-management component
	to the platform. This work presents an event-based fault-management
	system and presents the results of its evaluation.},
  doi = {10.1109/IAT.2004.1342989},
  issn = { },
  keywords = { domain specific dependability requirements; event streams; event-based
	fault-management system; multiagent system; fault diagnosis; multi-agent
	systems;}
}

@INPROCEEDINGS{4341915,
  author = {Yague, A. and Garbajosa, J.},
  title = {Applying the Knowledge Stored in Systems Models to Derve Validation
	Tools and Environments},
  booktitle = {Cognitive Informatics, 6th IEEE International Conference on},
  year = {2007},
  pages = {391 -400},
  month = {aug.},
  abstract = {It is frequent that test tools are basically independent from the
	application domain. This is usually the case for languages as well.
	This reflection becomes particularly relevant for validation tools,
	since the validation process is closely related to requirements.
	Therefore, test engineers must work at a much lower level of abstraction
	than that of the system domain. The problem is especially relevant
	when dealing with complex systems, in which hardware and software
	are closely related, such as ambient intelligence systems, automotive,
	power plants, telecommunication and autonomic systems in general.
	The authors of this paper claim that validation tools can and will
	be domain specific in the future. For this reason, they will have
	to include knowledge defined in the system requirements. As explained
	within this paper, this knowledge can be incorporated into the validation
	tools with limited involvement from the test engineer. These domain
	specific-tools will make the task easier for test engineers. This
	paper analyzes the different kinds of knowledge involved in domain-specific
	validation tools, specifically acceptance testing tools. The knowledge
	is expressed in terms of requirements and models. This paper also
	presents a model driven engineering approach to derive validation
	tools and environments from system models. This approach uses model
	transformations. Derivation can be performed in either a semiautomatic
	or fully automatic way depending on the system characteristics. Through
	these approaches, software and systems engineers can access to domain-specific
	validation tools during the early stages of the system life cycle.},
  doi = {10.1109/COGINF.2007.4341915},
  keywords = {abstraction;acceptance testing tools;application domain;languages;model
	driven engineering;software engineering;system life cycle;system
	requirements;validation tools;program testing;program verification;software
	tools;}
}

@INPROCEEDINGS{4150282,
  author = {Guoqing Yang and Minde Zhao and Hongli and Zhaohui Wu},
  title = {SmartC: A Component-Based Hierarchical Modeling Language for Automotive
	Electronics},
  booktitle = {Control, Automation, Robotics and Vision, 2006. ICARCV '06. 9th International
	Conference on},
  year = {2006},
  pages = {1 -6},
  month = {dec.},
  abstract = {This paper introduces SmartC, a language designed for programming
	automotive electronics embedded systems such as engine control systems.
	SmartC is a hierarchical modeling language and implements the SmartOSEK
	operating system model. The SmartC models are classified into four
	levels, namely module level, task level, subtask level and component
	level. In the SmartC models, control-flow oriented models and data-flow
	oriented models are integrated in the hybrid SmartC models. At the
	task level, the model is constructed based on the control flow, whereas
	the component level model is constructed based on the data flow.
	In SmartC programs, all inter-task communication, task triggering
	mechanisms, and access to guarded global variables, are automatically
	generated by the SmartC generator which generates the C code from
	the SmartC code. Having well-structured concurrency mechanisms, SmartC
	greatly reduces the risk of concurrency errors, such as deadlock
	and race conditions. The SmartC language is implemented on the automated
	manual transmission (AMT) control system and is compatible with the
	OSEK/VDX specifications. We use a continuous time (CT) model as an
	example to illustrate the effectiveness of the language},
  doi = {10.1109/ICARCV.2006.345212},
  keywords = {C code generation;SmartC modeling language;SmartOSEK operating system;automated
	manual transmission control system;automotive electronic programming;component-based
	hierarchical modeling language;concurrency errors;control-flow oriented
	model;data-flow oriented model;deadlock;embedded systems;engine control
	system;intertask communication;race condition;task triggering;automotive
	electronics;concurrency control;embedded systems;engines;object-oriented
	languages;program compilers;road vehicles;specification languages;}
}

@INPROCEEDINGS{1609882,
  author = {Guoqing Yang and Minde Zhao and Lei Wang and Zhaohui Wu},
  title = {Model-based design and verification of automotive electronics compliant
	with OSEK/VDX},
  booktitle = {Embedded Software and Systems, 2005. Second International Conference
	on},
  year = {2005},
  pages = { 7 pp.},
  month = {dec.},
  abstract = { Model-based approaches are gradually applied in embedded system design
	with Unified Modeling Language (UML) and its profiles, but in terms
	of automotive electronics domain, few developers adopt UML to design
	system models because of inadequate tools that support the domain-specific
	modeling. This paper puts forward a model-based approach for automobile
	electronics software design and verification with a dependable platform
	compliant with OSEK/VDX standard. In addition, a case study is presented
	to demonstrate the application of the approach. The contribution
	of the approach is threefold. First, the approach applies the theory
	of model-based design with OSEK/VDX standard in automotive electronics
	domain. Second, the approach solves the transformation between UML
	models and OSEK/VDX models through an efficient method. Third, the
	approach simulates the system models and provides the designer with
	the results to optimize the design at design-level.},
  doi = {10.1109/ICESS.2005.70},
  keywords = { OSEK; Unified Modeling Language; VDX; automobile electronics software
	design; automobile electronics software verification; automotive
	electronics; embedded system design; Unified Modeling Language; automotive
	electronics; electronic engineering computing; embedded systems;
	formal verification; object detection; object-oriented programming;
	open systems; standards; vehicles;}
}

@INPROCEEDINGS{5494305,
  author = {Junwei Yang and Zhongxiang Hu and Yujun Zheng},
  title = {Macml: A Domain-Specific Language for Machinery Service Management},
  booktitle = {Service Sciences (ICSS), 2010 International Conference on},
  year = {2010},
  pages = {293 -297},
  month = {may},
  abstract = {The paper presents Macml, a domain-specific language (DSL) that focuses
	on the effective specification, implementation, and verification
	of information systems in the domain of machinery services. As a
	meta-model of the application domain, the language precisely defining
	elements including entities, relationships, behaviors, constraints,
	and workflows, based on which the users, domain experts, and software
	engineers can effectively communicate with each other and work together
	to model a variety of machinery service management systems, which
	are all instances of the meta-model and which can be further transformed
	into executable systems mechanically. As a case study, a system model
	of Macml is presented to illustrate the implementation of our approach.},
  doi = {10.1109/ICSS.2010.12},
  keywords = {Macml;domain-specific language;information systems specification;information
	systems verification;machinery service management;meta-model;formal
	specification;formal verification;information systems;programming
	languages;}
}

@INPROCEEDINGS{4906782,
  author = {Yuhang Yang and Qin Lu and Tiejun Zhao},
  title = {A clustering based approach for domain relevant relation extraction},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2008. NLP-KE
	'08. International Conference on},
  year = {2008},
  pages = {1 -8},
  month = {oct.},
  abstract = {Most existing corpus based relation extraction techniques focus on
	predefined relations. In this paper, a clustering based method is
	presented for domain relevant relation extraction including both
	relation type discovery and relation instance extraction. Given two
	raw corpora, one in the general domain, one in an application domain,
	domain specific verbs connecting different instances are extracted
	based on syntactic dependency as well as a small set of domain concept
	instance seeds. Relation types are then discovered based on verb
	clustering followed by relation instance extraction. The proposed
	approach requires no predefined relation types, no prior training
	of domain knowledge, and no need for manually annotated corpora.
	This method is applicable to any domain corpus and it is especially
	useful for knowledge-limited and resource-limited domains. Evaluations
	conducted on Chinese football domain for relation extraction show
	that the approach discovers various relations with good performance.},
  doi = {10.1109/NLPKE.2008.4906782},
  keywords = {Chinese football domain;corpus based relation extraction techniques;domain
	corpus;domain relevant relation extraction;relation instance extraction;relation
	type discovery;verb clustering;information retrieval;natural language
	processing;pattern clustering;}
}

@INPROCEEDINGS{5070863,
  author = {Yazdanshenas, A.R. and Khosravi, R.},
  title = {Using Domain-Specific Languages to Describe the Development Viewpoint
	of Software Architectures},
  booktitle = {Information Technology: New Generations, 2009. ITNG '09. Sixth International
	Conference on},
  year = {2009},
  pages = {1595 -1596},
  month = {april},
  abstract = {In this paper, we introduced the idea of the development metamodel
	as a means to guide the developers throughout the development process.
	We suggested switching the focus from the software to the developers
	by introducing a new expressive model. We showed how a lightweight
	(Java-based) embedded domain-specific language can help the architect
	to describe intended guidelines and constraints clearly and concisely.
	We also showed how this DSL, can be a used to ensure the constraints
	in the final product.},
  doi = {10.1109/ITNG.2009.269},
  keywords = {development metamodel;embedded domain-specific language;software architecture
	development viewpoint;software architecture;}
}

@INPROCEEDINGS{5349322,
  author = {Yazdanshenas, A.R. and Khosravi, R.},
  title = {Using domain-specific languages to describe the development viewpoint
	of software architectures},
  booktitle = {Computer Conference, 2009. CSICC 2009. 14th International CSI},
  year = {2009},
  pages = {146 -151},
  month = {oct.},
  abstract = {The developers of a system are accepted as one of the most important
	stakeholders of an architecture description. The development viewpoint
	is suggested to satisfy the needs of the developers throughout the
	development process via codeline organization descriptions, programming
	models, etc. However, the available models for such purposes, if
	any, barely cross informal natural language descriptions and checklists.
	This paper introduces the idea of enhancing the description of the
	development viewpoint using lightweight domain-specific languages
	and presents the application of such languages in two industrial
	case studies. This language enables the architect to provide the
	necessary guidelines that constrains the implementers during the
	development process and it is also used as a means to discover the
	deviation of the code from the architecture as the development goes
	on.},
  doi = {10.1109/CSICC.2009.5349322},
  keywords = {architecture description;codeline organization descriptions;domain-specific
	languages;programming models;software architectures;codes;programming
	languages;software architecture;}
}

@INPROCEEDINGS{1493653,
  author = {Ye, Q. and Sloane, A.M. and Verity, D.R.},
  title = {Analysis, specification and generation of mobile computer data synchronisation},
  booktitle = {Mobile Business, 2005. ICMB 2005. International Conference on},
  year = {2005},
  pages = { 499 - 506},
  month = {july},
  abstract = { In current technologies, mobile computer data synchronisation protocols
	are typically programmed at a low-level. The disadvantages of this
	are that they are error-prone and time consuming. This paper analyses
	current Palm OS data synchronisation problems and applies embedded
	domain-specific language (EDSL) techniques in the problem domain
	to improve the current situation. The key advantage is that domain
	developers can describe problems using their natural terms and concepts
	at high conceptual level. Our approach allows equivalent code to
	be generated automatically from higher-level specifications, enabling
	domain developers to express their ideas quickly and concisely, to
	work more productively and to avoid certain kinds of coding error.},
  doi = {10.1109/ICMB.2005.19},
  keywords = { Palm OS; data synchronisation protocol; embedded domain-specific
	language; higher-level specification; mobile computer data synchronisation;
	formal specification; mobile computing; specification languages;
	synchronisation;}
}

@INPROCEEDINGS{5764678,
  author = {Qing Yi},
  title = {Automated programmable control and parameterization of compiler optimizations},
  booktitle = {Code Generation and Optimization (CGO), 2011 9th Annual IEEE/ACM
	International Symposium on},
  year = {2011},
  pages = {97 -106},
  month = {april},
  abstract = {We present a framework which effectively combines programmable control
	by developers, advanced optimization by compilers, and flexible parameterization
	of optimizations to achieve portable high performance. We have extended
	ROSE, a C/C++/Fortran source-to-source optimizing compiler, to automatically
	analyze scientific applications and discover optimization opportunities.
	Instead of directly generating optimized code, our optimizer produces
	parameterized scripts in POET, an interpreted program transformation
	language, so that developers can freely modify the optimization decisions
	by the compiler and add their own domain-specific optimizations if
	necessary. The auto-generated POET scripts support extra optimizations
	beyond those available in the ROSE optimizer. Additionally, all the
	optimizations are parameterized at an extremely fine granularity,
	so the scripts can be ported together with their input code and automatically
	tuned for different architectures. Our results show that this approach
	is highly effective, and the code optimized by the auto-generated
	POET scripts can significantly outperform those optimized using the
	ROSE optimizer alone.},
  doi = {10.1109/CGO.2011.5764678},
  keywords = {C-C++-Fortran source-to-source optimizing compiler;ROSE optimizer;autogenerated
	POET scripts;automated programmable control;compiler optimization;domain
	specific optimization;flexible parameterization;interpreted program
	transformation language;optimized code;parameterized scripts;portable
	high performance;C++ language;FORTRAN;optimising compilers;programmable
	controllers;}
}

@INPROCEEDINGS{796489,
  author = {Li Yingjun and Lu Jian},
  title = {Framework-based software reuse for interactive seismic processing
	applications},
  booktitle = {Technology of Object-Oriented Languages and Systems, 1999. TOOLS
	31. Proceedings},
  year = {1999},
  pages = {239 -244},
  abstract = {Domain-specific application frameworks have received considerable
	attention from researchers and developers and will become one of
	the future trends in object oriented framework research. Based on
	a deep understanding and analysis of OO framework techniques, we
	study the existing problems in oil and gas exploration applications,
	and propose a specific framework for interactive seismic data processing
	application software. Meanwhile, a set of design-pattern-oriented
	components are introduced so that software extensibility and code
	reusability can be considerably enhanced},
  doi = {10.1109/TOOLS.1999.796489},
  keywords = {OO framework techniques;code reusability;design-pattern-oriented components;domain-specific
	application frameworks;framework based software reuse;gas exploration
	applications;interactive seismic data processing application software;interactive
	seismic processing applications;object oriented framework research;software
	extensibility;geophysical prospecting;geophysics computing;interactive
	systems;object-oriented programming;seismology;software reusability;}
}

@INPROCEEDINGS{931573,
  author = {Aesun Yoon},
  title = {Development of a domain-specific multilingual terminology lexicon
	using DiET 2.5},
  booktitle = {Industrial Electronics, 2001. Proceedings. ISIE 2001. IEEE International
	Symposium on},
  year = {2001},
  volume = {2},
  pages = {816 -821 vol.2},
  abstract = {The Korean government has adopted the French TGV as a high-speed transportation
	system and the first service is scheduled at the end of 2003. TGV-relevant
	documents consist of huge volumes, of which over than 76% have been
	translated into English. A large part of the English version is,
	however, incomprehensible without referring to the original French
	version. The goal of this paper is to demonstrate how DiET 2.5, a
	lexicon builder, makes it possible to build, with ease, a domain-specific
	terminology lexicon that may contain multimedia and multilingual
	data with multi-layered logical information. The authors believe
	their work shows an important step in enlarging the language scope
	and the development of electronic lexica, and in providing the flexibility
	of defining any type of the DTD and the interconnectivity among collaborators.
	As an application of DiET 2.5, they would like to build a TGV-relevant
	lexicon in the near future},
  doi = {10.1109/ISIE.2001.931573},
  keywords = {DiET 2.5;Korea;TGV high-speed transportation system;collaborator interconnectivity;domain-specific
	multilingual terminology lexicon;multi-layered logical information;multilingual
	data;multimedia data;database management systems;language translation;multimedia
	computing;nomenclature;software packages;}
}

@INPROCEEDINGS{6004505,
  author = {Sanghyun Yoon and Jaeyeon Jo and Junbeom Yoo},
  title = {A Domain-Specific Safety Analysis for Digital Nuclear Plant Protection
	Systems},
  booktitle = {Secure Software Integration Reliability Improvement Companion (SSIRI-C),
	2011 5th International Conference on},
  year = {2011},
  pages = {68 -75},
  month = {june},
  abstract = {Rigorous safety demonstration through safety analysis is strongly
	mandated for safety-critical systems. Nuclear plant protection systems
	often use techniques such as FTA, FMEA and HAZOP. Safety experts
	perform them manually, and quality of the analysis totally depends
	on the ability and experience of the experts. If we restrict the
	application domain of safety analysis into specific critical failures,
	we could automate a large part of the analysis and also improve its
	quality too. This paper proposes a domain-specific safety analysis
	technique, NuFTA, for nuclear plant protection systems. NuFTA mechanically
	constructs a software fault tree of nuclear reactor protection systems
	specified with NuSCR requirement formal specification language. The
	root failures of the fault tree constructed through NuFTA are restricted
	into 'shutdown' events of nuclear reactors, which is the most important
	event in the domain. Within the domain specific restrictions, NuFTA
	can construct software fault trees mechanically and aid safety experts'
	analyses efficiently.},
  doi = {10.1109/SSIRI-C.2011.21},
  keywords = {FMEA technique;FTA technique;HAZOP technique;NuFTA technique;digital
	nuclear plant protection system;domain-specific safety analysis;formal
	specification language;nuclear reactor protection systems;safety-critical
	system;software fault tree;fission reactor safety;formal specification;nuclear
	engineering computing;safety-critical software;}
}

@INPROCEEDINGS{4195168,
  author = {Kim Ju Young and Shim Kwang Hyun},
  title = {Methodology for Automatic Synthesis of Wargame Simulator using DEVS},
  booktitle = {Advanced Communication Technology, The 9th International Conference
	on},
  year = {2007},
  volume = {1},
  pages = {436 -441},
  month = {feb.},
  abstract = {In specific domain such as wargame, simulator developers may not well
	understand domain knowledge, which domain experts know. In such a
	case, the developers may leave detail domain knowledge within simulation
	models as a black box which is filled by domain experts. Thus, a
	simulator can be synthesized by filling the black box with algorithms
	for domain specific objects. This paper proposes a methodology for
	automatic synthesis of wargame simulators which are developed by
	the "DEVS (discrete event systems specification)" framework. For
	the synthesis the co-modeling methodology is employed in the specification
	and implementation of discrete event models.},
  doi = {10.1109/ICACT.2007.358389},
  issn = {1738-9445},
  keywords = {DEVS;automatic wargame simulator synthesis;discrete event systems
	specification;domain knowledge;Unified Modeling Language;computer
	games;discrete event simulation;formal specification;}
}

@INPROCEEDINGS{5713414,
  author = {Yousefipour, A. and Neiat, A.G. and Mohsenzadeh, M. and Hemayati,
	M.S.},
  title = {An ontology-based approach for ranking suggested semantic web services},
  booktitle = {Advanced Information Management and Service (IMS), 2010 6th International
	Conference on},
  year = {2010},
  pages = {17 -22},
  month = {30 2010-dec. 2},
  abstract = {With the growing number of web services on the web that offer similar
	functions, it is required to use mechanisms for ranking and selecting
	them based on non-functional properties (i.e. QoS). Different QoS
	models and language may use different concepts, metrics, units, values
	for representing QoS information. It is difficult for an automatic
	semantic web service discovery system to be able to understand such
	QoS information collected from some semantic web services in order
	to decide about the best semantic web service in terms of their quality.
	In this paper, an ontology-based approach for ranking semantic web
	services has been proposed. A generic and domain-specific ontology
	is used to infer the semantic similarity between the parameters of
	the request and the advertisement, which will be applied in the process
	of SWSs ranking. This ontology is referred to as Extended OWL-Q (EOWL-Q).
	Furthermore some OWL-Q disadvantages and the way of resolving them
	in EOWL-Q were studied to achieve a same understanding of QoS properties.
	Moreover, how semantic web service ranking can be used in the context
	of semantic web service discovery in new QoS-aware framework is explained.},
  keywords = {QoS information;QoS model;QoS-aware framework;SWS ranking;World Wide
	Web;automatic semantic Web service discovery;domain-specific ontology;semantic
	Web service ranking;semantic similarity;Web services;ontologies (artificial
	intelligence);semantic Web;}
}

@INPROCEEDINGS{4608538,
  author = {Jianqi Yu and Lalanda, P.},
  title = {Integrating UPnP in a development environment for service-oriented
	applications},
  booktitle = {Industrial Technology, 2008. ICIT 2008. IEEE International Conference
	on},
  year = {2008},
  pages = {1 -5},
  month = {april},
  abstract = {Composing software services is a challenging activity that requires
	solving both low-level technical problems and high- level semantic
	issues. In this paper, we present a tool allowing service composition
	within domains. A service composition is described with a model description
	language with domain concepts. The tool integrates multiples technologies
	like UPnP or DPWS to automate the development of service composition
	for a developer without any specific technology knowledge. This environment
	has been used successfully for the development of Internet gateways
	in the building computing field.},
  doi = {10.1109/ICIT.2008.4608538},
  keywords = {DPWS;Internet gateways;UPnP;composing software services;development
	environment;model description language;service composition;service-oriented
	applications;Internet;object-oriented languages;}
}

@INPROCEEDINGS{131516,
  author = {Yu, L. and Osborn, S.L.},
  title = {An evaluation framework for algebraic object-oriented query models
	},
  booktitle = {Data Engineering, 1991. Proceedings. Seventh International Conference
	on},
  year = {1991},
  pages = {670 -677},
  month = {apr},
  abstract = {An evaluation framework consisting of five categories of criteria
	is developed for evaluating the relative merits of objects algebras,
	namely, object-orientedness, expressiveness, formalness, performance
	and database issues. Four recently proposed object algebras are evaluated
	against these criteria. It is shown that there exists no object algebra
	that satisfies all the criteria. It is argued that, since some of
	the criteria may not be compatible, a feasible object algebra has
	to make some tradeoffs to suit domain-specific needs. It is possible
	to identify a minimal subset of the criteria. The criterion that
	an object algebra should support encapsulation seems to be the most
	important. If an object algebra fails to support this criterion,
	its semantics is inconsistent with the concept of `data abstraction',
	which makes a language object-oriented},
  doi = {10.1109/ICDE.1991.131516},
  keywords = {algebraic object-oriented query models;data abstraction;database;domain-specific
	needs;evaluation framework;expressiveness;formalness;object-orientedness;objects
	algebras;performance;object-oriented databases;performance evaluation;}
}

@INPROCEEDINGS{4723041,
  author = {Zhenming Yuan and Guichao Jin},
  title = {Sketch Recognition Based Intelligent Whiteboard Teaching System},
  booktitle = {Computer Science and Software Engineering, 2008 International Conference
	on},
  year = {2008},
  volume = {5},
  pages = {867 -870},
  month = {dec.},
  abstract = {The virtual electronic whiteboard is one of the main tools of the
	CSCL. This paper realizes an intelligent whiteboard system based
	on sketch recognition. The system is build on multitier distributed
	architecture, which can support the users to communicate with each
	other by sketching and chatting on pen based HCI. It can recognize
	the gestures to help users modify, copy and move sketches as familiar
	pencil-and-paper process, and transform the free-hand sketches into
	the symbols of the domain-specific knowledge automatically using
	feature points detection and graphics recognition algorithms. Such
	novel AI-aid sketching communication can support building connection
	between the concrete graphics and the abstract concepts during the
	learning activities, especially for information technology education.
	The experimental whiteboard system fulfills a flowchart-to-C programming
	and an ER diagram-to-database recognition and communication framework.
	The result of the usability evaluation of the experiment suggests
	that this system has higher efficiency for collaborative learning
	and more sufficient for the concept learning of information science.},
  doi = {10.1109/CSSE.2008.123},
  keywords = {ER diagram-to-database recognition;artificial intelligent-aid sketching
	communication;computer supported cooperative learning;feature points
	detection;flowchart-to-C programming;gesture recognition;graphics
	recognition algorithms;information science;information technology
	education;intelligent whiteboard teaching system;multitier distributed
	architecture;pen based HCI;pencil-and-paper process;sketch recognition;usability
	evaluation;virtual electronic whiteboard;C language;artificial intelligence;computer
	aided instruction;computer science education;flowcharting;gesture
	recognition;groupware;human computer interaction;interactive devices;software
	architecture;teaching;}
}

@INPROCEEDINGS{5478911,
  author = {Zamil, M.A.L. and Can, A.B.},
  title = {Toward effective medical search engines},
  booktitle = {Health Informatics and Bioinformatics (HIBIT), 2010 5th International
	Symposium on},
  year = {2010},
  pages = {21 -26},
  month = {april},
  abstract = {In this paper, we present a domain specific search engine that relies
	on extracting the semantic relation among medical documents. Our
	goal is to maximize the contextual retrieval and ranking performance
	with minimum input from users. We have performed experiments to measure
	the effectiveness of the proposed technique by evaluating the performance
	of the retrieval process in terms of recall, precision and topical
	ranking. The results indicated that the proposed medical search engine
	achieved higher average precision in compare with highest scored
	runs submitted to TREC-9.},
  doi = {10.1109/HIBIT.2010.5478911},
  keywords = {contextual retrieval;medical documents;medical search engines;semantic
	relation;document handling;information retrieval;medical information
	systems;search engines;}
}

@INPROCEEDINGS{5276555,
  author = {Zamli, K.Z. and Isa, N.A.M. and Khamis, N.},
  title = {Implementing executable graph based visual language in a distributed
	environment},
  booktitle = {Computing Informatics, 2006. ICOCI '06. International Conference
	on},
  year = {2006},
  pages = {1 -6},
  month = {june},
  abstract = {One of the common difficulties in a graph based visual language is
	to develop its executable semantics and achieved its execution in
	a distributed environment. In order to address some of these issues,
	this paper outlines the general control flow semantics of a graph
	based visual language. In doing so, this paper also discusses a sound
	technique implementing such semantics permitting execution in a distributed
	environment. An implementation is sketched for a domain specific
	graph based visual language, called VRPML.},
  doi = {10.1109/ICOCI.2006.5276555},
  keywords = {control flow semantics;distributed environment;graph based visual
	programming language;distributed processing;graph theory;programming
	language semantics;visual languages;visual programming;}
}

@INPROCEEDINGS{4344883,
  author = {Hongying Zan and Guocheng Duan and Ming Fan},
  title = {Single Word Term Extraction Using a Bilingual Semantic Lexicon-Based
	Approach},
  booktitle = {Natural Computation, 2007. ICNC 2007. Third International Conference
	on},
  year = {2007},
  volume = {5},
  pages = {451 -456},
  month = {aug.},
  abstract = {The existing approaches to automatic term recognition include these
	types: dictionary-based, rule-based, statistical, etc. First, we
	discuss the dictionary-based methods briefly in this paper. Then
	we propose an approach for Chinese single word term extraction combining
	the dictionary-based method with seed knowledge-based method. Our
	method is based on two resources. One is the Chinese concept dictionary
	which is a general bilingual semantic lexicon and the other one is
	the bilingual seeds set extracted from a bilingual glossary of HK
	law. The approach is to recognize the legal domain-specific term.
	Our approach is applying general semantic lexicon for domain-specific
	term extraction. The experimental results show that our approach
	can get high precision in legal field. Keywords: automatic term recognition,
	bilingual seeds set, Chinese concept dictionary, legal terminology,
	single word term.},
  doi = {10.1109/ICNC.2007.667},
  keywords = {Chinese single word term extraction;automatic term recognition;bilingual
	glossary;bilingual semantic lexicon;dictionary-based methods;dictionaries;glossaries;natural
	language processing;text analysis;word processing;}
}

@INPROCEEDINGS{5194159,
  author = {Zarraonandia, T. and Diaz, P. and Guerra, E. and Vargas, M.R.R. and
	Aedo, I.},
  title = {A Framework for the Multi-disciplinary Design of Web-Based Educational
	Systems},
  booktitle = {Advanced Learning Technologies, 2009. ICALT 2009. Ninth IEEE International
	Conference on},
  year = {2009},
  pages = {45 -49},
  month = {july},
  abstract = {The communication and the collaboration in a multi-disciplinary team
	can be diminished due to the different specification tools and languages
	used by the diverse members of the team. The MODUWEB approach tackles
	this problem allowing the integration of different design perspectives
	through model-driven development (MDD) techniques. Such integration
	is completely transparent to the users of the modeling languages,
	that is, the designers of the Web-based educational systems, and
	in this way their productivity is not affected. This paper describes
	a prototype of a system which implements the MODUWEB approach to
	combine two domain specific languages dealing with different perspectives
	of the design of a Web-based educational system: IMS learning design
	for educational design and Labyrinth for Web design.},
  doi = {10.1109/ICALT.2009.121},
  keywords = {IMS learning design;MODUWEB approach;Web design;Web-based educational
	system;educational design;model-driven development techniques;modeling
	language;multidisciplinary design;Web design;computer aided instruction;simulation
	languages;}
}

@INPROCEEDINGS{4020070,
  author = {Zbib, R. and Jain, A. and Bassu, D. and Agrawal, H.},
  title = {Generating Domain Specific Graphical Modeling Editors from Meta Models},
  booktitle = {Computer Software and Applications Conference, 2006. COMPSAC '06.
	30th Annual International},
  year = {2006},
  volume = {1},
  pages = {129 -138},
  month = {sept.},
  abstract = {We describe an approach for automatically generating application aware
	graphical modeling environments from the meta-model specification
	of an application domain. A generated graphical modeling environment:
	a) provides domain-specific graphical metaphors in the modeling palette,
	b) imposes domain-specific modeling constraints to prevent semantically
	incorrect models, and c) provides domain-specific operators and languages
	to capture application domain constraints. The domain meta-model
	is specified using a meta-model which is an extension of the UML
	meta-model. One of the major advantage of using application domain-specific
	modeling environment is to make it easy for business analyst to create
	semantically correct models. We use UML2.0 to specify the domain
	metamodel. The advantage of using UML2.0 representation is the reuse
	of vendor-supported technologies including MDA tools. An implementation
	using the Eclipse framework is also discussed},
  doi = {10.1109/COMPSAC.2006.48},
  issn = {0730-3157},
  keywords = {Eclipse framework;MDA tools;UML metamodel;UML2.0 representation;application
	aware graphical modeling;domain-specific graphical metaphor;domain-specific
	modeling constraint;domain-specific operator;metamodel specification;vendor-supported
	technology;Unified Modeling Language;formal specification;software
	reusability;}
}

@INPROCEEDINGS{4547699,
  author = {Zebelein, C. and Falk, J. and Haubelt, C. and Teich, J.},
  title = {Classification of General Data Flow Actors into Known Models of Computation},
  booktitle = {Formal Methods and Models for Co-Design, 2008. MEMOCODE 2008. 6th
	ACM/IEEE International Conference on},
  year = {2008},
  pages = {119 -128},
  month = {june},
  abstract = {Applications in the signal processing domain are often modeled by
	data flow graphs which contain both dynamic and static data flow
	actors due to heterogeneous complexity requirements. Thus, the adopted
	notation to model the actors must be expressive enough to accommodate
	dynamic data flow actors. On the other hand, treating static data
	flow actors like dynamic ones hinders design tools in applying domain-specific
	optimization methods to static parts of the model, e.g., static scheduling.
	In this paper, we present a general notation and a methodology to
	classify an actor expressed by means of this notation into the synchronous
	and cyclo-static dataflow models of computation. This enables the
	use of a unified descriptive language to express the behavior of
	actors while still retaining the advantage to apply domain-specific
	optimization methods to parts of the system. In experiments we could
	improve both latency and throughput of a general data flow graph
	application using our proposed automatic classification in combination
	with a static single-processor scheduling approach by 57%.},
  doi = {10.1109/MEMCOD.2008.4547699},
  keywords = {data flow graph;domain-specific optimization method;dynamic data flow
	actor;heterogeneous complexity requirement;signal classification;signal
	processing;static data flow actor;static single-processor scheduling;unified
	descriptive language;data flow graphs;optimisation;signal classification;}
}

@INPROCEEDINGS{972765,
  author = {Zedan, H. and Zhou, S. and Sampat, N. and Chen, X. and Cau, A. and
	Yang, H.},
  title = {K-Mediator: Towards evolving information systems},
  booktitle = {Software Maintenance, 2001. Proceedings. IEEE International Conference
	on},
  year = {2001},
  pages = {520 -527},
  abstract = {Business processes and goals change rapidly due to the turbulent environment
	in which they operate. Their supporting and underpinning technologies
	may, as a result, require to change. Meanwhile, technological advances
	are being made at an increasing rate. This may trigger changes in
	business goals and processes to exploit these new advances; a notable
	example is e-commerce and e-learning. Understanding and analyzing
	the interplay and the dual effect between these two entities, technologies
	and business, is vital both for the prosperity of business and the
	success and further development of the technologies themselves. This
	paper proposes the K-Mediator framework together with its underpinning
	theory to facilitate the co-evolution between businesses and their
	supporting technologies},
  doi = {10.1109/ICSM.2001.972765},
  keywords = {K-Mediator;business processes;e-commerce;e-learning;evolving information
	systems;software maintenance;software process improvement;}
}

@INPROCEEDINGS{4296640,
  author = {Cong Zhang and Bakshi, A. and Prasanna, V.K.},
  title = {ModelML: a Markup Language for Automatic Model Synthesis},
  booktitle = {Information Reuse and Integration, 2007. IRI 2007. IEEE International
	Conference on},
  year = {2007},
  pages = {317 -322},
  month = {aug.},
  abstract = {Domain-specific modeling has become a popular way of designing and
	developing systems. It generally involves a systematic use of a set
	of object-oriented models to represent various facets of a domain.
	However, manually creating instances of these models is time-consuming
	and error-prone when a system in the domain is complex. Automatic
	model synthesis tools are thus usually developed to free users from
	the model creation process. In practice, most of these tools would
	hard code knowledge about the domain specific models in the program.
	A biggest problem with these tools is that their source code needs
	to be changed whenever the knowledge changes. In this paper, we define
	a model markup language (ModelML) to facilitate the development of
	automatic model synthesis tools. The language provides a complete
	self-describing representation of object-oriented models to be synthesized.
	Unlike other XML-based representations of models, ModelML reflects
	the structure of the models directly in the nesting of elements in
	the XML-based syntax. This feature allows the knowledge about the
	domain specific models to be decoupled from model synthesis tools.
	To demonstrate the usefulness of the markup language, we have developed
	a generic automatic model synthesis tool which is based on ModelML
	inputs.},
  doi = {10.1109/IRI.2007.4296640},
  keywords = {ModelML language;XML-based model markup language;automatic model synthesis
	tools;domain-specific modeling;model creation process;object-oriented
	models;XML;formal specification;object-oriented methods;}
}

@INPROCEEDINGS{4018549,
  author = {Cong Zhang and Bakshi, A. and Prasanna, V. and Da Sie, W.},
  title = {Towards a Model-based Application Integration Framework for Smart
	Oilfields},
  booktitle = {Information Reuse and Integration, 2006 IEEE International Conference
	on},
  year = {2006},
  pages = {545 -550},
  month = {sept.},
  abstract = {The increasing demand for cost-effective oil and gas production has
	led to an industry-wide push to develop smart oilfields for the future.
	Applications for smart oilfields are characterized with heterogeneous
	data and resources, complicated business processes, and changing
	business requirements from users. Existing software development process
	and techniques have become increasingly incapable of managing such
	complex software systems. Model-based integration frameworks are
	based on a domain-specific modeling language and a common model database.
	They offer the benefits of extensibility, modularity, and reusability
	of both code and design to the applications. In this paper, we describe
	a prototype integration framework for a class of oilfield applications.
	To demonstrate the advantages of the integration framework, we show
	how applications are developed and integrated in the framework in
	a systematic manner},
  doi = {10.1109/IRI.2006.252472},
  keywords = {business process;business requirement;common model database;complex
	software system;cost-effective oil;domain-specific modeling language;gas
	production;heterogeneous data resource;model-based application integration
	framework smart oilfield;software development process;integrated
	software;petroleum industry;software reusability;}
}

@INPROCEEDINGS{5234687,
  author = {Chunxia Zhang and Peng Jiang},
  title = {Automatic extraction of definitions},
  booktitle = {Computer Science and Information Technology, 2009. ICCSIT 2009. 2nd
	IEEE International Conference on},
  year = {2009},
  pages = {364 -368},
  month = {aug.},
  abstract = {The task of definition extraction aims to acquire definitions of terms
	from texts. This task is a subtask of terminology extraction, ontology
	construction, semantic relation learning, and question answering
	and so on. This paper presents a bootstrapping approach to automatic
	extracting definitions of domain-specific terms from unannotated
	Chinese free texts. Experimental results in three domains of computer,
	military, and archaeology show effectiveness of our algorithm. As
	an application of definition extraction, definitions identified by
	our method have been used to answer factual questions in a question
	answering system.},
  doi = {10.1109/ICCSIT.2009.5234687},
  keywords = {archaeology domain;automatic definition extraction;bootstrapping approach;computer
	domain;data mining;domain-specific term;military domain;ontology
	construction;question answering system;semantic relation learning;terminology
	extraction;unannotated Chinese free text;data mining;information
	retrieval;learning (artificial intelligence);natural language processing;ontologies
	(artificial intelligence);text analysis;}
}

@INPROCEEDINGS{1506514,
  author = {Zhang, C. and Viktor Prasanna and Orangi, A. and Da Sie, W. and Kwatra,
	A.},
  title = {Modeling methodology for application development in petroleum industry},
  booktitle = {Information Reuse and Integration, Conf, 2005. IRI -2005 IEEE International
	Conference on.},
  year = {2005},
  pages = { 445 - 451},
  month = {aug.},
  abstract = { The development of applications for monitoring, control, simulation
	and diagnosis in the petroleum industry involves a multitude of complex
	software tools. These tools have their own formalisms, semantics
	and use different abstractions to represent the system under development.
	They use different data formats to represent data in the software
	tools. Each application requires coupling of two or more different
	such complex software tools. Providing efficient interaction between
	these complex software tools using different abstractions, formalisms,
	data formats, etc. becomes a mammoth task. Thus there is a need to
	provide a unified environment that allows capturing the desired application
	and provide a framework for interaction between the necessary software
	tools. This paper discusses the formal metamodels to describe the
	individual formalisms in the desired unified environment. These metamodels,
	created by the Generic Modeling Environment (GME), define the domain-specific
	modeling language for application development in the petroleum industry.},
  doi = {10.1109/IRI-05.2005.1506514},
  issn = { },
  keywords = { Generic Modeling Environment; domain-specific modeling language;
	formal metamodels; modeling methodology; petroleum industry; software
	tools; formal specification; petroleum industry; software tools;
	specification languages;}
}

@INPROCEEDINGS{4370728,
  author = {Chun-Xia Zhang and Cun-Gen Cao and Lei Liu and Zhen-Dong Niu and
	Jun-Hong Lin},
  title = {Extracting Hyponymy Relations from Domain-Specific Free Texts},
  booktitle = {Machine Learning and Cybernetics, 2007 International Conference on},
  year = {2007},
  volume = {6},
  pages = {3360 -3365},
  month = {aug.},
  abstract = {Domain-specific ontologies have shown their powerful usefulness in
	many application areas, such as semantic web, information sharing,
	and natural language processing. However, manually building of domain
	ontologies still remains a tedious and cumbersome task. Hyponymy
	is a core component of domain-specific ontologies. In this paper,
	we propose three symbolic learning methods, which are integrated
	together to extract hyponymies from un-annotated domain-specific
	Chinese free texts. The three symbolic learning methods include seed-driven
	learning, pattern-mediated learning, and term composition based learning.
	Experimental results show that the algorithm is adequate to extracting
	the hyponymies from unstructured domain-specific Chinese corpus.},
  doi = {10.1109/ICMLC.2007.4370728},
  keywords = {domain-specific ontology;hyponymy extraction;information sharing;natural
	language processing;pattern-mediated learning;seed-driven learning;semantic
	web;symbolic learning method;term composition based learning;unannotated
	domain-specific Chinese free text;learning (artificial intelligence);ontologies
	(artificial intelligence);text analysis;}
}

@INPROCEEDINGS{1541161,
  author = {Hongyu Zhang and Bradbury, J.S. and Cordy, J.R. and Dingel, J.},
  title = {Implementation and verification of implicit-invocation systems using
	source transformation},
  booktitle = {Source Code Analysis and Manipulation, 2005. Fifth IEEE International
	Workshop on},
  year = {2005},
  pages = { 87 - 96},
  month = {sept.-1 oct.},
  abstract = { In this paper we present a source transformation-based framework
	to support uniform testing and model checking of implicit-invocation
	software systems. The framework includes a new domain-specific programming
	language, the Implicit-Invocation Language (IIL), explicitly designed
	for directly expressing implicit-invocation software systems, and
	a set of formal rule-based source transformation tools that allow
	automatic generation of both executable and formal verification artifacts.
	We provide details of these transformation tools, evaluate the framework
	in practice, and discuss the benefits of formal automatic transformation
	in this context. Our approach is designed not only to advance the
	state-of-the-art in validating implicit-invocation systems, but also
	to further explore the use of automated source transformation as
	a uniform vehicle to assist in the implementation, validation and
	verification of programming languages and software systems in general.},
  doi = {10.1109/SCAM.2005.15},
  keywords = { Implicit-Invocation Language; domain-specific programming language;
	formal rule-based source transformation; formal verification; implicit-invocation
	systems; model checking; formal verification; program testing;}
}

@INPROCEEDINGS{4959028,
  author = {Jingjun Zhang and Yuejuan Chen and Guangyuan Liu},
  title = {Modeling Aspect-Oriented Programming with UML Profile},
  booktitle = {Education Technology and Computer Science, 2009. ETCS '09. First
	International Workshop on},
  year = {2009},
  volume = {2},
  pages = {242 -245},
  month = {march},
  abstract = {Aspect-Oriented Programming (AOP), which allows for modularizing concerns
	that normally cause crosscutting in object oriented (OO) systems,
	has effectively solved the problem that the Object-Oriented Programming
	(OOP) has encountered such as the scattered codes and tangled codes
	resulting from the cross-cutting concerns. At present, the AOP paradigm
	has expanded to all the lifecycle of the software development, as
	a result, Aspect-Oriented Software Development (AOSD) is becoming
	a new technique, which has an important step of modeling the aspects.
	Modeling aspects is a hot topic these days, however, there is no
	uniform modeling technique to support this new method. Since the
	Unified Modeling Language (UML) is an extensible modeling language
	to facilitate domain specific modeling. At this paper, we use the
	extension mechanisms provided by the UML to model the aspects of
	a system to improve the stations of AOP in the software development.},
  doi = {10.1109/ETCS.2009.314},
  keywords = {Unified Modeling Language;aspect-oriented programming;aspect-oriented
	software development;domain specific modeling;extensible modeling
	language;object-oriented programming;software development lifecycle;Unified
	Modeling Language;object-oriented programming;software engineering;}
}

@INPROCEEDINGS{5587804,
  author = {Keliang Zhang and Qinlong Fei},
  title = {Co-construction of ontology-based knowledge base through the Web:
	Theory and practice},
  booktitle = {Natural Language Processing and Knowledge Engineering (NLP-KE), 2010
	International Conference on},
  year = {2010},
  pages = {1 -6},
  month = {aug.},
  abstract = {Ontology-based knowledge base plays an increasingly important role
	in improving the precision and recall rate of a retrieval system.
	Based on Distributed Learning theory, a novel approach for the co-construction
	of ontology-based knowledge base is explored. Making use of the platform
	set up for the co-construction and sharing of domain-specific knowledge
	through the Web, we constructed an ontology-based knowledge base
	of airborne radar field. This study is expected to contribute to
	the effective improvement of precision and recall rate of information
	retrieval in the airborne radar field. Hopefully, the mode we designed
	and adopted for the co-construction and sharing of domain-specific
	knowledge base could be enlightening for other similar studies.},
  doi = {10.1109/NLPKE.2010.5587804},
  keywords = {Web;airborne radar field;distributed learning theory;information retrieval;ontology-based
	knowledge base;Internet;airborne radar;information retrieval;ontologies
	(artificial intelligence);}
}

@INPROCEEDINGS{1311106,
  author = {Kang Zhang and Guang-Lei Song and Jun Kong},
  title = {Rapid software prototyping using visual language techniques},
  booktitle = {Rapid System Prototyping, 2004. Proceedings. 15th IEEE International
	Workshop on},
  year = {2004},
  pages = { 119 - 126},
  month = {june},
  abstract = { Rapid prototyping of domain-specific software requires a systematic
	software development methodology and user-friendly tools. Being both
	executable and easy to use, visual languages and their automatic
	generation mechanisms are highly suitable for software prototyping.
	This paper presents a software prototyping methodology based on the
	visual language generation technology, that allows visual prototyping
	languages to be specified and generated using an expressive graph
	grammar formalism. Executable prototypes and their verification and
	code generation are made possible by syntax-directed computations.
	The paper demonstrates this methodology through a prototyping example
	built on our current implementation.},
  doi = {10.1109/IWRSP.2004.1311106},
  issn = {1074-6005 },
  keywords = { automatic generation mechanisms; code generation; domain-specific
	software; graph grammar formalism; prototypes verification; rapid
	software prototyping; syntax-directed computations; systematic software
	development; user-friendly tools; visual language techniques; visual
	prototyping languages; graph grammars; program compilers; software
	prototyping; software tools; specification languages; visual languages;
	visual programming;}
}

@INPROCEEDINGS{4618305,
  author = {Lei Zhang and Yong Peng and Xiangwu Meng and Jie Guo},
  title = {Personalized domain-specific search engine},
  booktitle = {Industrial Informatics, 2008. INDIN 2008. 6th IEEE International
	Conference on},
  year = {2008},
  pages = {1308 -1313},
  month = {july},
  abstract = {As web information expands, personalized vertical search engine plays
	a more and more important role in search industry. With ldquobeing
	the best in specialized personalization domain utilizationrdquo in
	mind, we develop personalized domain-specific search engine (114
	PDSE, or 114). We introduce domain-specific effective and efficient
	Chinese segmentation based on multi-grading thesauruses as the base
	for industrial demand acquirement and index building, Next, as another
	key point of our paper, we present our methods of personalized search
	in domain utility, to differentiate the interests of terms in userpsilas
	each query based on domain interest profile which is acquired recurring
	to user-based collaborative filtering methods and statistics on queries.
	The reported experiments indicate that 114 PDSE holds the promise
	of supplying high match-degree search results in personalized domain
	utility well-pleasingly.},
  doi = {10.1109/INDIN.2008.4618305},
  issn = {1935-4576},
  keywords = {Chinese segmentation;Web information expands;domain interest profile;index
	building;industrial demand acquirement;multigrading thesauruses;personalization
	domain utilization;personalized domain-specific search engine;personalized
	vertical search engine;user-based collaborative filtering methods;Internet;information
	filtering;natural language processing;search engines;thesauri;}
}

@INPROCEEDINGS{4019054,
  author = {Li Zhang and Wenyu Zhang and Gang Chen and Yuzhu Wang and Jinxiang
	Dong},
  title = {A Semantic Web-based Architecture for Collaborative Multi-Agent Functional
	Modeling in Design},
  booktitle = {Computer Supported Cooperative Work in Design, 2006. CSCWD '06. 10th
	International Conference on},
  year = {2006},
  pages = {1 -6},
  month = {may},
  abstract = {This paper presents a novel architecture for collaborative multi-agent
	functional modeling in design on the semantic Web. The proposed architecture
	is composed of two visiting levels, i.e., local level and global
	level. The local level is an ontology-based functional modeling framework,
	which uses Web ontology language (OWL) to build a domain-specific
	local functional design ontology repository. Using this local ontology
	repository, the requests coming from the functional design agent
	can be parsed and performed effectively. The global level is a distributed
	multi-agent collaborative virtual environment, in which, OWL is used
	as a content language within the standard FIPA agent communication
	language (FIPA ACL) messages for describing ontologies, enabling
	diverse functional design ontologies between different agents to
	be communicated freely. The proposed architecture facilitates the
	exchange between diverse knowledge representation schemes in different
	functional modeling environments, and supports computer supported
	cooperative work (CSCW) between multiple functional design agents},
  doi = {10.1109/CSCWD.2006.253018},
  keywords = {CSCW;OWL;Web ontology language;agent communication language;collaborative
	multiagent functional modeling;computer supported cooperative work;functional
	design;knowledge representation;ontology;semantic Web-based architecture;groupware;knowledge
	representation languages;multi-agent systems;ontologies (artificial
	intelligence);semantic Web;}
}

@INPROCEEDINGS{5066655,
  author = {Liping Zhang and Minde Zhao and Chao Wang and Ruyi Wu and Hong Li
	and Wang Dongdong and Renfa Li},
  title = {A Bidirectional Generation Method of SmartC Models and Codes},
  booktitle = {Embedded Software and Systems, 2009. ICESS '09. International Conference
	on},
  year = {2009},
  pages = {249 -255},
  month = {may},
  abstract = {This paper proposes a bidirectional generation method with a set of
	consistent bidirectional generation rules between SmartC models and
	codes. Based on these rules, the consistency of the bidirectional
	generation between SmartC models and codes is demonstrated by a case
	study. Extensive tests are conducted to show the performance of this
	bidirectional generation method. And the efforts of different industrial
	applications are recorded to illustrate its advantage of decreasing
	project efforts and accelerating project progress.},
  doi = {10.1109/ICESS.2009.77},
  keywords = {SmartC codes;SmartC models;bidirectional generation method;automotive
	electronics;traffic engineering computing;}
}

@ARTICLE{5386505,
  author = {Zhang, L.-J. and Zhou, N. and Chee, Y.-M. and Jalaldeen, A. and Ponnalagu,
	K. and Sindhgatta, R. R. and Arsanjani, A. and Bernardini, F.},
  title = {SOMA-ME: A platform for the model-driven design of SOA solutions},
  journal = {IBM Systems Journal},
  year = {2008},
  volume = {47},
  pages = {397 -413},
  number = {3},
  month = { },
  abstract = {The service-oriented modeling and architecture modeling environment
	(SOMA-ME) is first a framework for the model-driven design of service-oriented
	architecture (SOA) solutions using the service-oriented modeling
	and architecture (SOMA) method. In SOMA-ME, Unified Modeling Language
	(UML #x2122;) profiles extend the UML 2.0 metamodel to domain-specific
	concepts. SOMA-ME is also a tool that extends the IBM Rational #x00AE;
	Software Architect product to provide a development environment and
	automation features for designing SOA solutions in a systematic and
	model-driven fashion. Extensibility, traceability, variation-oriented
	design, and automatic generation of technical documentation and code
	artifacts are shown to be some of the properties of the SOMA-ME tool.},
  doi = {10.1147/sj.473.0397},
  issn = {0018-8670}
}

@INPROCEEDINGS{4053534,
  author = {Pengfei Zhang and Chunhua Wu and Cong Wang and Xiaohong Huang},
  title = {Personalized Question Answering System Based on Ontology and Semantic
	Web},
  booktitle = {Industrial Informatics, 2006 IEEE International Conference on},
  year = {2006},
  pages = {1046 -1051},
  month = {aug.},
  abstract = {Semantic Web technologies bring new benefits to knowledge-based question
	answering system. Especially, ontology is becoming the pivotal methodology
	to represent domain-specific conceptual knowledge in order to promote
	the semantic capability of a QA system. In this paper we present
	a QA system in which the domain knowledge is represented by means
	of ontology. In addition, personalized services are enabled through
	modeling users' profiles in the form of ontology, and a Chinese natural
	language human-machine interface is implemented mainly through a
	NL parser in this system. An initial evaluation result shows the
	feasibility to build such a semantic QA system based on ontology,
	the effectivity of personalized semantic QA, the extensibility of
	ontology and knowledge base, and the possibility of self-produced
	knowledge based on semantic relations in the ontology.},
  doi = {10.1109/INDIN.2006.275742},
  keywords = {Chinese natural language human-machine interface;JavaTeller;domain-specific
	conceptual knowledge;knowledge-based question answering system;natural
	language parser;ontology;personalized question answering system;personalized
	service;self-produced knowledge;semantic Web;semantic capability;semantic
	relation;user profile modeling;Java;computer aided instruction;computer
	science education;grammars;information retrieval;information retrieval
	systems;natural language interfaces;object-oriented programming;ontologies
	(artificial intelligence);semantic Web;}
}

@INPROCEEDINGS{4368051,
  author = {Qinlong Zhang and Qin Lu and Zhifang Sui},
  title = {Measuring Termhood in Automatic Terminology Extraction},
  booktitle = {Natural Language Processing and Knowledge Engineering, 2007. NLP-KE
	2007. International Conference on},
  year = {2007},
  pages = {328 -335},
  month = {30 2007-sept. 1},
  abstract = {Automatic terminology extraction can be divided into two tasks. The
	first task measures the unithood which is used to identify a string
	as a lexical unit. The second task measures the so called termhood,
	used to identify a lexical unit being a domain specific term. This
	paper proposes a method to measure termhood in Chinese ATE. It considers
	the domain specificity of both the components of a candidate term
	as well as statistical information and other contextual information
	across different domains and applied to a support vector machine
	model for terminology extraction. The experiments are based on the
	Chinese corpus in the IT domain with cross validation of data from
	outside of the IT domain. Results show that the precision of the
	open tests can reach over 80% for the top 2,000 candidates and around
	50% for the top 20,000 candidate. Furthermore, experiments with different
	lexicon size shows that the algorithm does not require a comprehensive
	domain lexicon of a large size. A few thousand basic domain terms
	would be sufficient to achieve the above mentioned performance.},
  doi = {10.1109/NLPKE.2007.4368051},
  keywords = {Chinese ATE;automatic terminology extraction;lexical unit;statistical
	information;support vector machine model;termhood measurement;unithood
	measurement;information retrieval;natural languages;statistical analysis;support
	vector machines;}
}

@INPROCEEDINGS{5635237,
  author = {Zhang, R. and Hosking, J. and Grundy, J. and Mehandjiev, N. and Carpenter,
	M.},
  title = {Design of a Suite of Visual Languages for Supply Chain Specification},
  booktitle = {Visual Languages and Human-Centric Computing (VL/HCC), 2010 IEEE
	Symposium on},
  year = {2010},
  pages = {240 -243},
  month = {sept.},
  abstract = {Supply chain modelling and simulation by SMEs (Small-to-Medium Enterprises)
	is a challenging problem. This is due both to complexity of the supply
	chain models required and the lack of required expertise among the
	SMEs. The problem is important since SMEs need to represent and modify
	their evolving skills and processes to be visible in electronic marketplaces
	and supply chain design platforms. We demonstrate how this problem
	can be addressed by developing a suite of novel domain-specific visual
	languages and a support tool. The challenging setup of our research
	context motivated us to trial a new approach for the design of our
	visual languages and to employ a collaborative development process
	across our distributed research team.},
  doi = {10.1109/VLHCC.2010.41},
  issn = {1943-6092},
  keywords = {SME;collaborative development process;domain-specific visual language;small-to-medium
	enterprises;supply chain modelling;supply chain simulation;supply
	chain specification;groupware;small-to-medium enterprises;supply
	chain management;visual languages;}
}

@INPROCEEDINGS{4549893,
  author = {Tian Zhang and Jouault, F. and Bezivin, J. and Jianhua Zhao},
  title = {A MDE Based Approach for Bridging Formal Models},
  booktitle = {Theoretical Aspects of Software Engineering, 2008. TASE '08. 2nd
	IFIP/IEEE International Symposium on},
  year = {2008},
  pages = {113 -116},
  month = {june},
  abstract = {Different formal methods have presented plenty of formal models for
	system specification and proof. Hence the problem of bridging these
	formal models rises. MDE is a new paradigm in software engineering,
	which implements software by (meta-)modeling and model transforming.
	In this paper, we provide a MDE based approach for bridging heterogeneous
	formal models: Firstly, the heterogeneous formal models are introduced
	into MDE as domain specific languages by metamodeling. Then, transformation
	rules are built for semantics mapping. At last, model-text syntax
	rules are developed, so as to map models to programs. Our approach
	could be applied on formal models in both graphical style and grammatical
	style. A case study of bridging MARTE to LOTOS is also illustrated
	showing the validity and practicability of our approach.},
  doi = {10.1109/TASE.2008.21},
  keywords = {domain specific languages;formal methods;metamodeling;model transforming;model-driven
	engineeering;software engineering;system specification;formal specification;specification
	languages;}
}

@INPROCEEDINGS{6028598,
  author = {Xinxiao Zhao and Xueqing Li and Jiangman Xu},
  title = {Design of a multiple modeling language supported workflow architecture
	in educational information system},
  booktitle = {Computer Science Education (ICCSE), 2011 6th International Conference
	on},
  year = {2011},
  pages = {120 -125},
  month = {aug.},
  abstract = {Using workflow system to manage some business processes is an essential
	work to do in educational information system. In this paper, the
	author bring forward a solution for building a lightweight workflow
	component in some educational information systems based on J2EE technology.
	This workflow component has general function modules of workflow
	system such as workflow engine, process manager, and application
	extensible interface. The process modeling mechanism of this workflow
	component can help us extend different process description languages
	to model business processes in different domain. So, we can build
	some specific modeling languages suitable for modeling business processes
	in educational information system. All business process models described
	by different modeling languages can be translated into execution
	models owning a kind of unified structure to be executed. Rollback
	operation of the business process also can be supported by the way
	of backward resuming the execution track of process.},
  doi = {10.1109/ICCSE.2011.6028598},
  keywords = {J2EE technology;business processes;description language process;educational
	information system;extensible interface application;lightweight workflow
	component;process manager;rollback operation;workflow architecture
	support;workflow engine;Java;educational administrative data processing;information
	systems;}
}

@INPROCEEDINGS{4667833,
  author = {Yiqiang Zhao and Junfang Zeng and Yiping Yang and Lin Chen},
  title = {A Conceptual Network Based Modeling Framework for Semantic Representation
	of Chinese News},
  booktitle = {Natural Computation, 2008. ICNC '08. Fourth International Conference
	on},
  year = {2008},
  volume = {6},
  pages = {221 -225},
  month = {oct.},
  abstract = {This paper provides a novel framework for semantic representation
	of Chinese news based on conceptual network which is described hierarchically
	in three layers. we first define the structure of independent concept
	that is the basic element of the framework. Then the domain specific
	semantic unit called knowledge node is illustrated. Finally, the
	knowledge tree which gives a well formed semantic hierarchy is discussed.
	An example on Chinese economic news is used through this paper to
	illustrate the intrinsic mechanism of this framework. The proposed
	framework can be used to express the semantic information of the
	concepts and their relations in Chinese news for further processing.},
  doi = {10.1109/ICNC.2008.27},
  keywords = {Chinese news;conceptual network based modeling;knowledge node;knowledge
	tree;semantic representation;knowledge representation;natural language
	processing;}
}

@INPROCEEDINGS{4709156,
  author = {Jianhua Zheng and Di Li and Zhaogan Shu and Rong Zhu},
  title = {New Approach for Embedded Computer Numeric Control Development},
  booktitle = {Young Computer Scientists, 2008. ICYCS 2008. The 9th International
	Conference for},
  year = {2008},
  pages = {1272 -1278},
  month = {nov.},
  abstract = {After analyzing the shortcomings of traditional code-centric development
	process for embedded computer numeric control (CNC), this paper presents
	a new approach: Embedded cNc Development based on modEl Driven (ENDED)
	which enjoys the advantages of model-driven development (MDD) and
	domain specific modeling (DSM). This approach is a model-based methodology
	which includes three parts: construction of domain modeling language,
	model transformation, code auto-generation from domain model. Multi-view
	based meta-modeling method was employed in the first step of ENDED
	to construct the CNC modeling language. During the model transformation
	part, three requirements of model transformation were proposed and
	corresponding processing strategies were elaborated. In the last
	part, code auto generator based on domain library to improve the
	productivity was described in details and an illustrative example
	was tested to show the feasibility of ENDED.},
  doi = {10.1109/ICYCS.2008.459},
  keywords = {CNC modeling language;code auto-generation;domain modeling language;domain
	specific modeling;embedded computer numeric control development;model
	transformation;model-based software design;model-driven development;multiview
	based meta-modeling method;computerised numerical control;object-oriented
	programming;program compilers;software architecture;specification
	languages;}
}

@INPROCEEDINGS{1307900,
  author = {Yu Zhenhua and Cai Yuanli},
  title = {Novel architecture description language based on high-level Petri
	nets},
  booktitle = {Information and Communication Technologies: From Theory to Applications,
	2004. Proceedings. 2004 International Conference on},
  year = {2004},
  pages = { 589 - 590},
  month = {april},
  abstract = { We present a novel architecture description language - OPNADL. The
	domain specific software architecture based on OPNADL is employed
	to guide the large-scale software development. OPNADL (object-oriented
	Petri nets architecture description language) is a graphical and
	mathematical modeling language, which can be used to describe static
	and dynamic semantics, and analyze the dynamic behavior of software
	system. The overall and individual characteristics of a system can
	be visually and intuitively depicted with the proposed OPNADL. Software
	architecture is conveniently constructed, refined and verified by
	OPNADL.},
  doi = {10.1109/ICTTA.2004.1307900},
  issn = { },
  keywords = { architecture description language; formal specification; formal verification;
	object-oriented Petri nets; software architecture; Petri nets; formal
	specification; formal verification; object-oriented methods; object-oriented
	programming; software architecture; specification languages;}
}

@INPROCEEDINGS{983030,
  author = {Zhou Zhi and Hinny Kong Pe Hin and Kheng Leng Gay, R. and Goh Wee
	Lin and Lee Shaur Yang},
  title = {iTSum: one agent-based system for automated text summarizing},
  booktitle = {Info-tech and Info-net, 2001. Proceedings. ICII 2001 - Beijing. 2001
	International Conferences on},
  year = {2001},
  volume = {3},
  pages = {18 -25 vol.3},
  abstract = {With the explosive increase of information especially the news within
	the cyber world, it is necessary to find a way to illuminate the
	workload for a person to manually access the meta-data day by day.
	Based on a modified KPC algorithm, one improved engine is designed
	and the prototype system iTSum was developed by the project group
	within Nanyang Technological University of Singapore, which allows
	the users to specify the website URLs he wishes to visit and generates
	the summarized Web page for him automatically. The performance evaluation
	of iTSum and KPC shows the improvement is outstanding. iTSum is an
	agent-based, personalized and domain-specific text summarizing and
	publishing system, tests show it can perform better than KPC itself
	and most other current methodologies},
  doi = {10.1109/ICII.2001.983030},
  keywords = {automated text summarizing;domain-specific text summarizing system;iTSum;modified
	KPC algorithm;one agent-based system;publishing system;training corpus;information
	retrieval;natural languages;software agents;text analysis;}
}

@INPROCEEDINGS{4683126,
  author = {Hua Zhou and XingPing Sun and ZhiHong Liang and HongWei Kang and
	QingDuan and Hongji Yang},
  title = {XMML: A Visual Metamodeling Language for Domain-Specific Modeling
	and its Application in Distributed Systems},
  booktitle = {Future Trends of Distributed Computing Systems, 2008. FTDCS '08.
	12th IEEE International Workshop on},
  year = {2008},
  pages = {133 -139},
  month = {oct.},
  abstract = {As a practical method to simplify construction of distributed systems,
	Domain-Specific Modeling raises the level of abstraction beyond programming
	by specifying the solution directly using visual models to express
	domain concepts. This paper gives a visual metamodeling language
	which is suitable for specific domain modeling, namely XMML. It supports
	development and design of Domain-Specific Modeling languages (DSMLs)
	and domain model. A formal definition method of domain rules based
	on events is proposed for domain rules modeling. The paper also gives
	a loose coupling, scalable implementation schema for visual design
	of DSMLs. Finally, gives an example used XMML for metamodeling of
	a distributed application.},
  doi = {10.1109/FTDCS.2008.27},
  issn = {1071-0485},
  keywords = {XMML;distributed systems;domain rules modeling;domain-specific modeling;formal
	definition method;visual metamodeling language;formal specification;middleware;visual
	languages;}
}

@INPROCEEDINGS{761169,
  author = {Jianwen Zhu and Gajski, D.D.},
  title = {OpenJ: an extensible system level design language},
  booktitle = {Design, Automation and Test in Europe Conference and Exhibition 1999.
	Proceedings},
  year = {1999},
  pages = {480 -484},
  abstract = {There is an increasing research interest in system level design languages
	which can carry designers from specification to implementation of
	a system-on-a-chip. Unfortunately two of the most important goals
	in designing such a language, are at odds with each other: heterogeneity
	requires components of the system to be captured precisely by domain
	specific models to simplify analysis and synthesis; simplicity requires
	a consistent notation to avoid confusion. In this paper, we focus
	on our effort in resolving this dilemma in an extensible language
	called OpenJ. In contrast to the conventional monolithic languages,
	OpenJ has a layered structure consisting of the kernel layer which
	is essentially an object oriented language designed to be simple,
	modular and polymorphic; the open layer which exports parameterizable
	language constructs; the domain layer which precisely captures the
	computational models essential for embedded systems. The domain layer
	can be provided by vendors via a common protocol defined by an open
	layer which enables the supersetting or/and subsetting of the kernel.
	A compiler has been built for this language and experiments are conducted
	for popular models such as synchronous, discrete event and dataflow},
  doi = {10.1109/DATE.1999.761169},
  keywords = {OpenJ;compiler;computational models;dataflow model;discrete event
	model;domain layer;embedded systems;extensible system level design
	language;heterogeneity;implementation;kernel layer;layered structure;object
	oriented language;open layer;parameterizable language constructs;protocol;simplicity;specification;subsetting;supersetting;synchronous
	model;system-on-a-chip;circuit CAD;embedded systems;formal specification;integrated
	circuit design;object-oriented languages;program compilers;protocols;specification
	languages;}
}

@INPROCEEDINGS{4222650,
  author = {Liming Zhu and Yan Liu and Ngoc Bao Bui and Gorton, J.},
  title = {Revel8or: Model Driven Capacity Planning Tool Suite},
  booktitle = {Software Engineering, 2007. ICSE 2007. 29th International Conference
	on},
  year = {2007},
  pages = {797 -800},
  month = {may},
  abstract = {Designing complex multi-tier applications that must meet strict performance
	requirements is a challenging software engineering problem. Ideally,
	the application architect could derive accurate performance predictions
	early in the project life-cycle, leveraging initial application design-level
	models and a description of the target software and hardware platforms.
	To this end, we have developed a capacity planning tool suite for
	component-based applications, called Revel8tor. The tool adheres
	to the model driven development paradigm and supports benchmarking
	and performance prediction for J2EE, .Net and Web services platforms.
	The suite is composed of three different tools: MDAPerf MDABench
	and DSLBench. MDAPerf allows annotation of design diagrams and derives
	performance analysis models. MDABench allows a customized benchmark
	application to be modeled in the UML 2.0 Testing Profile and automatically
	generates a deployable application, with measurement automatically
	conducted. DSLBench allows the same benchmark modeling and generation
	to be conducted using a simple performance engineering Domain Specific
	Language (DSL) in Microsoft Visual Studio. DSLBench integrates with
	Visual Studio and reuses its load testing infrastructure. Together,
	the tool suite can assist capacity planning across platforms in an
	automated fashion.},
  doi = {10.1109/ICSE.2007.73},
  issn = {0270-5257},
  keywords = {.Net;J2EE;Microsoft Visual Studio;UML 2.0 testing profile;Web service;capacity
	planning tool suite;complex multitier application design;component-based
	application;customized benchmark application;design diagram annotation;domain
	specific language;load testing;model driven development;software
	architecture;software engineering;software measurement;software performance
	requirement;software project life-cycle;software reuse;Java;Unified
	Modeling Language;Web services;formal specification;network operating
	systems;object-oriented programming;program testing;project management;software
	architecture;software metrics;software performance evaluation;software
	reusability;}
}

@INPROCEEDINGS{1204901,
  author = {Qinwei Zhu and Goncalves, M.A. and Fox, E.A.},
  title = {5SGraph demo: a graphical modeling tool for digital libraries},
  booktitle = {Digital Libraries, 2003. Proceedings. 2003 Joint Conference on},
  year = {2003},
  pages = { 385},
  month = {may},
  abstract = { We present a domain-specific visual modelling tool, 5SGraph, aimed
	at modelling digital libraries. 5SGraph is based on a metamodel that
	describes DLs using the 5S theory [M.A. Goncalves et al., 2003].
	The output from 5SGraph is a digital library model that is an instance
	of the metamodel, expressed in the 5S description language (5SL)
	[M.A. Goncalves et al., 2002]. 5SGraph presents the metamodel in
	a structured toolbox, and provides a top-down visual building environment
	for designers. The visual proximity of the metamodel and instance
	model facilitates requirements gathering and simplifies the modelling
	process. Furthermore, 5SGraph maintains semantic constraints specified
	by the 5S metamodel and enforces these constraints over the instance
	model to ensure semantic consistency and correctness. 5SGraph enables
	component reuse to reduce the time and efforts of designers. 5SGraph
	also is designed to be flexible and extensible, able to accommodate
	and integrate several other complementary tools (e.g., to model scenarios
	or complex digital objects), reflecting the interdisciplinary nature
	of digital libraries. The tool has been tested with real users and
	several modelling tasks in a usability experiment [Zhu, Q., 2002]
	and its usefulness and learnability have been demonstrated.},
  doi = {10.1109/JCDL.2003.1204901},
  keywords = { 5S description language; 5S metamodel; 5S theory; 5SGraph; 5SL; digital
	library model; digital object; domain-specific visual modelling tool;
	graphical modelling tool; instance model; semantic constraint; structured
	toolbox; top-down visual building environment; visual proximity;
	digital libraries; graphical user interfaces; simulation languages;
	visual languages; visual programming;}
}

@INPROCEEDINGS{5430129,
  author = {Zhuhadar, L. and Nasraoui, O. and Wyatt, R. and Romero, E.},
  title = {Multi-language Ontology-Based Search Engine},
  booktitle = {Advances in Computer-Human Interactions, 2010. ACHI '10. Third International
	Conference on},
  year = {2010},
  pages = {13 -18},
  month = {feb.},
  abstract = {One of the first Multi-Language Information Retrieval (MLIR) systems
	was implemented in 1969 by Gerard Salton who enhanced his SMART system
	to retrieve multilingual documents in two languages, English and
	German. However, the research field of MLIR is still struggling since
	the majority of information retrieval systems are monolingual and
	more precisely English-based, even though only 6% of the world's
	population native language have as English [14]. This paper presents
	a Multi-Language Information Retrieval (MLIR) approach that falls
	into the area of Domain Specific Information Retrieval (E-learning
	being the domain). The approach we followed is a synergistic approach
	between (1) Thesaurus-based Approach and (2) Corpus-based Approach.
	This research has been implemented on a real platform called HyperManyMedia1
	at Western Kentucky University.},
  doi = {10.1109/ACHI.2010.43},
  keywords = {HyperManyMedia;SMART system;Thesaurus based approach;corpus based
	Approach;domain specific information retrieval;multilanguage information
	retrieval systems;multilanguage ontology based search engine;multilingual
	documents retrieval;information retrieval;information retrieval systems;natural
	languages;ontologies (artificial intelligence);search engines;thesauri;}
}

@INPROCEEDINGS{1220535,
  author = {Zibert, J. and Martincic-Ipsic, S. and Ipsic, I. and Mihelic, F.},
  title = {Bilingual speech recognition of Slovenian and Croatian weather forecasts},
  booktitle = {Video/Image Processing and Multimedia Communications, 2003. 4th EURASIP
	Conference focused on},
  year = {2003},
  volume = {2},
  pages = { 637 - 642 vol.2},
  month = {july},
  abstract = { In the paper we present some results of a joint project in speech
	data collection and speech recognition of Slovenian and Croatian
	weather forecasts. In the paper we describe the procedures we have
	performed in order to obtain domain specific speech databases from
	broadcast programmes. We further describe the speech recognition
	experiments for language identification and the speech recognition
	experiments of monolingual and bilingual speech.},
  doi = {10.1109/VIPMC.2003.1220535},
  issn = { },
  keywords = { Croatian weather forecast; Slovenian weather forecast; bilingual
	speech recognition; broadcast programme; language identification;
	monolingual speech; speech data collection; speech database; spoken
	dialogue system; broadcasting; interactive systems; linguistics;
	speech recognition; weather forecasting;}
}

@INPROCEEDINGS{205188,
  author = {Zimmerman, D.L. and Lowry, M.R.},
  title = {Algorithm design and parallel program development through formal
	specifications},
  booktitle = {System Sciences, 1990., Proceedings of the Twenty-Third Annual Hawaii
	International Conference on},
  year = {1990},
  volume = {ii},
  pages = {189 -197 vol.2},
  month = {jan},
  abstract = {The authors describe two components of the Kestrel interactive development
	system (KIDS). The overall methodology is a top-down refinement of
	formal specifications; software design decisions are factored into
	logic steps along an extended `what-to-how' spectrum. The algorithm
	design component is based upon reusing the structure common to a
	class of algorithms, such as local search. By formalizing this structure
	as an abstract theory, the system can then instantiate its parameters
	to domain-specific functions, thus obtaining a high-level program
	specification. The component dealing with parallel program development
	supports a generalized data flow model, in which a computation is
	broken down into a collection of distinct atomic units. These are
	then scheduled and assigned to individual processors through formalisms
	expressing resource allocation constraints. The authors illustrate
	algorithm design with the development of the simplex algorithm from
	a high-level specification and develop a parallel version of one
	suboperation, the dot product computation},
  doi = {10.1109/HICSS.1990.205188},
  keywords = {Kestrel interactive development system;abstract theory;algorithm design;atomic
	units;domain-specific functions;dot product computation;formal specifications;generalized
	data flow model;high-level program specification;local search;logic
	steps;parallel program development;parallel version;resource allocation
	constraints;simplex algorithm;software design decisions;top-down
	refinement;formal specification;parallel programming;programming
	environments;}
}

@INPROCEEDINGS{4459154,
  author = {Zimmermann, O. and Zdun, U. and Gschwind, T. and Leymann, F.},
  title = {Combining Pattern Languages and Reusable Architectural Decision Models
	into a Comprehensive and Comprehensible Design Method},
  booktitle = {Software Architecture, 2008. WICSA 2008. Seventh Working IEEE/IFIP
	Conference on},
  year = {2008},
  pages = {157 -166},
  month = {feb.},
  abstract = {When constructing software systems, software architects must identify
	and evaluate many competing design options and document the rationale
	behind any selections made. Two supporting concepts are pattern languages
	and architectural decision models. Unfortunately, both concepts only
	provide partial support: Extensive upfront education is needed for
	practitioners to be in command of' the full pattern literature relevant
	in their field; retrospective architectural decision modeling is
	viewed as a painful extra responsibility without immediate gains.
	In this paper, we combine pattern languages and reusable architectural
	decision models into a design method that is both comprehensive and
	comprehensible. Our design method identifies the required decisions
	in requirements models systematically, gives domain-specific pattern
	selection advice, and provides traceability from platform-independent
	patterns to platform-specific decisions. We validate our approach
	by applying it to enterprise applications as an exemplary application
	genre and a SOA case study from the finance industry.},
  doi = {10.1109/WICSA.2008.19},
  keywords = {pattern languages;reusable architectural decision models;service-oriented
	architecture;software architecture;object-oriented languages;software
	architecture;}
}

@INPROCEEDINGS{5934789,
  title = {Title Page i},
  booktitle = {Engineering of Computer Based Systems (ECBS), 2011 18th IEEE International
	Conference and Workshops on},
  year = {2011},
  pages = {i},
  month = {april},
  abstract = {The following topics are dealt with: Cyberphysical system; model driven
	development knowledge; hardware-software communication middleware;
	component based distributed real-time embedded system; intrusive
	online monitoring; software safety standard; computer based system;
	domain specific modeling languages; pushdown model checking; multicore
	induced behavioral deviation; autonomous ground vehicle; service
	oriented product lines; reachability analysis; formal verification;
	distributed transaction management; open-source pivot language; model
	driven in-the-loop validation; UAV software testing; and data adaptable
	reconfigurable embedded system.},
  doi = {10.1109/ECBS.2011.1},
  keywords = {Cyberphysical system;UAV software testing;autonomous ground vehicle;component
	based distributed real-time embedded system;computer based system;data
	adaptable reconfigurable embedded system;distributed transaction
	management;domain specific modeling languages;formal verification;hardware-software
	communication middleware;intrusive online monitoring;model driven
	development knowledge;model driven in-the-loop validation;multicore
	induced behavioral deviation;open-source pivot language;pushdown
	model checking;reachability analysis;service oriented product lines;software
	safety standard;embedded systems;formal verification;middleware;object-oriented
	programming;reconfigurable architectures;service-oriented architecture;software
	standards;}
}

@ARTICLE{5370756,
  title = {Kudos to Bob Glass and Rebecca Wirfs-Brock},
  journal = {Software, IEEE},
  year = {2010},
  volume = {27},
  pages = {7 -9},
  number = {1},
  month = {jan.-feb. },
  abstract = {These letters deal with the retirement of Rebecca Wirfs-Brock and
	Bob Glass, systems architecture, domain-specific languages, the certification
	of requirements analysts, measurement, and reading classics.},
  doi = {10.1109/MS.2010.13},
  issn = {0740-7459}
}

@INPROCEEDINGS{5069878,
  title = {Hub page},
  booktitle = {Modeling in Software Engineering, 2009. MISE '09. ICSE Workshop on},
  year = {2009},
  pages = {i -ii},
  month = {may},
  abstract = {The following topics are dealt with: modeling in software engineering;
	software evolution; domain-specific languages; software verification
	and validation; model transformation.},
  doi = {10.1109/MISE.2009.5069878},
  keywords = {domain-specific language;model transformation;software engineering;software
	evolution;software validation;software verification;software engineering;}
}

@INPROCEEDINGS{5069879,
  title = {Session list},
  booktitle = {Modeling in Software Engineering, 2009. MISE '09. ICSE Workshop on},
  year = {2009},
  pages = {1},
  month = {may},
  doi = {10.1109/MISE.2009.5069879}
}

@INPROCEEDINGS{5076606,
  title = {Title Page iii},
  booktitle = {Software Engineering Conference, 2009. ASWEC '09. Australian},
  year = {2009},
  pages = {iii},
  month = {april},
  abstract = {The following topics are dealt with: software engineering; concurrent
	program; software evolution; change propagation analysis; model-driven
	code generation; specification based testing; supporting multi-path
	UI development with vertical refinement; evidence-based paradigm;
	critic authoring templates for specifying domain-specific visual
	language tool; service oriented architecture; integrated data mapping;
	software meta-tool; semantic impact and faults in source code changes;
	architecture level change impact analysis methods; Web systems evolution;
	Web services; comprehensive feature-oriented traceability model;
	software product line development; component-based software system;
	inter-agent data flow analysis; abstract state machines; formal semantics
	based translator generation; multithreaded programs; event-driven
	system-level language; software project organization structures;
	software development cost estimation; WAD workflow system:data-centric
	workflow system; qualitative software process simulation modeling;
	quantitative software process simulation modeling and UML.},
  doi = {10.1109/ASWEC.2009.2},
  issn = {1530-0803},
  keywords = {UML;WAD workflow system;Web system evolution;abstract state machine;change
	propagation analysis;component-based software system;concurrent program;critic
	authoring template;data-centric workflow system;domain-specific visual
	language tool specification;event-driven system-level language;evidence-based
	paradigm;formal semantics;integrated data mapping;inter-agent data
	flow analysis;model-driven code generation;multipath UI development;multithreaded
	program;product line development;service oriented architecture;software
	development cost estimation;software engineering;software evolution;software
	meta-tool;software process simulation modeling;software project organization
	structure;source code change;specification-based testing;Unified
	Modeling Language;Web services;concurrency control;data flow analysis;multi-threading;object-oriented
	programming;product development;program compilers;program testing;software
	engineering;}
}

@INPROCEEDINGS{5295313,
  title = {Visual domain-specific languages},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {5},
  month = {sept.},
  doi = {10.1109/VLHCC.2009.5295313},
  issn = {1943-6092}
}

@INPROCEEDINGS{5295324,
  title = {Title page},
  booktitle = {Visual Languages and Human-Centric Computing, 2009. VL/HCC 2009.
	IEEE Symposium on},
  year = {2009},
  pages = {i},
  month = {sept.},
  abstract = {The following topics are dealt with: visual domain specific languages;
	human-centric computing; end-user programming; Web technology; visual
	query languages; spreadsheets; Internet; user interfaces; visual
	programming languages.},
  doi = {10.1109/VLHCC.2009.5295324},
  issn = {1943-6092},
  keywords = {Internet;Web technology;end-user programming;human-centric computing;spreadsheets;user
	interfaces;visual domain specific languages;visual programming languages;visual
	query languages;Internet;spreadsheet programs;user interfaces;visual
	programming;}
}

@INPROCEEDINGS{5336434,
  title = {Contents},
  booktitle = {Visualizing Software for Understanding and Analysis, 2009. VISSOFT
	2009. 5th IEEE International Workshop on},
  year = {2009},
  pages = {iv -v},
  month = {sept.},
  abstract = {This the following topics are dealt with: code visualization; open
	source newcomers; UML class diagrams; software visualization tools;
	source code examples; domain-specific programs' behavior; visual
	analytics in software product assessments; Java heap; C/C++ code
	bases; generating visualization-based analysis scenarios from maintenance
	task descriptions; sv3D meets Eclipse; and enhancing structural views
	of software systems by dynamic information.},
  doi = {10.1109/VISSOF.2009.5336434},
  keywords = {C/C++ code bases;Eclipse;Java heap;UML;code visualization;domain-specific
	program behavior;dynamic information;maintenance task descriptions;open
	source newcomers;software product assessments;software systems;software
	visualization tools;source code;structural views;C++ language;Java;Unified
	Modeling Language;product development;program diagnostics;program
	visualisation;public domain software;software architecture;software
	maintenance;software reusability;source coding;task analysis;}
}

@INPROCEEDINGS{5352772,
  title = {2nd Workshop on advances in programming languages},
  booktitle = {Computer Science and Information Technology, 2009. IMCSIT '09. International
	Multiconference on},
  year = {2009},
  pages = {633},
  month = {oct.},
  abstract = {PROGRAMMING languages are programmers' most basic tools. With appropriate
	programming languages one can drastically reduce the cost of building
	new applications as well as maintaining existing ones. In the last
	decades there have been many advances in programming languages technology
	in traditional programming paradigms such as functional, logic, and
	object-oriented programming, as well as the development of new paradigms
	such as aspect-oriented programming. The main driving force was and
	will be to better express programmers' ideas. Therefore, research
	in programming languages is an endless activity and the core of computer
	science. New language features, new programming paradigms, and better
	compile-time and run-time mechanisms can be foreseen in the future.
	The aims of this event is to provide a forum for exchange of ideas
	and experience in topics concerned with programming languages and
	systems. Original papers and implementation reports are invited in
	all areas of programming languages. Major topics of interest include
	but are not limited to the following: #x2022;Automata theory and
	applications #x2022;Compiling techniques #x2022;Domain-specific languages
	#x2022;Formal semantics and syntax #x2022;Generative and generic
	programming #x2022;Grammarware and grammar based systems #x2022;Knowledge
	engineering languages, integration of knowledge engineering and software
	engineering #x2022;Languages and tools for trustworthy computing
	#x2022;Language theory and applications #x2022;Language concepts,
	design and implementation #x2022;Markup languages (XML) #x2022;Metamodeling
	and modeling languages #x2022;Model-driven engineering languages
	and systems #x2022;Practical experiences with programming languages
	#x2022;Program analysis, optimization and verification #x2022;Program
	generation and transformation #x2022;Programming paradigms (aspect-oriented,
	functional, logic, object-oriented, etc.) #x2022;Programming tools
	and environments - #x2022;Proof theory for programs #x2022;Specification
	languages #x2022;Type systems #x2022;Virtual machines andjust-in-time
	compilation #x2022;Visual programming languages},
  doi = {10.1109/IMCSIT.2009.5352772}
}

@INPROCEEDINGS{4639039,
  title = {Title},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {i},
  month = {sept.},
  abstract = {The following topics are dealt with: visual language; human-centric
	computing; animated visualization; software quality evolution; end-user
	programming; domain-specific language and visual programming tool.},
  doi = {10.1109/VLHCC.2008.4639039},
  issn = {1943-6092},
  keywords = {animated visualization;domain-specific language;end-user programming;human-centric
	computing;software quality evolution;visual language;visual programming
	tool;computer animation;software prototyping;software quality;software
	tools;visual languages;visual programming;}
}

@INPROCEEDINGS{4639065,
  title = {Domain-specific languages},
  booktitle = {Visual Languages and Human-Centric Computing, 2008. VL/HCC 2008.
	IEEE Symposium on},
  year = {2008},
  pages = {91 -92},
  month = {sept.},
  doi = {10.1109/VLHCC.2008.4639065},
  issn = {1943-6092}
}

@INPROCEEDINGS{4279195,
  title = {Early Aspects at ICSE: Workshops in Aspect-Oriented Requirements
	Engineering and Architecture Design-Cover},
  booktitle = {Aspect-Oriented Requirements Engineering and Architecture Design,
	2007. Early Aspects at ICSE: Workshops in},
  year = {2007},
  pages = {c1},
  month = {may},
  abstract = {The following topics are dealt with: a clustering technique for early
	detection of dominant and recessive cross-cutting concerns; aspectual
	support for specifying requirements in software product lines; modeling
	and evolving cross-cutting concerns in ADORA; AGOL; and aspect-oriented
	domain-specific language for MAS; towards the architectural definition
	of the health watcher system with AO-ADL; revisiting a formal framework
	for modeling aspects in the design phase; a traceability method for
	cross-cutting concerns with transformation rules; and on the contributions
	of an end-to-end AOSD testbed.},
  doi = {10.1109/EARLYASPECTS.2007.5},
  keywords = {ADORA;AGOL;AO-ADL;AOSD testbed;MAS;aspect-oriented domain-specific
	language;clustering technique;cross-cutting;formal framework;health
	watcher system;software product lines;transformation rules;object-oriented
	programming;software engineering;}
}

@INPROCEEDINGS{4351309,
  title = {IEEE Symposium on Visual Languages and Human-Centric Computing-Title},
  booktitle = {Visual Languages and Human-Centric Computing, 2007. VL/HCC 2007.
	IEEE Symposium on},
  year = {2007},
  pages = {i -iii},
  month = {sept.},
  abstract = {The following topics are dealt with: software engineering; program
	maintenance; teaching and learning; software configuration management
	system; online learning environment; logic programming; domain specific
	visual language meta tool; visual interactive programming; end user
	programming evaluations; nonhierarchical visualization component;
	formal method; incremental specification; end-user Web development.},
  doi = {10.1109/VLHCC.2007.61},
  keywords = {Web development;data visualization;formal method;incremental specification;logic
	programming;online learning environment;program maintenance;software
	configuration management system;software engineering;visual interactive
	programming;visual language;Web design;computer aided instruction;configuration
	management;data visualisation;formal specification;interactive systems;logic
	programming;software maintenance;visual languages;}
}

@ARTICLE{4375380,
  title = {Call-for-Papers: Special Issue on Software Language Engineering},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2007},
  volume = {33},
  pages = {891},
  number = {12},
  month = {dec. },
  doi = {10.1109/TSE.2007.70760},
  issn = {0098-5589}
}

@INPROCEEDINGS{1509469,
  title = {Proceedings. 2005 IEEE Symposium on Visual Languages and Human-Centric
	Computing},
  booktitle = {Visual Languages and Human-Centric Computing, 2005 IEEE Symposium
	on},
  year = {2005},
  month = {sept.},
  abstract = {The following topics are discussed: domain specific visual languages;
	human-centric computing; end-user debugging and testing; formal foundations;
	interaction and user interface design; algorithm visualization and
	demonstration; software visualization and program comprehension;
	visual language design, specification, and implementation; animation;
	and end user development.},
  doi = {10.1109/VLHCC.2005.2},
  keywords = {algorithm visualization;animation;domain specific visual languages;end-user
	debugging;end-user testing;formal foundations;formal specification;human
	computer interaction;human-centric computing;program comprehension;software
	visualization;user interface design;computer animation;formal specification;program
	debugging;program testing;program visualisation;reverse engineering;simulation
	languages;user centred design;user interfaces;visual languages;}
}

@INPROCEEDINGS{1531012,
  title = {Proceedings. 13th IEEE International Conference on Requirements Engineering},
  booktitle = {Requirements Engineering, 2005. Proceedings. 13th IEEE International
	Conference on},
  year = {2005},
  month = {29 2005-sept. 2},
  abstract = {The following topics are dealt with: requirements engineering; personalized
	software; product lines; aligning requirements with business goals;
	elicitation; requirements management; policy-oriented requirements;
	modeling; domain-specific requirements engineering; requirements
	analysis; prioritizing and merging requirements; constrained natural
	language notations; goals and nonfunctional requirements; and quality
	improvement.},
  doi = {10.1109/RE.2005.3},
  keywords = {business goals;constrained natural language notations;elicitation;goals
	requirements;merging requirements;nonfunctional requirements;personalized
	software;policy-oriented requirements;prioritizing requirements;product
	lines;quality improvement;requirements analysis;requirements engineering;requirements
	management;requirements merging;commerce;formal specification;formal
	verification;software development management;systems analysis;}
}

@INPROCEEDINGS{1260198,
  title = {Programming at runtime: Requirements paradigms for nonprogrammer
	web application development},
  booktitle = {Human Centric Computing Languages and Environments, 2003. Proceedings.
	2003 IEEE Symposium on},
  year = {2003},
  pages = {23 -30},
  month = {oct.},
  abstract = {We investigate the femibiliy of nonprogramnier web application development
	and propose the creation of end-user programming tools that address
	the issue at a high level of abstraction. The results of three related
	empirical studies and one protoping effort are reported. We surveyed
	nonprogrammers needs for web applications and studied how nonprogrammers
	would naturally approach web development. To express what a tool
	should provide we summarize high-level components and concepts employed
	by web applications. To express how a tool may provide its functionality,
	we propose "Programming-at-Runtime" - a programming paradigm that
	is in its core similar to the automatic recalculation in spreadsheets.
	Finally, we introduce "FlashLight" - a protoype web development tool
	for nonprogrammers.},
  doi = {10.1109/HCC.2003.1260198},
  keywords = {domain-specific programming;educationally disadvantage programming;end-user
	programming;formal methods;human centric computing languages;multimedia
	languages;multimedia software engineering;visual languages;visual
	software engineering;formal specification;high level languages;multimedia
	computing;software engineering;visual programming;}
}

@INPROCEEDINGS{905162,
  title = {Proceedings 2001 Symposium on Applications and the Internet},
  booktitle = {Applications and the Internet, 2001. Proceedings. 2001 Symposium
	on},
  year = {2001},
  pages = {x+239},
  abstract = {The following topics were dealt with: Internet; information retrieval;
	data mining; agents; domain specific languages; distributed and parallel
	applications; multimedia; mobile computing; and collaboration technology},
  doi = {10.1109/SAINT.2001.905162},
  keywords = {Internet;agents;collaboration technology;data mining;distributed applications;domain
	specific languages;information retrieval;mobile computing;multimedia;parallel
	applications;Internet;groupware;high level languages;information
	resources;information retrieval;mobile computing;multimedia systems;parallel
	processing;software agents;}
}

@INPROCEEDINGS{685723,
  title = {Proceedings. Fifth International Conference on Software Reuse (Cat.
	No.98TB100203)},
  booktitle = {Software Reuse, 1998. Proceedings. Fifth International Conference
	on},
  year = {1998},
  pages = {xiii+388},
  month = {jun},
  abstract = {The following topics were dealt with: engineering domains; objects
	and components; domain modelling and process issues; domain-specific
	languages and generators; industrial experience reports; Internet
	support for reuse; software architecture; advanced topics in software
	reuse; linking domain analysis and domain implementation; the Reusably
	Incorrect Forum; and how to solve the reuse problem},
  doi = {10.1109/ICSR.1998.685723},
  issn = {1085-9098},
  keywords = {Internet support;Reusably Incorrect Forum;application generators;domain
	analysis;domain implementation;domain modelling;domain-specific languages;engineering
	domains;industrial experience;object-oriented programming;process
	issues;software architecture;software components;software reuse;software
	reusability;}
}

